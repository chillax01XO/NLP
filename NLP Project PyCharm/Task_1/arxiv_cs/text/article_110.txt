arXiv:2505.20888v1  [cs.CL]  27 May 2025EasyDistill : A Comprehensive Toolkit for Effective Knowledge Distillation
of Large Language Models
Chengyu Wang1, Junbing Yan1, Wenrui Cai1,2, Yuanhao Yue1, Jun Huang1
1Alibaba Cloud Computing2Shanghai Jiao Tong University
chengyu.wcy@alibaba-inc.com
Abstract
In this paper, we present EasyDistill , a compre-
hensive toolkit designed for effective black-box
and white-box knowledge distillation (KD) of
large language models (LLMs). Our frame-
work offers versatile functionalities, including
data synthesis, supervised fine-tuning, ranking
optimization, and reinforcement learning tech-
niques specifically tailored for KD scenarios.
The toolkit accommodates KD functionalities
for both System 1 (fast, intuitive) and System
2 (slow, analytical) models. With its modular
design and user-friendly interface, EasyDistill
empowers researchers and industry practition-
ers to seamlessly experiment with and imple-
ment state-of-the-art KD strategies for LLMs.
In addition, EasyDistill provides a series of ro-
bust distilled models and KD-based industrial
solutions developed by us, along with the cor-
responding open-sourced datasets, catering to
a variety of use cases. Furthermore, we de-
scribe the seamless integration of EasyDistill
into Alibaba Cloud’s Platform for AI (PAI).
Overall, the EasyDistill toolkit makes advanced
KD techniques for LLMs more accessible and
impactful within the NLP community.1
1 Introduction
The proliferation of large language models (LLMs)
has been transformative for NLP (Zhao et al., 2023;
Yadagiri and Pakray, 2025), pushing the bound-
aries of what machines can understand and gen-
erate in human language. However, the extensive
size and complexity of these models present sig-
nificant challenges, including high computational
costs and substantial energy consumption. Knowl-
edge distillation (KD) offers a viable solution to
this dilemma, where smaller models are trained to
replicate the performance of their larger counter-
parts, enabling efficient use of resources without
1The toolkit, model checkpoints and datasets are
released at: https://github.com/modelscope/
easydistill .sacrificing much accuracy (Xu et al., 2024; Yang
et al., 2024c). Despite its potential, effective KD
of LLMs is not straightforward, often requiring
advanced algorithms and domain expertise. In ad-
dition, the lack of tools for LLM-based KD can
exacerbate these challenges, limiting exploration
and adaptation in industrial settings.2
In this paper, we introduce EasyDistill , a compre-
hensive toolkit designed to simplify the KD process
for LLMs under both black-box and white-box set-
tings, utilizing proprietary and open-source LLMs
as teacher models. EasyDistill offers a wide ar-
ray of functionalities, including data synthesis and
augmentation, supervised fine-tuning (SFT), rank-
ing optimization, and reinforcement learning (RL),
all tailored for KD scenarios. By supporting both
System 1 (fast, intuitive) and System 2 (slow, ana-
lytical) models (Li et al., 2025), EasyDistill facili-
tates the KD process across various types of LLMs.
EasyDistill is easy to use and extend; it provides a
modular design with a simple command-line inter-
face for invoking these algorithms.
In addition, EasyDistill is more than just an open-
source toolkit; it integrates several techniques to
support KD for industrial practitioners. The contri-
butions are threefold: i) It includes a series of ro-
bust distilled models (e.g., DistilQwen ), along with
open-source datasets, to demonstrate the effective-
ness of KD. ii) It features several KD-based indus-
trial solutions (i.e., EasyDistill-Recipes ) that serve
as practical guides for diverse application needs.
iii)EasyDistill is integrated into Alibaba Cloud’s
Platform for AI (PAI), showcasing its adaptability
and potential for large-scale deployment. By bridg-
ing the gap between cutting-edge KD techniques
and practical applicability, EasyDistill enhances
2Note that there are a few open-source toolkits that support
KD for LLMs, such as DistillKit ( https://github.com/
arcee-ai/DistillKit ). To the best of our knowledge,
there is a lack of support for various types of KD algorithms
and practical solutions (as described below) within the open-
source community.KnowledgeDistillationAlgorithmsBlack-Box SFTLogits Forward KLDBlack-Box/White-Box TrainingEasyDistill-RecipesEnvironmentsLocal EnvironmentCloud Environment(PAI Products)
DataSynthesis&AugmentationDistilQwen2DistilQwen2.5DistilQwen2.5-DS3-0324DistilQwen2.5-R1Domain-SpecificKDRecipes
Logits Reverse KLDOthersDPORank OptimizationCogPOReward Model TrainingReinforcement LearningPPOGRPOOthersProprietary LLMs Teacher LLMsOpen-Source LLMsGeneral NLP InstructionsSeed KnowledgeDomain-Specific InstructionsReasoning ProblemsOthersInvokeQwen SeriesStudent LLMsLlama SeriesOthersTrain
Instruction ExpansionInstruction RefinementInstruction-Response ExtractionInstruction Data OperationsCoT GenerationCoT SimplificationCoT ExpansionChain-of-Thought (CoT) Data OperationsDistilQwen-ThoughtXGeneralRecipesInstruct-FollowingDatasetsReleasedDatasetsCoTReasoningDatasetsFigure 1: The overall architecture of the EasyDistill toolkit.
the accessibility within the NLP community.
2 Architecture and Functionalities
In this section, we formally introduce the EasyDis-
tilltoolkit. The overall architecture is shown in
Figure 1. We begin by presenting the basic KD
functionalities, alongside the command-line tool
for invoking these functionalities. Following that,
we describe the collection of practical solutions
(i.e., EasyDistill-Recipes ). Finally, we briefly de-
scribe the integration of the toolkit into PAI prod-
ucts for users to perform KD on the cloud.
2.1 Basic KD Functionalities
2.1.1 Data Synthesis & Augmentation
Synthetic data plays a pivotal role in developing
robust LLMs (Liu et al., 2024), especially given
the typically limited size of seed datasets for KD.
This enhances the KD process, ensuring that the
distilled student models not only replicate the out-
put behavior of teacher LLMs but also extend their
generalization abilities to previously unseen tasks.
InEasyDistill , we offer various data synthesis and
augmentation operators, utilizing both proprietary
and open-source teacher LLMs. These operators
are designed to create high-quality seed datasetsfor KD, enriching them not only in volume but also
in diversity across tasks, topics or domains.
The first group of operators supported by Easy-
Distill focuses on the enhancement of instructional
data corresponding to a variety of NLP tasks, which
form the core inputs (i.e., seed knowledge) to ev-
ery KD algorithm for LLMs. In this context, we
extend our previous work (Yue et al., 2024a) to en-
gineer several functionalities, including instruction
expansion, instruction refinement, and the auto-
matic generation of instruction-response pairs from
the knowledge expressed in raw texts.
The second group of our operators in EasyDistill
concentrates on Chain-of-Thoughts (CoTs) (Wei
et al., 2022), which are particularly important for
distilling the problem-solving capacities of large
reasoning models (LRMs) (Li et al., 2025), also
known as System 2 models. In addition to the
basic operator for producing CoTs grounded in in-
structions or instruction-response pairs, we further
integrate operators for simplifying and extending
CoTs to effectively address reasoning problems, as
overly long or short CoTs may not be suitable for
developing strong LRMs (Yang et al., 2025) We
further suggest that combining these two types of
operators enhances the KD process for LRMs, as
they enable the creation of enriched training setsaccompanied by high-quality CoTs.
2.1.2 Training Algorithms for KD Scenarios
The core KD pipeline for LLMs is straightforward.
The input is seed knowledge, consisting of instruc-
tions for any target tasks, which is leveraged to
prompt the selected teacher LLM to generate de-
tailed outputs. In our framework, we support both
proprietary and open-source LLMs as teacher mod-
els, and open-source LLMs as student models. In
the following, we elaborate different types of algo-
rithms tailored to the KD scenarios.
Black-Box/White-Box Training. Since we can
only obtain output tokens from proprietary LLMs,
the direct KD approach involves supervised fine-
tuning (SFT), treating these output tokens as the
ground truth for student LLMs.
For open-source teacher LLMs, in addition to
SFT, leveraging the models’ hidden knowledge as
guidance often leads to improved KD performance.
In this approach, we obtain the token-level logits
from the teacher model and minimize the diver-
gence between the logits distributions of the teacher
and student models. The loss functions employed
inEasyDistill include Kullback–Leibler divergence
(KLD) (Gu et al., 2024), reverse KLD (Wu et al.,
2025), among others. In the implementation, the
forward pass of the teacher model is performed
prior to the training of the student model to op-
timize GPU memory consumption. Furthermore,
based on our previous findings (Wang et al., 2025),
the sum of the probabilities of the top-10 tokens is
almost equal to 1. Thus, EasyDistill offers users op-
tions to leverage only the top- ktoken logits from
the teacher model and match the corresponding
logits from the student model. Subsequently, the
computation of loss functions is approximated by
considering only kselected logits. This approach
not only reduces computation time but also en-
hances the speed of storing and reading the logits.
We do not recommend minimizing the gap between
hidden representations, such as attention matrices,
of teacher and student LLMs due to the excessive
computational requirements.
Reinforcement Learning (RL). A basic princi-
ple of KD is to make the student model mimic
the behavior of the teacher models. However, this
approach may cause the student model to “over-
fit” the teacher outputs, rather than exploring more
possibilities to enhance its generalization abilities.
RL-based approaches, on the other hand, leverage
feedback from the teacher to train student models.The first type of RL-based KD functionalities in
EasyDistill involves training reward models using
feedback from teacher models, similar to the Re-
inforcement Learning from AI Feedback (RLAIF)
framework (Lee et al., 2024). Specifically, we em-
ploy teacher models to generate synthetic “chosen”
and “rejected” responses as preference data, which
are then used to train the reward model based on
any targeted LLM backbone with scalar outputs of
the predicted reward values.
The second type involves supporting RL opti-
mization to obtain the policy model, i.e., the RL-
optimized student model. EasyDistill integrates
popular RL algorithms for training LLMs, particu-
larly Proximal Policy Optimization (PPO) (Schul-
man et al., 2017) for System 1 models, and Group
Relative Policy Optimization (GRPO) (Shao et al.,
2024) for System 2 models. Unlike general RL
toolkits, EasyDistill emphasizes the entire pipeline
of distilling knowledge from teacher models to de-
velop more robust student models, as demonstrated
in previous works (Bai et al., 2022; Trung et al.,
2024; Yang et al., 2024d).
Preference Rank Optimization. A potential draw-
back of RL-based algorithms is the instability
in training. Preference rank optimization-based
approaches directly incorporate preferences into
LLMs, making the training process more stable.
InEasyDistill , we integrate the direct preference
optimization (DPO) method (Rafailov et al., 2023)
based on the training pipeline from Tunstall et al.
(2023) for KD. For System 2 models, which pos-
sess strong reasoning capabilities, it is important
that distilled smaller models have different capaci-
ties and cognitive trajectories than their larger coun-
terparts. To address this, EasyDistill integrates our
cognitive preference optimization (CogPO) algo-
rithm (Cai et al., 2025b) to enhance the reasoning
abilities of smaller models by aligning their cogni-
tive processes with their inherent capacities.
2.2 Command-Line Interface
To facilitate the KD process using our framework,
we provide a user-friendly command-line tool that
supports running KD jobs with a simple JSON con-
figuration file, specifying the input, output, and all
necessary arguments. For example, typical SFT
training jobs for black-box KD can be configured
as shown in Code 1 and Code 2, utilizing differ-
ent sources of teacher models. For white-box KD,
users can provide additional hyper-parameters and
specify the path to store the teacher logits (as shownin Code 3). Once the JSON configuration is set,
the KD process can be invoked simply by one line
of command, shown as follows:
easydistill -config=kd.json
with the entire pipeline running automatically.
In the provided sample codes, the inference
section contains essential information, particularly
the URL of the model service and its API key,
for making inferences with the teacher model. In
the online mode, any APIs compatible with the
OpenAI API format can be utilized. Therefore,
EasyDistill is compatible with any teacher models
in this case. For offline batch inference, we sup-
port vLLM for accelerated model inference (Kwon
et al., 2023) when the model can be downloaded
to local storage. The training configuration
includes critical hyper-parameters for the training
phase. EasyDistill supports all DeepSpeed accel-
eration techniques by default (Rasley et al., 2020),
such as ZeRO and CPU offloading, which can be
customized for advanced uses. In the future, other
distributed learning frameworks will be supported
inEasyDistill as well.
{
"job_type": "black_box_kd_api",
"dataset": {
"instruction_path": "train.json",
"labeled_path": "train_labeled.json",
"template" : "chat_template.jinja",
"seed": 42
},
"inference":{
"base_url": "ENDPOINT",
"api_key": "TOKEN",
"stream": "true",
"system_prompt" : "You are a helpful
assistant.",
"max_new_tokens": 512
},
"models": {
"student": "student/Qwen/Qwen2.5-0.5B-Instruct/"
},
"training": {
"output_dir": "result/",
"num_train_epochs": 3,
"per_device_train_batch_size": 1,
"gradient_accumulation_steps": 8,
"save_steps": 1000,
"logging_steps": 1,
"learning_rate": 2e-5,
"weight_decay": 0.05,
"warmup_ratio": 0.1,
"lr_scheduler_type": "cosine"
}
}
Code 1: Sample JSON configuration for black-box
KD (online inference with a proprietary or open-source
teacher model where the teacher model can be accessed
by any inference API in the OpenAI format and does
not need to be specified in the configuration).
{
"job_type": "black_box_kd_local",
"dataset": {
...
}
"inference":{"enable_chunked_prefill": true,
"seed": 777,
"gpu_memory_utilization": 0.9,
"temperature": 0.8,
"trust_remote_code": true,
"enforce_eager": false,
"max_model_len": 4096,
"max_new_tokens": 512
},
"models": {
"teacher": "teacher/Qwen/Qwen2.5-32B-Instruct/",
"student": "student/Qwen/Qwen2.5-0.5B-Instruct/"
},
"training": {
...
}
}
Code 2: Sample JSON configuration for black-box KD
(offline inference with an open-source teacher model).
{
"job_type": "white_box_kd_local",
"dataset": {
"logits_path": "logits.json",
...
}
"inference":{
"enable_chunked_prefill": true,
"seed": 777,
"gpu_memory_utilization": 0.9,
"temperature": 0.8,
"trust_remote_code": true,
"enforce_eager": false,
"max_model_len": 4096,
"max_new_tokens": 512
},
"distillation": {
"kd_ratio": 0.5,
"max_seq_length": 512
},
"models": {
"teacher": "teacher/Qwen/Qwen2.5-7B-Instruct/",
"student": "student/Qwen/Qwen2.5-0.5B-Instruct/"
},
"training": {
...
}
}
Code 3: Sample JSON configuration for white-box KD.
2.3 EasyDistill-Recipes : Practical Solutions
In this section, we further introduce EasyDistill-
Recipes , a collection of KD-based solutions that
produce lightweight LLMs built on EasyDistill .
2.3.1 General KD Recipes: DistilQwen
In general KD recipes, we offer detailed solutions
for producing the DistilQwen series using EasyDis-
till. This series includes both System 1 and System
2 models, which are lightweight LLMs built upon
the Qwen series. We make these solutions available
to enable users to create their own models utiliz-
ing the KD techniques in our framework. A brief
summary of these models are shown in Table 1,
with the model trees shown in Figure 2. The perfor-
mance of DistilQwen models is shown in Section C
(in the appendix). Detailed descriptions of these
models can be found in their respective Hugging
Face model cards.Model Series Model Type Parameter Sizes Teacher LLMs
DistilQwen2 System 1 models 1.5B, 7B GPT-4, Qwen-max
DistilQwen2.5 System 1 models 0.5B, 1.5B, 3B, 7B GPT-4, Qwen-max, Qwen2.5-72B-Instruct
DistilQwen2.5-R1 System 2 models 7B, 14B, 32B DeepSeek-R1
DistilQwen2.5-DS3-0324 System 2 models 7B, 14B, 32B DeepSeek-R1, DeepSeek-V3-0234
DistilQwen-ThoughtX System 2 models 7B, 32B DeepSeek-R1, QwQ-32B
Table 1: A summary of the DistilQwen model series.
DistilQwen2
Qwen2
DistilQwen2.5
Qwen2.5
DistilQwen2.5-R1
DistilQwen2.5-V3-0324
Student LLMsProduced LLMsInitializeInitialize
Qwen-maxTeacher LLMs
Qwen2.5-72B-InstructGPT-4
DeepSeek-R1
DeepSeek-V3-0324
DistillDistillDistillSystem 1 Models
System 2 Models
QwQ-32BDistilQwen-ThoughtX
Distill
Figure 2: Model trees of the DistilQwen series.
The first collection is DistilQwen2 , an enhanced
version of the Qwen2 models (Yang et al., 2024a),
equipped with improved instruction-following ca-
pabilities for various NLP tasks. During the distil-
lation training of DistilQwen2 , we employ GPT-4
and Qwen-max as teacher models to generate high-
quality responses. Specifically, before conducting
black-box SFT training, we utilize the method de-
scribed in (Yue et al., 2024b) to balance the task
distributions of input instructions. Following SFT,
a rank optimization process is performed using
the DPO algorithm (Rafailov et al., 2023) to en-
hance alignment between the student models and
the teacher models.
In response to the release of the Qwen2.5 model
series (Yang et al., 2024b), DistilQwen2.5 mod-
els are trained using a combination of black-box
and white-box KD algorithms. We adhere to the
same instruction data processing and black-box
SFT procedure as employed in the production
ofDistilQwen2 , due to the similarity in architec-
tures and model abilities. Subsequently, white-box
training is applied to refine the students’ acquisi-
tion of intricate knowledge from the teacher mod-els, specifically utilizing Qwen2.5-72B-Instruct as
open-source teacher models. For further details,
readers may refer to the report (Wang et al., 2025).
With the release of large System 2 models such
as DeepSeek-R1 (DeepSeek-AI, 2025), the concept
of “LLM with slow thinking” has become a stan-
dard strategy to extend the intelligent boundaries of
LLMs. We introduce the DistilQwen2.5-R1 model
series, which utilizes DeepSeek-R1 as the teacher
model, based on fine-tuning over a collection of
DeepSeek-R1’s CoT distillation data. To align the
reasoning abilities of smaller distilled models with
their intrinsic cognitive capacities, the models are
further refined using our CogPO algorithm (Cai
et al., 2025b), which outperforms other training
methods. Thus, the DistilQwen2.5-R1 model series
can be regarded as “small reasoning models”.
Additionally, we transfer the fast-thinking rea-
soning capabilities from DeepSeek-V3-03243to
theDistilQwen2.5-DS3-0324 models. The KD pro-
cess is similar to that of DistilQwen2.5-R1 . To
shorten the reasoning process, the CoT simplifica-
tion operator are employed to reduce the number
of tokens in the training data for DistilQwen2.5-
R1. Combined with a rewritten dataset comprising
DeepSeek-V3-0324’s CoT distillation data, we de-
velop the DistilQwen2.5-DS3-0324 models, which
promote the new “LLM + fast thinking” paradigm.
The most recent DistilQwen series is DistilQwen-
ThoughtX (Cai et al., 2025a), which exhibits im-
proved reasoning abilities and generates CoTs with
more optimal lengths compared to its predecessors.
This model series is developed from the innova-
tive OmniThought dataset by utilizing the novel
Reasoning Verbosity (RV) and Cognitive Difficulty
(CD) scores introduced in OmniThought, which
ensure that models receive rich, high-quality train-
ing data reflecting optimal CoT output length and
difficulty. Overall, DistilQwen-ThoughtX repre-
sents new reasoning models with “adaptive think-
ing” paradigms.
3https://huggingface.co/deepseek-ai/
DeepSeek-V3-03242.3.2 Domain-Specific KD Recipes
Within the EasyDistill-Recipes module, we further
integrate domain-specific recipes for real-world
applications. Taking code generation as an ex-
ample, it generates executable code snippets, as-
sisting developers in writing functional blocks, re-
fining logic, and adapting boilerplate code based
on prompts. This capability significantly acceler-
ates software development. In the context of code
generation tasks, the primary evaluation metric is
Pass@1 , which measures the model’s ability to pro-
duce correct, runnable code in a single attempt. A
key challenge lies in balancing model capability
and inference efficiency: while larger models may
achieve higher Pass@1 scores, they often incur
higher computational costs, impacting deployment
scalability. Thus, the core optimization goal is to
maximize generation accuracy while maintaining a
lightweight architecture. To verify the efficacy of
EasyDistill , we distill two models using prompts
and outputs distilled from DeepSeek-R1 based on
the OpenCodeReasoning dataset4. The detailed
performance of the models is presented in Table
3, demonstrating their effectiveness in improving
performance in specific tasks.
2.3.3 Released Datasets
To assist community developers in improving
instruction-following capabilities of LLMs, we
have open-sourced two datasets: DistilQwen 100K
andDistilQwen 1M, which are part of the distilled
training sets of the DistilQwen model series. These
datasets cover a range of contents, including mathe-
matics, code, knowledge-based QA, instruction fol-
lowing, and creative generation, with a total dataset
size of 100K and 1M entries. For CoT reasoning,
we have released OmniThought , which is a large-
scale dataset featuring 2M CoT processes gener-
ated and validated by DeepSeek-R1 and QwQ-32B.
Each CoT process is annotated with novel Reason-
ing Verbosity (RV) and Cognitive Difficulty (CD)
scores, which describe the appropriateness of CoT
verbosity and cognitive difficulty level for models
to comprehend these reasoning processes. For de-
tails, please refer to (Cai et al., 2025a). The links
of all released datasets are shown in Appendix A.
2.4 Integration to PAI Products
Apart from releasing our toolkit to the open-source
community for users to run the algorithms in local
4https://huggingface.co/datasets/
nvidia/OpenCodeReasoning
Figure 3: A snapshot of the model cards of the Dis-
tilQwen series in the PAI-Model Gallery.
environments, we have integrated its key function-
alities into Alibaba Cloud’s Platform for AI (PAI)5,
a cloud-native machine learning platform. This
integration facilitates low-cost model KD and de-
ployment on the cloud in the following aspects: i)
All the distilled models produced using EasyDistill
(e.g., the DistilQwen series) are available in the
PAI-Model Gallery. This platform supports the en-
tire lifecycle of the LLM usage, including training,
evaluation, compression, and deployment. A snap-
shot of the model cards of the DistilQwen series is
shown in Figure 3. ii) The KD pipelines and practi-
cal solutions can be seamlessly executed on deep
learning containers on PAI. Note that our EasyDis-
tillis not platform-dependent; it can be run in any
environment satisfying the Python requirements,
including other cloud platforms.
In the future, we will continue to develop KD-
based model training products built on the Easy-
Distill toolkit. As this is not our main focus, we do
not elaborate further.
3 Conclusion and Future Work
In this paper, we have introduced EasyDistill , a
comprehensive toolkit focusing on KD for LLMs.
It encompasses a suite of advanced algorithms, in-
cluding data synthesis, SFT, ranking optimization,
and RL techniques, all specifically tailored for KD
scenarios. Additionally, it includes several practi-
cal solutions and is integrated with Alibaba Cloud’s
Platform for AI (PAI) for large-scale deployment.
In the future, we aim to extend the toolkit by sup-
porting a wider range of advanced KD algorithms
and by adding more domain-specific solutions to
align it even more closely with practical needs.
5https://www.alibabacloud.com/en/
product/machine-learningLimitations
There are a few limitations that should be acknowl-
edged. Firstly, the toolkit primarily focuses on
established methods for KD, which may limit the
exploration of non-standard KD techniques that
require further manual integration into the toolkit.
Secondly, although EasyDistill includes a variety
of industrial solutions, the effectiveness can vary
based on the specific domains and the quality of
available datasets. Finally, while EasyDistill en-
hances accessibility within the NLP community,
the toolkit assumes a certain level of technical profi-
ciency for effective utilization. Users lacking deep
familiarity with KD processes or LLMs may face a
steep learning curve when attempting to leverage
the advanced features provided by the toolkit.
Ethic Considerations and Broader Impact
The development of EasyDistill makes complex
KD processes more accessible to both academic
researchers and industry practitioners. It offers a
means for companies and educational institutions
with limited resources to implement cutting-edge
AI models. EasyDistill ’s integration into Alibaba
Cloud’s Platform for AI (PAI) and its practical
solutions further enhance the toolkit’s impact by
demonstrating its viability for large-scale deploy-
ment. Moreover, the open source of EasyDistill
encourages community involvement, which could
lead to new enhancements in KD techniques.
However, the deployment of models distilled us-
ingEasyDistill also requires careful consideration
of ethical implications, including the potential for
bias inherent in LLMs. Ensuring ethical standards
are upheld will be crucial to mitigating potential
negative social impacts.
References
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
Amanda Askell, Jackson Kernion, Andy Jones, Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon, Carol Chen, Catherine Olsson, Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau, Kamal Ndousse, Kamile Lukosiute, Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer, Noemí Mercado, Nova DasSarma, Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
ston, Shauna Kravec, Sheer El Showk, Stanislav Fort,
Tamera Lanham, Timothy Telleen-Lawton, Tom Con-
erly, Tom Henighan, Tristan Hume, Samuel R. Bow-
man, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,Nicholas Joseph, Sam McCandlish, Tom Brown, and
Jared Kaplan. 2022. Constitutional AI: harmlessness
from AI feedback. CoRR , abs/2212.08073.
Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang,
and Xiangzhong Fang. 2025a. Reasoning with om-
nithought: A large cot dataset with verbosity and cog-
nitive difficulty annotations. CoRR , abs/2505.10937.
Wenrui Cai, Chengyu Wang, Junbing Yan, Jun Huang,
and Xiangzhong Fang. 2025b. Training small reason-
ing llms with cognitive preference alignment. CoRR ,
abs/2504.09802.
DeepSeek-AI. 2025. Deepseek-r1: Incentivizing rea-
soning capability in llms via reinforcement learning.
CoRR , abs/2501.12948.
Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024.
Minillm: Knowledge distillation of large language
models. In The Twelfth International Conference on
Learning Representations . OpenReview.net.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gon-
zalez, Hao Zhang, and Ion Stoica. 2023. Efficient
memory management for large language model serv-
ing with pagedattention. In Proceedings of the 29th
Symposium on Operating Systems Principles , pages
611–626. ACM.
Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas
Mesnard, Johan Ferret, Kellie Lu, Colton Bishop,
Ethan Hall, Victor Carbune, Abhinav Rastogi, and
Sushant Prakash. 2024. RLAIF vs. RLHF: scaling
reinforcement learning from human feedback with
AI feedback. In Forty-first International Conference
on Machine Learning . OpenReview.net.
Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Ji-
axin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu,
Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying
Zhang, Fei Yin, Jiahua Dong, Zhijiang Guo, Le Song,
and Cheng-Lin Liu. 2025. From system 1 to sys-
tem 2: A survey of reasoning large language models.
CoRR , abs/2502.17419.
Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe
Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi
Yang, Denny Zhou, and Andrew M. Dai. 2024. Best
practices and lessons learned on synthetic data for
language models. CoRR , abs/2404.07503.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christo-
pher D. Manning, Stefano Ermon, and Chelsea Finn.
2023. Direct preference optimization: Your language
model is secretly a reward model. In Advances in
Neural Information Processing Systems 36: Annual
Conference on Neural Information Processing Sys-
tems 2023 .
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and
Yuxiong He. 2020. Deepspeed: System optimiza-
tions enable training deep learning models with over
100 billion parameters. In The 26th ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 3505–3506. ACM.John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
Radford, and Oleg Klimov. 2017. Proximal policy
optimization algorithms. CoRR , abs/1707.06347.
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,
Junxiao Song, Mingchuan Zhang, Y . K. Li, Y . Wu,
and Daya Guo. 2024. Deepseekmath: Pushing the
limits of mathematical reasoning in open language
models. CoRR , abs/2402.03300.
Luong Quoc Trung, Xinbo Zhang, Zhanming Jie, Peng
Sun, Xiaoran Jin, and Hang Li. 2024. Reft: Reason-
ing with reinforced fine-tuning. In Proceedings of the
62nd Annual Meeting of the Association for Compu-
tational Linguistics , pages 7601–7614. Association
for Computational Linguistics.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, Nathan Sarrazin, Omar San-
seviero, Alexander M. Rush, and Thomas Wolf. 2023.
Zephyr: Direct distillation of LM alignment. CoRR ,
abs/2310.16944.
Chengyu Wang, Junbing Yan, Yuanhao Yue, and Jun
Huang. 2025. Distilqwen2.5: Industrial practices of
training distilled open lightweight language models.
CoRR , abs/2504.15027.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V . Le,
and Denny Zhou. 2022. Chain-of-thought prompting
elicits reasoning in large language models. In Ad-
vances in Neural Information Processing Systems 35:
Annual Conference on Neural Information Process-
ing Systems 2022 .
Taiqiang Wu, Chaofan Tao, Jiahao Wang, Runming
Yang, Zhe Zhao, and Ngai Wong. 2025. Rethinking
kullback-leibler divergence in knowledge distillation
for large language models. In Proceedings of the
31st International Conference on Computational Lin-
guistics , pages 5737–5755. Association for Compu-
tational Linguistics.
Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen,
Reynold Cheng, Jinyang Li, Can Xu, Dacheng
Tao, and Tianyi Zhou. 2024. A survey on knowl-
edge distillation of large language models. CoRR ,
abs/2402.13116.
Annepaka Yadagiri and Partha Pakray. 2025. Large lan-
guage models: a survey of their development, capabil-
ities, and applications. Knowl. Inf. Syst. , 67(3):2967–
3022.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,
Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan
Li, Dayiheng Liu, Fei Huang, Guanting Dong, Hao-
ran Wei, Huan Lin, Jialong Tang, Jialin Wang,
Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin
Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai,
Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Ke-
qin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni,
Pei Zhang, Peng Wang, Ru Peng, Rui Men, RuizeGao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan,
Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge,
Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren,
Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing
Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan,
Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang,
Zhifang Guo, and Zhihao Fan. 2024a. Qwen2 techni-
cal report. CoRR , abs/2407.10671.
An Yang, Baosong Yang, Beichen Zhang, Binyuan
Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayi-
heng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian
Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang,
Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang,
Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei
Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men,
Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren,
Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang,
Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and
Zihan Qiu. 2024b. Qwen2.5 technical report. CoRR ,
abs/2412.15115.
Chuanpeng Yang, Wang Lu, Yao Zhu, Yidong Wang,
Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang
Chen. 2024c. Survey on knowledge distillation for
large language models: Methods, evaluation, and
application. CoRR , abs/2407.01885.
Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng,
and Yuandong Tian. 2024d. RLCD: reinforcement
learning from contrastive distillation for LM align-
ment. In The Twelfth International Conference on
Learning Representations . OpenReview.net.
Wenkai Yang, Shuming Ma, Yankai Lin, and Furu
Wei. 2025. Towards thinking-optimal scaling of
test-time compute for LLM reasoning. CoRR ,
abs/2502.18080.
Yuanhao Yue, Chengyu Wang, Jun Huang, and Peng
Wang. 2024a. Building a family of data augmentation
models for low-cost LLM fine-tuning on the cloud.
CoRR , abs/2412.04871.
Yuanhao Yue, Chengyu Wang, Jun Huang, and Peng
Wang. 2024b. Distilling instruction-following abil-
ities of large language models with task-aware cur-
riculum planning. In Findings of the Association
for Computational Linguistics: EMNLP 2024 , pages
6030–6054. Association for Computational Linguis-
tics.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang
Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023. A survey of large language models. CoRR ,
abs/2303.18223.Dataset Size Task Type URL
DistilQwen 100K 100K Instruction following [URL]
DistilQwen 1M 1M Instruction following [URL]
OmniThought 2M CoT reasoning [URL]
Table 2: Summarization of our released datasets.
Model LiveCodeBench V2 Speedup
Qwen2.5-3B-Instruct 11.35 2.3x
Qwen2.5-3B-Code 16.62 2.3x
Qwen2.5-7B-Instruct 30.72 -
Qwen2.5-7B-Code 35.32 -
Table 3: Performance comparison of code generation
models on LiveCodeBench V2 and inference speedup.
A List of Released Datasets
The information of our released datasets is shown
in Table 2.
B Performance of Code Generation
Models
The performance of our code generation models in
EasyDistill-Recipes is shown in Table 3.
CPerformance Summary of the Proposed
DistilQwen Models
In this section, we provide a summary of the per-
formance of the DistilQwen models that we have
released to the public. We recommend readers refer
to their model cards on Hugging Face for details.
TheDistilQwen2 andDistilQwen2.5 series, dis-
tilled from GPT-4 and Qwen-max, are System
1 models, with the instruction-following abilities
shown in Table 4 and Table 5, respectively. Note
that we further leverage Qwen2.5-72B-Instruct as
the white-box open-source teacher model to refine
theDistilQwen2.5 series.
TheDistilQwen2.5-R1 ,DistilQwen2.5-V3-0324 ,
andDistilQwen-ThoughtX series, distilled from
DeepSeek-R1, DeepSeek-V3-0324, and QwQ-32B,
are System 2 models, demonstrating deep reason-
ing abilities as shown in Table 6. Among the three
series, DistilQwen2.5-R1 represents slow-thinking
models with longer CoTs and higher reasoning
capabilities. In contrast, DistilQwen2.5-V3-0324
are fast-thinking models with significantly shorter
CoTs. Meanwhile, the DistilQwen-ThoughtX mod-
els are adaptive-thinking models with more optimal
CoT lengths.Model AlpacaEval 2.0 MT-Bench MT-Bench IFEval IFEval
(length control) (single) (instruct-loose) (strict-prompt)
Qwen2-1.5B-Instruct 5.22 5.85 6.45 41.37 28.10
DistilQwen2-1.5B-Instruct 8.28 6.42 7.12 49.76 36.04
Qwen2-7B-Instruct 24.33 8.27 8.68 66.67 52.31
DistilQwen2-7B-Instruct 25.35 8.40 9.03 71.46 60.26
Table 4: Performance comparison between the original Qwen2 models and the DistilQwen2 models in terms of
instruction-following abilities across two parameter sizes: 1.5B and 7B.
Model AlpacaEval 2.0 MT-Bench MT-Bench IFEval IFEval
(length control) (single) (instruct-loose) (strict-prompt)
Qwen2.5-0.5B-Instruct 2.46 5.49 6.26 42.81 30.31
DistilQwen2.5-0.5B-Instruct 4.89 5.78 6.83 52.61 37.82
Qwen2.5-1.5B-Instruct 6.69 7.09 7.66 55.40 40.11
DistilQwen2.5-1.5B-Instruct 13.69 7.35 7.99 61.10 74.49
Qwen2.5-3B-Instruct 17.98 7.92 8.40 61.18 74.58
DistilQwen2.5-3B-Instruct 20.91 8.37 8.97 67.03 77.36
Qwen2.5-7B-Instruct 31.43 8.52 8.83 81.53 72.10
DistilQwen2.5-7B-Instruct 34.86 8.76 9.22 83.48 73.27
Table 5: Performance comparison between the original Qwen2.5 models and the DistilQwen2.5 models in terms of
instruction-following abilities across four parameter sizes: 0.5B, 1.5B, 3B, and 7B.
Model AIME2024 MATH-500 GPQA Diamond LiveCodeBench V2
Qwen2.5-3B-Instruct 6.67 62.6 32.83 11.35
DistilQwen2.5-DS3-0324-3B 16.67 70.0 34.34 18.00
Qwen2.5-7B-Instruct 10.0 73.6 33.30 30.72
DistilQwen2.5-7B-R1 23.33 77.8 37.88 36.40
DistilQwen2.5-DS3-0324-7B 43.33 88.4 42.93 46.38
DistilQwen-ThoughtX-7B 56.67 90.2 50.00 56.80
Qwen2.5-14B-Instruct 16.7 78.2 43.43 37.38
DistilQwen2.5-14B-R1 26.67 82.6 45.45 41.49
DistilQwen2.5-DS3-0324-14B 46.67 90.8 51.52 54.40
Qwen2.5-32B-Instruct 16.67 81.4 45.50 47.36
DistilQwen2.5-32B-R1 46.67 87.0 48.99 55.97
DistilQwen2.5-DS3-0324-32B 70.00 93.8 62.12 65.95
DistilQwen-ThoughtX-32B 80.00 92.6 64.00 73.40
Table 6: Performance comparison between the original Qwen2.5 models and the DistilQwen2.5-R1 /DistilQwen2.5-
DS3-0324 /DistilQwen-ThoughtX models in terms of deep reasoning abilities.