arXiv:2505.20751v1  [cs.RO]  27 May 2025Interactive OT Gym: A Reinforcement Learning-Based Interactive
Optical tweezer (OT)-Driven Microrobotics Simulation Platform
Zongcai Tan, Dandan Zhang
Abstract — Optical tweezers (OT) offer unparalleled capabili-
ties for micromanipulation with submicron precision in biomed-
ical applications. However, controlling conventional multi-trap
OT to achieve cooperative manipulation of multiple complex-
shaped microrobots in dynamic environments poses a signifi-
cant challenge. To address this, we introduce Interactive OT
Gym, a reinforcement learning (RL)-based simulation platform
designed for OT-driven microrobotics. Our platform supports
complex physical field simulations and integrates haptic feed-
back interfaces, RL modules, and context-aware shared control
strategies tailored for OT-driven microrobot in cooperative
biological object manipulation tasks. This integration allows for
an adaptive blend of manual and autonomous control, enabling
seamless transitions between human input and autonomous
operation. We evaluated the effectiveness of our platform using
a cell manipulation task. Experimental results show that our
shared control system significantly improves micromanipulation
performance, reducing task completion time by approximately
67% compared to using pure human or RL control alone
and achieving a 100% success rate. With its high fidelity,
interactivity, low cost, and high-speed simulation capabilities,
Interactive OT Gym serves as a user-friendly training and
testing environment for the development of advanced interactive
OT-driven micromanipulation systems and control algorithms.
For more details on the project, please see our website
https://sites.google.com/view/otgym
I. I NTRODUCTION
Efficient, safe, and precise micromanipulation is crucial in
biomedical applications such as cell sorting, tissue engineer-
ing, and microassembly [1]. Optical tweezers (OT), with their
piconewton-level precision at micro-nanoscale [2], are fre-
quently used alongside microfluidic chips to manipulate and
sort rare cells in small sample volumes [3]–[5]. However, for
cells highly sensitive to phototoxicity and thermal damage,
such as red blood cells, neurons, and circulating tumor cells
(CTCs) [6], direct laser exposure can be unsafe during OT-
based manipulation. Hence, manipulating microrobots via
OT to achieve indirect cell handling is a more attractive
solution [7]–[9].
Using multi-trap optical tweezers (OT) to control complex-
shaped microrobots for indirect biological manipulation
presents significant challenges, including precise alignment
and stable cooperative control [10]–[13]. Precise force con-
trol is particularly crucial, as excessive force can damage
delicate biological structures. Therefore, developing haptic
feedback systems to enhance users’ perception of force is
essential [14]–[19]. Existing haptic devices primarily assist
users in path following or obstacle avoidance, and the
Zongcai Tan, Dandan Zhang are with the Department of Bioengineering,
Imperial-X Initiative, Imperial College London, London, United Kingdom.
Corresponding: d.zhang17@imperial.ac.uk.
High -End Workstation
(for Autonomous  control)Haptic  Devices
(for Human  control)
Simulation  PlatformOptical 
Tweeze r CellStatic  Deionized 
Obstacle  Water
Dynamic
ObstacleMicro
RobotFig. 1. Overview of the Interactive OT Gym Components: Haptic Devices
for Human Control, High-End Workstation for Autonomous Control, and
Simulation Platform.
manipulated objects are typically simple geometric shapes
such as spheres or ellipsoids [20], [21].
Furthermore, most robotic simulation platforms are de-
signed for macro-scale systems, making them unsuitable
for OT-driven microrobotics, which require adaptability and
a force-sensitive human interaction interface. The lack of
simulation platforms further limits accessibility and slows
innovation. Additionally, autonomous control of OT-driven
microrobots remains underexplored. While reinforcement
learning (RL) has been used to improve efficiency and adapt-
ability in optical manipulation [22], [23], it has not yet been
applied to interactive control of complex-shaped microrobots
or the unique challenges inherent in OT environments.
To overcome these limitations,we developed a simulation
platform, which integrates physical principles, capable of
simulating complex phenomena like fluid resistance, dy-
namic obstacles, and random thermal motion in OT envi-
ronments. This high-fidelity platform supports the evalua-
tion of autonomous, semi-autonomous, and shared control
algorithms. However, in complex tasks, the limited range
of optical traps and the challenges in stable long-distance
navigation lead to high computational costs and long training
times for training RL model. To address these issues, we
designed a progressive RL algorithm that enhances path
planning efficiency with A* and adaptability in dynamic con-
trol with RL, providing a data-efficient solution for optical
microrobot autonomy [24].
While full automation can enhance efficiency, ensuring
safety and dexterity in micromanipulation within dynamic
and uncertain biological environments remains a critical
challenge [25]. Manual control provides immediate feedback
through human real-time decision-making; however, it of-ten suffers from low efficiency due to the complexity of
micromanipulation tasks and operator fatigue in dynamic
environments [26]. Mela Coffey et al. [20] proposed a
collaborative control framework that integrates manual con-
trol with automatic algorithms, but its fixed control weight
allocation and switching time limit its adaptability in dy-
namic environments. Similarly, Quang Minh Ta et al. [27]
introduced a haptic feedback system, but operators must
simultaneously manage obstacle avoidance and navigation
speed, which increases their cognitive load and limits au-
tomation. In contrast, this paper presents a context-aware
shared control strategy that combines RL-driven autonomous
control with enhanced haptic feedback, dynamically adjust-
ing control weights between human and robot in response
to environmental changes. This approach offers a promising
solution by improving safety, flexibility, and efficiency in
micromanipulation systems.
To this end, this platform integrates a RL autonomous
control system and a haptic feedback-enhanced teleoperation
system, achieving for the first time the combination of
manual and autonomous control in a simulation environment
of OT and microfluidic chips to support human-robot col-
laboration, thereby improving the safety and efficiency of
micromanipulation.
In summary, to address the current limitations in OT-driven
microrobotics, we propose the Interactive OT Gym , the
first simulation platform (to the best of our knowledge) that
supports complex physical field simulations. This platform
offers high-fidelity force feedback and an environment for
training and deploying reinforcement learning (RL) models
for autonomous microrobot control. The main contributions
of this paper are as follows:
•High-fidelity, cost-effective RL training environment :
The platform provides a realistic and interactive simu-
lation environment that is both low-cost and capable of
rapid testing.
•Integration of shared control strategies : Interactive
OT Gym uniquely combines RL-based autonomous
control with haptic feedback-enhanced teleoperation.
Furthermore, unlike previous platforms that only support
autonomous algorithm simulation, our platform includes a
haptic feedback teleoperation interface and can realistically
reproduce complex physical phenomena in OT microma-
nipulation environments, such as fluid resistance, dynamic
obstacles, and random thermal motion.
II. M ETHODOLOGY
A. System Overview
The Interactive OT Gym was designed as a distributed
architecture-based simulation platform for OT-driven micro-
robot collaborative manipulation, which was constructed by a
pair of Geomagic Touch haptic device, high-end workstation,
and a simulation environment (see Fig. 1). This integrated
system is composed of three main modules:
•Simulator: A high-fidelity physical simulation environ-
ment that provides accurate motion modeling and visual
rendering. (See Fig.2 (a))•Autonomous Control: A navigation module based on
reinforcement learning (RL) for autonomous microma-
nipulation. (See Fig.2 (b))
•Manual Control: An interactive teleoperation interface
enhanced with haptic feedback for precise and intuitive
user control.
Each module is deployed on a separate device, with com-
munication managed by the Robot Operating System (ROS).
This distributed architecture prevents resource contention
by enabling parallel execution and ensuring computational
efficiency. In this study, we use a complex-shaped optical
microrobot developed by Zhang et al. [13] as an example.
B. Construction of the Physical Simulation Environment
The high cost and complexity of traditional physical
experiments limit the training and validation of algorithms.
To accelerate this process and save time, we developed a
virtual environment using NVIDIA Omniverse’s Isaac Sim
physics engine. This environment accurately simulates a mi-
cromanipulation system that combines OT and microfluidics,
which can be used for RL model training. More specifically,
we created a simulated microfluidic environment (Fig.2 (a)),
in which optical traps are simulated to control microrobots
for indirect cell manipulation. The simulated microfluidic
chip features a four-branch structure with two inlets and
two outlets, and microchannels measuring 25 µm in width
and 50 µm in height, filled with buffer fluid to ensure
stability. The key forces acting on the objects, as described
by Equation (1), include optical trapping Foptical , Brownian
motion FBrownian , van der Waals forces Fvan der Waals , viscous
dragFdrag, hydrodynamic forces Fhydrodynamic , contact forces
(including collision force) Fcontact , and other forces Fother.
Additionally, four randomly moving obstacles are included
to increase the training complexity.
F(t) =Foptical+Fhydrodynamic +Fvan der Waals +Fdrag
+FBrownian +Fcontact +Fother(1)
C. Autonomous Control with Progressive RL Navigation
We proposed a progressive training strategy (Fig. 3) that
integrates A* path planning with RL-driven speed control to
achieve fully autonomous navigation. In this approach, A* is
used for long-range path planning due to its computational
efficiency and stability, as RL-based path planning would
impose prohibitively high training costs. Meanwhile, precise
speed control is essential in the microenvironment, where the
limited range of optical trapping fields and environmental
dynamics present significant challenges. Excessive speed
may lead to cell or robot loss, whereas slower speeds exacer-
bate Brownian motion-induced disturbances and reduce nav-
igation efficiency. Since directly modeling the relationship
between navigation speed and dynamic environmental factors
is impractical due to its complexity and uncertainty, RL
provides a feasible solution for real-time speed adaptation,
while A* ensures stable path planning. This hybrid approach
reduces computational burden and enhances training effi-
ciency in complex navigation scenarios.Binary 
image
θ, x, y
Target –network 
(MLP)(s, a, s’)Position 
calculation
Action 
(a)Reward 
Function
Experience Replay 
Buffer(s, a, r, s’)
Q learning 
updateQ –network 
(MLP)sample
minibatch
Simulated Microfluidic Chip
Target Point for Cell A
Target Point for Cell B
Optical MicrorobotsOptical Traps       
Cells for TransportationDynamic ObstaclesStatic ObstaclesMicrorobot 
Initial PositionState 
(s)Reinforcement Learning -Based Framework
Microrobots
Microrobots(a) (b)Fig. 2. The distributed architecture of shared control (connected by ROS), the motion of microrobots is determined by both RL control and human
teleoperation. (a) Simulated Microfluidic Chip (b) Hardware & Interactive environment & DQN-Optimized Speed Control for Microrobots.
Fig. 3. Progressive training process for autonomous navigation.
In the preprocessing stage, grayscale images are bina-
rized to separate obstacles from channels. To ensure safety,
morphological dilation expands obstacle regions based on
the microrobot’s size, taking into account both the robot’s
occupied volume and potential fine-tuning of the path, thus
providing sufficient collision avoidance margins. The image
coordinates are then converted to physical map coordinates
for planning.
During path planning, the A* algorithm, extended
with eight-directional movement, generates a more flexible
pathfinding around obstacles, while B-spline interpolation
smooths the path, eliminating sharp turns for smoother
robot motion. After path planning, an RL agent performs
local speed control training along the A*-generated path.
As shown in Fig.2 (b), the DQN framework uses a state
space sthat includes the robot’s real-time coordinates, cell
positions, and obstacles. The action space aconsists of
discrete speed levels, and the Q-network is updated using
mini-batch sampling from the experience replay buffer D,
following Equation (2):
Q(st, at) =Q(st, at) +αh
rt+γmax
aQ(st+1, a)−Q(st, at)i
(2)
Here, stis the state at time t,atis the action, rtis
the immediate reward, αis the learning rate, and γis thediscount factor. The reward function R(s, a)balances contact
force optimization, collision avoidance, and speed control:
R(s, a) =Rcontact-force (s, a) +Rcollision-penalty (s, a) +Rspeed(s, a)
(3)
where Rcontact-force rewards the agent for maintaining an opti-
mal contact force, avoiding both excessive force, which could
damage the cell, and insufficient force, which could lead
to cell detachment, Rcollision-penalty penalizes collisions with
obstacles or loss of the optical trap, and Rspeed encourages
maintaining a high but safe speed along the path.
During training, an epsilon-greedy strategy is used to
balance exploration and exploitation, with epsilon decaying
from 1.0 to 0.01 over time. The target network parameters
are updated periodically to ensure stable convergence.
Training was conducted on an NVIDIA GeForce RTX
4060 Laptop GPU (Nvidia Corporation, USA) colorredwith
8GB of GDDR6 memory, for 15 hours across 1000 episodes,
with each episode consisting of approximately 200 time
steps.
D. Manual Control with Haptic Feedback
Based on the physical simulation environment, this study
developed a bimanual haptic feedback system for optical
tweezer-driven microrobot teleoperation. The system en-
hances the user’s perception of micro-scale forces during
indirect cell manipulation, preventing excessive pressure that
could damage cells and ensuring the stability and safety of
OT operations.
In this system, two haptic devices (Geomagic Touch, 3D
Systems, USA) are used as input devices. The velocity
of the device’s tips incrementally controls the positions of
two optical traps, which drive two optical microrobots. A
vision-based force sensor measures the relative distance and
direction between the robots and the tweezers’ focal points.
The system calculates the three-dimensional optical forces
based on the optical trap force model (Equation (4)) and
scales these forces to provide real-time feedback through the
haptic devices.
fOT(X) =(K· ∥Xtrap−XObject∥,if∥Xtrap−XObject∥< δ
C+A
∥Xtrap−XObject∥2,if∥Xtrap−XObject∥ ≥δ
(4)where XtrapandXObject denote the positions of the optical
trap and the microrobot, respectively. K= 0.455 is the
trap stiffness, δ= 0.446defines the threshold between near
and far field interactions, A= 0.058 scales the attractive
force in the far field, and C= 0.01serves as an offset for
distant interactions. To enhance stability and improve user
experience, the optical forces are empirically scaled. Both
high-frequency noise from haptic feedback and disturbances
from the operator’s manual input are filtered using low-pass
filters. These noise sources, primarily caused by Brownian
motion and human tremor, can lead to jittery control.
E. Human-Robot Collaborative control
To enable safer manipulation in complex microscopic
environments, a shared control strategy was developed based
on real-time distance adjustments (see Fig. 4). The system
dynamically adjusts the weight distribution between human
control and automated navigation by calculating the real-
time distance between the robot and dynamic obstacles,
offering three operation modes: fine-tuning, balanced, and
autonomous.
Fig. 4. Framework for human-robot shared control based on context-aware
adaptation. Here we substitute d1= 1, d2= 2 into Equation (5).
Specifically, the displacement increment of the robot con-
trol is defined as ∆Ps
t, where ∆Pr
tis determined by the
automated navigation algorithm, and ∆Ph
tis the control
command from the human operator. The motion scaling
factor is τ, and the weight parameter αis dynamically
adjusted based on the real-time distance d. The control
formula is ∆Ps
t=τ[α∆Ph
t+(1−α)∆Pr
t], where the value
ofαis dynamically adjusted according to the distance d, as
defined by:
α=

0.5 ifd≤d1,
0.5−0.4×d−d1
d2−d1ifd1< d≤d2,
0.1 ifd > d 2.(5)
This shared control strategy ensures accurate fine manip-
ulation while enhancing automation efficiency, providing an
effective and safe solution for OT-driven microrobot-assisted
cell sorting tasks in complex environments.
III. E XPERIMENTS AND RESULTS
A. Evaluation of Path Planning Performance
1) Task Description: We evaluated the performance of
manual and algorithm-generated trajectories using the motion
capture module of our proposed Interactive OT Gym. OTcontrolled two microrobots to grasp and transport cells from
a starting point to a destination in a cooperative manner. The
platform recorded the trajectories, and we conducted qualita-
tive and quantitative analyses—including frequency domain
smoothness analysis—to assess manual paths, A* algorithm-
generated paths, and B-spline smoothed paths based on A*.
2) Results and Analysis: Fig. 5 (a) visualizes the three
paths. The manual path shows noticeable detours and jitter;
the A* path has rigid turns; the B-spline smoothed path
demonstrates a smoother trajectory closely approximating the
shortest path.
TABLE I. Comparison of different trajectory types using selected metrics.
Trajectory Type Total Path Length Mean Curvature Angular Deviation
Manual 9.6193 314.34 45.962
A* 9.4362 3.3893 9.5632
B-Spline 9.1352 0.64264 0.30768
Quantitative analysis (Table I) demonstrates that the B-
spline path outperforms the others in total path length,
mean curvature, and angular deviation. It achieves a shorter
path and eliminates the sharp turns of the A* path through
smoothing, resulting in a more continuous trajectory. The
A* algorithm, constrained by discrete movements, introduces
high curvature at turns, which is significantly reduced by the
B-spline smoothing.
Frequency domain analysis (Fig. 5 (b)) shows:
•the manual path contains significant high-frequency
components, indicating substantial jitter and noise.
•the A* path reduces some high-frequency elements but
still retains some noise.
•the B-spline smoothed path effectively eliminates high-
frequency noise, confirming its superior smoothness.
The B-spline smoothed path performs best across all eval-
uation metrics. Its shorter path length, smooth curves, and
low noise make it highly suitable for applications requiring
high-precision path planning.
B. Evaluation of RL Training Effectiveness
1) Experiment Description: This experiment aimed to op-
timize microrobot speed control along a predefined trajectory
using RL to balance the navigation navigation efficiency and
safety in OT micromanipulation. The RL model focused
on optimizing speed control along a B-spline smoothed
trajectory, minimizing inefficiency from slow speeds and
fluctuations caused by Brownian motion, while preventing
trap loss or cell detachment at high speeds. The reward
function included penalties for contact forces above 10pN
and deviations of more than 0.2µm from the trap center, as
well as rewards for selecting higher speed levels.
As shown in Fig. 5 (c), the RL model’s reward values
steadily increased and converged after around 700 episodes,
indicating that the model successfully learned to balance
speed and navigation precision.
Initially, violin and box plots (Fig. 6 (a) and (b)) show
frequent contact forces above 10pN, and median robot dis-
tance from the trap center exceeded 0.06µm, with some
values approaching the 0.2µm threshold. However, as trainingEpisodeReinforcement Learning Curve
Reward
(um)(um)(a) (b) (c)Fig. 5. (a) Visualization of the three paths: manual, A*, and B-spline smoothed paths. (b) Spectral analysis results of the three paths: manual, A*, and
B-spline smoothed paths. (c) learning curve of cumulative reward.
Fig. 6. Safety evolution during RL-based robot navigation training: distri-
butions of contact force and robot-trap distance. (a), (b) before training; (c),
(d) after training.
progressed, the RL agent improved its strategy. By the end of
training, contact forces remained below 10pN, and the robot
stayed within 0.06µm of the trap center (Fig. 6 (c) and (d)),
demonstrating that RL training effectively reduced high-risk
behaviors, improving stability and safety.
RL training successfully optimized speed control, reduced
contact force fluctuations, and improved navigation preci-
sion. By balancing speed and safety, the model achieved
stable and efficient control in complex micromanipulation
tasks, demonstrating strong potential for applications requir-
ing safety and rapid operations.
C. Evaluation of RL Dynamic Speed Control Performance
1) Experiment Description: We evaluated the perfor-
mance of RL dynamic speed control by comparing it to a
constant speed control using six speed levels, ranging from
0.27µm/s to 0.51µm/s, with speeds increasing from group
1 to group 6. The metrics for comparison included task
completion time and success rate (the percentage of the total
distance completed).TABLE II. Average Success Rates (ASR) and Task Completion Times
(ATCT) for RL-based control and six groups of constant speed control
Metric RL G1 G2 G3 G4 G5 G6
ASR 1.000 0.209 0.689 0.836 1.000 1.000 1.000
ATCT (s) 38.92 / 36.20 41.41 44.80 53.40 66.56
Note: ‘G’ indicates ’Group’. The slash (/) indicates that all experiments in the
group failed, so no task completion time is available for statistics.
2) Results and Analysis: As shown in Table II, the
success rate of constant speed control decreased as the
speed increased, eventually dropping below 21%. Although
slower speeds achieved a 100% success rate, they signifi-
cantly increased task completion time, making this approach
inefficient. In contrast, RL-controlled dynamic speed main-
tained a 100% success rate while significantly reducing
task completion time. This was achieved through dynamic
speed adjustment, which prevents loss of control over the
microrobots at higher speeds.
The evaluation showed that RL dynamic speed control
outperformed constant speed control by maintaining a higher
success rate and significantly reducing task completion time.
This improvement stems from RL’s ability to adaptively
adjust navigation speed based on the dynamic environment.
D. Evaluation of Shared Control Effectiveness
We conducted experiments to validate the performance of
the OT shared micromanipulation system in terms of task
efficiency and user experience, comparing it with manual
and autonomous (RL-based) control.
1) Experimental Setup: Eight volunteers (ages 22–27, two
females and six males) from Imperial College London par-
ticipated. Five had prior experience with micromanipulation
devices, and four had used haptic feedback devices. The
task involved using an optical microrobot to grasp cells and
navigate through a microfluidic chip with dynamic obstacles,
transporting cells from start to destination.
Participants initially operated the OT via a Geomagic
Touch device, receiving haptic feedback for haptic perception
of the gripping force. After grasping the cells, the RL
algorithm handled navigation, with users making fine adjust-
ments when needed. As the microrobot approached dynamic
obstacles, the manual control weight gradually increased
through a context-aware adaptation algorithm, prompting
users through visual cues from the user interface for precise
”fine-tune” interventions (Fig. 8). Haptic feedback adjustedFig. 7. Comparison of success rate (a) and task completion time (b) under manual, autonomous, and shared control modes. (c) NASA-TLX scores under
manual and shared control modes. (d) UEQ-S scores for manual and shared control modes. Statistical significance between groups is indicated by stars,
where one star ( ∗) represents a significant difference with p < 0.05, two stars ( ∗∗) represent p < 0.01, and three stars ( ∗ ∗ ∗ ) represent p < 0.001.
resistance to prevent loss of control or cell detachment if
users moved too quickly.
Fig. 8. User interface during microrobot manipulation, showing local and
global views with situational awareness cues, haptic force, and motion
direction guidance.
2) Quantitative Results: Key metrics were task comple-
tion time and success rate (the proportion of dynamical ob-
stacles avoided). Each mode (manual, autonomous, shared)
was tested eight times for reliability.
According to Fig. 7 (a), manual control had the longest
completion time ( 163.5±8.3s), while autonomous and
shared control times were much shorter ( 54.1±0.1s and
53.9±0.2s, respectively). ANOV A confirmed manual control
was significantly slower than the other modes ( p <0.01).
According to Fig. 7 (b), shared control achieved a 100% suc-
cess rate, outperforming manual ( 94%±4%) and autonomous
control ( 59%±5%), which struggled with dynamic obstacles.
Shared control combined RL-based global navigation with
user intervention, improving overall performance.
3) User Experience Evaluation: After the experiments,
participants completed the NASA Task Load Index (NASA-
TLX) and User Experience Questionnaire Short version
(UEQ-S) to assess workload and user satisfaction. As shown
in Fig. 7 (c), shared control significantly reduced work-
load across all six NASA-TLX dimensions compared to
manual control, especially in mental and physical demand,
highlighting the reduced user burden due to task delegation
to automation. The UEQ-S results (Fig. 7 (d)) showed
shared control scored higher across all seven dimensions,
particularly in supportiveness and effectiveness ( p <0.001).
Although manual control also scored well in supportivenessand clarity, shared control significantly improved overall user
satisfaction.
In conclusion, the shared control system outperformed
manual and autonomous modes by combining RL-based
automation with human adaptability. It reduced task com-
pletion time, improved success rates, and lowered workload,
demonstrating its potential for complex micromanipulation
tasks requiring precision and safety.
IV. C ONCLUSION
In this paper, We introduced Interactive OT Gym, the first
interactive simulation platform for OT-driven microrobotics
that incorporates complex physical field simulations. By inte-
grating a high-fidelity simulator, haptic feedback-based man-
ual control modules, progressive RL-based autonomous con-
trol modules, and shared control strategies, we constructed
and effective human-robot collaboration through seamless
integration of manual and autonomous control. Experimental
results demonstrated that our shared control system signifi-
cantly enhanced micromanipulation performance, reducing
task completion time by 67% and increasing the success
rate to 100%. User workload was also substantially reduced,
highlighting the platform’s effectiveness in improving effi-
ciency, precision, and safety in OT-driven micromanipulation
tasks in simulated cell transportation. This simulation plat-
form offers an efficient and low-cost training environment
for the development of next-generation collaborative OT-
driven microrobotics systems. It holds significant potential
in areas such as tissue engineering, micro-assembly, and
biological object manipulation, where precise and efficient
micromanipulation is crucial.
Future work will focus on improving simulation fidelity to
bridge the sim-to-real gap, enabling more accurate RL-based
control models to be transferred to real-world biomedical
applications after training in the simulated environment.
ACKNOWLEDGE
This research was conducted in accordance with ethical
guidelines and was approved by the Imperial College London
Research Ethics Committee (Approval ID: 7134867).REFERENCES
[1] N. Bhagwat, K. Dulmage, C. H. Pletcher Jr, L. Wang, W. DeMuth,
M. Sen, D. Balli, S. S. Yee, S. Sa, F. Tong et al. , “An integrated flow
cytometry-based platform for isolation and molecular characterization
of circulating tumor single cells and clusters,” Scientific reports , vol. 8,
no. 1, p. 5035, 2018.
[2] D. G. Grier, “A revolution in optical manipulation,” nature , vol. 424,
no. 6950, pp. 810–816, 2003.
[3] X. Wang, S. Chen, M. Kong, Z. Wang, K. D. Costa, R. A. Li,
and D. Sun, “Enhanced cell sorting and manipulation with combined
optical tweezer and microfluidic chip technologies,” Lab on a Chip ,
vol. 11, no. 21, pp. 3656–3662, 2011.
[4] X. Wang, Z. Wang, and D. Sun, “Cell sorting with combined optical
tweezers and microfluidic chip technologies,” in 2010 11th Interna-
tional Conference on Control Automation Robotics & Vision . IEEE,
2010, pp. 201–206.
[5] T. Xu, Y . Li, X. Han, L. Kan, J. Ren, L. Sun, Z. Diao, Y . Ji, P. Zhu,
J. Xu et al. , “Versatile, facile and low-cost single-cell isolation, culture
and sequencing by optical tweezer-assisted pool-screening,” Lab on a
Chip , vol. 23, no. 1, pp. 125–135, 2023.
[6] A. Bl ´azquez-Castro, “Optical tweezers: Phototoxicity and thermal
stress in cells and biomolecules,” Micromachines , vol. 10, no. 8, p.
507, 2019.
[7] U. G. B ¯utait˙e, G. M. Gibson, Y .-L. D. Ho, M. Taverne, J. M.
Taylor, and D. B. Phillips, “Indirect optical trapping using light driven
micro-rotors for reconfigurable hydrodynamic manipulation,” Nature
communications , vol. 10, no. 1, p. 1215, 2019.
[8] M. Xie, A. Shakoor, and C. Wu, “Manipulation of biological cells
using a robot-aided optical tweezers system,” Micromachines , vol. 9,
no. 5, p. 245, 2018.
[9] Y . Hou, H. Wang, R. Fu, X. Wang, J. Yu, S. Zhang, Q. Huang,
Y . Sun, and T. Fukuda, “A review on microrobots driven by optical
and magnetic fields,” Lab on a Chip , vol. 23, no. 5, pp. 848–868,
2023.
[10] S. Chowdhury, A. Thakur, P. ˇSvec, C. Wang, W. Losert, and S. K.
Gupta, “Automated manipulation of biological cells using gripper
formations controlled by optical tweezers,” IEEE Transactions on
Automation Science and Engineering , vol. 11, no. 2, pp. 338–347,
2013.
[11] Q. M. Ta and C. C. Cheah, “Stochastic control for orientation and
transportation of microscopic objects using multiple optically driven
robotic fingertips,” IEEE Transactions on Robotics , vol. 35, no. 4, pp.
861–872, 2019.
[12] A. Thakur, S. Chowdhury, P. ˇSvec, C. Wang, W. Losert, and S. K.
Gupta, “Indirect pushing based automated micromanipulation of bi-
ological cells using optical tweezers,” The International Journal of
Robotics Research , vol. 33, no. 8, pp. 1098–1111, 2014.
[13] D. Zhang, A. Barbot, B. Lo, and G.-Z. Yang, “Distributed force control
for microrobot manipulation via planar multi-spot optical tweezer,”
Advanced Optical Materials , vol. 8, no. 21, p. 2000543, 2020.
[14] M. Yin, E. Gerena, C. Pacoret, S. Haliyo, and S. Regnier, “High-
bandwidth 3d force feedback optical tweezers for interactive bio-
manipulation,” in 2017 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems (IROS) . IEEE, 2017, pp. 1889–1894.
[15] E. Gerena, F. Legendre, Y . Vitry, S. R ´egnier, and S. Haliyo, “Improving
optical micromanipulation with force-feedback bilateral coupling,” in
2020 IEEE International Conference on Robotics and Automation
(ICRA) . IEEE, 2020, pp. 10 292–10 298.
[16] Y . Tanaka and K. Fujimoto, “Dual-arm visuo-haptic optical tweezers
for bimanual cooperative micromanipulation of nonspherical objects,”
Micromachines , vol. 13, no. 11, p. 1830, 2022.
[17] Z. Ni, C. Pacoret, R. Benosman, and S. R ´egnier, “2d high speed
force feedback teleoperation of optical tweezers,” in 2013 IEEE
International Conference on Robotics and Automation . IEEE, 2013,
pp. 1700–1705.
[18] C. Pacoret and S. R ´egnier, “Invited article: A review of haptic
optical tweezers for an interactive microworld exploration,” Review
of Scientific Instruments , vol. 84, no. 8, 2013.
[19] K. Sakamoto, T. Aoyama, M. Takeuchi, and Y . Hasegawa, “Intuitive
cell manipulation microscope system with haptic device for intracy-
toplasmic sperm injection simplification,” Sensors , vol. 24, no. 2, p.
711, 2024.
[20] M. Coffey and A. Pierson, “Collaborative teleoperation with haptic
feedback for collision-free navigation of ground robots,” in 2022IEEE/RSJ International Conference on Intelligent Robots and Systems
(IROS) . IEEE, 2022, pp. 8141–8148.
[21] J. Lee, X. Zhang, C. H. Park, and M. J. Kim, “Real-time teleoperation
of magnetic force-driven microrobots with 3d haptic force feedback
for micro-navigation and micro-transportation,” IEEE Robotics and
Automation Letters , vol. 6, no. 2, pp. 1769–1776, 2021.
[22] M. Praeger, Y . Xie, J. A. Grant-Jacob, R. W. Eason, and B. Mills,
“Playing optical tweezers with deep reinforcement learning: in virtual,
physical and augmented environments,” Machine Learning: Science
and Technology , vol. 2, no. 3, p. 035024, 2021.
[23] Y . Lee and E. Chae, “Machine learning-enhanced optical tweezers
for defect-free rearrangement,” Current Applied Physics , vol. 61, pp.
150–159, 2024.
[24] S. A. Abbasi, A. Ahmed, S. Noh, N. L. Gharamaleki, S. Kim,
A. M. B. Chowdhury, J.-y. Kim, S. Pan ´e, B. J. Nelson, and H. Choi,
“Autonomous 3d positional control of a magnetic microrobot using
reinforcement learning,” Nature Machine Intelligence , vol. 6, no. 1,
pp. 92–105, 2024.
[25] M. Ammi and A. Ferreira, “Robotic assisted micromanipulation sys-
tem using virtual fixtures and metaphors,” in Proceedings 2007 IEEE
International Conference on Robotics and Automation . IEEE, 2007,
pp. 454–460.
[26] G. Hwang and H. Hashimoto, “Development of a human-robot-
shared controlled teletweezing system,” IEEE Transactions on Control
Systems Technology , vol. 15, no. 5, pp. 960–966, 2007.
[27] Q. M. Ta and C. C. Cheah, “Human–machine interaction control for
stochastic cell manipulation systems,” Automatica , vol. 131, p. 109721,
2021.