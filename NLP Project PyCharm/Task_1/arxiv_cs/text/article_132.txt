arXiv:2505.20740v1  [cs.AI]  27 May 2025MSEarth: A Benchmark for Multimodal Scientific
Comprehension of Earth Science
Xiangyu Zhao1,2†, Wanghan Xu1,3†, Bo LIU2, Yuhao Zhou1, Fenghua Ling1,
Ben Fei1, 4,Xiaoyu Yue1,Lei Bai1,Wenlong Zhang1,Xiao-Ming Wu2
1Shanghai Artificial Intelligence Laboratory2The Hong Kong Polytechnic University
3Shanghai Jiao Tong University4The Chinese University of Hong Kong
zhangwenlong@pjlab.org.cn, xiang-yu.zhao@connect.polyu.hk
Atmosphere Hydrosphere CryosphereLithosphere BiosphereMSEarth -Bench
5 Sphere
Lithosphere
Cryosphere
Hydrosphere
Atmosphere
Biosphere13+ Advanced Models
Qwen2.5 -VL-7/32/72B 
Llama3.2 -11/90B -Vision 
InternVL -8/78B
GPT -4o Gemini -1.5-flash
Claude -3-5-haiku/sonnet3 Sub -Tasks
Figure Captioning
Multi -Choice VQA
Open -Ended VQACollaborative Evaluator
Multi -Agent Evaluator
Expert Evaluator
Figure 1: Illustrative examples of the diverse types of scientific figures in MSEarth, sourced from
open-access articles available from website.
Abstract
The rapid advancement of multimodal large language models (MLLMs) has un-
locked new opportunities to tackle complex scientific challenges. Despite this
progress, their application in addressing earth science problems, especially at the
graduate level, remains underexplored. A significant barrier is the absence of bench-
marks that capture the depth and contextual complexity of geoscientific reasoning.
Current benchmarks often rely on synthetic datasets or simplistic figure-caption
pairs, which do not adequately reflect the intricate reasoning and domain-specific
insights required for real-world scientific applications. To address these gaps, we
introduce MSEarth, a multimodal scientific benchmark curated from high-quality,
open-access scientific publications. MSEarth encompasses the five major spheres
of Earth science: atmosphere, cryosphere, hydrosphere, lithosphere, and biosphere,
featuring over 7K figures with refined captions. These captions are crafted from
the original figure captions and enriched with discussions and reasoning from the
papers, ensuring the benchmark captures the nuanced reasoning and knowledge-
intensive content essential for advanced scientific tasks. MSEarth supports a
†Equal Contribution.variety of tasks, including scientific figure captioning, multiple choice questions,
and open-ended reasoning challenges. By bridging the gap in graduate-level bench-
marks, MSEarth provides a scalable and high-fidelity resource to enhance the
development and evaluation of MLLMs in scientific reasoning. The benchmark is
publicly available to foster further research and innovation in this field. Resources
related to this benchmark can be found at https://huggingface.co/MSEarth and
https://github.com/xiangyu-mm/MSEarth.
1 Introduction
The advent of multimodal large language models (MLLMs) [ 1,2] has revolutionized artificial
intelligence, driving groundbreaking advancements across diverse scientific disciplines. Prominent
examples include ChemVLM [ 3] in chemistry, GeoChat [ 4] in geography, and WeatherQA [ 5] in
atmospheric science. These models excel in domain-specific visual question answering by integrating
specialized knowledge. For example, ChemVLM facilitates the analysis of molecular structures,
chemical reactions, and chemistry-related examination questions, while WeatherQA enables reasoning
about severe weather events in real-world scenarios.
Refined  Caption :This figure shows the cyclonic west circulation (Wz)identified forthe period 26-30
October 1824 .During this period, alow-pressure system centered over theBritish Isles, incontrast toahigh -
pressure system over theMediterranean , created  a strong  pressure  gradient .This ledtotheinflow ofwarm
and humid airmasses from theAtlantic into Central Europe . The resulting  weather  conditions  included  gales,  
thunderstorms,  and heavy  precipitation,  particularly  affecting  southwest  Germany . Notably,  the low-
pressure  system  triggered  intense  rainstorms  in both  the northern  and southern  regions  of the Black  Forest .
Raw Caption : Atmospheric circulation pattern over Central 
Europe from 26 -28 October 1824 .
Explanation
Step 1: The figure shows a low -pressure system over the British Isles and a high -pressure system over the 
Mediterranean.
Step 2: The pressure gradient would drive air masses from the Atlantic Ocean into Central Europe .
Step 3: The Atlantic Ocean is a source of warm, humid air, which would dominate the inflow during this 
circulation pattern.Question : What type of air mass  is likely brought into Central Europe based on the pressure gradient shown?
A. Cold and dry air
B. Warm and humid air
C. Cold and wet air
D. Warm and dry air
Answer : B. Warm and humid airQuestion: What type of synoptic -scale 
weather system does this map most 
likely illustrate? 
A. Tropical cyclone
B. Extratropical (mid -latitude)
cyclone
C. Tornado
D. Mesoscale convective
thunderstorm
Answer : B
Explanation
Step 1:The system islarge inscale (hundreds tothousands ofkm)
and located around 50°N.
Step 2:Itfeatures aclosed low with concentric isobars—
characteristic ofmid-latitude cyclones .
Step 3:Therefore, it’sanextratropical (temperature -gradient -driven)
cyclone .
Generated VQA with raw caption Generated VQA with refined caption
(a) (b)
Figure 2: Illustration of VQA generation methodologies: (a) VQA relying exclusively on figure
captions, and (b) VQA utilizing refined captions that integrate figure captions with content from
academic papers. Highlighted areas denote questions and answers supported by evidence.
Developing multimodal large language models (MLLMs) that understand advanced geoscientific
knowledge necessitates rigorous benchmarks to evaluate their ability to solve complex, discipline-
specific problems. Existing benchmarks often rely on synthetic datasets or materials from high school
and undergraduate textbooks [ 6,7], which lack the depth required for professional, graduate-level
tasks. Recent efforts [ 8,9,10] have turned to academic papers for constructing multimodal scientific
benchmarks, leveraging the complexity of graduate-level content. Yet, these approaches typically
extract only figures and captions, neglecting the critical scientific reasoning and insights in the paper
context. Consequently, tasks based on these benchmarks tend to be oversimplified, focusing on basic
figure-caption matching , which offers limited insight into a model’s reasoning capabilities. Another
obstacle to the advancement of current benchmarks is the difficulty in formulating questions for
observational analysis images within the Earth science discovery process. As shown in Figure 2 (a),
question generation based solely on captions oversimplifies tasks, as the quality and difficulty of
questions are constrained by the limitations of the generation models, and they lack the contextual
support from the papers, making verification challenging. Existing benchmarks focus solely on figure-
caption matching while neglecting the high knowledge density in scientific discovery processes.
This raises a new challenge: How to effectively align high-value Earth science figures with
knowledge-intensive long-context information?
To address these challenges, we propose a novel approach to benchmark construction that overcomes
the limitations of existing methods through two key innovations. First, we introduce the concept of
‘refined caption’, which combine basic figure caption with additional domain-specific text content
extracted from the paper. By leveraging refined captions, we enable MLLMs to generate VQA tasks
that are grounded in enriched contextual information, ensuring that the questions are both original and
scientifically meaningful. As illustrated in Figure 2 (b), questions generated using refined captions
2Table 1: Comparison with previous multimodal scientific benchmarks. Task types include OE (Open-
ended QA), MCQ (Multiple-choice QA), and CG (Caption Generation).
Benchmark Dataset Science Topics Tasks Difficulty Original Questions Validated
Multimodal Scientific Benchmarks
ScienceQA [6] General Science MCQ Primary ✗ ✓
SceMQA [11] General Science MCQ,OE Pre-College ✗ ✓
Mmmu [7] General Science MCQ,OE College-level ✗ ✓
OlympiadBench [12] Math, Physics OE Competition ✗ ✓
EMMA [13] General Science MCQ Mixed ✓ ✓
Paper-Based Multimodal Scientific Benchmarks
SciFIBench [9] General Science MCQ Graduate-Level ✗ ✓
ArxivCap/QA [10] General Science CG,MCQ Graduate-Level ✓ ✗
MMSci [8] General Science CG,MCQ Graduate-Level ✗ ✗
MSEarth(Ours) Geoscience CG,MCQ,OE Graduate-Level ✓ ✓
not only exhibit higher quality but are also supported and validated by the content of the academic
papers, ensuring the professionalism and accuracy of the generated questions. Second, we implement
a rigorous quality control process to validate the generated benchmark. This process combines a
multi-agent automated evaluation and expert evaluation, ensuring that the generated questions are
relevant, coherent, and aligned with the complexity of professional-level geoscientific reasoning. By
addressing the reliability and accuracy issues faced by current LLMs in scientific question generation,
our approach ensures that the benchmark is both robust and valuable for evaluating MLLMs in
advanced scientific domains.
Building on this methodology, we introduce a comprehensive multimodal scientific benchmark,
MSEarth, curated from more than 14,000 high-quality open-access scientific publications. MSEarth
encompasses five spheres of Earth science, eight subjects and 66 sub-subjects, featuring more than
7,000 figures that represent a wide range of visual scientific data. Each figure is accompanied by
carefully crafted captions and tasks designed to rigorously assess MLLMs in professional-level
geoscientific applications. The benchmark includes tasks such as scientific figure captioning, multiple
choice questions, and open-ended reasoning challenges, offering a holistic and interdisciplinary eval-
uation framework. Our work not only fills the gap in benchmark with graduate-level content but also
introduces a scalable framework for constructing high-quality benchmarks. These contributions make
MSEarth a robust and reliable tool for evaluating the potential of MLLMs in advanced geoscientific
applications in the real world.
In summary, our contributions are as follows:
•Creation of a graduate-level Earth science benchmark: we presented MSEarth, a com-
prehensive benchmark for graduate-level Earth science, filling a significant void in MLLM
evaluation.
•Development of a scalable adaptive framework: our innovative ’refined caption’ concept
and collaborative framework offer a robust, scalable solution for developing high-fidelity,
domain-specific benchmarks extendable to other sciences.
•Comprehensive evaluation of state-of-the-art MLLMs: our extensive MLLM evaluations
on MSEarth provide crucial insights into current limitations and future directions.
2 Related Work
Multimodal Scientific Benchmarks. Numerous multimodal benchmarks have been developed to
evaluate scientific understanding across various domains. These benchmarks often integrate text,
images, and other modalities to assess models’ reasoning and cross-modal capabilities. However, their
creation typically requires significant manual effort in data collection and validation. ScienceQA [ 6]
is an early multimodal benchmark that features multiple-choice questions (MCQs) collected from
online resources and manually filtered for quality. It covers general science topics such as physics,
chemistry, and biology, with a focus on elementary and high school-level reasoning. SceMQA [ 11]
and Mmmu [ 11] extended this by incorporating both MCQs and open-ended questions (OE) from
textbooks and online resources, targeting pre-college and college-level difficulty. OlympiadBench [ 12]
3introduced competition-level problems in mathematics and physics, offering open-ended tasks sourced
from Olympiad exams. These problems are highly challenging but limited to specific domains. More
recently, EMMA [ 13] combined manually designed questions with existing benchmarks, covering
a broader range of topics with mixed difficulty levels. While these benchmarks have advanced
multimodal evaluation, many rely on questions derived from existing resources, limiting their ability
to assess models’ performance in generating or reasoning with original, contextually grounded
scientific content.
Paper-Based Multimodal Scientific Benchmarks. Benchmarks based on academic papers aim to
leverage the rich, domain-specific content found in scientific literature. FigureSeer [ 14] first extracts
figures from academic papers, focusing on chart figures to evaluate the understanding of chart figures.
SciFiBench [ 9] extended this by introducing figure-to-caption and caption-to-figure matching tasks,
while MMSci [ 8] further advanced this approach using figures from Nature papers. However, these
benchmarks lack original questions, limiting their ability to assess advanced reasoning and contextual
understanding. ArxivQA/Cap [ 10] expanded the scope by generating new questions for figures from
32 subjects on arXiv. However, these questions were generated solely using the inherent capabilities
of GPT-4V [ 15] and did not have contextual support from the relevant text in the papers, raising
concerns about their scientific validity. In contrast, our proposed benchmark, MSEarth, addresses
these limitations by introducing original, evidence-supported questions grounded in refined captions.
This approach enables a more rigorous evaluation of MLLMs in professional-level geoscientific
applications.
3 MSEarth - Multimodal Scientific Benchmark for Earth Science
This section offers a comprehensive overview of the construction process for MSEarth. As illustrated
in Figure 3, we describe the framework used to develop MSEarth from open-access scientific
publications. The section is organized into three main parts: first, we detail the data collection and
preprocessing steps. Next, we elaborate on the construction procedures for the two benchmarks
within MSEarth, specifically MSEarthCap and MSEarthQA.
3.1 Data Preparation
The first part of the benchmark construction focuses on data collection and preprocessing. The data
collection begins with more than 100K Earth science papers obtained in PDF format. These are
uniformly converted into structured JSON text using the MinerU [ 16] parser. To classify the papers,
semantic similarity is calculated between the abstracts and keywords from the five Earth spheres:
hydrosphere, biosphere, lithosphere, atmosphere, and cryosphere. Based on this, the papers are
assigned to respective disciplinary categories. Details are provided in Appendix C.3.
Then we initially obtained approximately 100,000 papers. We then further refined this dataset by
selecting papers based on the criterion of containing high-quality, Earth science-related images,
resulting in a subset of around 40,000 papers. Specifically, Qwen-2.5-VL-72B [ 17] is utilized to filter
and select images, with the filtering prompts detailed in the Appendix C.4.
3.2 MSEarthCap
Figure-Caption Extraction : Figures and their corresponding captions are extracted from the JSON
files processed by MinerU. As shown in Appendix C.2, MinerU has already extracted the figures
along with their original captions, which can be directly utilized for subsequent processing. To ensure
accurate alignment between figures and their references within the text, we employ a regex-based
method to identify the labels of each figure. This approach enables precise matching between the
figures and the relevant sections of the articles.
Relevant Context Extraction : To further enrich the captions with contextual information, we use the
figure labels obtained in the previous step to perform approximate matching against the main body of
the paper. Since MinerU processes papers with segmented paragraphs, we apply regular expression
matching to each paragraph to extract contextual text that references the target figure. This ensures
the inclusion of descriptions and reasoning associated with each figure within the paper. To guarantee
that the extracted context provides sufficient detail about the target figure, only paragraphs exceeding
4Extract relevant text parts to 
figures from the articles.
Refined Captions
Generated VQA PairsImage FilteringI. Generate from  figure, caption & 
text parts
II. Generate from figure & refined 
caption
Human Evaluators
Multi -agent EvaluatorsInformation Extraction
 VQA Generation
Content to json  Figure to png
a. Extract paper’s figure -caption pairs.
b. Extract paper’s background information.Atmosphere
Hydrosphere
Cryosphere
Biosphere
lithosphere
Captioning Benchmark
MCQ Benchmark
Open -Ended BenchmarkMSEarth Benchmark
Quality ControllingDownload Geoscience articles
Convert PDFs to machine readable formats.Raw captions
High quality figures
Relevant context
Subject ClassificationPrompt Designer
Generative modelVQA Benchmark
Input promptsFigure 3: Data curation process for MSEarth. The two parts on the left represent data preprocessing,
while the two parts on the right encompass the automated generation of VQA and expert-AI collabo-
rative filtering.
Generated Questions
Refined CaptionsFigures
GPT-4o agent
2 Qwen -VL agent
2 Intern -VL agent
2 Llama Vision agentNormal answer
Ground Truth
Enhanced Answer
 Human evaluatorsSubpar QA
Specialized  QAEasy QA
Hard QASamplingV oting Results Voting Mechanism
Multiple -choice VQA Benchmark
Open -Ended VQA Benchmark
annotatingAnnotated VQA Benchmark
Figure 4: Overall approach of our multi-agent, voting-based approach to automate the validation of
generated questions.
two sentences were included in the final dataset. From this filtered subset, we selected around 14K
papers that met the criteria for subsequent processing. For more details, refer to Appendix C.5.
Refined Caption Generation : To create professional-level figure descriptions, we employ GPT-4o
for refined caption generation. The model takes as input the extracted figure, its original caption, and
the contextual text from the relevant sections of the paper. Figure refinement is performed only for
data that includes valid relevant contexts. The specific prompts used for this process are detailed
in the Appendix C.6. After statistical analysis, we observe that the average word length of the raw
captions is 37.71, while the average length of the refined captions increases to 137.47, reflecting the
incorporation of richer, domain-specific content.
3.3 MSEarthQA
To generate high-quality multiple-choice questions (MCQs) and open-ended questions, we use a
question generation pipeline that takes the figure, its original caption, and the refined caption as input.
The generation prompts, detailed in the Appendix C.6, are crafted to encourage the model to highlight
differences between the original and refined captions, ensuring that the generated questions are
grounded in evidence from the paper. The questions are constructed using GPT-4o to maintain high
detail and relevance. However, due to inherent challenges such as self-inconsistency and uncertainty
in the generation process, the generated questions undergo an automated and expert validation process
to ensure quality.
3.3.1 Automated Validation
Inspired by the LLM V oting [ 18,19,20] method, we developed a multi-agent, voting-based approach
to automate the validation of generated questions. Specifically, we employ a Majority Voting strategy,
where multiple agents independently generate responses, and the final decision is based on the
majority consensus of these agents. In our setup, we utilize the following MLLMs for decision-
making: Qwen2.5-VL-72B, Qwen2.5-VL-7B, InternVL2.5-7B, InternVL2.5-78B, and GPT-4o. A
key aspect of our evaluation process is the use of the refined caption , which incorporates scientists’
reasoning and insights about the figure extracted from the paper. This refined caption provides
additional context and domain-specific information that goes beyond the original caption. By
comparing model performance with and without the refined caption, we can assess the quality of the
5questions and determine whether they effectively test the model’s ability to grasp scientific reasoning
and insights. The detailed decision-making process is outlined below:
Phase A: The question and original caption are provided to a suite of models {M1, M2, ..., M n}.
The correctness of the model responses is used to evaluate the types and quality of the questions. A
threshold of 60% is defined for supermajority voting. Specifically, if more than 40% of the models
produce incorrect responses, the question is flagged for further analysis. Questions that fail this phase
are categorized as either potentially difficult or of poor quality. The distinction between these two
categories is determined in subsequent phases.
Phase B: In this phase, the question and refined caption are provided to the same suite of models.
If more than 60% of the models answer the question correctly with the refined caption, it indicates
that the question requires relatively specialized scientific knowledge to answer. Such questions are
categorized as specialized QA , as their answers rely on the model’s ability to understand and apply
specific domain knowledge rather than simply perceiving the image or relying on commonsense
reasoning. Questions that fail this phase proceed to the next stage for further evaluation.
Phase C: In this phase, only models with 70B+ parameters are used for voting, including GPT-4o (the
same model used for question generation), InternVL2.5-78B, and Qwen2.5-VL-72B. The question
and its refined caption are provided to these large-scale models. If more than 60% of the large
models answer the question correctly, it suggests that the difficulty of the question likely lies in the
model’s ability to perceive and interpret the image content. Such questions are categorized as hard
QA. Subsequent human validation will involve sampling and additional annotation across QA filtered
in all phases to ensure the overall quality and accuracy of the benchmark.
This pipeline identifies high-quality questions by filtering out overly simplistic or poorly constructed
ones. In Phase A , approximately 70% of the questions were categorized as easy, as most models
could answer them correctly without refined captions. After Phase B , around 20% were classified
asspecialized QA , where refined captions enabled correct answers, indicating the need for domain-
specific knowledge. In Phase C , 5% were labeled as hard QA , requiring high-performing models
to interpret image content accurately, suggesting that these questions test the model’s ability to
perceive and interpret image content. The remaining 5% were deemed flawed and discarded. Detailed
processes and examples are provided in Appendix D.
3.3.2 Expert Validation
Ensuring that synthetic data closely mirrors real-world distributions is critical for evaluation tasks. To
achieve this, domain experts are engaged to review and annotate the curated QA pairs for accuracy
and relevance. During this process, low-quality or invalid questions are identified and filtered out to
ensure the overall quality of the dataset. The annotation process is conducted from two perspectives:
image types and question types. For image types, we categorize the data into three groups: single-
image question answering, single-image-focused question answering within multi-image figures, and
multi-image relational question answering. For question types, we define two categories: scientific
discovery and perception questions. Scientific discovery questions are constructed based on inferences
or scientific discoveries presented in research papers, making them more specialized and challenging.
In contrast, image perception questions focus on interpreting images and require less background
knowledge of scientific concepts. This expert-AI collaborative process, combined with rigorous
quality control, results in a high-quality dataset comprising 1,500 open-ended questions and 3,000
MCQs, forming the MSEarthQA benchmark. The annotated results are summarized in Table 2, with
further details and analysis provided in Appendix E.
4 Experiments
4.1 Baselines
We evaluate different families of MLLMs on our benchmark. We evaluate the following closed-source
models: GPT-4 series [ 21], Gemini-2.5 series [ 22] and Claude-3 series [ 23]. We also evaluate the
following open-source models: LLaV A-OneVision [ 24], Qwen-2.5-VL [ 25], InternVL2.5/3 [ 26,
27] and Llama-3.2-Vision-Instruct [ 28]. We use chat/instruction-tuned variants of each model
and compare the performance of multiple model sizes where available. Details can be found in
Appendix G.
6Table 2: Main statistics in MSEarth.
Statistic Number
Total questions 7,195
MCQ 2,784
Questions with single images 1,255 (45.1%)
Questions with multiple images 1,529 (54.9%)
* Single-image focus ≈1,164 (41.8%)
* Multi-image relational ≈365 (13.1%)
Discovery Question 2117 (76.0%)
Perception Question 667 (24.0%)
Open-Ended 1,411
Questions with single images 679 (45.1%)
Questions with multiple images 832 (45.1%)
* Single-image focus ≈619 (41.8%)
* Multi-image relational ≈113 (13.1%)
Captioning 3,000
Average caption length 37.71
Average refined caption length 137.47
Hydrology18.8%Atmospheric Sciences
15.7%
Geography
15.7%
Geology
14.6%
Solid Earth Geophysics11.7%Ecology and Biosciences
10.6%Oceanography
8.9%Polar Science
4.0%
River & Estuarine Hydrology 7.5%
Groundwater Hydrology7.3%
Limnology4.1%
Meteorology8.5%
Climatology
5.7%
Atmospheric Remote Sensing
1.5%
Physical Geography
11.8%
Urban Geography
2.6%
Geomorphology
1.3%
Sedimentology9.9%
Quaternary Geology2.8%
Structural Geology2.0%
Seismology7.8%
T ectonophysics3.2%
Exploration Geophysics
0.7%
Aquatic & Limnological Ecology
5.2%
Landscape Ecology
2.8%
Ecosystem Ecology
2.6%
Ocean Physics
6.5%
Ocean Geology
1.5%
Environmental Oceanography
1.0%
Glaciology
3.3%
Polar Climate Science
0.4%
Permafrost Science
0.3% Figure 5: Subjects distribution in MSEarth.
4.2 Evaluation Metrics
Both captioning and open-ended QA tasks require generating freeform textual outputs grounded
in complex scientific data and reasoning. To evaluate these tasks, we use lexical overlap-based
metrics such as BLEU [ 29], ROUGE [ 30], and METEOR [ 31] for surface-level similarity, while
BERTScore [ 32] assesses deeper semantic alignment. Given the importance of factual correctness, we
employ a factual entailment classifier to measure the consistency of generated outputs with reference
answers or captions. Additionally, following G-Eval [ 33], we utilize the Qwen2.5-VL-72B model
with a specialized prompt to compute a factual scientific score. For the captioning task, we define
a Cap-Eval score ranging from 1 to 5, where higher scores indicate better caption quality. For the
open-ended QA task, we introduce OE-Eval, which evaluates the reasonableness of generated answers
using a binary 0/1 scoring system.
Evaluating MCQs is relatively straightforward, as these tasks require selecting the correct answer
from a predefined set of options. In our experiments, models were guided by carefully structured
prompts to ensure adherence to a specific output format. Regular expression rules were employed to
extract the selected choice, ensuring strict alignment with the predefined answer format. We observed
that some models occasionally failed to consistently follow the formatting instructions. To address
this issue and preserve the integrity of the evaluation process, we implemented a similarity-based
method to identify the closest matching option when the selected choice could not be extracted using
regular expressions. Detailed evaluation metrics are provided in Appendix H.
4.3 Main Results
Current models struggle with scientific question-answering tasks, particularly on questions
requiring specialized knowledge and reasoning across multiple images. The performance of
MCQs is summarized in Table 3. The results reveal that most models do not perform exceptionally
well on scientific question-answering tasks, with proprietary models generally achieving better
results. Further analysis of the models’ failure rates on discovery and perception-based questions
is provided in the Appendix (Figure 9). This analysis shows that models are more prone to errors
on questions requiring specialized knowledge, underscoring significant room for improvement in
scientific discovery question-answering. In contrast, for relatively simpler perception-based questions,
which require less domain-specific knowledge, the models tend to perform better. Similarly, when
analyzing performance across different image types, we observe that most models achieve their best
results on tasks involving single-image inputs. However, for tasks requiring multi-image inputs,
particularly those that demand reasoning across multiple images to derive an answer, the models
perform the worst. Additional experimental results can be found in Appendix I.
Proprietary models consistently outperform open-source models in both Scientific Figure
Captioning and Open-Ended VQA tasks, with LLM-based metrics providing a more reasonable
7Table 3: Accuracies (%) of different models on multiple-choice questions.The best results are
highlighted in bold, with the second-best underlined.
ModelImage-Type Task Type Overall
SINGLE MULTI CROSS DISCOVERY PERCEPT ACC
Open-source Models
LLaV A-onvision-72B 49.40 45.52 41.10 41.92 61.86 46.69
Qwen2.5-VL-7B 39.12 35.65 39.18 37.27 38.98 37.68
Qwen2.5-VL-32B 42.07 39.78 40.00 37.03 52.92 40.84
Qwen2.5-VL-72B-Chat 47.65 43.30 43.84 41.43 57.72 45.33
InternVL2-8B 35.94 34.11 32.33 34.25 36.13 34.70
InternVL2.5-78B 48.13 45.88 45.21 43.27 58.02 46.80
InternVL3-78B 57.53 51.37 45.48 47.00 73.61 53.38
Llama3.2-90B-Vision 44.30 40.64 36.16 38.64 51.42 41.70
DeepSeek-VL2 52.43 49.23 44.66 46.06 62.82 50.07
Proprietary Models
Gemini-2.5-Flash 58.33 54.55 53.42 49.98 75.56 56.11
Gemini-2.5-Flash-Thinking 60.64 54.64 53.70 51.35 75.86 57.22
Gemini-2.5-Pro-Thinking 64.78 59.36 55.34 56.31 77.06 61.28
Claude-3.5-Haiku 49.48 47.16 42.47 42.18 64.77 47.59
Claude-3.7-Sonnet 59.52 56.53 57.53 51.68 78.11 58.01
GPT-4o-mini 52.51 48.63 43.01 43.65 68.67 49.64
GPT-4o 63.03 55.76 47.67 50.45 81.86 57.97
Table 4: Performance on scientific figure captioning. The best results are highlighted in bold, with
the second-best underlined.
ModelOverlap Similarity MLLM
ROUGE 1 R OUGE 2 R OUGE L M ETEOR BLEU B ERTSCORE CAP-EVAL
Open-source Models
LLaV A-onevision-72B 30.81 5.99 17.82 18.35 2.15 83.47 2.07
Qwen2.5-VL-7B 27.01 5.21 15.72 16.57 1.55 83.82 2.22
Qwen2.5-VL-32B 29.90 5.88 15.53 25.81 2.14 82.86 2.66
Qwen2.5-VL-72B 30.68 5.85 16.53 21.35 2.36 83.87 2.56
InternVL2.5-8B 29.07 5.16 16.71 19.28 1.58 83.25 1.91
InternVL2.5-78B 30.66 5.95 17.24 20.66 2.27 83.56 2.30
InternVL3-78B 30.73 5.87 16.95 20.95 2.32 83.72 2.43
Llama3.2-90B-Vision 19.93 4.32 12.98 21.21 1.49 78.89 1.82
DeepSeek-VL2 29.06 5.42 16.71 18.22 1.69 83.64 2.22
Proprietary Models
Gemini2.5-Flash 32.89 7.30 17.79 23.01 3.10 83.96 2.98
Gemini2.5-Flash-Thinking 32.67 7.00 17.42 23.47 2.97 83.85 3.04
Gemini2.5-Pro-Thinking 33.47 7.69 17.15 27.45 3.33 83.62 3.35
Claude-3.5-haiku 26.63 4.47 14.97 17.58 1.38 83.46 2.36
Claude-3.7-Sonnet 29.89 5.52 15.94 21.15 2.15 83.62 2.71
GPT-4o-mini 28.39 5.00 15.96 18.45 1.59 83.90 2.40
GPT-4o 29.89 5.57 16.33 20.19 2.15 83.93 2.72
evaluation. The captioning results are presented in Table 4, where overlap-based, similarity-based,
and LLM-based metrics exhibit similar trends, with no significant differences observed among the
overlap-based and similarity-based metrics. The Gemini-2.5-Pro model achieves the best performance
across most metrics. The LLM-based metric, designed to evaluate the professionalism and accuracy
of generated captions, demonstrates greater variance compared to similarity-based metrics, making it
more suitable for assessing the Scientific Figure Captioning task. Open-source models still show a
noticeable gap compared to proprietary models, consistent with the findings from the MCQ results,
suggesting a close interconnection between a model’s understanding and reasoning capabilities.
Similarly, the results for open-ended question answering, presented in Table 5, show that overlap-
based and similarity-based metrics tend to yield higher scores due to the shorter nature of both
ground truth answers and model-generated responses. However, for open-ended questions, the focus
should be on the rationality and correctness of the answers, making the LLM-based metric a more
reasonable evaluation method. This metric also reveals trends consistent with the previous tasks,
further highlighting the performance gap between open-source and proprietary models.
8Table 5: Performance on scientific open-ended question answering. The best results are highlighted
in bold, with the second-best underlined.
ModelOverlap Similarity LLM
ROUGE 1 R OUGE 2 R OUGE L M ETEOR BLEU B ERTSCORE OE-E VAL(%)
Open-source Models
LLaV A-onevision-72B 38.07 20.62 37.91 27.88 1.94 89.72 41.56
Qwen2.5-VL-7B-Chat 35.52 20.71 35.24 29.00 2.40 88.62 40.68
Qwen2.5-VL-32B-Chat 36.82 21.33 36.51 29.33 1.83 89.20 41.96
Qwen2.5-VL-72B-Chat 38.57 23.49 38.34 30.65 2.20 89.22 44.82
InternVL2.5-8B 36.12 21.05 35.95 28.25 2.04 89.14 39.05
InternVL2.5-78B 40.59 24.14 40.34 31.15 2.23 90.05 45.64
InternVL3-78B 40.59 23.87 40.42 31.77 2.37 89.98 47.00
Llama3.2-90B-Vision 37.64 22.26 37.53 29.08 1.99 89.16 42.72
DeepSeek-VL2 36.49 20.44 36.37 27.24 1.83 89.38 40.68
Proprietary Models
Gemini-2.5-Flash 40.26 22.98 40.02 32.34 2.06 89.39 52.00
Gemini-2.5-Flash-Thinking 39.59 22.30 39.47 30.77 1.87 89.58 46.49
Gemini-2.5-Pro-Thinking 38.67 22.30 38.38 31.73 2.50 88.93 47.70
Claude-3.7-Sonnet 40.61 23.58 40.21 30.75 1.73 89.37 48.33
GPT-4o-mini 36.49 21.37 36.27 28.89 1.63 89.06 41.81
GPT-4o 41.30 24.74 41.03 32.70 2.04 89.78 48.55
Gemini2.5-Pro-thinkingGemini2.5-Flash InternVL3-78B Qwen2.5-VL-72B25303540455055Performance50.67
49.0050.3351.67
45.0049.00
46.3347.00 46.67
39.6748.0047.33
33.67
29.3333.67 34.00Method
Base
CoT
Voting (N=4)
Voting (N=8)
Figure 6: Performance of different strategies on MSEarth-mini.
4.3.1 Results with Compute Scaling
From the main experiments, it is evident that the performance of various models declines significantly
on questions requiring specialized knowledge. To explore whether existing methods can enhance
model performance on such questions, we sampled 300 specialized questions from the MCQ dataset
to create the MSEarth-mini set. We then evaluated the effectiveness of Chain-of-Thought (CoT)
reasoning and majority voting mechanism, which selects the most frequent response among (N)
candidate responses; in the case of a tie, one of the most frequent answers is randomly chosen. The
results are presented in figure 6. Notably, for the Gemini-Pro-thinking model, which inherently
incorporates a thinking mechanism, introducing CoT reasoning led to a decline in performance.
Similarly, for some open-source models, such as Qwen and InternVL, the addition of CoT reasoning
also resulted in performance degradation. However, the majority voting mechanism proved effective
for most models.
5 Conclusion
In this work, we introduce MSEarth, a graduate-level multimodal benchmark for MLLMs in geo-
scientific applications. By leveraging refined captions that integrate figure captions with domain-
specific content, MSEarth enables rigorous assessment of MLLMs on tasks such as figure captioning,
multiple-choice questions, and open-ended reasoning. Our evaluation reveals significant gaps in
current MLLMs’ ability to handle complex, graduate-level geoscientific reasoning, highlighting
opportunities for improvement. We believe MSEarth will serve as a valuable resource for advancing
MLLMs in scientific reasoning and plan to expand its scope to other scientific domains in future
work. The limitation of this work lies in the vastness and diversity of geosciences as a discipline.
While we have made efforts to cover a wide range of topics, it is inevitable that certain niche or highly
specialized subfields may not be adequately represented in MSEarth.
9Acknowledgments
This work was primarily conducted during the author’s internship at the Shanghai Artificial Intelli-
gence Laboratory.
References
[1]Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances
in neural information processing systems , 36:34892–34916, 2023.
[2]Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. A survey
of multimodel large language models. In Proceedings of the 3rd International Conference on
Computer, Artificial Intelligence and Control Engineering , pages 405–409, 2024.
[3]Junxian Li, Di Zhang, Xunzhi Wang, Zeying Hao, Jingdi Lei, Qian Tan, Cai Zhou, Wei Liu,
Yaotian Yang, Xinrui Xiong, et al. Chemvlm: Exploring the power of multimodal large language
models in chemistry area. In Proceedings of the AAAI Conference on Artificial Intelligence ,
volume 39, pages 415–423, 2025.
[4]Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan,
and Fahad Shahbaz Khan. Geochat: Grounded large vision-language model for remote sensing.
InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
pages 27831–27840, 2024.
[5]Chengqian Ma, Zhanxiang Hua, Alexandra Anderson-Frey, Vikram Iyer, Xin Liu, and Lianhui
Qin. Weatherqa: Can multimodal language models reason about severe weather? arXiv preprint
arXiv:2406.11217 , 2024.
[6]Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought
chains for science question answering. Advances in Neural Information Processing Systems ,
35:2507–2521, 2022.
[7]Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal
understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition , pages 9556–9567, 2024.
[8]Zekun Li, Xianjun Yang, Kyuri Choi, Wanrong Zhu, Ryan Hsieh, HyeonJung Kim, Jin Hyuk
Lim, Sungyoung Ji, Byungju Lee, Xifeng Yan, et al. Mmsci: A multimodal multi-discipline
dataset for phd-level scientific comprehension. In AI for Accelerated Materials Design-Vienna
2024 , 2024.
[9]Jonathan Roberts, Kai Han, Neil Houlsby, and Samuel Albanie. Scifibench: Benchmarking
large multimodal models for scientific figure interpretation. arXiv preprint arXiv:2405.08807 ,
2024.
[10] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu.
Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language
models. arXiv preprint arXiv:2403.00231 , 2024.
[11] Zhenwen Liang, Kehan Guo, Gang Liu, Taicheng Guo, Yujun Zhou, Tianyu Yang, Jiajun Jiao,
Renjie Pi, Jipeng Zhang, and Xiangliang Zhang. Scemqa: A scientific college entrance level
multimodal question answering benchmark. arXiv preprint arXiv:2402.05138 , 2024.
[12] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu,
Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for
promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint
arXiv:2402.14008 , 2024.
[13] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and
Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning
benchmark. arXiv preprint arXiv:2501.05444 , 2025.
10[14] Noah Siegel, Zachary Horvitz, Roie Levin, Santosh Divvala, and Ali Farhadi. Figureseer:
Parsing result-figures in research papers. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14 ,
pages 664–680. Springer, 2016.
[15] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4
technical report. arXiv preprint arXiv:2303.08774 , 2023.
[16] Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen
Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document
content extraction. arXiv preprint arXiv:2409.18839 , 2024.
[17] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang,
Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 ,
2025.
[18] Joshua C Yang, Damian Dalisan, Marcin Korecki, Carina I Hausladen, and Dirk Helbing. Llm
voting: Human choices and ai collective decision-making. In Proceedings of the AAAI/ACM
Conference on AI, Ethics, and Society , volume 7, pages 1696–1708, 2024.
[19] Xian Yeow Lee, Shunichi Akatsuka, Lasitha Vidyaratne, Aman Kumar, Ahmed Farahat, and
Chetan Gupta. Reliable decision-making for multi-agent llm systems. 2025.
[20] Lars Benedikt Kaesberg, Jonas Becker, Jan Philip Wahle, Terry Ruas, and Bela Gipp. V oting or
consensus? decision-making in multi-agent debate. arXiv preprint arXiv:2502.19130 , 2025.
[21] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,
AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv
preprint arXiv:2410.21276 , 2024.
[22] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
[23] AI Anthropic. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card , 1:1,
2024.
[24] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan
Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint
arXiv:2408.03326 , 2024.
[25] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint
arXiv:2412.15115 , 2024.
[26] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shen-
glong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source
multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271 ,
2024.
[27] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Yuchen Duan,
Hao Tian, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time
recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479 , 2025.
[28] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama
3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.
[29] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association
for Computational Linguistics , pages 311–318, 2002.
11[30] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization
branches out , pages 74–81, 2004.
[31] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with
improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic
and extrinsic evaluation measures for machine translation and/or summarization , pages 65–72,
2005.
[32] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore:
Evaluating text generation with bert. arXiv preprint arXiv:1904.09675 , 2019.
[33] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:
Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634 ,
2023.
[34] Conghui He, Wei Li, Zhenjiang Jin, Chao Xu, Bin Wang, and Dahua Lin. Opendatalab:
Empowering general artificial intelligence with open datasets. arXiv preprint arXiv:2407.13773 ,
2024.
[35] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep
self-attention distillation for task-agnostic compression of pre-trained transformers. Advances
in neural information processing systems , 33:5776–5788, 2020.
[36] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems
Principles , pages 611–626, 2023.
12Figure Caption GenerationMultiple -choice Question
Caption : Radius–time  cross  sections  of the azimuthal -mean  tangential  wind  velocity  and radar  reﬂectivity  at 
the height  of 2km in (a) CTL  and (b) WARM . The red lines  denote  the period  of the ERC .
Question : How  does  the timing  of secondary  eyewall  formation  differ  between  CTL  and WARM  simulations?
A. It occurs  earlier  in WARM .    B. It occurs  simultaneously  in both  simulations .
B. C. It occurs  earlier  in CTL .     D. Neither  simulation  shows  secondary  eyewall  formation .
Answer : C. It occurs  earlier  in CTL .
Explanation
Step  1: Observe  the red lines  marking  the period  of secondary  eyewall  formation  in both  panels .
Step  2: In the CTL  simulation  (left panel),  the formation  starts  at about  105 hours .
Step  3: In the WARM  simulation  (right  panel),  it starts  later,  around  162 hours .
Step  4: Determine  that the secondary  eyewall  formation  occurs  earlier  in CTL  than  in WARM .
Open -Ende d Question
Caption : Slip distributions  of the Nepal  earthquake . (a) Temporal  and spatial  distributions  of slip on 
earthquake  fault  plane . (b) A three -dimensional  view  of the finite  fault  model .
Question : Which  area  saw the most  significant  slip during  the rupture ?
Answer : East  of epicenter
Explanation
Step  1: Using  the spatial  visualization  in Figure  (a), locate  the regions  of highest  slip intensity  marked  in red.
Step  2: This  area  is to the east of the epicenter,  as indicated  by the spatial  markings .
Query : Create an in -depth description of the visual elements in the image.
Answer : Seismic  line Abbotsbury -Compton  Valence . This  seismic  line illustrates  the geological  structure  and 
stratigraphy  in the western  part  of the region,  north  of the Abbotsbury -Ridgeway  Fault  and west  of Waddock  
Cross . The Budleigh  Salterton  Pebble  Beds  (BSPB)  are visible  resting  unconformably  on the Aylesbeare  
Mudstone,  which  itself  lies unconformably  on the Palaeozoic  basement . Both  the base  of the Budleigh  Salterton  
Pebble  Beds  and the base  of the Aylesbeare  Mudstone  are characterized  by relatively  strong  reflectors  due to 
their  sonic  contrasts . The seismic  section  highlights  the northwest -southeast -trending  fault  system  that bounds  
the Mid Dorset  Platform  to the west,  forming  a half-graben  structure  on its southwest  side. This  fault  system,  
which  aligns  with  the projection  of the Watchet -Cothelstone  Fault,  is interpreted  as a reactivated  Variscan  
lineation,  with  evidence  suggesting  strike -slip movement  during  the Tertiary .  
Figure 7: Examples of the three types of scientific question-answering tasks presented in our
benchmark.
A Limitations
The limitation of this work lies in the vastness and diversity of geosciences as a discipline. While
we have made efforts to cover a wide range of topics, it is inevitable that certain niche or highly
specialized subfields may not be adequately represented in MSEarth.
B Ethics and Broader Impacts
Data is primarily collected from free online resources, such as arXiv and Nature, with strict adherence
to avoiding sources that prohibit copying or redistribution. This work establishes a benchmark for
evaluating the multimodal Earth scientific exploration capabilities of MLLMs in the field of Earth
sciences. It has broader positive impacts, including promoting the responsible use of AI in scientific
research and enhancing public understanding of Earth sciences. We believe MSEarth will serve as a
valuable resource for advancing multimodal language models (MLLMs) in scientific reasoning, and
we plan to expand its scope to other scientific domains in future work.
C Benchmark Details
C.1 Field Explanation of MSEarth
In Table 6, we provide an explanation of each field for the three tasks in MSEarth.
C.2 Format conversion
Specifically, our data source is a collection of papers gathered by OpenDataLab [ 34] from online
resources. These papers were processed using MinerU , which converted the textual content of the
PDFs into JSON format and saved the images as PNG files. In Figure 8, we present a portion of
the content list from a processed PDF paper, highlighting the original caption (raw caption) of the
13Field Name Input Description
Multiple-choice Question
question_id ✗ The unique identifier for the question.
query ✓ Contains the original caption, question, and options.
response ✗ The correct answer to the question.
images ✓ The file path to the associated image(s).
refined_caption ✗ The enhanced image description based on the paper content.
classification ✗ The classification of the question, including its domain and discipline.
reasoning_chain ✗ The reasoning steps to arrive at the answer.
Open-Ended Question
question_id ✗ The unique identifier for the question.
query ✓ Contains the original caption and the question.
response ✗ The correct answer to the question.
images ✓ The file path to the associated image(s).
refined_caption ✗ The enhanced image description based on the paper content.
classification ✗ The classification of the question, including its domain and discipline.
reasoning_chain ✗ The reasoning steps to arrive at the answer.
Caption Generation
question_id ✗ The unique identifier for the question.
query ✓ The question.
response ✗ The correct answer (refined caption).
images ✓ The file path to the associated image(s).
context ✗ The text from the paper that describes the image.
original_caption ✗ The original caption of the image.
classification ✗ The classification of the question, including its domain and discipline.
Table 6: Field Descriptions for Different Tasks. The table provides details about each field, whether
it is used as input, and its description. Fields are grouped by task type: MCQ, OE, and Caption
Generation.
image and the corresponding discussion section. It is evident that the discussion of figures in the
paper contains substantial scientific reasoning, which is crucial for a comprehensive understanding of
scientific figures.
Structure of Content List Field
"content_list ": [
{
"type": "text",
"text": "LUMINESCENCE STUDIES ON NEOTECTONIC EVENTS IN SOUTH -CENTRAL KUMAUN HIMALAYA -- A FEASIBILITY STUDY",
"text_level ": 1
},
........
{
"type": "text",
"text": " The continued northward movement of the Indian plate has caused accumulation of stresses which get periodically released. res ulting in 
earthquakes and neotectonic activity along faults. Geophysical and structural studies suggest that seismicity in the Himalaya is related to movements alo ng
three major longitudinal thrusts/faults (Fig. 1) viz. the Himalayan Frontal Fault (HFF), the Main Boundary Thrust (MBT) and t he Main Central Thrust 
(MCT) ( Valdiya , 1986, 1988; Nakata. 1989). These thrusts divide the Himalaya into three distinct lithotectonic zones. These zones are further dissected by 
numerous transverse faults ( Valdiya . 1976; Khattri and Tyagi, 1983a). A concept of locked segments has been propounded to assess the seismogenic
potential of various sectors and is related to accumulation of stresses and their eventual release along the faulted zones ( Khattri and Tyagi 1983b). The 
repeat frequency of this process of locking of stresses and release of energy is not yet well established due to lack of dati ng methods. Needless to say, this is 
an aspect cardinal to the estimation of seismogenic hazards involved in planning large scale engineering and societal projects in the Himalaya .“
"text_level ": 0
},
{
"type ": "image",
"img_path ": "s3://llm-pipeline -media/pdf -imgs /f5def6d72e7dbfe47ada87d0bc9084997c67f989011db77a255d1817a33fe86b.png",
"img_caption ": "FIG. 1. Geological map indicating fault zones and locked segments in Himalaya “
},
........
]
Figure 8: Examples of the content list field in a paper.
14C.3 Paper Filtering
To classify scientific papers into relevant Earth system categories, we employ a similarity-based
approach using pre-trained sentence embeddings and cosine similarity. The process begins by
generating embeddings for both the paper’s title and predefined keywords using the pre-trained
all-MiniLM-L6-v2 [ 35] model, which captures the semantic meaning of textual data. First, we
calculate the similarity between the paper’s title and a set of general positive keywords, such as
“Earth,” “Earth system,” “hydrosphere,” “biosphere,” “lithosphere,” “atmosphere,” and “cryosphere.”
The cosine similarity is computed between the embedding of the paper’s title and the embedding of
the general positive keywords. If the similarity score is below a predefined threshold (0.2), the paper
is excluded from further analysis, as it is deemed irrelevant to the sciences of the Earth system.
To further filter out irrelevant papers, we calculate the similarity between the paper’s title and a
set of general negative keywords, such as “cell biology,” “virus,” “pharmaceuticals,” “chemistry,”
“physics,” and “astronomy.” If the similarity score exceeds a predefined threshold (0.1), the paper is
excluded, as it is likely to belong to unrelated disciplines. For papers that pass the initial filtering,
we calculate their similarity with predefined positive classification keywords for each Earth system
category (e.g., hydrosphere, biosphere, lithosphere, atmosphere, cryosphere). Each category contains
a list of domain-specific keywords. For example, the hydrosphere category includes keywords such
as “water cycle,” “ocean,” “rivers,” “lakes,” and “groundwater,” while the biosphere category includes
“ecosystem,” “biodiversity,” “habitat,” and “species.” The cosine similarity is computed between the
paper’s title and each keyword within a category, and the average similarity score for each category is
calculated.
The category with the highest average similarity score is selected as the most relevant classification
for the paper, provided the score exceeds a predefined threshold (0.15). To ensure robustness, we
also calculate the similarity between the paper’s title and negative classification keywords for each
category. For example, the hydrosphere negative keywords include “chemistry,” “universe,” “planets,”
and “astronomy,” while the biosphere negative keywords include “cell biology,” “medicine,” and
“pharmacology.” The final relevance score for each category is computed as the difference between
the positive and negative similarity scores, ensuring that papers with high relevance to unrelated
fields are excluded. The final classification of a paper is determined by passing the general positive
and negative keyword thresholds, identifying the category with the highest positive relevance score
adjusted by subtracting the negative relevance score, and ensuring the adjusted relevance score
exceeds the classification threshold (0.15). This approach allows us to systematically classify papers
into Earth system categories while filtering out irrelevant content, leveraging semantic embeddings
and cosine similarity to ensure that the classification is both accurate and interpretable.
C.4 Image Filtering
Next, we further filtered the images in these 103,108 papers. Our goal was to retain Earth observation
images, as our task focuses on evaluating the model’s ability to understand and reason about scientific
phenomena in Earth sciences. These images include various types of visual data, such as those
representing geophysical processes, atmospheric phenomena, geographic features, weather patterns,
and cartographic representations.
To achieve this, we employed a systematic filtering pipeline based on the Qwen2-VL-7B-Instruct
model. The filtering process was guided by a carefully designed prompt, which instructed the
model to classify each image as either an Earth observation image or not. Specifically, the prompt
defined Earth observation images as those depicting remote sensing imagery, atmospheric data
visualizations, aerial views of geographical features (e.g., rivers, urban landscapes), weather-related
images (e.g., precipitation maps, typhoon tracks), and cartographic representations. Conversely, the
prompt explicitly excluded images containing biological entities (e.g., humans, plants, animals),
artificial objects (e.g., vehicles, device structures), data visualizations (e.g., statistical charts, line
graphs, scatter plots), text-based content, or blank images.
The filtering process was implemented as follows: for each image, the model was provided with both
the image and the prompt, and it generated a binary output (“1” for Earth observation images and
“0” otherwise). To ensure robustness, the model’s output was validated through multiple sampling
attempts with slight variations in generation parameters (e.g., temperature). If the model consistently
classified an image as “1,” it was retained; otherwise, it was discarded. This iterative and robust
15Category Type Keywords
General Positive Earth, Earth system, hydrosphere, biosphere, lithosphere, atmosphere,
cryosphere
General Negative cell biology, virus, pharmaceuticals, chemistry, physics, astronomy, food
science, proteins, microbiology
Hydrosphere Positive water cycle, ocean, rivers, lakes, groundwater, ice caps, aquifers, precipi-
tation, evaporation, humidity
Hydrosphere Negative chemistry, universe, planets, astronomy, astrophysics, space, stars,
galaxy, cosmology
Biosphere Positive ecosystem, biodiversity, habitat, species, biomes, ecological balance,
carbon cycle
Biosphere Negative cell biology, chemistry, medicine, pharmacology, microbiology, bio-
chemistry, toxicology, pathology, clinical
Lithosphere Positive earthquake, tectonic plates, earth’s crust, minerals, rocks, soil, sediments,
mountains, volcanoes, landforms, geological processes
Lithosphere Negative ancient texts, archaeology, culture, history, artifacts, civilization, prehis-
toric, mythology, anthropology
Atmosphere Positive stratosphere, troposphere, weather, climate, greenhouse gases, ozone
layer, air pressure, humidity, winds, carbon dioxide, temperature
Atmosphere Negative universe, galaxy, astronomy, astrophysics, space, stars, planets, cosmol-
ogy, black holes, nebula, solar system
Cryosphere Positive glaciers, ice sheets, sea ice, permafrost, snowpack, icebergs, frozen
ground, climate change, albedo effect, polar regions
Cryosphere Negative frozen food, ice cream, refrigeration, freezing, cold storage, ice cubes,
food preservation, chilling, frost
Table 7: Keywords for positive and negative classifications across different Earth system categories.
The table includes general keywords as well as specific keywords for hydrosphere, biosphere,
lithosphere, atmosphere, and cryosphere.
classification approach allowed us to minimize false positives and negatives in the filtering process.
After this step, we retained 44,963 papers, which contained images classified as Earth observation
images. These filtered images form the basis for subsequent analysis and evaluation of the model’s
capabilities in understanding and reasoning about Earth science phenomena.
The following prompt was used to retain Earth observation images:
Analyze the provided image and classify it as an Earth observation image or not.
Earth observation images include, but are not limited to:
• Remote sensing imagery,
• Atmospheric data visualizations,
• Aerial views of geographical features (e.g., rivers, urban landscapes),
• Weather-related images (e.g., precipitation maps, typhoon tracks),
• Cartographic representations.
Exclude images depicting:
• Biological entities (humans, plants, animals),
• Artificial objects (vehicles, device structures),
• Data visualizations (statistical charts, line graphs, scatter plots),
• Text-based content or blank images.
Output format:
• Return "1" if the image is an Earth observation image.
• Return "0" if the image does not meet the Earth observation criteria.
Provide only the numerical output (1 or 0) without any additional text or explanation.
16C.5 Content Filtering
To construct our benchmark, which requires generating VQA tasks supported by the content of
the papers, we ensured that the selected figures not only had captions but were also discussed in
detail within the text of the papers. Using regular expressions, we extracted the figure numbers
and identified corresponding discussions in the main body of the papers. Figures with discussions
exceeding two sentences were included in the final dataset. Considering the data requirements for
constructing the benchmark, we randomly selected around 14K papers from this set, resulting in a
total of 44,374 figures for further processing.
C.6 Prompt Designer for MSEarth
The following prompt was used to generate a refined caption:
You are an expert assistant in scientific image analysis and caption generation. Your task is to
rewrite or generate a new, detailed caption for the provided figure using the original caption
and only the sentences or information from the Relevant Content that are directly associated
with this figure.
Please strictly follow these guidelines:
• Assume the figure does not reference or depend on other figures in the document.
• Exclude any mention of other figures, their content, or references in the caption.
•If subfigures are present, provide specific descriptions for each subfigure accordingly.
Otherwise, assume it represents a single figure.
•The new caption must be detailed, precise, and include only the relevant details from
the provided content.
Inputs for caption generation:
• Original Caption: {caption}
• Relevant Content: {content}
Now write a detailed, high-quality caption for this figure below:
The following prompt was used to generate diverse VQAs:
You are an advanced AI model specialized in generating high-quality Visual Question An-
swering (VQA) tasks. Your role is to generate a diverse set of VQA questions, answers, and
reasoning chains based on the provided visual input (a figure) and its captions.
Definitions:
1.Figure: A scientific or illustrative figure provided as the primary visual input.
Test-takers will analyze this image to answer the questions.
2.Caption: A concise summary describing key aspects of the Figure.
3.Supplementary: In-depth information (e.g., summarized expert insight, detailed
analysis, or background knowledge) that you can use to assist in designing advanced
and meaningful questions. However, test-takers cannot access this information.
Input Information Provided:
•Caption: {raw caption}
•Supplementary: {refined caption}
Task Instructions:
Your task is to create a variety of advanced VQA tasks designed to test visual and contextual
understanding based on the Figure and Caption. Below are key rules and guidelines you must
follow:
171. Use of Input Sources:
•Ensure that no question can be answered entirely using Caption without observa-
tions from the Figure. Figure content should always serve as a primary source for
reasoning.
•Supplementary Usage: The correct answers are encouraged to be derived from the
Supplementary information. Focus on crafting questions where the Supplementary
plays a crucial role in providing the answer.
2. Question Types:
•Multiple Choice Questions (MCQs): At least 2questions must be of this type, with
4 distinct options (A-D) and one correct answer.
–Ensure that only one option can be logically correct based on the provided
information (Figure, Caption, and/or Supplementary). Avoid creating options
that could lead to ambiguous interpretations or alternate correct answers.
–Incorrect options must be plausible and relevant to the context but should
contain subtle logical flaws or lack supporting evidence when compared to the
correct option.
•Open-Ended Questions: At least 2questions must be open-ended, requiring concise
and precise answers (no more than 4 words).
3. Reasoning Chains:
•For every question, you must include a reasoning chain. The chain explains the
logical process by which the correct answer can be determined.
• The reasoning chain must:
–Be clear, step-by-step, and never explicitly mention Caption or Supplementary
inreasoning_chain (e.g., "According to the Supplementary" or "The Caption
states").
–Use different levels of reasoning complexity.
4. Output Structure:
The output must be written in JSON format using the structure below:
[
{
"question_type": MCQ or OE
"question": "Your question here",
"options": [A,B,C,D],
"answer": "Correct option or short answer",
"reasoning_chain": ["Step 1: ...", ...]
},
// Additional questions in the same format...
]
5. Task Guidelines:
1.Questions that are grounded in the Supplementary context are highly encouraged.
These questions should require the test-taker to refer to in-depth knowledge and
insights not immediately visible in the Figure or Caption.
2.Avoid referencing the Supplementary in any question and reasoning_chain (e.g.,
"According to the Supplementary" or "The Supplementary states").
Provide your response below:
D Multi-Agent Voting
D.1 Prompt
The following prompt was used to generate a normal answer for MCQ:
18You are tasked with answering a multiple-choice question about the given input image.
Instructions:
1. Carefully analyze the input image and the provided query.
2.Based on the image, select the correct option (e.g., ’A’, ’B’, ’C’) or directly state the
correct option content.
3. Provide reasoning explaining how to derive the correct answer.
Input:
•Query: {query}
Output Format:
The output must be written in JSON format using the structure below:
{
"answer": "Correct option or short answer",
"Explanation": "Explaining how to derive the correct answer."
}
The following prompt was used to generate a enhanced answer for MCQ:
You are tasked with answering a multiple-choice question about the given input image.
Input:
•Question: {question}
•Refined Caption: {caption}
Instructions:
1. Carefully analyze the input image and its caption.
2.Based on the image and caption, select the correct option (e.g., ’A’, ’B’, ’C’) or
directly state the correct option content.
Output Format:
The output must be written in JSON format using the structure below:
{
"answer": "Correct option or short answer",
"Explanation": "Explaining how to derive the correct answer."
}
D.2 Example of Different Levels of Questions
Figure 10 illustrates an example of a simple problem in multi-agent voting, while Figure 11 presents
an example of a domain-specific problem, and Figure 12 demonstrates an example of a challenging
problem. The most notable distinction lies in the varying levels of perceptual ability required by
the model: simple and challenging problems primarily differ in the model’s ability to perceive and
interpret images, whereas domain-specific problems emphasize the model’s knowledge in specialized
fields. Additionally, the answers to domain-specific questions are often supported by evidence found
in the "refined caption" field provided in the paper.
To construct the benchmark datasets, we sampled data from the multi-agent automated filtering
process as follows: 900 questions from Phase A, 1800 questions from Phase B, and 300 questions
from Phase C were selected to form the multiple-choice question (MCQ) set, while 500 questions
from Phase A and 1000 questions from Phase B were selected to form the open-ended question set.
All sampled data were subsequently validated by experts to ensure accuracy and quality.
19gemini-2.5-pro-thinkingclaude-3-7-sonnet-latestgpt-4o-2024-11-20gemini-2.5-flash qwen2.5-vl-72B internvl-2.5-78B01020304050607080Accuracy (%)56.31%
51.68%50.45%51.35%
41.43%43.27%77.06%78.11%81.86%
75.86%
57.72% 58.02%Discovery Problems
Perception ProblemsFigure 9: Models’ accuracy on discovery and perception problems.
E Expert Validation
E.1 Details
We recruited annotators with a background in Earth sciences and a master’s degree through an
annotation company to label the data. The annotated dataset consists of 3,000 MCQs and 1,500 open-
ended QAs. We provided the annotators with figures, queries, reasoning chains, and our processed
refined captions to assist them in evaluating whether the provided answers were reasonable. For
questions where the answers could not be found in the refined captions, the annotators were required
to use their own knowledge to determine the correctness of the answers. If they were unable to make
a judgment, such questions were discarded to ensure that the filtered dataset contained only accurate
and complete questions. The tasks assigned to the annotators are described below:
The evaluation framework categorizes questions based on several criteria. First, the Image Type
of Reasoning Required distinguishes between questions involving a single image, where the input
consists of just one image, and those with a Single-image focus , where multiple images are present
but the question pertains to one specific image. Additionally, Multi-image reasoning questions
require comparing or reasoning across multiple images.
Next, the Type of Scientific Question is considered. Perception Questions are those where answers
can be derived through basic observation, such as identifying position or color. These questions do
not have answers in the refined captions and require manual evaluation of their validity. In contrast,
Discovery Questions necessitate domain-specific knowledge for answering, and annotators must
verify if the answer can be derived from the refined caption field.
TheCompleteness of Questions is another criterion, where questions are classified as Complete if
all necessary information is provided in the question or image, and Incomplete if missing information
makes it difficult or impossible to answer.
Finally, the Correctness of Questions assesses whether the provided answer is accurate, categorizing
them as Correct orIncorrect based on the accuracy of the answer.
20Easy Multiple -choice Question
Image Caption :
Coastline totally covered by seawall and concrete armor units on Shimizu coast (July 1995).
Question :
How has the natural aesthetic of the coastal environment likely changed due to the structures visible in the figure?
Options :
A. Enhanced beauty due to added greenery
B. Preservation of the sandy beach's appearance
C. Loss of natural shoreline aesthetics
D. Creation of diverse wildlife habitats
Response :
C. Loss of natural shoreline aesthetics
Reasoning Chain:
Step 1:Use visual cues from the figure to observe the dominance of artificial structures, such as seawalls and concrete 
units.
Step 2:Reflect on the visual impact of manmade elements completely covering the coastline.
Step 3: Identify that no sandy beaches or natural aesthetics remain visible.
Step 4: Determine that "Loss of natural shoreline aesthetics" best describes the impact based on both image and caption .
Image Caption :
Hydrogeological cross section in the Grombalia  basin
Question :
Which layer lies directly below the shallow aquifer in most parts of the Grombalia  basin?
Options :
A. Topsoil
B. Clayey sands aquicluds
C. Semi -deep aquifer
D. Sandstone
Response :
B. Clayey sands aquicluds
Reasoning Chain:
Step 1:The visual information from the figure shows a cross -sectional view of the stratigraphy.
Step 2 :Observing the labeling in the legend and the diagram, clayey sands aquicluds  are consistently shown directly below 
the shallow aquifer.
Step 3 :Correlating these observations, the layer below the shallow aquifer is identified as the clayey sands aquicluds .
Figure 10: An example of easy multiple-choice VQA.
After manual screening, 216 invalid en-
tries were identified in the MCQ task,
and 89 invalid entries were found in
the open-ended task. To evaluate the
effectiveness of our multi-agent filter-
ing process, we conducted a statistical
analysis of the three phases of data. In
Phase A, 59 out of 900 sampled ques-
tions were deemed invalid after manual
review; in Phase B, 80 out of 1800 ques-
tions were invalid; and in Phase C, 77
out of 300 questions were invalid. These
results demonstrate the utility of the
initial model-based filtering: questions
supported by refined captions and cor-
rectly answered by most models (Phases
A and B) tend to be of higher quality,
while questions filtered in Phase C ex-
hibit lower quality.
Phase A Phase B Phase C020406080100Percentage (%)93.4%95.7%
74.3%6.6%4.3%
25.7%Percentage of Valid and Invalid Data Across Phases
Valid Data
Invalid DataFigure 13: Proportion of valid and invalid data after
manual screening across different phases. Phase B, or
Specialized VQA, exhibits the highest quality.
21Moderate Multiple -choice Question
Image Caption:
Change in sea level pressure between different pairs of sensitivity experiments with the sea surface temperature gradient 
altered in the Atlantic Ocean, Pacific Ocean, or in combinations in both oceans.
….
Bottom Panel :Difference between experiments AdPi  and AiPd . Most changes outside of the range shown ($ -2.2$ to 
$2.2 ~\mathrm {mb}$) are significant at the 95% level .
Question:
What mechanism best explains the Southern Hemisphere pressure changes observed in the 
bottom panel (AdPi -AiPd )?
Options:
A. Pacific tropical warming amplifies wave refraction.
B. High -latitude cooling destabilizes atmospheric conditions.
C. Sea ice variations influence atmospheric circulation.
D. Increased sensible heat transports planetary momentum.
Response:
C.Sea ice variations influence atmospheric circulation .
Reasoning Chain:
Step 1:Look at the bottom panel (AdPi -AiPd ), which demonstrates significant pressure
responses in the Southern Hemisphere.
Step 2 :Identify pressure anomalies during Southern Hemisphere summer that align with
locations of seasonal sea ice.
Step 3 :Use context from the caption and figure dynamics to deduce influence from sea ice
variations.
Refined Caption:
Sea level pressure differences between sensitivity experiments with altered sea surface temperature (SST) gradients in the 
Atlantic and Pacific Oceans :
…..
•Bottom Panel :The difference between Experiment AdPi  (Atlantic high -latitude warming and tropical cooling combined
with Pacific tropical warming and high -latitude cooling) and Experiment AiPd  (the reverse gradient alterations). This
configuration produces a large positive NAO change, with warm tropical Pacific SSTs driving equatorward wave refraction
and poleward angular momentum transport, while warm northern North Atlantic SSTs reduce low -altitude northward
sensible heat transport and destabilize the local atmosphere. Notable responses are also observed in the Southern
Hemisphere, even during summer, which are linked to sea ice variations. Most changes outside the range of $ -2.2$ to
$2.2 ~\mathrm {mb}$ are statistically significant at the 95% confidence level.Figure 11: An example of specialized multiple-choice VQA.
F Classification of Research Problems in Earth Sciences
Under the framework of the five major spheres, we further categorized the generated research
problems into specific academic disciplines according to a standardized classification system. Within
the broad category of Earth Sciences, we refined the classification into detailed sub-disciplines or
sub-fields. The classification process involves three main steps: first, identifying the primary sphere
to which the research problem belongs, selecting from eight major disciplines (referred to as primary
spheres), including Atmospheric Sciences, Ecology and Biosciences, Hydrology, Oceanography,
Geology, Geography, Solid Earth Geophysics, and Polar Science. Second, the classification is further
refined by selecting the most appropriate sub-discipline or sub-field from a detailed hierarchy. Third,
for interdisciplinary problems, the primary classification is clearly stated, and any relevant secondary
classifications are noted. This hierarchical approach ensures a systematic and precise categorization
of research problems, enabling a deeper understanding of their academic and scientific context.
Summary of Classification: The classification system includes a total of 8 first-level disciplines and
66 second-level disciplines. Each research problem is assigned to one of the primary disciplines and
further refined into a specific sub-discipline based on its characteristics and context.
You are tasked with classifying a research problem into one of the Earth’s spheres and refining
it into a specific sub-discipline or sub-field.
Instructions:
1.Carefully analyze the input, which includes the research question, paper title, and
any additional information derived from images (e.g., visual data descriptions or
extracted features).
22Har d Multiple -choice Question
Image Caption :
MERIS RGB image of scene over Mediterranean Sea from 2 May 2003 (left) and zoom into daily composite of TCWV 
(middle) and uncertainty of TCWV (right) from MERIS and SSM/I measurements .
Question:
In which regions is TCWV density the highest based on the middle panel?
Options:
A. Over the Mediterranean Sea
B. Over land near coastal areas
C. Over desert areas in the bottom left
D. In the southernmost water regions
Response:
D. In the southernmost water regions
Reasoning Chain:
Step 1: Observe the middle panel, which shows TCWV values represented by a color gradient.
Step 2: The highest values correspond to the red and yellow sections in the southernmost part of the image.
Step 3: These sections lie over the near -equatorial regions of the southern water zones where higher water vapour  levels 
are observed.
Refined  Caption:
MERIS RGB image of the Mediterranean Sea area from 2 May 2003 (left), zoomed -in view showing the daily composite of 
Total Column Water Vapour  (TCWV) (middle), and the uncertainty of TCWV measurements (right ). The middle panel 
illustrates the smooth transition of the water vapour  field between land and ocean, with increased uncertainty in coastal 
areas. This uncertainty is linked to the use of MERIS data to fill gaps in the SSM/I measurements. The right panel 
highlights the regions of elevated uncertainty, particularly along the coast. Additionally, the figure emphasizes MERIS's 
high sensitivity to small -scale variations in the water vapour  field, as seen over Western Turkey.
Figure 12: An example of hard multiple-choice VQA.
2.Identify the primary sphere ( Atmospheric Sciences, Ecology and Biosciences,
Hydrology, Oceanography, Geology, Geography, Solid Earth Geophysics, or
Polar Science ) that the problem belongs to.
3.Refine the classification by selecting the most appropriate sub-discipline or sub-field
from the hierarchy.
4.If the problem spans multiple spheres or disciplines, clearly state the primary classi-
fication and mention any relevant secondary classifications.
Classification Hierarchy:
Atmospheric Sciences: Atmospheric Chemistry, Meteorology, Climatology, Hydrometeorol-
ogy, Paleoclimatology, Atmospheric Physics, Numerical Weather Prediction and Simulation,
Atmospheric Remote Sensing.
Ecology and Biosciences: Regional Ecology, Population Ecology, Community Ecology,
Ecosystem Ecology, Ecological Engineering, Restoration Ecology, Landscape Ecology,
Aquatic Ecology and Limnological Ecology, Biogeochemistry, Biogeography.
Hydrology: Hydrology, Hydrogeology, Limnology, River Hydrology and Estuarine Hydrol-
ogy, Groundwater Hydrology, Regional Hydrology, Ecohydrology, Hydrological Physics,
Hydrological Geography, Hydrological Meteorology, Hydrological Measurement, Hydrologi-
cal Cartography.
Oceanography: Ocean Chemistry, Ocean Physics, Ocean Biology, Ocean Geology, Remote
Sensing Oceanography, Environmental Oceanography, Marine Resources Science.
Geology: Economic Geology, Engineering Geology, Environmental Geology, Quaternary Ge-
ology, Sedimentology, Stratigraphy, Paleogeography, V olcanology, Mineralogy and Petrology,
Regional Geology, Remote Sensing Geology.
Geography: Physical Geography, Human Geography, Regional Geography, Urban Ge-
ography, Tourism Geography, World Geography, Historical Geography, Geomorphology,
Biogeography, Chemical Geography, Other Disciplines in Geography.
Solid Earth Geophysics: Geodynamics, Seismology, Geomagnetism, Gravimetry, Geo-
electricity, Geothermal Science, Tectonophysics, Exploration Geophysics, Computational
Geophysics, Experimental Geophysics, Other Disciplines in Solid Earth Geophysics.
Polar Science: Polar Ecology, Polar Oceanography, Glaciology, Permafrost Science, Polar
Climate Science.
23Input:
•Paper Title: {paper_title}
•Research Question: {research_question}
•Image Information: {image_caption}
Output Format:
The output must be written in JSON format using the structure below:
{
"primary_sphere": ,
"primary_sub_discipline": ,
"secondary_sphere": "Ecology and Biosciences",
"secondary_sub_discipline": "Aquatic Ecology"
}
Table 8: Top Sub-disciplines in Various Scientific Subjects. The table lists the top three sub-
disciplines by count within each major scientific subject.
Subject Top 1 Sub-subject Top 2 Sub-subject Top 3 Sub-subject
Hydrology River Hydrology and Estuarine Hydrology 805 Groundwater Hydrology 790 Limnology 439
Ecology and Biosciences Aquatic Ecology and Limnological Ecology 562 Landscape Ecology 298 Ecosystem Ecology 280
Geology Sedimentology 1068 Quaternary Geology 298 Structural Geology 215
Solid Earth Geophysics Seismology 845 Tectonophysics 343 Exploration Geophysics 74
Geography Physical Geography 1575 Urban Geography 76 Geomorphology 40
Polar Science Glaciology 352 Polar Climate Science 46 Permafrost Science 31
Atmospheric Sciences Meteorology 920 Climatology 619 Atmospheric Remote Sensing 159
Oceanography Ocean Physics 698 Ocean Geology 163 Environmental Oceanography 104
G MLLMs’ versions
Table 9: Evaluated MLLMs in our experiments with their versions or Huggingface model paths.
Open-source Models
Model Model path
Qwen2.5-VL-7B https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
Qwen2.5-VL-32B https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct
Qwen2.5-VL-72B https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct
InternVL2.5-8B https://huggingface.co/OpenGVLab/InternVL2_5-8B
InternVL2.5-78B https://huggingface.co/OpenGVLab/InternVL2_5-78B
InternVL3-78B https://huggingface.co/OpenGVLab/InternVL3-78B
LLaV A-onvision-72B https://huggingface.co/llava-hf/llava-onevision-qwen2-72b-ov-hf
Llama3.2-90B-Vision https://huggingface.co/meta-llama/Llama-3.2-90B-Vision
DeepSeek-VL2 https://huggingface.co/deepseek-ai/deepseek-vl2
Proprietary Models
Model Model versioning
GPT-4o gpt-4o-2024-11-20
Gemini-2.5-Pro-Thinking gemini-2.5-pro-preview-05-06
Gemini-2.5-Flash gemini-2.5-flash-preview-04-17
Gemini-2.5-Flash-Thinking gemini-2.5-flash-preview-04-17
Claude-3.7-Sonnet claude-3-7-sonnet-20250219
Claude-3.5-Haiku claude-3-5-haiku-20241022
GPT-4o-mini gpt-4o-mini-2024-07-18
For open-source models, we use vllm [ 36] for local testing; for proprietary models, we conduct tests
via API calls. The download paths for specific models and the versions of models accessed via API
are provided in Figure 9.
24H Evaluation Metrics
H.1 MLLM-based Metrics
Following G-Eval [ 33], we utilize MLLM (Qwen2.5-VL-72B) with a specialized prompt to compute
a factual scientific score. For the captioning task, we define a Cap-Eval score ranging from 1 to
5, where higher scores indicate better caption quality. For the open-ended QA task, we introduce
OE-Eval, which evaluates the reasonableness of generated answers using a binary 0/1 scoring system.
The following prompt was used for Cap-Eval:
Evaluate the quality of a generated caption for a geoscience research paper figure or image.
Evaluation Criteria:
1.Scientific Accuracy: Does the generated caption accurately describe the scientific
content of the figure or image?
2.Clarity and Coherence: Is the caption well-structured, logically organized, and
easy to understand?
3.Relevance and Completeness: Does the caption provide all necessary information
to understand the figure or image?
Evaluation Steps:
1.Compare the Generated Caption to the Standard Caption . Assess whether the
generated caption aligns with the scientific content and intent of the standard caption.
2.Assign a score for coherence on a scale of 1 to 5, where 1 is the lowest and 5 is the
highest, based on the Evaluation Criteria.
Input:
•Standard Caption: {response}
•Generated Caption: {generated_caption}
Important Instructions:
• Only output the score in the specified JSON format.
• Do not provide any explanations, comments, or additional text.
Output Format:
The output must be written in JSON format using the structure below:
{
"score": 1-5
}
The following prompt was used for OE-Eval:
You are tasked with evaluating the correctness of a generated answer to an open-ended
question about a given input image.
Input:
•Question: {query}
•Refined Caption: {refined caption}
•Standard Answer: {response}
•Generated Answer: {generated_answer}
25Instructions:
1.Based on the refined caption, question, and standard answer, determine if the gener-
ated answer is correct.
2. Only output the determination in the specified JSON format.
3. Do not provide any explanations, comments, or additional text.
Output Format:
The output must be written in JSON format using the structure below:
{
"is_correct": true or false
}
H.2 Similarity-based Metrics
In cases where some models fail to strictly follow instructions and only output the correct answer,
resulting in regular expression matching failures, we use the all-MiniLM-L6-v2 model [ 35] to
calculate the similarity between the model’s output and each option. The option with the highest
similarity is then selected as the model’s answer.
I Detailed MSEarth-MCQ Results
The inputs to our model, MSEarth, consist of images, questions, and the original captions. The
original captions provide contextual information about the images, such as the meanings of specific
symbols. Therefore, we conducted tests on different models to evaluate their performance with and
without the original captions.
Table 10: Accuracies (%) of different models on multiple-choice questions. The best results are
highlighted in bold, with the second-best underlined. OC: original caption.
ModelInput Image-Type Task Type Overall
OC SINGLE MULTI CROSS DISCOVERY PERCEPT ACC
Open-source Models
LLaV A-onvision-72B ✗ 49.40 45.52 41.10 41.92 61.86 46.69
Qwen2.5-VL-7B ✗ 39.12 35.65 39.18 37.27 38.98 37.68
Qwen2.5-VL-32B ✗ 42.07 39.78 40.00 37.03 52.92 40.84
Qwen2.5-VL-72B ✗ 47.65 43.30 43.84 41.43 57.72 45.33
InternVL2-8B ✗ 35.94 34.11 32.33 34.25 36.13 34.70
InternVL2.5-78B ✗ 48.13 45.88 45.21 43.27 58.02 46.80
InternVL3-78B ✗ 51.95 44.85 45.75 44.54 59.67 48.17
Llama3.2-90B-Vision ✗ 44.30 40.64 36.16 38.64 51.42 41.70
DeepSeek-VL2 ✗ 45.42 42.70 46.85 43.74 46.78 44.47
LLaV A-onvision-72B ✓ 53.55 49.48 47.95 46.58 65.52 51.11
Qwen2.5-VL-7B ✓ 47.65 44.07 37.53 40.53 58.47 44.83
Qwen2.5-VL-32B ✓ 52.59 46.99 43.84 42.47 70.16 49.10
Qwen2.5-VL-72B ✓ 52.11 50.43 46.30 44.40 70.46 50.65
InternVL2-8B ✓ 44.86 43.99 38.36 38.97 58.47 43.64
InternVL2.5-78B ✓ 53.23 49.74 44.38 43.17 74.21 50.61
InternVL3-78B ✓ 57.53 51.37 45.48 47.00 73.61 53.38
Llama3.2-90B-Vision ✓ 45.98 40.46 38.90 38.26 56.97 42.74
DeepSeek-VL2 ✓ 52.43 49.23 44.66 46.06 62.82 50.07
Proprietary Models
Gemini-2.5-Flash ✓ 58.33 54.55 53.42 49.98 75.56 56.11
Gemini-2.5-Flash-Thinking ✓ 60.64 54.64 53.70 51.35 75.86 57.22
Gemini-2.5-Pro-Thinking ✓ 64.78 59.36 55.34 56.31 77.06 61.28
Claude-3.5-Haiku ✓ 49.48 47.16 42.47 42.18 64.77 47.59
Claude-3.7-Sonnet ✓ 59.52 56.53 57.53 51.68 78.11 58.01
GPT-4o-mini ✓ 52.51 48.63 43.01 43.65 68.67 49.64
GPT-4o ✓ 63.03 55.76 47.67 50.45 81.86 57.97
26For open source models, we performed experiments ino settings: with and without the original
caption. The results show that providing the original caption improves performance in all tasks.
Notably, the improvement is more significant for perception tasks compared to discovery tasks. This
is likely because perception tasks rely more heavily on understanding the image content, and the
original caption provides helpful contextual information for interpreting the image.
We have compiled several case studies to illustrate the necessity of the original caption when
answering questions in certain situations. In example 14, if the original caption is not provided,
InternVL3-78B will be unable to accurately determine that the geographical location is in Germany,
resulting in an incorrect answer. In contrast, some proprietary models may possess stronger perceptual
capabilities and can correctly identify the location as Germany even without the original caption.
Similarly, in example 15, providing the original caption aids the model in understanding the image,
thereby facilitating task completion. Both scenarios are prevalent in scientific question-answering
contexts. To address this, we conducted separate experiments and explicitly integrated these settings
into the design of the MSEarth-MCQ task.
Raw Caption : Heat -flow pattern in Germany and in adjacent areas (after Hurtig et al., 1992) together with the position of the 
Central Segment of the European Geotraverse and shotpoints $D-K$
Question : Which region along the EGT segment has the most sparse  heat -flow data ?
A. North German Lowland
B. Molasse Basin
C. Saxothuringian
D. Rhenohercynian
Answer : A
Refined Caption: Heat -flow pattern in Germany and adjacent areas, illustrating the position of the Central Segment of the 
European Geotraverse  (EGT). The EGT extends from the southern rim of the Molasse Basin near Chur, northward to the Baltic 
Sea near Kiel. This section traverses significant geological formations, including the Alpine Molasse Basin, Hercynian (Variscan ) 
units of the Swabian Jura, South German Basin, Mid -German Crystalline Rise, Hessian Depression, and Leine Graben, 
terminating in the North German Lowland. The heat -flow data, particularly sparse in the North German Basin, range from values 
north of Hamburg to east of Hannover. The pattern depicted is based on previously obtained temperature data. Shotpoints  D-K 
are marked along the traverse .
InternVL3 -78B w original caption : A - The North German Lowland region shows fewer heat -flow contour lines compared to other regions, indicating 
sparser data. The contour lines are less dense, suggesting fewer measurements or less variation recorded, which implies spars e data collection in this area.
InternVL3 -78B w/o original caption : B - The Molasse Basin region shows fewer heat -flow data points compared to other regions along the EGT segment. 
The contour lines are less dense, indicating sparser data collection in this area.
gemini -2.5-flash w/o caption : A - The image shows heat flow contours (dashed lines) and geological regions. The EGT segment is represented by the thick 
black line with points D through K. We need to identify the region along the EGT segment with the most sparse  heat -flow data, which is indicated by the 
density of heat flow contours. Examining the map, the northernmost part of the EGT segment, around point K and extending furt her north, is likely 
within the North German Lowland. In this area, the heat flow contours are relatively sparse (e.g., 60, 70). In contrast, the Rhenohercynian  region along 
the EGT (from G to J) shows a higher density of contours (60, 70, 80, 90). The Saxothuringian  region along the EGT (around I and J) also has several 
contours (50, 60, 70, 80). The Moldanubic  region (D to E) has a high density of contours (80, 90, 100, 110, 120). The Molasse Basin is not shown along 
the EGT segment. Comparing the density of contours along the EGT in the given options, the North German Lowland (northernmost  part) exhibits the 
most sparse  heat -flow data.
gpt-4o-2024 -11-20 w/o caption : A - The North German Lowland region along the EGT segment has the most sparse  heat -flow data, as indicated by the 
lack of detailed contour lines and data points in this area compared to the other regions. The other regions, such as the Mol asse Basin, Saxothuringian , 
and Rhenohercynian , show more detailed heat -flow contour lines and data points, indicating denser data coverage.
Figure 14: Performance comparison of different models under two settings: with and without the
original caption. The results indicate that more powerful models exhibit less dependence on the
original caption, highlighting their ability to interpret image content independently.
J More Results
For MCQ and OE questions, we used radar charts to illustrate the performance of various models
across different disciplines. We also give some case studies in Figure 17 and Figure 18:
27Raw Caption : Defining the mud volcano boundary, layers, and faults on the CDS stack migrated section. Black rectangle 
shows part of the section showing in subsection in Fig. 14 b.
Question : What distinguishes the NW section of the Figure from the SE section?
A. The NW section shows more layered reflectors.
B. The SE section contains more faults and disrupted layers
C. The NW section has deeper visible structures
D. The SE section is closer to the surface .
Answer : B
Refined Caption: Migrated section of the common diffraction surface (CDS) stacked data, illustrating the boundary, 
internal layers, and faults of the mud volcanoes. The left portion of the section, which is of particular interest, shows an 
improved imaging of the mud volcano boundary. Reflectors and other subsurface structures responsible for diffraction events 
in the stacked section are clearly visible, providing enhanced detail and clarity of the geological features .
InternVL3 -78B w original caption : B - The SE section within the black rectangle shows more disrupted layers and faults compared to the NW section, 
which appears to have more continuous and less disturbed reflectors. This indicates that the SE section contains more structu ral disturbances.
InternVL3 -78B w/o original caption : A - The NW section of the figure shows more distinct and continuous layered reflectors compared to the SE 
section, which has more disrupted and less continuous layers. This indicates that the NW section has more pronounced stratifi cation.
gemini -2.5-flash w/o caption : A - The image displays a seismic section. The NW section, particularly in the upper part, shows more continuous and 
distinct layered reflectors compared to the SE section, which appears more complex with folding and potentially faulting with in the boxed area. 
Option B is incorrect as the SE section, while complex, doesn't necessarily show more faults and disrupted layers than the NW  section, and the NW 
section also exhibits some disruption. Option C is incorrect as both sections show structures down to similar depths. Option D is incorrect as the 
depth scale is the same for both sections, and the SE section is not closer to the surface than the NW section.
gemini -2.5-pro w/o caption : B - Upon examining the seismic reflection profile, the SE section (roughly from 20 km to 45 km distance) displays 
prominent folding of the geological layers. Associated with this folding, there are also clear indications of faults, such as  offsets and abrupt 
terminations of reflectors. This intense deformation results in disrupted layers. In contrast, the NW section (roughly from 0  km to 20 km distance) 
shows layers that are generally flatter or gently dipping, with less evidence of significant folding or faulting. Therefore, the SE section is distinguished 
by containing more faults and disrupted layers compared to the NW section.
gpt-4o-2024 -11-20 w/o caption : B - In the image, the NW section shows relatively continuous and layered reflectors, while the SE section displays 
more disrupted and faulted layers, as evidenced by the irregular and broken patterns in the SE region. This indicates the pre sence of faults and 
structural deformation in the SE section.
Figure 15: Performance comparison of different models under two settings: with and without the
original caption.
HydrologyGeologyGeography
Polar Science
Atmospheric
Oceanography
Solid Earth GeophysicsEcology and Biosciences20%40%60%80%
MCQ Accuracy by Primary Sphere
HydrologyGeologyGeography
Polar Science
Atmospheric
Oceanography
Solid Earth GeophysicsEcology and Biosciences10%30%50%70%
OE Accuracy by Primary Sphere
InternVL2.5-78B Qwen-2.5-VL-72B Claude-3.7-sonnet gpt-4o-2024-11-20 gemini-2.5-pro-thinking
Figure 16: Performance comparison of different models across various subjects.
28Multiple Choice VQA Case Study
Original Caption : (a) Temporal distribution of precipitation, wind direction, and wind 
intensity (maximum and mean) on 27 February observed at the weather station at 
Navarredonda  (see Fig. 1b); (b) spatial pattern of precipitation (isolines in mm) observed 
on 27 February over the study area (circle, Navarredonda  automatic weather station).
Question: What does the temporal distribution of precipitation in Figure 9(a) indicate 
about the morning hours of February 27?
Options:
A.Minimal precipitation occurred.
B. A brief period of heavy precipitation occurred.
C. Precipitation was evenly distributed throughout the morning.
D. Precipitation was absent in the morning.
Response:  B. A brief period of heavy precipitation occurred.
Reasoning chain: 
Step 1: Observe the precipitation graph in Figure (a), which shows bar heights representing 
precipitation recorded in 10 -minute intervals.
Step 2: Analyze the morning hours (e.g., 06Z to 09Z); the bar graph shows a significant 
increase in precipitation during this time.
Step 3: Conclude that there was a brief period of heavy precipitation in the morning.Refined caption : (a) Temporal distribution of precipitation, wind direction, and wind 
intensity (maximum and mean) on 27 February as observed at the Navarredonda  weather 
station. The data highlights a brief period of increased instability indices, Froude number, 
and meridional moisture flux during the morning and central hours , attributed to the 
short -lived flow perpendicular to the mountain range. (b) Spatial pattern of precipitation 
on 27 February over the study area, with isolines indicating precipitation levels in 
millimeters. The pattern confirms a broad area of precipitation with measured values 
between 30 mm and higher, with maximum values observed around the Navarredonda  
automatic weather station.
Answer: A
Explanation: The top graph in Figure (a) shows the temporal distribution of precipitation in 10 -minute intervals. In the morning hours 
(from 00:00 UTC to around 08:00 UTC), the bars representing precipitation are relatively low, indicating minimal precipitatio n during 
this time frame. There is no evidence of heavy precipitation or even distribution throughout the morning, nor is there an ind ication that 
precipitation was completely absent.
Answer: B
Explanation: Figure (a) shows the temporal distribution of precipitation. The x -axis represents time in UTC, and the y -axis repr esents 
precipitation in mm/10min. The morning hours of February 27 can be considered as the period from approximately 06:00 UTC to 1 2:00 
UTC. Observing the precipitation graph (top panel) during these hours: \n1. Precipitation is clearly present throughout this peri od, so 
option D ('Precipitation was absent in the morning') is incorrect. \n2. The amount of precipitation is not minimal; there are con sistent bars, 
many of which are around 0.5 mm/10min to 1 mm/10min, and some even higher. Thus, option A ('Minimal precipitation occurred') is 
incorrect. \n3. The height of the precipitation bars varies significantly throughout the morning, indicating that the precipitati on was not 
evenly distributed. For instance, there are peaks and troughs in intensity. Therefore, option C ('Precipitation was evenly di stributed 
throughout the morning') is incorrect. \n4.  Between 06:00 UTC and 07:00 UTC, there is a noticeable increase in precipitation int ensity, 
with several 10 -minute intervals showing precipitation rates of 1 mm/10min or more, and one peak reaching approximately 1.5 mm/1 0min 
around 06:30 UTC. This can be characterized as a period of heavier precipitation compared to other parts of the morning (e.g. , 08:00 -
10:00 UTC where precipitation is generally lower). This period of higher intensity is relatively brief in the context of the entire morning. 
Another similar period of increased intensity occurs around 11:00 -12:00 UTC. Therefore, option B ('A brief period of heavy preci pitation 
occurred') accurately describes the observed pattern.
Answer: C. Precipitation was evenly distributed throughout the morning.
Explanation: Looking at Figure (a), the top panel shows precipitation data for February 27, 2010, with the x -axis representing h ours from 
00 to 24 UTC. During the morning hours (approximately 06 -12 UTC), the blue precipitation bars display a relatively consistent pa ttern 
with similar heights. There are no major gaps in precipitation or extremely intense brief episodes during this period. Instea d, the 
precipitation appears to be occurring steadily throughout the morning hours with a fairly even distribution, making option C thecorrect 
answer.
Answer: B. A brief period of heavy precipitation occurred.
Explanation: Figure (a) shows the temporal distribution of precipitation with vertical bars indicating precipitation levels o ver10-minute 
intervals. During the morning hours of February 27 (especially between 06:00 UTC and 08:00 UTC), there is a noticeable spike in 
precipitation levels, indicating a brief period of heavy precipitation compared to other time intervals.gpt-4o-2024 -11-20claude -3-7-sonnet -thinkinggemini -2.5-pro-thinkingqwen -2.5-VL-instruct
Figure 17: Case Study of Multiple Choice VQA.
29Open -Ended VQA Case Study  1
Response: Baruun -Urt
Reasoning chain: 
Step 1: From the Figure, Baruun -Urt is outside the Gobi Desert's shaded area, 
suggesting it is not in the arid region .
Step 2: The semi -arid steppe zone is consistent with areas having denser 
vegetation, implied by Baruun -Urt's location in the Figure .
Refined caption : Schematic illustration of the locations of the photometric stations used for atmospheric AOD measurements in the 
Eastern Gobi region of Mongolia. The stations are situated at approximately the same latitude and include Sainshand  (located in an arid 
zone), Dalanzadgad  (also in an arid zone), and Baruun -Urt (located in a semi -arid steppe zone with denser vegetation cover ).
Answer: Baruun -Urt
Explanation: The Gobi Desert is an arid zone. Baruun -Urt is located north of the Gobi Desert, in an area likely to be a semi -arid steppe 
transition zone .
Answer: Baruun -Urt
Explanation: Located at northern edge of Gobi Desert region where desert transitions to semi -arid steppe ecological zone.
Answer: Sainshand ,
Explanation: The map shows Sainshand  located within the semi -arid steppe zone of the Gobi Desert.gpt-4o-2024 -11-20claude -3-7-sonnetgemini -2.5-pro-thinkingqwen -2.5-VL-instructOriginal Caption : Schematic illustration of locations of the photometric stations.
Question : Which station lies in a semi -arid steppe zone ?
Answer :Dalanzadgad
Explanation: Located in semi -arid area
Open -Ended VQA Case Study 2
Original Caption : Map of Gö sing  with marked earth buildings from the cadastre .
Question : Which natural feature primarily impacts settlement patterns visible in the 
map ?
Response : Terrain slope
Refined caption: Detailed map of Gö sing  illustrating the distribution of earth buildings as recorded in the earth building cadastre . The 
map highlights how the topography of the terrain influences the settlement patterns of these structures, providing insight into the 
construction techniques and methods that shape the appearance of entire streets within the village. Reasoning chain: 
Step 1: Observing the map shows that areas near slopes have higher earth building clusters.
Step 2: Caption reveals that terrain influences settlement and construction choices .
Step 3: Terrain slope emerges as the most significant natural factor influencing patterns .
Answer: Terrain elevation differences
Explanation: The map shows settlement patterns concentrated along roads and flatter areas, avoiding steeper or elevated terrain, which is 
typical in areas with significant elevation differences .gpt-4o-2024 -11-20
Answer: River proximity
Explanation: Settlements cluster near river.qwen -2.5-VL-instructAnswer: Terrain or topography
Eexplanation: The map shows buildings (settlements) concentrated along roads, and the presence of dashed lines, likely representing 
contour lines, suggests varying elevation. Settlements often follow flatter areas or valleys in hilly terrain for easier cons truction and access. 
Therefore, the terrain/topography is the primary natural feature influencing where settlements are located .gemini -2.5-pro-thinking
Figure 18: Case Study of Open-Ended VQA.
30