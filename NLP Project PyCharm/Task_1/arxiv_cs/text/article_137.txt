Jigsaw-Puzzles: From Seeing to Understanding to Reasoning in
Vision-Language Models
Zesen Lyu1,3, Dandan Zhang3, Wei Ye1, Fangdi Li2,3, Zhihang Jiang1,3, Yao Yang3*
1Hangzhou Institute for Advanced Study, UCAS
2Zhejiang University,3Zhejiang Lab
{lvzesen23, yewei23, jiangzhihang23}@mails.ucas.ac.cn,
FdbendyLi@zju.edu.cn, {danae.zdd, yangyao}@zhejianglab.com
Abstract
Spatial reasoning is a core component of hu-
man cognition, enabling individuals to perceive,
comprehend, and interact with the physical
world. It relies on a nuanced understanding
of spatial structures and inter-object relation-
ships, serving as the foundation for complex
reasoning and decision-making. To investi-
gate whether current vision-language models
(VLMs) exhibit similar capability, we introduce
Jigsaw-Puzzles, a novel benchmark consisting
of 1,100 carefully curated real-world images
with high spatial complexity. Based on this
dataset, we design five tasks to rigorously eval-
uate VLMs’ spatial perception, structural un-
derstanding, and reasoning capabilities, while
deliberately minimizing reliance on domain-
specific knowledge to better isolate and as-
sess the general spatial reasoning capability.
We conduct a comprehensive evaluation across
24 state-of-the-art VLMs. The results show
that even the strongest model, Gemini-2.5-Pro,
achieves only 77.14% overall accuracy and per-
forms particularly poorly on the Order Gener-
ation task, with only 30.00% accuracy, far be-
low the 90%+ performance achieved by human
participants. This persistent gap underscores
the need for continued progress, positioning
Jigsaw-Puzzles as a challenging and diagnos-
tic benchmark for advancing spatial reasoning
research in VLMs.
1 Introduction
The road to artificial general intelligence (AGI)
demands more than language or vision alone: it
requires models to possess a human-like spatial rea-
soning capability by constructing structured repre-
sentations of the physical world (Lake et al., 2017).
Spatial reasoning refers not just to the perception
of visual input, but to the capability to comprehend
spatial arrangements, model structural relations,
and infer the geometry and layout of a scene. These
*Corresponding author.
Figure 1: Jigsaw-Puzzles example. While human partic-
ipants effortlessly reconstruct the original spatial layout,
all tested VLMs fail to recover the correct order.
Figure 2: Evaluation of VLMs on Jigsaw-Puzzles. The
plot reports the accuracy of 8 representative VLMs on 5
tasks.
capabilities are fundamental to human cognition
and develop naturally through everyday perception
and interaction (Ishikawa and Newcombe, 2021).
In contrast, current VLMs, while highly capable in
tasks such as image captioning (Young et al., 2014;
Lin et al., 2014; Sharma et al., 2018), visual ques-
tion answering (Krishna et al., 2017; Singh et al.,
2019; Marino et al., 2019), and image-text retrieval
(Schuhmann et al., 2021; Thapliyal et al., 2022;
Bitton-Guetta et al., 2023), consistently struggle
Preprint. Under review.arXiv:2505.20728v1  [cs.AI]  27 May 2025with tasks requiring spatial reasoning (Stogiannidis
et al., 2025). We show an example in Figure 1 and
report the performance of some tested VLMs on
Jigsaw-Puzzles in Figure 2. This gap underscores a
critical limitation: while current VLMs have made
substantial progress in basic visual understanding,
they continue to struggle with structured spatial
reasoning, which is essential for grounded under-
standing in real-world scenarios. Bridging this gap
is essential for progressing towards generalizable
human-like spatial cognition and ultimately AGI.
However, existing benchmarks have yet to pro-
vide a comprehensive evaluation of spatial rea-
soning capability in VLMs under complex, real-
world visual scenarios. Some works (Fu et al.,
2024; Li et al., 2024; Liu et al., 2024; Yue et al.,
2024) focus primarily on foundational visual un-
derstanding by systematically evaluating percep-
tion, comprehension, and basic visual reasoning,
revealing notable limitations in these areas. Al-
though a few recent efforts (Pothiraj et al., 2025;
Stogiannidis et al., 2025; Ren et al., 2025; Tang
et al., 2025) have attempted to evaluate the spatial
reasoning capability of VLMs, they often rely on
overly synthetic settings, task-specific constraints,
or domain-dependent priors (Song et al., 2025) (See
Appendix A.1 for examples), limiting the capabil-
ity to capture generalizable spatial reasoning under
natural visual conditions. A truly effective eval-
uation of human-like spatial reasoning capability
should model the task as a multi-stage cognitive
process—beginning with perception, advancing
through structural understanding, and culminating
in high-level reasoning. Such reasoning must be
grounded in the visual richness and ambiguity of
real images, requiring the integration of spatial
structural modeling and goal-directed reasoning
(Chen et al., 2024). Yet, this critical dimension of
spatial cognition remains largely overlooked in ex-
isting benchmarks, underscoring the need for new
benchmarks that move beyond narrow task formu-
lations and embrace the full complexity of spatial
reasoning.
To overcome the limitations of existing bench-
marks in evaluating spatial reasoning capability of
VLMs under real-world conditions, we introduce
Jigsaw-Puzzles, a novel benchmark inspired by the
cognitive mechanisms underlying human puzzle-
solving. Puzzle-solving naturally reflects the multi-
stage cognitive process (Fissler et al., 2018), mak-
ing it a compelling and effective testbed for spatial
reasoning in VLMs.Benchmark Understanding ReasoningHigh Visual
ComplexityGreat
ScalabilityAutomated
Construction
Capture (Pothiraj et al., 2025) ✓ ✓ × × ×
Mind the Gap (Stogiannidis et al., 2025) ✓ ✓ × × ×
VGRP (Ren et al., 2025) ✓ ✓ × × ×
LEGO-Puzzles (Tang et al., 2025) ✓ ✓ × ✓ ✓
Jigsaw-Puzzles (Ours) ✓ ✓ ✓ ✓ ✓
Table 1: Comparison of spatial reasoning benchmarks.
In total, Jigsaw-Puzzles comprises 1,100 care-
fully curated real-world images and features five
different tasks. First, we begin with the Missing
Piece Selection task to evaluate VLMs’ basic spa-
tial understanding capability, which serves as the
essential foundation for spatial reasoning. Building
on this foundation, we introduce four reasoning-
centric tasks: Piece Localization ,Connection Ver-
ification ,Anomaly Detection , and Order Restora-
tion. These tasks are designed to assess various
facets of spatial reasoning, including adjacency
modeling, local structural consistency, spatial lo-
calization, geometric transformation understanding,
and multi-step spatial reasoning.
Compared to existing benchmarks for evaluat-
ing spatial reasoning capability in VLMs, Jigsaw-
Puzzles offers three key advantages, as summarized
in Table 1: (1) Higher visual complexity. Jigsaw-
Puzzles uses real-world images with diverse and
rich visual elements, and significantly outperforms
benchmarks based on synthetic images (Ren et al.,
2025; Stogiannidis et al., 2025; Tang et al., 2025)
and simple visual scenes (Pothiraj et al., 2025).
This enables more realistic and challenging spatial
reasoning evaluation. (2) Greater scalability. Any
natural image that satisfies the construction rules
can be directly used to generate puzzle tasks, with-
out the need to manually synthesize target images.
(3) Fully automated construction pipeline. All
Jigsaw-Puzzles tasks are generated automatically
without manual annotation, with each question
paired with a unique deterministic answer. This
feature enables low-cost dataset construction and
facilitates continuous expansion and refinement.
In summary, we introduce Jigsaw-Puzzles, a
novel benchmark for systematically evaluating the
human-like spatial reasoning capability of VLMs
in realistic visual settings. Our main contributions
are as follows:
A new benchmark for spatial reasoning. We
introduce Jigsaw-Puzzles, a puzzle-inspired bench-
mark constructed through a fully automated
pipeline that improves existing benchmarks in vi-
sual complexity and scalability, while enabling
structured evaluation of spatial reasoning in VLMs.
Comprehensive evaluation and analysis. Weevaluate 24 state-of-the-art VLMs on Jigsaw-
Puzzles and conduct detailed analysis. Our findings
expose consistent limitations in current VLMs and
provide insights to guide future improvements in
spatial reasoning capability.
Open-sourced dataset and construction tools.
We release the full dataset along with the auto-
mated generation scripts to support the evaluation
and continued advancement of spatial reasoning in
VLMs under real-world visual scenarios, as well
as to facilitate future benchmark expansion.
2 Related Work
General VLMs Evaluation Benchmarks. With
the rapid progress of VLMs, systematically eval-
uating their diverse capabilities has become a key
challenge. Although many benchmarks have been
introduced, most focus primarily on visual under-
standing. MME (Fu et al., 2024) evaluates instruc-
tion following, perception, and basic reasoning
across 14 subtasks, revealing persistent issues like
object hallucination and limited spatial understand-
ing. SEED-Bench (Li et al., 2024) includes 19,000
multiple-choice questions across 12 dimensions
and shows continued struggles with text recogni-
tion and temporal reasoning. MMBench (Liu et al.,
2024) offers fine-grained bilingual evaluations, en-
hancing the robustness of multilingual assessment.
MMMU (Yue et al., 2024) provides 11,500 ques-
tions across 183 subfields and 30 image types to
test expert-level reasoning, yet even advanced mod-
els like Gemini display notable knowledge gaps.
While these benchmarks have advanced the eval-
uation of perceptual and semantic understanding,
none systematically assess spatial reasoning—the
core aspect of human cognition. This highlights the
pressing need for more challenging and diagnostic
benchmarks specifically targeting spatial reasoning
capability in VLMs.
Spatial Reasoning Evaluation Benchmarks in
VLMs. Several recent benchmarks have aimed to
evaluate the spatial reasoning capability of VLMs.
Capture (Pothiraj et al., 2025) assesses occluded
object counting, revealing that VLMs struggle to
form coherent spatial representations under occlu-
sion. Mind the Gap (Stogiannidis et al., 2025)
evaluates spatial relations, navigation, and men-
tal rotation, showing that VLMs often perform near
chance level, indicating limited spatial cognition.
VGRP (Ren et al., 2025) introduces 20 visual grid
puzzles across varying difficulty levels to assessvisual perception, rule-following, and logical rea-
soning. LEGO-Puzzles (Tang et al., 2025) provides
1,100 visual QA pairs over 11 subtasks to mea-
sure basic and multi-step spatial reasoning. Re-
sults consistently show that current VLMs struggle
with perceptual complexity, rotation reasoning, and
sequential reasoning. Despite these efforts, most
benchmarks rely on simplified scenarios, failing to
reflect the complexity of real-world spatial environ-
ments, thereby limiting their generalizability. More
diagnostic benchmarks grounded in natural visual
settings are needed to advance human-level spatial
reasoning in VLMs.
3 Jigsaw-Puzzles
In this section, we introduce Jigsaw-Puzzles, a scal-
able and comprehensive benchmark designed to
evaluate the spatial reasoning capability of VLMs
in realistic visual environments. Specifically, Sec-
tion 3.1 outlines the motivation and definition of
each task, while Section 3.2 describes the dataset
construction process, including image selection and
the automated generation of question–answer pairs.
3.1 Task Definition
To systematically evaluate the spatial reasoning
capability of VLMs, we design tasks around the
core cognitive stages underlying human spatial rea-
soning—seeing, understanding, and reasoning. In-
spired by the human process of solving jigsaw puz-
zles, our benchmark simulates how individuals inte-
grate fragmented visual information into a coherent
whole: beginning with the perception of local vi-
sual cues, followed by the comprehension of spatial
relationships, and culminating in multi-step spatial
reasoning to reconstruct the original scene. This
sequence naturally reflects the progression from
low-level perception to high-level spatial reason-
ing. Accordingly, the tasks span spatial understand-
ing, single-step, and multi-step spatial reasoning,
collectively providing a comprehensive evaluation
across different levels of spatial reasoning. Figure 3
shows examples of each task in Jigsaw-Puzzles.
Task 1: Missing Piece Selection. The task evalu-
ates VLMs’ spatial understanding capability. Given
an image with a missing region, VLMs need to se-
lect the correct patch from four candidates. We
define two difficulty levels: Easy, where distractors
are randomly chosen, and Hard, where distractors
are selected using CLIP (Radford et al., 2021) sim-
ilarity to closely resemble the ground-truth patch,Figure 3: Task examples of Jigsaw-Puzzles. Note: the questions above are slightly simplified for clarity and brevity,
and the blue option indicates the correct answer.
Figure 4: Dataset curation pipeline. Step 1 filters candidate images through expert-defined rules to build a spatial
reasoning dataset. Step 2 uses automated templates to generate task-specific QA pairs from the curated images.
increasing the task’s difficulty. Task 2: Piece Lo-
calization . This task evaluates spatial localization
as a representative single-step spatial reasoning
capability. Given a partially masked image and
one masked patch, VLMs must identify the patch’s
original position. Difficulty is controlled by grid
size and number of masked regions: Easy (2 ×2
with two masks), Hard (3 ×3 with four masks), in-
creasing spatial complexity. Task 3: Connection
Verification . This task evaluates adjacency reason-ing, which also falls under single-step spatial rea-
soning. The full image is divided into 2 ×2 grids,
and two patches are randomly selected. VLMs
are asked to determine their spatial relationship in
the original image (e.g., above-below, left-right, or
non-adjacent). Task 4: Anomaly Detection . This
task targets local spatial transformation detection, a
process that inherently involves single-step spatial
reasoning. One region in 2 ×2 grids is randomly
rotated, mirrored, or left unchanged. The modelmust detect the change, locate the region, and iden-
tify the transformation. Task 5: Order Restoration .
This task integrates the capabilities assessed in the
previous tasks and serves as a complex multi-step
spatial reasoning challenge. A complete image is
split into four shuffled patches. VLMs must iden-
tify the correct order to reconstruct the original
spatial layout.
Overall, the five puzzle-inspired tasks in Jigsaw-
Puzzles cover a broad spectrum of spatial rea-
soning challenges—from basic spatial understand-
ing to single-step and multi-step spatial reason-
ing—enabling a comprehensive evaluation of spa-
tial reasoning in VLMs.
3.2 Dataset Curation
As illustrated in Figure 4, our dataset curation
pipeline consists of two main stages: data collec-
tion and QA generation.
Data Collection. We integrate data collection and
quality control into a unified process. Starting from
the CC3M (Sharma et al., 2018) dataset, we ap-
ply task-specific filtering criteria—including min-
imum resolution and aspect ratio constraints—to
construct an initial image pool of approximately
10,000 candidate images. Two human experts itera-
tively review the image pool while incrementally
refining a shared set of filtering rules. Based on
these evolving rules, they collaboratively filter the
initial dataset to obtain the final set of high-quality,
structurally diverse images. See Appendix A.2 for
the rules pool. To enhance generalizability, we em-
phasize semantic and structural diversity through-
out the dataset.
QA Generation. To support scalable and consis-
tent QA pairs generation, each task type is asso-
ciated with a specific construction template. QA
pairs are automatically generated using these tem-
plates. Figure 3 illustrates simplified examples of
the templates, full versions are provided in Ap-
pendix A.2.
4 Evaluation on Jigsaw-Puzzles
4.1 Experimental Setting
Benchmark Models. We evaluate 24 VLMs
on Jigsaw-Puzzles, covering a diverse range
of model scales and training paradigms. For
open-source models, we evaluate Qwen2-VL-
72B (Wang et al., 2024a), QvQ-72B-Preview
(Qwen, 2024), Qwen2.5-VL-[7B/32B/72B] (Bai
et al., 2025), InternVL3-[8B/14B/38B/78B] (Zhuet al., 2025), Kimi-VL-A3B-[Instruct/Thinking]
(Du et al., 2025), Phi-4-multimodal-instruct
(Abouelenin et al., 2025), Aya-Vision-[8B/32B]
(Dash et al., 2025), and Mistral-Small-3.1-24B-
Instruct (Mistral, 2025). For proprietary mod-
els, we evaluate Claude-[3.5/3.7]-Sonnet (An-
thropic, 2024), Gemini-[2.0/2.5]-Flash, Gemini-
2.5-Flash-Thinking, Gemini-2.5-Pro (Anil et al.,
2023), GPT-4o, GPT-4o-mini (Achiam et al., 2023),
and Grok-2-Vision (Grok, 2024). Notably, QvQ-
72B-Preview, Kimi-VL-A3B-Thinking, Gemini-
2.5-Flash-Thinking, and Gemini-2.5-Pro are cat-
egorized as reasoning-enhanced models. All mod-
els, supporting multi-image input, are evaluated in
a zero-shot setting with hardware scaled to their
parameter size, see details in Appendix A.2.
Evaluation Metric. Since each QA pair in Jigsaw-
Puzzles has a single correct answer, we use exact
match accuracy (%) as the primary metric to evalu-
ate VLMs’ performance on each task.
Baselines. We provide two baselines for compari-
son: (1) Random, which assumes equal probability
across all options and calculates expected accuracy
accordingly. (2) p-value-based critical value, which
reports the minimum accuracy required to outper-
form random guessing at a significance level of
p=0.05.
Human Performance. To evaluate human per-
formance, we construct a subset called Jigsaw-
Puzzles-Lite by sampling 220 images from the full
dataset. Three human participants complete all
tasks on this subset under the same conditions as
VLMs—without access to any external tools or the
internet. Their performance serves as an empirical
upper bound for spatial reasoning capability.
4.2 Main Results
Table 2, 3 report the performance of 24 VLMs
on Jigsaw-Puzzles. Building on these results, we
conduct a comprehensive and systematic analysis.
We summarize several key findings as below.
Spatial Reasoning Remains a Challenge for
VLMs. As shown in Table 3, human participants
consistently outperform VLMs, achieving an over-
all accuracy of 96.36%. By comparison, current
VLMs perform considerably worse, with even the
strongest models—Gemini-2.5-Pro—lagging more
than 20 percentage points behind human accuracy
across all tasks. The persistent gap between hu-
mans and VLMs highlights the demanding nature
of Jigsaw-Puzzles and affirms its utility as a robust
benchmark for spatial reasoning evaluation.Missing Piece Selection Piece LocalizationModelsEasy Hard Easy HardConnection
VerificationAnomaly
DetectionOrder
RestorationOverall
Baseline
Random Guessing 25.00 25.00 50.00 25.00 33.33 28.13 25.00 30.21
↑Random (p < 0.05) 27.30 27.30 52.50 27.30 35.70 30.50 27.30 32.56
Proprietary Models
Grok2-Vision 64.55 52.45 53.00 41.00 34.91 27.73 25.27 42.70
GPT-4o-mini 96.45 83.64 59.45 37.82 44.36 57.91 33.18 58.97
GPT-4o 95.00 89.18 61.55 53.45 41.09 53.18 31.55 60.71
Claude-3.5-Sonnet 99.73 94.55 62.45 41.09 45.64 67.45 35.00 63.70
Claude-3.7-Sonnet 99.55 95.09 60.27 44.55 47.91 67.00 39.82 64.88
Gemini-2.0-Flash 92.09 85.64 63.55 54.00 44.91 68.73 34.27 63.31
Gemini-2.5-Flash 98.82 92.45 64.55 54.55 48.82 67.36 34.45 65.86
Gemini-2.5-Flash-Thinking 99.55 94.73 76.64 51.27 57.91 62.00 64.82 72.42
Gemini-2.5-Pro 99.91 97.18 78.82 61.09 59.36 70.00 73.64 77.14
Open-source Models
Kimi-VL-A3B-Instruct 67.91 52.55 51.45 29.82 37.91 21.82 32.82 42.04
Kimi-VL-A3B-Thinking 84.64 58.09 56.36 32.64 28.00 30.91 20.36 44.43
Phi-4-multimodal-instruct 63.45 51.64 60.91 37.45 36.64 43.36 27.64 45.87
Qwen2.5-VL-7B 87.18 63.27 54.27 36.18 38.55 45.00 28.09 50.36
Aya-Vision-8B 26.82 27.27 49.64 26.55 35.00 12.91 24.73 28.99
InternVL3-8B 98.09 85.45 53.91 35.45 44.91 46.82 34.36 57.00
InternVL3-14B 99.73 88.73 59.64 40.18 51.73 49.09 40.91 61.43
Mistral-Small-3.1-24B-Instruct 26.91 28.55 54.27 31.27 38.27 52.36 26.45 36.87
Qwen2.5-VL-32B 97.27 76.82 62.09 40.36 50.09 61.55 33.64 60.26
Aya-Vision-32B 24.27 25.27 51.45 28.36 37.18 41.45 25.00 33.28
InternVL3-38B 99.00 91.36 63.45 42.64 56.73 30.73 54.64 62.65
Qwen2-VL-72B 95.55 75.36 55.27 40.64 40.55 42.36 33.55 54.75
QVQ-72B-Preview 77.82 52.18 53.09 36.73 41.82 47.73 34.64 49.14
Qwen2.5-VL-72B 99.36 87.82 65.91 45.27 43.36 58.82 41.00 63.08
InternVL3-78B 99.73 95.55 69.45 52.27 49.27 58.18 57.09 68.79
Table 2: Full Evaluation Results of 24 VLMs on Jigsaw-Puzzles. VLMs are grouped into proprietary and open-
source categories. Dark Green and Light Green indicate the top-1 and top-2 performance within each group,
respectively. Results of reasoning-enhanced are marked in bold . We also highlight the top three models based on
their overall performance, using Dark Blue , Medium Blue , and Light Blue , respectively.
Missing Piece Selection Piece LocalizationModelsEasy Hard Easy HardConnection
VerificationAnomaly
DetectionOrder
RestorationOverall
Human Performance 99.55 100.00 95.45 91.36 93.18 97.27 97.73 96.36
Proprietary Models
Claude-3.7-Sonnet 100.00 95.45 55.45 47.27 42.73 68.18 38.64 63.96
Gemini-2.5-Flash 98.18 93.18 58.18 55.45 42.27 66.82 32.27 63.76
Gemini-2.5-Flash-Thinking 99.55 95.91 71.82 51.82 55.00 60.91 57.27 70.33
Gemini-2.5-Pro 100.00 96.36 77.73 56.82 57.27 71.36 70.91 75.78
Open-source Models
Qwen2.5-VL-72B 99.09 86.82 67.73 42.27 40.00 57.73 33.64 61.04
InternVL3-78B 99.55 95.45 70.45 53.64 44.55 61.36 56.82 68.83
Table 3: Comparing Top-Performing VLMs with Human Performance on Jigsaw-Puzzles-Lite. The human
performance is highlighted in Dark Green . Results of reasoning-enhanced are marked in bold . The top three
overall performance are highlighted in Dark Blue , Medium Blue , and Light Blue , respectively.
Significant Gap Between Open-Source and
Proprietary VLMs. As shown in Tables 2,
proprietary VLMs consistently outperform open-
source VLMs on Jigsaw-Puzzles. Among them,
non-reasoning-enhanced proprietary VLMs typi-
cally exceed 60% overall accuracy, whereas most
open-source VLMs fall short—only InternVL3-
[14B/38B/78B] and Qwen2.5-VL-72B surpass this
threshold. Reasoning-enhanced proprietary mod-
els, such as Gemini-2.5-Flash-Thinking (72.42%)and Gemini-2.5-Pro (77.14%), further widen this
gap. These results reveal a persistent disparity in
spatial reasoning performance, suggesting that pro-
prietary VLMs benefit from advantages in model
architecture, training strategy, and access to large-
scale data. Meanwhile, this finding highlights
substantial room for improvement in open-source
VLMs toward achieving more robust and general-
izable spatial reasoning.
Model Performance in Different Tasks. IntheMissing Piece Selection task, which primar-
ily targets spatial understanding, most proprietary
VLMs perform well under both Easy and Hard
settings, demonstrating strong perceptual capabil-
ity. Although open source models generally per-
form poorly by comparison, certain models, such
as the InternVL3 series and Qwen2.5-VL-72B,
achieve perceptual understanding on par with pro-
prietary VLMs. Notably, both the Aya-Vision se-
ries and the Mistral-Small-3.1-24B-Instruct mod-
els perform poorly across all settings, even at the
32B scale, accuracy remains near random, reveal-
ing severe deficits in spatial understanding and
instruction following. In single-step spatial rea-
soning tasks— Piece Localization ,Connection Ver-
ification , and Anomaly Detection —most VLMs
surpass the p-value-based critical value, indicat-
ing emerging competence in basic spatial reason-
ing. However, strong performance remains concen-
trated in only a few models, particularly reasoning-
enhanced proprietary models and the latest open-
source InternVL3 series. This disparity becomes
even more evident in the multi-step spatial reason-
ing task— Order Restoration , indicating that most
VLMs struggle with complex spatial reasoning.
In conclusion, Jigsaw-Puzzles effectively distin-
guishes VLMs across a spectrum of spatial reason-
ing capability—from basic understanding to com-
plex multi-step reasoning. As shown by the results
in Table 2, substantial room for improvement re-
mains, particularly in multi-step spatial reasoning.
Foundational Spatial Understanding Shapes
Reasoning Performance. We analyze task similar-
ity on Jigsaw-Puzzles by computing Pearson cor-
relation coefficients between each task and all oth-
ers, as proposed by Zhang et al. (2025), as shown
in Figure 5. The results show that performance
on the Missing Piece Selection task—a proxy for
spatial understanding, is strongly correlated with
performance on spatial reasoning tasks. In con-
trast, VLMs with weaker spatial understanding of-
ten struggle with reasoning tasks, with some per-
forming worse than random on reasoning-intensive
tasks. This pattern reflects the human cognitive
progression from perception to understanding to
reasoning, underscoring the foundational role of
spatial understanding in enabling higher-level spa-
tial reasoning in VLMs.
Spatial Reasoning Scales with VLM size. We
analyze the relationship between VLM size and
overall performance on Jigsaw-Puzzles. As shown
in Figure 6, our results reveal that VLM ac-
Figure 5: Task Similarity Heatmap. The heatmap il-
lustrates the pairwise correlation between tasks in our
benchmark, measured using Pearson correlation coeffi-
cients. Task names are abbreviated using the initials of
each word (e.g., Missing Piece Selection →MPS). The
suffixes _e and _h indicate the Easy and Hard settings,
respectively.
Figure 6: Relationship between VLM size and perfor-
mance on Jigsaw-Puzzles. Each point represents a VLM,
with its accuracy plotted against log-scaled parameter
size. A clear positive correlation is observed both across
and within model families, indicating that larger models
tend to exhibit stronger performance.
curacy consistently increases with model size,
both across all models and within specific fami-
lies (e.g., InternVL3, Qwen2.5-VL). This positive
correlation suggests that spatial reasoning capa-
bility—like other cognitive competencies (Wang
et al., 2024b)—benefits from larger model capacity,
which scales with parameter count.
Reasoning-Enhanced Models Show Superior
Spatial Reasoning Performance. To assess the
spatial reasoning capability of reasoning-enhanced
VLMs, we evaluate Gemini-2.5-Flash-Thinking,
Gemini-2.5-Pro, Kimi-VL-A3B-Thinking andFigure 7: An example of self-correction. Red shows the
initial incorrect answer generated by Gemini-2.5-Pro;
Blue indicates the ground-truth answer; Green illustrates
the model’s self-correction process.
QvQ-72B-Preview. Except for Gemini-2.5-Pro,
each model has a corresponding base version for
comparison. As shown in Table 2, these enhanced
VLMs consistently achieve higher overall accuracy.
For example, Kimi-VL-A3B-Thinking improves
from 42.04% to 44.43%, and Gemini-2.5-Flash-
Thinking rises from 65.86% to 72.42%. Although
QvQ-72B-Preview overall underperforms Qwen2-
VL-72B, it achieves better results on spatial rea-
soning tasks. Notably, Gemini-2.5-Pro achieves
the highest overall accuracy (77.14%) among all
VLMs tested. Furthermore, the largest improve-
ments occur in the multi-step spatial reasoning
task, Order Restoration , where reasoning-enhanced
VLMs outperform their base counterparts more sub-
stantially than in single-step tasks. To explain this,
we analyze cases where only Gemini-2.5-Pro an-
swers correctly, with Figure 7 presenting one such
example. Gemini-2.5-Pro demonstrates a form of
self-correction: when the model’s initial predic-
tion is not among the provided options, it will re-
evaluate the visual input and revise its judgment.
This behavior, facilitated by the reduced answer
space under choice constraints, may contribute to
the superior performance of reasoning-enhanced
Figure 8: Evaluation of Order Restoration and Or-
der Generation tasks on Jigsaw-Puzzles-Lite. With-
out option constraints, VLM accuracy drops signifi-
cantly—peaking at just 30.00% and falling far short
of human performance.
models in the Order Restoration task.
Further Exploring Multi-Step Spatial Reason-
ing in VLMs. To further evaluate VLMs’ multi-
step spatial reasoning beyond the constraints of
predefined choices, we introduce the Order Gen-
eration task based on Jigsaw-Puzzles-Lite. In this
setting, VLMs must directly generate the correct
sequence of puzzle pieces without relying on an-
swer options, thereby more authentically simulat-
ing open-ended spatial reasoning. As shown in Fig-
ure 8, current VLMs consistently struggle with this
task—Gemini-2.5-Pro, the best-performing model,
achieves only 30.00% accuracy, in stark contrast
to 94.09% by human participants. This finding re-
veals that, despite exhibiting strong self-correction
behavior under option constraints, existing VLMs
face considerable challenges in autonomously con-
structing coherent spatial reasoning chains. This
highlights a significant gap between current VLMs
and human-level spatial reasoning in open-ended
scenarios.
5 Conclusion
We propose Jigsaw-Puzzles, a novel benchmark
for systematically evaluating the spatial reasoning
capability of VLMs in real-world visual scenes.
Through extensive experiments on 24 representa-
tive VLMs, we identify persistent gaps between
current VLMs and human-level spatial reason-
ing—especially in multi-step spatial reasoning
tasks. Jigsaw-Puzzles provides a scalable and cog-
nitively grounded benchmark to advance future re-search on spatial reasoning in VLMs.
Limitations
While Jigsaw-Puzzles provides a structured bench-
mark tailored for 2D spatial reasoning in static im-
ages, it currently does not address 3D perception,
temporal sequences, or embodied contexts—each
of which represents an important and orthogonal
axis of spatial cognition. We view this as a natural
next step and encourage future work to extend the
benchmark in these directions.
References
Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkin-
son, Hany Awadalla, Nguyen Bach, Jianmin Bao,
Alon Benhaim, Martin Cai, Vishrav Chaudhary, Con-
gcong Chen, and 1 others. 2025. Phi-4-mini tech-
nical report: Compact yet powerful multimodal lan-
guage models via mixture-of-loras. arXiv preprint
arXiv:2503.01743 .
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, and 1 others. 2023. Gpt-4 techni-
cal report. arXiv preprint arXiv:2303.08774 .
Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac,
Jiahui Yu, Radu Soricut, Johan Schalkwyk, An-
drew M Dai, Anja Hauth, Katie Millican, and 1 oth-
ers. 2023. Gemini: a family of highly capable multi-
modal models. arXiv preprint arXiv:2312.11805 .
Anthropic. 2024. The claude 3 model family: Opus,
sonnet, haiku.
Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wen-
bin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie
Wang, Jun Tang, and 1 others. 2025. Qwen2. 5-vl
technical report. arXiv preprint arXiv:2502.13923 .
Nitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel,
Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky,
and Roy Schwartz. 2023. Breaking common sense:
Whoops! a vision-and-language benchmark of syn-
thetic and compositional images. In Proceedings
of the IEEE/CVF International Conference on Com-
puter Vision , pages 2616–2627.
Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter,
Dorsa Sadigh, Leonidas Guibas, and Fei Xia. 2024.
Spatialvlm: Endowing vision-language models with
spatial reasoning capabilities. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 14455–14465.
Saurabh Dash, Yiyang Nan, John Dang, Arash Ah-
madian, Shivalika Singh, Madeline Smith, Bharat
Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter
Beller-Morales, and 1 others. 2025. Aya vision: Ad-
vancing the frontier of multilingual multimodality.
arXiv preprint arXiv:2505.08751 .Angang Du, Bohong Yin, Bowei Xing, Bowen Qu,
Bowen Wang, Cheng Chen, Chenlin Zhang, Chen-
zhuang Du, Chu Wei, and 1 others. 2025. Kimi-vl
technical report. arXiv preprint arXiv:2504.07491 .
Patrick Fissler, Olivia Caroline Küster, Daria Laptin-
skaya, Laura Sophia Loy, Christine AF V on Arnim,
and Iris-Tatjana Kolassa. 2018. Jigsaw puzzling taps
multiple cognitive abilities and is a potential protec-
tive factor for cognitive aging. Frontiers in aging
neuroscience , 10:408085.
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,
Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng,
Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji.
2024. Mme: A comprehensive evaluation benchmark
for multimodal large language models. Preprint ,
arXiv:2306.13394.
Grok. 2024. Bringing grok to everyone.
Toru Ishikawa and Nora S Newcombe. 2021. Why
spatial is special in education, learning, and everyday
activities.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin John-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, and 1
others. 2017. Visual genome: Connecting language
and vision using crowdsourced dense image anno-
tations. International journal of computer vision ,
123:32–73.
Brenden M Lake, Tomer D Ullman, Joshua B Tenen-
baum, and Samuel J Gershman. 2017. Building ma-
chines that learn and think like people. Behavioral
and brain sciences , 40:e253.
Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui
Wang, Ruimao Zhang, and Ying Shan. 2024. Seed-
bench: Benchmarking multimodal large language
models. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
13299–13308.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. 2014. Microsoft coco:
Common objects in context. In Computer vision–
ECCV 2014: 13th European conference, zurich,
Switzerland, September 6-12, 2014, proceedings,
part v 13 , pages 740–755. Springer.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Wang, Conghui He, Ziwei Liu, and 1 others. 2024.
Mmbench: Is your multi-modal model an all-around
player? In European conference on computer vision ,
pages 216–233. Springer.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
and Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-
tion answering benchmark requiring external knowl-
edge. In Proceedings of the IEEE/cvf conference
on computer vision and pattern recognition , pages
3195–3204.Mistral. 2025. Mistral small 3.1.
Atin Pothiraj, Elias Stengel-Eskin, Jaemin Cho, and
Mohit Bansal. 2025. Capture: Evaluating spatial
reasoning in vision language models via occluded
object counting. arXiv preprint arXiv:2504.15485 .
Qwen. 2024. Qvq: To see the world with wisdom.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark, and
1 others. 2021. Learning transferable visual models
from natural language supervision. In International
conference on machine learning , pages 8748–8763.
PmLR.
Yufan Ren, Konstantinos Tertikas, Shalini Maiti, Junlin
Han, Tong Zhang, Sabine Süsstrunk, and Filippos
Kokkinos. 2025. Vgrp-bench: Visual grid reasoning
puzzle benchmark for large vision-language models.
arXiv preprint arXiv:2503.23064 .
Christoph Schuhmann, Richard Vencu, Romain Beau-
mont, Robert Kaczmarczyk, Clayton Mullis, Aarush
Katta, Theo Coombes, Jenia Jitsev, and Aran Komat-
suzaki. 2021. Laion-400m: Open dataset of clip-
filtered 400 million image-text pairs. arXiv preprint
arXiv:2111.02114 .
Piyush Sharma, Nan Ding, Sebastian Goodman, and
Radu Soricut. 2018. Conceptual captions: A cleaned,
hypernymed, image alt-text dataset for automatic im-
age captioning. In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2556–2565.
Amanpreet Singh, Vivek Natarajan, Meet Shah,
Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,
and Marcus Rohrbach. 2019. Towards vqa models
that can read. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition ,
pages 8317–8326.
Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li,
Graham Neubig, and Xiang Yue. 2025. Visu-
alpuzzles: Decoupling multimodal reasoning eval-
uation from domain knowledge. arXiv preprint
arXiv:2504.10342 .
Ilias Stogiannidis, Steven McDonagh, and Sotirios A
Tsaftaris. 2025. Mind the gap: Benchmarking spatial
reasoning in vision-language models. arXiv preprint
arXiv:2503.19707 .
Kexian Tang, Junyao Gao, Yanhong Zeng, Haodong
Duan, Yanan Sun, Zhening Xing, Wenran Liu,
Kaifeng Lyu, and Kai Chen. 2025. Lego-puzzles:
How good are mllms at multi-step spatial reasoning?
arXiv preprint arXiv:2503.19990 .
Ashish V Thapliyal, Jordi Pont-Tuset, Xi Chen, and
Radu Soricut. 2022. Crossmodal-3600: A massively
multilingual multimodal evaluation dataset. arXiv
preprint arXiv:2205.12522 .Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhi-
hao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin
Wang, Wenbin Ge, and 1 others. 2024a. Qwen2-
vl: Enhancing vision-language model’s perception
of the world at any resolution. arXiv preprint
arXiv:2409.12191 .
Xinglin Wang, Peiwen Yuan, Shaoxiong Feng, Yiwei Li,
Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. 2024b.
Coglm: Tracking cognitive development of large lan-
guage models. arXiv preprint arXiv:2408.09150 .
Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-
enmaier. 2014. From image descriptions to visual
denotations: New similarity metrics for semantic in-
ference over event descriptions. Transactions of the
association for computational linguistics , 2:67–78.
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,
Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,
Weiming Ren, Yuxuan Sun, and 1 others. 2024.
Mmmu: A massive multi-discipline multimodal un-
derstanding and reasoning benchmark for expert agi.
InProceedings of the IEEE/CVF Conference on Com-
puter Vision and Pattern Recognition , pages 9556–
9567.
Zicheng Zhang, Xiangyu Zhao, Xinyu Fang, Chunyi
Li, Xiaohong Liu, Xiongkuo Min, Haodong Duan,
Kai Chen, and Guangtao Zhai. 2025. Redundancy
principles for mllms benchmarks. arXiv preprint
arXiv:2501.13953 .
Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu,
Shenglong Ye, Lixin Gu, Yuchen Duan, Hao Tian,
Weijie Su, Jie Shao, and 1 others. 2025. Internvl3:
Exploring advanced training and test-time recipes
for open-source multimodal models. arXiv preprint
arXiv:2504.10479 .
A Appendix
A.1 Examples of Other Spatial Reasoning
Benchmarks
Figure 9 and Figure 10 illustrate two representative
question types commonly used to evaluate spatial
reasoning capability of VLMs. The tested images
are not based on real-world scenes, which limits the
capability to evaluate spatial reasoning in VLMs
under realistic conditions.
A.2 Dataset Curation
Rules Pool. Figure 11 shows examples of images
that were rejected and accepted based on the fil-
tering rules, the following are the rules defined by
experts during the image selection process:
•Removing images containing explicit or vio-
lent content.
•Filtering out blurry, low-resolution, or visually
ambiguous images.Figure 9: An example of Mind the Gap (Stogiannidis
et al., 2025).
Figure 10: An example of LEGO-Puzzles (Tang et al.,
2025).
Figure 11: Top: examples of images rejected by expert-
defined filtering rules. Bottom: examples of high-
quality images that pass the rules.
•Excluding images lacking semantic clarity or
spatial structure.
•Discarding images with structural ambiguity
(e.g., multiple valid puzzle arrangements).
•Eliminating misaligned images or those with
overly small visual elements after cropping,
which hinder spatial reasoning.
Task-Specific Template. The following are de-
tailed templates for each task. Note that <image_x>
denotes a placeholder for the corresponding image
input.
Figure 12: Template of Missing Piece Selection , no-
tably, the templates for the Easy and Hard settings are
identical.
Figure 13: Template of Piece Localization (Easy) .
Figure 14: Template of Piece Localization (Hard) .
Figure 15: Template of Connection Verification .
Figure 16: Template of Anomaly Detection .Figure 17: Template of Order Restoration . Note: <op-
tion list> serves as a placeholder for the answer choices.
The text in parentheses is an example and should be
removed in actual use. One option is the correct answer,
while the remaining three are randomly drawn from the
other 23 candidates.
Figure 18: Template of Order Generation .
Hardware Setup for Evaluating VLMs. We eval-
uate open-source VLMs using hardware configu-
rations scaled to model size. Models with fewer
than 20B parameters run on a single NVIDIA A100
80GB GPU. Those between 20B and 40B use two
NVIDIA A100 80GB GPUs, while models exceed-
ing 40B are evaluated on four NVIDIA A100 80GB
GPUs to meet their greater memory and computa-
tional demands.