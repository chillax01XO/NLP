arXiv:2505.20707v1  [cs.CL]  27 May 2025Dissecting Physics Reasoning in Small Language Models:
A Multi-Dimensional Analysis from an Educational Perspective
Nicy Scaria*, Silvester John Joseph Kennedy*, Diksha Seth, Deepak Subramani
Computational and Data Sciences, Indian Institute of Science, India
nicyscaria@iisc.ac.in
Abstract
Small Language Models (SLMs) offer com-
putational efficiency and accessibility, making
them promising for educational applications.
However, their capacity for complex reason-
ing, particularly in domains such as physics,
remains underexplored. This study investigates
the high school physics reasoning capabilities
of state-of-the-art SLMs (under 4 billion pa-
rameters), including instruct versions of Llama
3.2, Phi 4 Mini, Gemma 3, and Qwen series.
We developed a comprehensive physics dataset
from the OpenStax High School Physics text-
book, annotated according to Bloom’s Taxon-
omy, with L ATEX and plaintext mathematical no-
tations. A novel cultural contextualization ap-
proach was applied to a subset, creating cultur-
ally adapted problems for Asian, African, and
South American/Australian contexts while pre-
serving core physics principles. Using an LLM-
as-a-judge framework with Google’s Gemini
2.5 Flash, we evaluated answer and reasoning
chain correctness, along with calculation accu-
racy. The results reveal significant differences
between the SLMs. Qwen 3 1.7B achieved high
‘answer accuracy’ ( ≈85%), but ‘fully correct
reasoning’ was substantially low ( ≈38%). The
format of the mathematical notation had a neg-
ligible impact on performance. SLMs exhibited
varied performance across the physics topics
and showed a decline in reasoning quality with
increasing cognitive and knowledge complex-
ity. In particular, the consistency of reasoning
was largely maintained in diverse cultural con-
texts, especially by better performing models.
These findings indicate that, while SLMs can
often find correct answers, their underlying rea-
soning is frequently flawed, suggesting an over-
reliance on pattern recognition. For SLMs to
become reliable educational tools in physics,
future development must prioritize enhancing
genuine understanding and the generation of
sound, verifiable reasoning chains over mere
answer accuracy.
1 Introduction
Small Language Models (SLMs) are neural lan-
guage models distinguished by their smaller param-
*Equal contributioneter counts and greater computational efficiency
relative to Large Language Models (LLMs). This
compact design allows SLMs to operate effectively
on common consumer hardware without requiring
specialized high-performance infrastructure that is
typically essential for LLMs. Notable examples
of SLMs, which generally range from several hun-
dred million to a few billion parameters, include
the Phi series (Gunasekar et al., 2023; Li et al.,
2023; Abdin et al., 2024b), Gemma (Team et al.,
2024, 2025), Pythia (Biderman et al., 2023), Llama
(Grattafiori et al., 2024), Qwen (Qwen et al., 2025;
Yang et al., 2025), and TinyLlama (Zhang et al.,
2024).
The capacity for reasoning in language models
has traditionally been associated with substantial
scale, often emerging in models with hundreds of
billions of parameters (Team et al., 2023; Hurst
et al., 2024). Initial research indeed suggested that
complex, multi-step reasoning was primarily a fea-
ture of these LLMs. However, this scale-centric
view is increasingly being contested by newer find-
ings (Yang et al., 2025; Srivastava et al., 2025).
The physics domain is characterized by a wide
array of sub-disciplines, each with distinct concep-
tual frameworks and problem-solving paradigms.
Effective engagement with physics requires a broad
spectrum of cognitive skills, from foundational re-
call of laws and definitions to the application of
principles, analysis of complex systems, evalua-
tion of evidence, and even creative problem for-
mulation, aligning with the hierarchical levels of
Bloom’s Taxonomy (Krathwohl, 2002). This pro-
cess is further complicated by the mathematical
problem solving required for physics proficiency,
adding another layer of complexity for language
models (Xu et al., 2025). A common limitation in
evaluating reasoning for such complex tasks is an
over-reliance on the correctness of the final answer,
often neglecting the reasoning steps or the chain
of thought that led to the solution (Srivastava et al.,
2025). For this study, we define physics reason-
ing as the ability to work effectively with physics
knowledge and problems by recalling facts andbasic principles, understanding concepts, apply-
ing physical laws through single-step or multi-step
processes (both conceptual and mathematical), an-
alyzing scenarios, explaining phenomena, solving
problems, and reaching well-supported conclusions
about physical systems.
SLMs enable efficient on-device processing
without requiring internet connectivity, signifi-
cantly improving data privacy through local compu-
tation (Sun et al., 2020; Abdin et al., 2024b). These
attributes are particularly advantageous within ed-
ucational frameworks where SLMs can promote
more equitable access to AI-driven learning re-
sources, a significant benefit in contexts with lim-
ited internet stability or financial constraints (Wei
et al., 2025; Schick and Schütze, 2021). The confi-
dentiality of student data, a critical factor in digital
learning environments, is significantly improved
through local processing, avoiding the privacy risks
associated with the API-based LLM service (Das
et al., 2025). Beyond these operational benefits,
effective application of SLMs in education also re-
quires appropriate pedagogical approaches, such
as contextualizing learning materials to specific
cultural or regional settings to enhance student en-
gagement and comprehension (Cordova and Lep-
per, 1996).
However, the performance of SLM reasoning
across multiple dimensions, such as evaluating the
reasoning chain, navigating the various cognitive
demands and types of knowledge of physics, and
adapting culturally contextualized educational con-
tent for diverse regions, remains largely underex-
plored, highlighting a significant gap in current
research.
To systematically evaluate these reasoning ca-
pabilities in SLMs, our approach included se-
lecting state-of-the-art models and developing a
specialized physics dataset from the OpenStax
High School Physics textbook (Urone and Hin-
richs, 2020). This dataset is annotated accord-
ing to Bloom’s Taxonomy to categorize questions
across cognitive and knowledge dimensions, span-
ning multiple physics topics. To assess the impact
of the representation format, we maintain parallel
versions with both the L ATEX notation and plain
text equivalents. Furthermore, we develop a novel
cultural contextualization approach, systematically
adapting a substantial subset of problems to incor-
porate authentic elements from underrepresented
regions in Asia, Africa, and South America while
preserving the underlying physics principles. Theevaluation framework assesses the correctness of
both the answers and the reasoning chains.
Based on the identified research gaps, our study
addresses the following research questions: (1)
How effectively can SLMs perform high school
physics reasoning? (2) Does mathematical symbol
representation alter the quality of physics reason-
ing? (3) Do SLMs exhibit consistent performance
across different physics topics? (4) How does cog-
nitive and knowledge complexity influence physics
reasoning in SLMs? (5) Can SLMs maintain con-
sistent physics reasoning chains across different
cultural contexts?
2 Related Work
Recent advances in training methodologies, such as
specialized fine-tuning of SLMs using reasoning-
intensive datasets (Li et al., 2023; Gunasekar et al.,
2023), knowledge distillation from larger models
(Guo et al., 2025), and targeted post-training com-
pression techniques (Egashira et al., 2024) are en-
hancing the ability of SLMs. However, the extent
to which these improvements translate across di-
verse domains and reasoning types requires sys-
tematic investigation, particularly in complex fields
like physics, where multi-step problem solving and
conceptual understanding are essential (Polverini
and Gregorcic, 2024; Kahaleh and Lopez, 2025).
Evaluating reasoning capabilities presents signif-
icant challenges. While rule-based parsing offers
precision, it struggles with format variations. Hu-
man evaluation remains the gold standard but faces
scalability constraints. LLM-as-a-Judge frame-
works have emerged as effective alternatives, with
studies showing a strong correlation between LLM
judgments and human assessments (Thakur et al.,
2024; Chiang and Lee, 2023; Gu et al., 2024;
Chang et al., 2024). Though promising, systematic
comparisons of evaluation methodologies specifi-
cally for assessing SLM reasoning capabilities re-
main limited.
Recent work by (Karim et al., 2025) demon-
strates that LLM reasoning processes can be af-
fected by contextual changes. While evaluation
frameworks like WorldView-Bench (Mushtaq et al.,
2025) exist for assessing cultural perspectives in
larger models, the specific impacts of contextualiza-
tion on SLM reasoning pathways and error patterns
represent an underexplored research area requiring
dedicated investigation.3 Methodology
To answer the research questions, we require a
suite of small language models, a dataset with
physics questions, a contextualization framework
to include culturally relevant components, infer-
ence with SLMs and evaluation of generated re-
sponses. In what follows, we describe each part of
our methodology.
3.1 Model selection
For the purposes of the study, SLMs are defined
as models with fewer than 4 billion (4B) param-
eters. The chosen models include instruct ver-
sions of Llama 3.2 (1B and 3B) (Grattafiori et al.,
2024), Phi 4 Mini (3.84B) (Abdin et al., 2024a)
and its reasoning-focused variant (3.84B) (Abdin
et al., 2025), Gemma 3 (1B and 4B) (Team et al.,
2025), Qwen 2.5 Instruct (1.5B) (Qwen et al.,
2025), Qwen 2.5 DeepSeek Distil (1.5B) (Guo
et al., 2025), and Qwen 3 Instruct (0.6B and 1.7B)
(Yang et al., 2025). This selection represents the
current state-of-the-art SLMs, with each model cho-
sen for its specific architectural innovations, train-
ing methodologies, or performance characteristics.
The inclusion of multiple model sizes from the
same families (Llama 3.2, Gemma 3, and Qwen
3) enables analysis of how parameter count affects
physics reasoning capabilities within the same ar-
chitecture choices of SLMs. Similarly, the com-
parison between Phi 4 Mini and its reasoning vari-
ant provides insight into how specialized training
for reasoning tasks affects performance on physics
problems.
3.2 Dataset creation
A dataset of physics questions was developed on
the basis of the end-of-chapter exercises in the
OpenStax High School Physics Textbook (Urone
and Hinrichs, 2020). This textbook encompasses
23 chapters covering diverse physics domains, in-
cluding Introduction, Mechanics, Electricity and
Magnetism, Waves and Acoustics, Thermodynam-
ics, Optics, and Modern Physics. The extraction
process resulted in a set of physics questions, en-
compassing various question types such as con-
ceptual items, critical thinking challenges, short
answer questions, true/false statements, extended
response tasks, and numerical problems. This di-
versity ensured a broad assessment across physics
topics and cognitive demands. The challenges and
specific processes involved in dataset generationare detailed in the Appendix A.1.
We converted mathematical equations from im-
ages to L ATEX format using OCR tools, followed
by a rigorous data cleaning process to address in-
consistencies in the source material. This itera-
tive refinement produced a final curated dataset
(Dopenstax ) containing 1,306 questions. We also
created a plain text version ( Dplaintext ) by systemat-
ically converting all L ATEX expressions to standard
text notation.
We annotated each physics problem according
to the knowledge and cognitive dimensions of
Bloom’s Taxonomy to enable a systematic anal-
ysis of the cognitive skills these tasks require from
SLMs. This annotation scheme was essential for
systematically evaluating how SLMs handle in-
creasingly complex reasoning tasks, particularly
as they progress from lower-order thinking skills
(Remember, Understand) to higher-order cogni-
tive processes (Apply, Analyze, Evaluate, Create).
Similarly, distinguishing between knowledge types
(Factual, Conceptual, Procedural, and Metacogni-
tive) allowed us to identify specific strengths and
limitations in how these models process and manip-
ulate different forms of physics knowledge. The
details and composition of the dataset is given in
Appendix A.2.
3.3 Contextualization of dataset
From the comprehensive dataset, we selected a
subset of 393 questions to investigate how contex-
tualization affects SLM performance. By incorpo-
rating diverse cultural and geographical elements
into standard physics problems, we could evaluate
whether these models exhibit consistent reasoning
abilities across differently contextualized versions
of identical physics concepts.
To ensure geographical and cultural diversity,
we developed a cultural context database drawing
from countries selected using the United Nations
Geoscheme, focusing on underrepresented regions
in Asia, Africa, South America, and Australia, com-
bining South America and Australia into a single
regional dataset. This was an intentional choice
due to the smaller number of countries within these
continents individually. For each region, we com-
piled authentic cultural elements including com-
mon names, festivals, landmarks, foods, transporta-
tion, sports, and traditions. Using Google’s Gemini
models with integrated search capabilities, we gen-
erated and verified this cultural information.
For each original question, we created five dis-tinct contextualized variations that maintained the
original physics principles while incorporating cul-
tural elements. This approach produced three
culturally adapted datasets ( DAsia,DAfrica , and
DSA_AU ), each containing 1,965 questions. De-
tails on the contextualization process, including
cultural database creation, verification procedures,
and the adaptation methodology, are provided in
Appendix B. An example of a contextual question
is given in Figure 3.
3.4 Model inference
We conducted inference using several models men-
tioned in Section 3.1 to address the research ques-
tions given in Section 1. For multiple choice ques-
tions, we supplied the question text and all options
in the prompt, requiring the models to generate
the selected option, explanation, and supporting
reasoning. For open-ended questions, the mod-
els generated both answers and detailed reasoning
without predefined options.
To perform a systematic comparison, six dis-
tinct evaluation sets were used as follows. (1)
the entire original dataset with LaTeX notation
(Dopenstax ) consisting of 1,306 physics questions
as our primary baseline; (2) a plain text version
of the entire dataset ( Dplaintext ); (3) the subset of
original questions selected for contextualization
(Dcontextual ) comprising 393 questions; and three
sets of culturally adapted versions including (4)
questions adapted with Asian cultural elements
(DAsia); (5) African cultural elements ( DAfrica );
and (6) South American and Australian cultural
elements ( DSA_AU ). This structured approach en-
ables us to systematically evaluate: (1) baseline
reasoning capabilities (using dataset 1, Dopenstax );
(2) mathematical notation effects (by comparing
datasets 1 and 2); (3) performance across cognitive
and knowledge dimensions (through the Bloom’s
Taxonomy annotations applied to dataset 1); (4)
variations across physics topics (using the topic cat-
egorizations within dataset 1); and (5) the impact
of cultural adaptation across different regions (by
comparing dataset 3 with datasets 4, 5, and 6).
3.5 Model evaluation
In this study, the LLM-as-a-judge model evalua-
tion approach with Google’s Gemini 2.5 Flash was
utilized. Multiple evaluation models were initially
tested, and Gemini was ultimately selected on the
basis of its superior balance of cost-effectiveness
and evaluation accuracy.The evaluation strategy implemented three spe-
cialized assessment prompts tailored to different
question formats: (1) Multiple choice: for re-
sponses where answer selection and reasoning
were clearly delineated, this prompt compared
the model’s selected option and reasoning directly
with the ground truth; (2) Multiple choice unstruc-
tured response: for free-form responses to multiple-
choice questions, this prompt first extracted the se-
lected option and reasoning from the generated text
before performing comparative assessment; and (3)
Open ended: for questions without predefined an-
swers, this prompt assessed whether the response
adequately addressed the required physics concepts
while allowing for valid alternative approaches.
All evaluation prompts assessed responses
across three dimensions: answer correctness (bi-
nary classification of correct/incorrect), reasoning
quality (categorized as fully correct, partially cor-
rect, or incorrect), and calculation accuracy (not
required, correct, or incorrect when calculations
were present). Reasoning quality distinguished be-
tween responses with complete understanding of
physics (fully correct), those with partly correct
reasoning (partially correct), and those containing
fundamental misunderstandings (incorrect).
To provide a more nuanced evaluation of rea-
soning capabilities, we implemented a weighted
reasoning accuracy measure. This metric assigned
different weights to each level of reasoning quality:
2 points for fully correct reasoning, 1 point for par-
tially correct reasoning, and 0 points for incorrect
reasoning. The weighted reasoning accuracy was
calculated as
WRA =Pn
i=1wi
2n×100% ,
where wirepresents the reasoning score (0, 1, or 2)
for the i-th question, and nis the total number of
questions.
The reliability of this automated approach was
verified by a manual review of randomly selected
samples across different types of questions and
physics topics for all SLMs. We examined approx-
imately 185 randomly selected questions covering
various question formats and topic areas to verify
the quality and consistency of the automated evalu-
ations. After benchmarking several LLMs for their
evaluation capabilities, Gemini 2.5 Flash emerged
as the best choice, demonstrating the most reliable
evaluation performance while maintaining compu-
tational cost.This methodology enabled a quantitative com-
parison of model performance in different nota-
tion formats, cognitive and knowledge dimensions,
physics topics, and cultural contexts. Detailed eval-
uation prompts are provided in Figure 4, 5 and 6.
4 Results and Analysis
Our findings address all research questions, reveal-
ing patterns in SLMs’ physics reasoning capabili-
ties across notation formats, topics, knowledge, and
cognitive demands, and cultural contexts. Table 1
summarizes the evaluation metrics across datasets.
4.1 How effectively can SLMs perform high
school physics reasoning?
The data in Table 1 reveals a striking gap between
answer and reasoning correctness for SLMs. While
Qwen 3 1.7B achieves the highest answer accu-
racy at 84.68%, its fully correct reasoning accu-
racy is only 38.25%, demonstrating that even the
best-performing model produces flawless reason-
ing chains in fewer than two fifths of cases. This
substantial discrepancy between getting the right
answer and showing entirely correct reasoning is
consistent across all SLMs.
The Phi 4 Reasoning 3.8B significantly outper-
forms the standard Phi 4 3.8B in both answer ac-
curacy (77.18% compared to 50.61%) and fully
correct reasoning (30.47% compared to 11.22%),
highlighting the impact of reasoning-focused train-
ing. Smaller models like Gemma 3 1B and Llama
3.2 1B demonstrate particularly low fully correct
reasoning rates of 9.65% and 10.41% respectively,
despite achieving answer accuracies above 40%.
The difference between weighted reasoning ac-
curacy (which considers both fully and partially
correct reasoning) and fully correct reasoning fur-
ther reveals issues with the reasoning chains gen-
erated by the SLMs. For instance, Qwen 3 1.7B
shows 84.99% weighted reasoning but only 38.25%
fully correct reasoning, indicating that in many
cases, models reach correct answers despite reason-
ing that contains errors or misconceptions.
Calculation accuracy shows considerable varia-
tion across models, with Qwen 3 1.7B (87.85%)
and Qwen 2.5 Distil 1.5B (83.67%) demonstrating
particularly strong mathematical capabilities. This
suggests that some models can execute calculations
correctly for the reasoning chain generated.
Our analysis revealed that in multiple choice
questions, SLMs often select options closest totheir derived answers, which partially explains the
higher answer correctness relative to reasoning
quality. Additionally, these models are typically
optimized during training to produce correct an-
swers rather than flawless reasoning chains, poten-
tially leading them to leverage pattern recognition
to select correct options even without complete
physical understanding. These factors collectively
contribute to the significant gap observed between
the models’ ability to select correct answers and
their capacity to produce sound reasoning chains.
4.2 Does mathematical symbol representation
alter the quality of physics reasoning?
The comparison between performance in Dopenstax
andDplaintext (Table 1) reveals that the format of
representation of mathematical symbols has a neg-
ligible effect on the quality of physics reasoning in
all the SLM tested.
For the ‘answer accuracy’ metric, most mod-
els show only slight variations between Dopenstax
andDplaintext . Qwen 3 1.7B achieves 84.68% with
Dopenstax and 86.13% with Dplaintext , while Phi 4
3.8B shows 50.61% with Dopenstax and 49.12% with
Dplaintext .
Similarly, for the ‘fully correct reasoning accu-
racy’ metric, there are only marginal differences
between notation formats. Qwen 3 1.7B achieves
38.25% with Dopenstax and 39.35% with plain text,
while Llama 3.2 3B shows 24.85% with Dopenstax
and 23.83% with Dplaintext .
The ‘weighted reasoning accuracy’ metric also
demonstrates this pattern, with most models show-
ing differences of roughly ±2% between the two
datasets. For the ‘calculation accuracy’ metric,
some models show slightly better performance with
Dplaintext (Phi 4 3.8B improves from 48.33% to
53.12%), while others perform marginally better
withDopenstax notation (Gemma 3 4B decreases
from 64.72% to 62.54%), but the differences re-
main relatively small.
The comparable performance across notation
formats indicates that modern SLMs effectively
process both L ATEX and plain text mathematical rep-
resentations in physics problems. This indicates
that mathematical symbol representation has only a
minor influence on the quality of physics reasoning
in these models.Table 1: Performance metrics of SLMs across different datasets.
Model Dopenstax Dplaintext Dcontextual DAsiaDAfrica DSA_AU
Answer Accuracy (%)
Qwen 3 0.6B 66.85 68.81 60.05 68.45 67.91 66.80
Gemma 3 1B 43.79 45.21 33.33 30.68 29.38 29.50
Llama 3.2 1B 45.71 46.59 33.84 34.66 34.98 34.36
Qwen 2.5 1.5B 66.76 64.52 55.73 56.71 58.96 56.66
Qwen 2.5 Distil 1.5B 69.61 69.50 73.79 67.81 67.31 67.66
Qwen 3 1.7B 84.68 86.13 87.53 87.59 86.61 86.72
Llama 3.2 3B 65.69 65.82 56.74 56.16 56.11 57.43
Phi 4 3.8B 50.61 49.12 46.56 47.12 45.93 44.86
Phi 4 Reasoning 3.8B 77.18 79.16 75.32 71.54 71.89 71.92
Gemma 3 4B 71.67 70.11 70.74 70.23 69.40 68.32
Fully Correct Reasoning Accuracy (%)
Qwen 3 0.6B 25.46 26.36 25.19 25.35 24.72 24.71
Gemma 3 1B 9.65 10.42 5.98 5.20 4.53 5.09
Llama 3.2 1B 10.41 11.23 7.25 8.50 8.25 8.92
Qwen 2.5 1.5B 22.82 22.11 17.68 17.89 18.89 18.68
Qwen 2.5 Distil 1.5B 23.39 23.64 23.66 24.65 24.64 24.48
Qwen 3 1.7B 38.25 39.35 39.44 38.93 38.72 38.95
Llama 3.2 3B 24.85 23.83 18.83 20.53 19.96 20.58
Phi 4 3.8B 11.22 12.15 12.60 12.64 12.55 12.37
Phi 4 Reasoning 3.8B 30.47 30.92 28.88 27.22 26.83 26.84
Gemma 3 4B 27.99 27.20 25.45 24.95 23.73 23.90
Weighted Reasoning Accuracy (%)
Qwen 3 0.6B 65.92 67.51 67.94 68.01 67.54 66.78
Gemma 3 1B 37.17 38.28 25.83 23.99 22.58 24.51
Llama 3.2 1B 38.51 40.96 31.30 33.35 32.15 33.50
Qwen 2.5 1.5B 61.95 60.99 53.05 54.59 56.64 55.63
Qwen 2.5 Distil 1.5B 67.31 67.70 68.70 68.31 68.71 68.47
Qwen 3 1.7B 84.99 86.48 87.91 87.01 86.84 87.10
Llama 3.2 3B 64.58 63.03 56.74 58.48 58.15 58.16
Phi 4 3.8B 45.94 46.82 46.95 47.28 46.92 46.81
Phi 4 Reasoning 3.8B 78.59 79.39 76.59 74.24 74.31 74.25
Gemma 3 4B 70.63 69.73 69.47 68.29 67.31 67.13
Calculation Accuracy (%)
Qwen 3 0.6B 69.92 71.15 70.07 72.35 72.95 70.21
Gemma 3 1B 19.09 20.00 22.11 20.05 18.54 20.58
Llama 3.2 1B 23.25 24.71 22.26 24.50 23.84 23.55
Qwen 2.5 1.5B 52.88 52.42 47.64 51.48 52.49 51.48
Qwen 2.5 Distil 1.5B 83.67 82.85 83.89 78.25 78.65 78.15
Qwen 3 1.7B 87.85 88.18 88.40 89.17 89.14 88.47
Llama 3.2 3B 50.43 48.54 48.67 51.21 51.02 53.16
Phi 4 3.8B 48.33 53.12 45.79 48.32 49.32 49.08
Phi 4 Reasoning 3.8B 78.36 80.72 78.31 78.43 76.83 77.27
Gemma 3 4B 64.72 62.54 68.51 65.81 63.76 65.12
4.3 Do SLMs exhibit consistent performance
across different physics topics?
Our analysis reveals notable variations in SLM per-
formance across different physics topics, as illus-
trated in the heat map (Figure 7 in the Appendix).
Most models demonstrate stronger performance
on Introduction and Thermodynamics topics while
struggling relatively more with Optics and Modern
Physics.
For fully correct reasoning, the variation be-
tween topics is substantial. Models generally pro-duce better reasoning chains for foundational top-
ics compared to those requiring advanced math-
ematical formalism or abstract conceptualization.
This variation in reasoning performance is more
pronounced than differences in answer accuracy,
suggesting that SLMs may be leveraging pattern
recognition rather than physics understanding for
more challenging topics.
Performance inconsistency is most evident in
smaller models. Gemma 3 1B and Llama 3.2 1B
have dramatic reasoning performance drops for
complex topics. In contrast, larger models andthose with specialized training demonstrate some
robustness. Qwen 3 1.7B and Phi 4 Reasoning
3.8B maintain more consistent reasoning capabili-
ties across different physics domains, though they
still show a decline in fully correct reasoning for
more abstract topics.
These topic-dependent variations likely stem
from differences in conceptual complexity, mathe-
matical demands, and the representation of training
data. These findings indicate that current SLMs
have not yet achieved physics reasoning capabili-
ties across different topics.
4.4 How do cognitive and knowledge
complexity influence the physics reasoning
in SLMs?
The complexity of cognitive and knowledge sig-
nificantly affects the performance of SLM physics
reasoning (Figures 8 and 9 in the appendix). Across
all models, there is a clear performance gradient
along both complexity dimensions, with capabil-
ities declining as tasks become more cognitively
demanding or require more sophisticated knowl-
edge application.
In the cognitive dimension, SLMs show strong
performance on lower-order thinking skills (Re-
member, Understand) but struggle progressively
with higher-order cognitive processes (Apply, Ana-
lyze, Evaluate, Create). The fully correct reasoning
metric reveals this pattern most prominently, with
even the best-performing model (Qwen 3 1.7B)
showing substantial degradation from Remember
(88.46%) to Create (35.71%) tasks. This decline
indicates that current SLMs have not yet developed
robust capabilities for complex reasoning processes
that require evaluation and creation.
The results of the knowledge dimension re-
veal that factual knowledge is handled most effec-
tively across models, while procedural knowledge
presents the greatest challenge. This pattern is evi-
dent in both reasoning and calculation metrics, sug-
gesting that SLMs struggle most with knowledge
requiring systematic application of procedures to
solve problems or reach conclusions. Models with
specialized training (Phi 4 Reasoning 3.8B) show
relatively better performance on procedural knowl-
edge, indicating that targeted training can partially
address these limitations.
Our findings suggest that performance degra-
dation is most pronounced at the higher levels of
Bloom’s Taxonomy, likely due to the compounding
nature of errors that becomes especially problem-atic for achieving fully correct reasoning. These
advanced tasks require coherent multi-stage rea-
soning and accurate execution of procedures at
each step, creating a cascade effect where even
minor inaccuracies in intermediate steps make it
increasingly difficult to maintain a coherent reason-
ing chain throughout the entire process.
4.5 Can SLMs maintain consistent physics
reasoning chains across different cultural
contexts?
SLMs demonstrate varying degrees of consistency
in physics reasoning when identical principles are
presented within different cultural frameworks (Ta-
ble 1). The data reveal important patterns in how
contextual variations affect reasoning chains.
Larger models and those with specialized train-
ing maintain remarkably stable reasoning in all con-
texts. Qwen 3 1.7B shows almost identical fully
correct reasoning accuracy in all cultural adapta-
tions (38.93-38.95% for Asian, African, and South
American/Australian contexts compared to 39.44%
for the baseline contextual dataset). Similarly,
Qwen 3 0.6B and Qwen 2.5 Distil 1.5B exhibit
minimal variation in reasoning performance in dif-
ferent cultural contexts.
Smaller models show modest variations in rea-
soning performance in different cultural contexts.
Gemma 3 1B shows differences of approximately
1.5 percentage points between the contextual base-
line dataset (5. 98%) and the culturally adapted
versions (4.53-5.20%). Llama 3.2 3B and Gemma
3 4B exhibit similar patterns with performance dif-
ferences of approximately 2-4 percentage points in
different contexts.
Interestingly, the weighted reasoning metric re-
veals that, while complete reasoning chains may
vary across contexts, models often maintain par-
tially correct reasoning chains. This suggests that
cultural adaptations primarily affect specific rea-
soning steps rather than fundamental physics under-
standing. Calculation accuracy remains particularly
stable across contexts for most models, suggest-
ing that mathematical operations maintain consis-
tency even when the same physics problems are
presented with different cultural elements.
These findings highlight a critical challenge for
educational applications of SLMs: while they can
often produce correct answers to physics problems,
generating completely sound reasoning chains re-
mains difficult. This distinction is particularly im-
portant in educational contexts, where the qualityof the explanation may be as valuable as the cor-
rectness of the answers.
5 Discussion
The evaluation of physics reasoning capabilities
of SLMs reveals several significant insights. A
critical observation is the substantial discrepancy
between answer accuracy and reasoning quality
across all models. Qwen 3 1.7B, despite not being
the largest model tested, consistently outperformed
larger counterparts like Gemma 3 4B and Phi 4
3.8B, achieving the highest answer accuracy across
all datasets. Qwen 3 0.6B also demonstrated no-
table capabilities despite its lower parameter count.
However, even the best-performing model reached
only 40% fully correct reasoning, highlighting a
disturbing pattern throughout the model spectrum.
These findings suggest that SLMs often rely on
pattern recognition rather than genuine physical
understanding, particularly in multiple choice sce-
narios. Specialized training significantly improves
reasoning capabilities, as demonstrated by Phi 4
Reasoning 3.8B outperforming the standard Phi
4 3.8B, indicating that targeted optimization can
improve reasoning without increasing the model
size.
The mathematical symbol representation has
minimal impact on the quality of reasoning in
all the SLMs evaluated. This notation-agnostic
performance indicates robust capabilities for han-
dling diverse mathematical representations, which
is valuable for educational applications where con-
tent appears in various formats. For educational
applications, this highlights that improvements in
SLM-assisted learning should prioritize enhanc-
ing fundamental reasoning capabilities rather than
adapting to specific mathematical formats.
Performance across Bloom’s Taxonomy shows
a clear decline from foundational to advanced rea-
soning tasks, with fully correct reasoning suffer-
ing most dramatically as cognitive demands in-
crease. This suggests SLMs struggle with the com-
pounding nature of errors in multi-step reasoning
chains, where small inaccuracies cascade through
the problem-solving process. Although factual
knowledge is handled adequately, conceptual and
procedural knowledge that is essential for physics
presents challenges across all models tested. In
physics education, particularly, where procedural
problem-solving and development of increasingly
complex cognitive skills are central to building ex-pertise, SLMs with flawed reasoning chains may
reinforce superficial understanding and impede stu-
dents’ progression toward advanced analytical and
creative problem-solving abilities, especially when
these models demonstrate correct answers despite
faulty reasoning processes.
SLMs exhibit notable performance variations
across physics domains, excelling in foundational
topics while struggling with advanced topics, likely
due to differing conceptual complexity and math-
ematical demands. Despite topic-dependent per-
formance, models maintain stable reasoning across
cultural contexts when physics principles remain
unchanged, with calculation accuracy particularly
consistent across contextual variations. This con-
textual robustness suggests promising applications
for supporting physics education in diverse set-
tings, underrepresented geographies and under-
served communities.
6 Conclusion
Our study provided a systematic and multi-
dimensional evaluation of the physics reasoning
capabilities of contemporary SLMs. Our inves-
tigation, spanning diverse SLMs, physics topics,
knowledge, and cognitive demands as per Bloom’s
Taxonomy, mathematical notation formats, and cul-
tural contextualizations, reveals critical insights
into the current strengths and limitations of these
SLMs. The findings consistently demonstrate a no-
table disparity between SLMs’ ability to produce
correct final answers and their capacity for generat-
ing entirely sound reasoning chains.
The implications of these findings are signifi-
cant for both the development of SLMs and their
application in educational settings. For SLM ad-
vancement, efforts should prioritize enhancing gen-
uine physics understanding, multi-step reasoning
abilities, and the generation of coherent and cor-
rect reasoning chains, rather than solely optimizing
for final answer correctness. Specialized training
appears to be a promising avenue for such improve-
ments. This could involve developing sophisticated
verifiers capable of scrutinizing step-by-step de-
ductions and exploring methods to improve the
grounding of SLM outputs in fundamental physical
laws and validated knowledge. Investigating hy-
brid architectures that synergize SLMs with other
reasoning paradigms, such as symbolic systems or
knowledge graphs, also presents a promising di-
rection. For physics education, while SLMs offerpotential benefits in terms of accessibility, privacy,
and contextual adaptability, educators and develop-
ers must exercise caution. The tendency of SLMs
to provide correct answers despite flawed reasoning
could inadvertently reinforce superficial learning if
not carefully managed. The quality of explanatory
reasoning is paramount in educational tools.
With continued focus on reasoning quality rather
than mere answer correctness, SLMs have the po-
tential to evolve from pattern matching systems to
genuine reasoning assistants. By addressing the
fundamental limitations identified in this study, fu-
ture developments could transform these efficient
models into valuable educational tools that support
physics education across different dimensions and
resource settings, potentially expanding access to
quality physics instruction where specialized teach-
ing resources remain limited.
Limitations
The LLM-as-a-judge evaluation method, while
scalable for rapidly evolving models, has inher-
ent limitations such as potential evaluator bias and
reduced nuance compared to resource-intensive hu-
man expert review. However, human evaluation is
not scalable with new models arriving fast and con-
sidering the range of reasoning tasks. Furthermore,
the cultural contextualization, though regionally
diverse, was not globally exhaustive. The country-
level focus might have missed finer local nuances
and the full depth of cultural integration.
Ethics Statement
We obtained necessary permissions from OpenStax
for the use of their high school physics textbook
content in our evaluation of SLMs. The dataset
creation and model evaluation processes were de-
signed to respect intellectual property rights while
facilitating research on physics reasoning capabili-
ties.
References
Marah Abdin, Sahaj Agarwal, Ahmed Awadallah,
Vidhisha Balachandran, Harkirat Behl, Lingjiao
Chen, Gustavo de Rosa, Suriya Gunasekar, Mo-
jan Javaheripi, Neel Joshi, and 1 others. 2025.
Phi-4-reasoning technical report. arXiv preprint
arXiv:2504.21318 .
Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien
Bubeck, Ronen Eldan, Suriya Gunasekar, Michael
Harrison, Russell J Hewett, Mojan Javaheripi, PieroKauffmann, and 1 others. 2024a. Phi-4 technical
report. arXiv preprint arXiv:2412.08905 .
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan,
Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harki-
rat Behl, and 1 others. 2024b. Phi-3 technical report:
A highly capable language model locally on your
phone. arXiv preprint arXiv:2404.14219 .
Stella Biderman, Hailey Schoelkopf, Quentin Gregory
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
lahan, Mohammad Aflah Khan, Shivanshu Purohit,
USVSN Sai Prashanth, Edward Raff, and 1 others.
2023. Pythia: A suite for analyzing large language
models across training and scaling. In International
Conference on Machine Learning , pages 2397–2430.
PMLR.
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,
Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,
Cunxiang Wang, Yidong Wang, and 1 others. 2024.
A survey on evaluation of large language models.
ACM transactions on intelligent systems and technol-
ogy, 15(3):1–45.
Cheng-Han Chiang and Hung-Yi Lee. 2023. Can large
language models be an alternative to human evalua-
tions? In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 15607–15631.
Diana I Cordova and Mark R Lepper. 1996. Intrin-
sic motivation and the process of learning: Bene-
ficial effects of contextualization, personalization,
and choice. Journal of educational psychology ,
88(4):715.
Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu.
2025. Security and privacy challenges of large lan-
guage models: A survey. ACM Computing Surveys ,
57(6):1–39.
Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He,
and Martin Vechev. 2024. Exploiting llm quantiza-
tion. In Advances in Neural Information Processing
Systems , volume 37, pages 41709–41732. Curran As-
sociates, Inc.
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten,
Alex Vaughan, and 1 others. 2024. The llama 3 herd
of models. arXiv preprint arXiv:2407.21783 .
Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,
Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan
Shen, Shengjie Ma, Honghao Liu, and 1 others.
2024. A survey on llm-as-a-judge. arXiv preprint
arXiv:2411.15594 .
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
César Teodoro Mendes, Allie Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero Kauffmann, Gus-
tavo de Rosa, Olli Saarikivi, and 1 others. 2023.
Textbooks are all you need. arXiv preprint
arXiv:2306.11644 .Daya Guo, Dejian Yang, Haowei Zhang, Junxiao
Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-
rong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.
Deepseek-r1: Incentivizing reasoning capability in
llms via reinforcement learning. arXiv preprint
arXiv:2501.12948 .
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam
Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow,
Akila Welihinda, Alan Hayes, Alec Radford, and 1
others. 2024. Gpt-4o system card. arXiv preprint
arXiv:2410.21276 .
Rabih Kahaleh and Victor Lopez. 2025. Evaluat-
ing large language models in high school physics
education: addressing misconceptions and foster-
ing conceptual understanding. Physics Education ,
60(2):025013.
Aabid Karim, Abdul Karim, Bhoomika Lohana, Matt
Keon, Jaswinder Singh, and Abdul Sattar. 2025.
Lost in cultural translation: Do llms struggle with
math across cultural contexts? arXiv preprint
arXiv:2503.18018 .
David R Krathwohl. 2002. A revision of bloom’s taxon-
omy: An overview. Theory into practice , 41(4):212–
218.
Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie
Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023.
Textbooks are all you need ii: phi-1.5 technical report.
arXiv preprint arXiv:2309.05463 .
Abdullah Mushtaq, Imran Taj, Rafay Naeem, Ibrahim
Ghaznavi, and Junaid Qadir. 2025. Worldview-
bench: A benchmark for evaluating global cultural
perspectives in large language models. arXiv preprint
arXiv:2505.09595 .
Giulia Polverini and Bor Gregorcic. 2024. How under-
standing large language models can inform the use
of chatgpt in physics education. European Journal
of Physics , 45(2):025701.
Qwen, :, An Yang, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan
Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin
Yang, Jiaxi Yang, Jingren Zhou, and 25 others.
2025. Qwen2.5 technical report. arXiv preprint
arXiv:2412.15115 .
Timo Schick and Hinrich Schütze. 2021. It’s not just
size that matters: Small language models are also few-
shot learners. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies , pages 2339–2352.
Gaurav Srivastava, Shuxiang Cao, and Xuan Wang.
2025. Towards reasoning ability of small language
models. arXiv preprint arXiv:2502.11569 .Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,
Yiming Yang, and Denny Zhou. 2020. Mobilebert: a
compact task-agnostic bert for resource-limited de-
vices. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
2158–2170.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-
Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, Katie Mil-
lican, and 1 others. 2023. Gemini: a family of
highly capable multimodal models. arXiv preprint
arXiv:2312.11805 .
Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya
Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin,
Tatiana Matejovicova, Alexandre Ramé, Morgane
Rivière, and 1 others. 2025. Gemma 3 technical
report. arXiv preprint arXiv:2503.19786 .
Gemma Team, Thomas Mesnard, Cassidy Hardin,
Robert Dadashi, Surya Bhupatiraju, Shreya Pathak,
Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale,
Juliette Love, and 1 others. 2024. Gemma: Open
models based on gemini research and technology.
arXiv preprint arXiv:2403.08295 .
Aman Singh Thakur, Kartik Choudhary, Venkat Srinik
Ramayapally, Sankaran Vaidyanathan, and Dieuwke
Hupkes. 2024. Judging the judges: Evaluating align-
ment and vulnerabilities in llms-as-judges. arXiv
preprint arXiv:2406.12624 .
Paul Peter Urone and Roger Hinrichs. 2020. Physics .
OpenStax, Houston, Texas.
Yumou Wei, Paulo Carvalho, and John Stamper. 2025.
Small but significant: On the promise of small lan-
guage models for accessible aied. arXiv preprint
arXiv:2505.08588 .
Xin Xu, Tong Xiao, Zitong Chao, Zhenya Huang, Can
Yang, and Yang Wang. 2025. Can LLMs solve longer
math word problems better? In The Thirteenth Inter-
national Conference on Learning Representations .
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang
Gao, Chengen Huang, Chenxu Lv, and 1 others.
2025. Qwen3 technical report. arXiv preprint
arXiv:2505.09388 .
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and
Wei Lu. 2024. Tinyllama: An open-source small
language model. arXiv preprint arXiv:2401.02385 .A Dataset Development and Annotation
A.1 Dataset extraction and preprocessing
A challenge encountered during the data extrac-
tion phase was the representation of mathematical
equations, which were predominantly embedded
as images rather than text within the exercise docu-
ments. To address this challenge, we used multiple
tools, including specialized optical character recog-
nition (OCR) models (Google Vision API, Math-
pix), to convert equation images into L ATEX format.
This approach significantly reduced the need for
manual transcription of mathematical notation and
preserved the integrity of the mathematical content.
Following the extraction of questions, answer
choices, correct answers, and the reasoning for
why the answer is correct from the textbook and
teacher resources from OpenStax (which served
as our ground truth for evaluating the model re-
sponses), a comprehensive data cleaning and re-
finement process was implemented. This system-
atic review, which combined automated checks and
manual verification, found several inconsistencies
in the source material. These included references
to non-existent textbook figures in the extracted
context, explanations with logical flaws or incom-
plete reasoning, and questions that lacked sufficient
standalone information as they referenced specific
textbook sections. Each question was meticulously
evaluated to ensure clarity, correctness, and com-
pleteness. Entries with irreparable flaws were re-
moved, and others were revised to meet the quality
standards required for the present study.
A.2 Bloom’s taxonomy annotations
We annotated each physics problem according to
the revised Bloom’s Taxonomy (Krathwohl, 2002),
which consists of two dimensions:
Cognitive Process Dimension:
•Remember: Retrieving relevant knowledge
from long-term memory, such as recalling
facts, basic concepts, or definitions
•Understand: Constructing meaning from in-
structional materials, including interpreting,
summarizing, and explaining ideas
•Apply: Using procedures or learned methods
in a given situation to solve problems or carry
out tasks•Analyze: Breaking down information into
components, identifying relationships or pat-
terns, and understanding structure and func-
tion
•Evaluate: Judging or determining the value
of material or methods based on criteria or
standards
•Create: Generating new ideas, products, or
structures by combining elements into a co-
herent or functional whole
Knowledge Dimension:
•Factual: The basic elements or facts students
must know to solve problems, including ter-
minology and specific details
•Conceptual: The interrelationships between
elements, such as theories, principles, and
models, that enable function within a domain
•Procedural: Knowing how to perform tasks,
techniques, and methods, and when to apply
them
•Metacognitive: Knowledge about one’s own
cognition and how to regulate it, including
self-awareness of learning strategies
The distribution of questions across the Cogni-
tive Process and Knowledge Dimensions revealed
important characteristics of high school physics
education and resulting limitations in our dataset:
Table 2: Distribution of questions across Cognitive Pro-
cess Dimension
Cognitive Level Percentage Count
Remember 18.3% 239
Understand 26.1% 341
Apply 29.5% 385
Analyze 16.7% 218
Evaluate 7.4% 97
Create 2.0% 26
As evidenced by this distribution, the dataset
contained questions spanning different levels of
cognitive skills, but with notable limitations.
Lower-order cognitive processes (Remember, Un-
derstand, Apply) accounted for approximately 74%
of the questions, while higher-order processes (An-
alyze, Evaluate, Create) comprised only 26%. ThisTable 3: Distribution of questions across Knowledge
Dimension
Knowledge Type Percentage Count
Factual 16.2% 212
Conceptual 42.5% 555
Procedural 41.3% 539
Metacognitive 0% 0
distribution reflects the typical emphasis in high
school physics education.
Notably, no questions were classified as address-
ing metacognitive knowledge. This absence re-
flects the traditional focus of high school physics
curricula on factual, conceptual, and procedural
knowledge rather than on developing students’
awareness of their own cognitive processes. Ad-
ditionally, metacognitive questions are challeng-
ing to assess in standardized formats and are often
addressed through reflective exercises or learning
journals rather than end-of-chapter problems.
A.3 Topic composition in the dataset
The distribution of physics topics in our dataset
reflects the comprehensive coverage of a typical
high school physics curriculum. Table 4 shows
the percentage and count breakdown of questions
across different physics domains.
Table 4: Distribution of questions across physics topics
Physics Topic Percentage Count
Introduction 4.7% 61
Mechanics 32.5% 424
Electricity & Mag-
netism21.9% 286
Thermodynamics 8.6% 112
Waves & Acoustics 10.2% 133
Optics 12.3% 161
Modern Physics 9.8% 129
This topic distribution ensured comprehensive
coverage of the physics curriculum, allowing us
to evaluate SLM performance across the full spec-
trum of physics concepts typically encountered in
high school education. The contextualization pro-
cess maintained this topic distribution in each cul-
turally adapted dataset, ensuring that comparative
analyses across different cultural contexts were not
confounded by variations in topic coverage.
The Introduction category includes foundationalconcepts such as scientific notation, measurement,
and dimensional analysis. Mechanics covers mo-
tion, forces, energy, and momentum. Electricity &
Magnetism encompasses electric charge, current,
circuits, and magnetic fields. Thermodynamics
includes heat, temperature, and the laws of thermo-
dynamics. Waves & Acoustics covers mechanical
waves, sound, and basic wave phenomena. Op-
tics includes light, mirrors, lenses, and optical in-
struments. Modern Physics covers topics such as
quantum mechanics, atomic physics, and nuclear
physics.
B Cultural Contextualization
Methodology
Our cultural contextualization approach required
developing comprehensive regional databases to
ensure authentic representation. Countries were se-
lected systematically based on the United Nations
Geoscheme1, with particular emphasis on under-
represented regions. We organized cultural infor-
mation into three distinct regional datasets:
1.Asian Context: Included information from
countries such as India, China, Indonesia,
Philippines, and many more (51 countries in
total)
2.African Context: Incorporated elements
from Nigeria, Kenya, South Africa, Ethiopia,
and many more (58 countries in total)
3.South American and Australian Context:
Featured Brazil, Argentina, Colombia, Peru,
Australia, and many more (41 countries in
total)
For each country, we compiled structured infor-
mation on common names and honorifics, cultural
festivals and celebrations, geographical landmarks
and natural features, local foods and culinary tra-
ditions, region-specific modes of transportation,
popular sports and recreational activities, cultural
traditions, rituals, and practices, and local indus-
tries and occupations.
We selected Google’s Gemini 2.5 Pro model
for generating cultural information because of
Google’s international presence and search index
across virtually all countries provided the model
with exposure to authentic cultural elements from
the different countries across the regions. The
1https://unstats.un.org/unsd/methodology/m49/model’s integrated ‘tool use’ capabilities with
Google Search enabled real-time retrieval and ver-
ification of cultural information. This search-
augmented generation approach, combined with
the model’s built-in reasoning capabilities, allowed
for systematic fact-checking and refinement of cul-
tural details during the generation process itself.
The generated cultural data was subsequently veri-
fied through additional targeted internet searches,
review of materials from relevant cultural heritage
websites and reputable online encyclopedias, and
cross-referencing with publicly available demo-
graphic and cultural information. The verification
process helped ensure cultural accuracy and repre-
sentativeness while avoiding stereotypical portray-
als. This carefully vetted cultural information was
provided as contextual input to guide the model
during the question generation process.
With these three regional cultural context
databases, we proceeded to the question adapta-
tion phase using Google’s Gemini 2.5 Flash model,
chosen for its ability to efficiently generate a large
volume of adaptations while consistently producing
high-quality, well-formed contextualized questions.
For each of the 393 selected original physics ques-
tions, the model was instructed to analyze the orig-
inal question, deconstruct its underlying physics
principles, and then integrate elements from our
cultural context database. A critical directive in
this process was to maintain physics fidelity. The
model was instructed to ensure that all contextual-
ized variations retained the original core physics
concepts, mathematical relationships, and formu-
lae (preserved in L ATEX notation), and the over-
all difficulty level. To foster diversity, we gen-
erated five distinct contextualized variations for
each original question. This approach produced
three culturally adapted datasets, each containing
393×5 = 1 ,965questions. The system main-
tained a history of previously generated questions
for each country within the region in each genera-
tion instance, which helped prevent repetition and
ensure authenticity, addressing the tendency of lan-
guage models to produce a similar output when
creating multiple items. For multiple choice ques-
tions, options and correct answers were generally
preserved, unless contextual adaptation required
modification for coherence. For open-ended ques-
tions, the underlying reasoning remained consistent
with the physics tested in the original problem.
A custom implementation managed the entire
workflow from question selection to final outputprocessing. The system maintained comprehen-
sive records of all generation attempts, producing
a parallel dataset of original and culturally contex-
tualized physics problems for comparative analysis
of the physics reasoning abilities of SLMs across
different cultural contexts. The prompts used for
multiple choice and open ended question genera-
tions are given in Figure 1 and 2.
CModel Inference and Evaluation details
C.1 Parameters for cultural context
generation
For the cultural context database creation and con-
textualized question generation phases, we used
slightly different parameters. Based on Google’s
recommendations for their models, we set temper-
ature to 0.2 and top_p to 0.95 when using Gemini
2.5 Pro (for cultural information generation) and
Gemini 2.5 Flash (for question adaptation). This
slightly higher temperature value provided an ap-
propriate balance between creativity and consis-
tency, allowing for diverse cultural elements and
problem formulations while maintaining coherence
and factual accuracy.
C.2 Parameters for SLM inference
For our primary experiments evaluating reasoning
capabilities in SLMs, we used consistent inference
parameters across all models to ensure fair compar-
ison. All SLMs were run with a temperature of 0.1
and top_p of 0.95. These low temperature settings
were selected to minimize randomness and pro-
mote deterministic outputs, which is particularly
important for assessing reasoning capabilities.
C.3 Parameters for evaluation
For our LLM-as-a-judge framework, we utilized
Google’s Gemini 2.5 Flash model with conserva-
tive sampling parameters (temperature = 0.1, top_p
= 0.95). These settings were selected to minimize
stochasticity in the evaluation process, ensuring
consistent and reliable assessment of both answer
correctness and reasoning quality across all model
outputs.Contextual_Multiple_ChoiceYou are tasked with creating culturally contextualized physics questions.Input:Question:{question}Options:{options}Ground Truth:- Correct Option: {correct_option}- Correct Option Answer: {correct_option_answer}- Reasoning: {ground_truth_reasoning}Context:{context}Question History:{question_history}Step-by-step Instructions:Step !: Carefully read and understand the physics question- Analyze the physical scenario described- Identify the core physics concepts and principles involved- Note any formulas or equations usedStep ": Examine the provided answer options- Understand what each option represents- Note the format and units of measurementStep #: Identify the correct answer and understand why it’s correct- Review the correct option letter/number- Study the reasoning explanation thoroughly- Understand the solution method and calculations involvedStep $: Review the question history for this country- Analyze previously generated questions in the question history- Note which cultural elements, scenarios, and contexts have already been used- Identify patterns to avoid repeatingStep %: Analyze the cultural context provided in the context JSON- Identify the country- Review the available cultural elements:* names* festivals* locations*f o o d s* transportation* sports* other_elements (clothing, music_and_dance, art_and_crafts, traditions_and_customs, etc.)- Prioritize cultural elements that have NOT been used in the question historyStep &: Create cultural variations of the question- While keeping the core physics problem identical:a) Replace Western/generic names with culturally speci’c names from the contextb) Change the setting to culturally relevant locations from the contextc) Incorporate cultural elements like festivals, foods, transportation, sports, etc.d) Use traditional objects, instruments, or clothing when applicablee) Maintain the same level of di(culty and mathematical relationshipsf) Preserve all mathematical formulas using LaTeX notationg) Ensure the new questions are distinct from those in the question historyStep ): Generate % distinct cultural variations- Ensure each variation uses di*erent combinations of cultural elements like names- Avoid repetition of the same cultural details across questions- Each variation should focus on di*erent aspects of the culture (e.g., one on festivals, one on sports, etc.)- Thoroughly avoid Western cultural elements and previously used scenarios- Each variation should feel authentic to the speci’ed countryStep +: Format the output as a JSON array with % objects- Include the country name in each object- Keep the original options, correct answer, reasoning, and answer text unchangedOutput format:[{"Country": "country name from context","ContextualQuestion": "Culturally adapted question text 1","ContextOptions": "same as original options if context doesn’t affect options", //modify if changes arerequired be made to options taking context into account like ["<first>", "<second>", ..]"ContextCorrectOption": "same as original","ContextReasoning": "same as original if context doesn’t affect reasoning", // modify if changes arerequired be made to reasoning taking context into account"ContextCorrectOptionAnswer": "same as original if context doesn’t affect answer"},// 4 more similar JSON objects with different cultural elements]IMPORTANT: Return ONLY this JSON object with no additional text.Figure 1: Prompt template used for creating 5 contextual multiple choice physics questions for each question in the
dataset.Contextual_Open_EndedYou are tasked with creating culturally contextualized physics questions.Input:Question:{question}Ground Truth:- Reasoning: {ground_truth_reasoning}Context:{context}Question History:{question_history}Step-by-step Instructions:Step !: Carefully read and understand the reference physics question- Analyze the physical scenario described- Identify the core physics concepts and principles involved- Note any formulas or equations usedStep ": Identify the expected approach to answering- Study the reasoning explanation thoroughly- Understand the solution method and explanations involvedStep #: Review the question history for this country- Analyze previously generated questions in the question history- Note which cultural elements, scenarios, and contexts have already been used- Identify patterns to avoid repeatingStep $: Analyze the cultural context provided in the context JSON- Identify the country- Review the available cultural elements:* names* festivals* locations*f o o d s* transportation* sports* other_elements (clothing, music_and_dance, art_and_crafts, traditions_and_customs, etc.)- Prioritize cultural elements that have NOT been used in the question historyStep %: Create cultural variations of the question- While keeping the core physics problem identical:a) Replace Western/generic names with culturally speci&c names from the contextb) Change the setting to culturally relevant locations from the contextc) Incorporate cultural elements like festivals, foods, transportation, sports, etc.d) Use traditional objects, instruments, or clothing when applicablee) Maintain the same level of di’culty and mathematical relationshipsf) Preserve all mathematical formulas using LaTeX notationg) Ensure the new questions are distinct from those in the question historyStep (: Generate % distinct cultural variations- Ensure each variation uses di)erent combinations of cultural elements like names- Avoid repetition of the same cultural details across questions- Each variation should focus on di)erent aspects of the culture (e.g., one on festivals, one on sports, etc.)- Thoroughly avoid Western cultural elements and previously used scenarios- Each variation should feel authentic to the speci&ed countryStep *: Format the output as a JSON array with % objects- Include the country name in each object- Keep the original reasoning unchangedOutput format:[{"Country": "country name from context","ContextualQuestion": "Culturally adapted question text 1","ContextReasoning": "same as original if context doesn’t affect reasoning", // modify ifchanges are required be made to reasoning taking context into account},// 4 more similar JSON objects with different cultural elements]IMPORTANT: Return ONLY this JSON object with no additional text.Figure 2: Prompt template used for creating 5 contextual open ended physics questions for each question in the
dataset.Cultural Contextualization ExampleOriginal Question:How many10!resistors must be connected in series to make an equivalent resistance of80!?Options:A. !", B. !, C. #", D. $"Asian Context (India):Aarav is building a circuit for his science project in Delhi. He has a box full of10!resistors.How many ofthese10!resistors mustheconnect in series toachieve a totalequivalent resistance of80!for a speci!cpart of his circuit?Options:A. !", B. !, C. #", D. $"African Context (Nigeria):Adetokunbo is setting up a sound system for a community gathering in Lagos. He needs a speci!c part ofthe circuit to have anequivalent resistance of80!.If he is connecting identical resistors of10!each inseries, how many10!resistors must be connected in series to make an equivalent resistance of80!?Options:A. !", B. !, C. #", D. $"South American Context (Brazil):João is working on the electrical system for a small ’barraca’ (stall) at a Festa Junina celebration. He needsas e c t i o no ft h ec i r c u i tt oh a v ea nequivalent resistance of80!.If he only has10!resistors available,how manyof these10!resistors must be connected in series to make an equivalent resistance of80!?Options:A. !", B. !, C. #", D. $"Correct Answer:B. !Figure 3: Example of a contextual question across different regions.Multiple_ChoiceYou are an expert physics teacher evaluating AI-generated responses to physics problems.Analyze the following with great care:Question: {question}Options: {options}Ground Truth:Correct Option: {correct_option}Correct Option Answer: {correct_option_answer}Reasoning: {ground_truth_reasoning}Model Response:Selected Option: {model_answer}Model Reasoning: {model_reasoning}Model Explanation: {model_explanation}Evaluation Instructions:Step !: Check if the model’s selected option ({model_answer}) matches the correct option ({correct_option}).Step ": Carefully trace through the model’s reasoning step-by-step.Step #: Compare the model’s explanation with the ground truth reasoning.Step $: For ALL responses, identify which parts of the reasoning are correct and which are incorrect.Step %: Categorize the response’s answer into one of these numeric categories:(!) Correct answer: Model’s selected option matches the correct option(&) Wrong answer: Model’s selected option does not match the correct optionStep ’: Categorize the reasoning into one of these numeric categories:(") Fully correct reasoning: All physics principles, concepts, and logic are correct(!) Partially correct reasoning: Some correct physics principles but with errors or misconceptions(&) Incorrect reasoning: Fundamental (aws in the physics concepts, formulas, or approachStep ): For calculations, use these categories:(") No calculations required for this problem(!) Calculations required and performed correctly(&) Calculations required but performed incorrectly or with errorsVeri*cation Guidelines:For partially correct reasoning, identify both the correct reasoning elements and the speci*c errors or misconceptionsFor incorrect reasoning, identify the fundamental (aws in the physics understandingFor calculation errors, specify exactly what went wrong in the mathematical stepsFor numerical problems: approximations within→!-"% of calculated values are reasonableIMPORTANT: Your response MUST be in the following JSON format:{"answer": <1 for correct or 0 for wrong>,"reasoning": <2 for fully correct, 1 for partially correct, or 0 for incorrect>,"calculations": <2 for no calculations needed, 1 for correct calculations, 0 for incorrectcalculations>,"explanation": "<brief explanation highlighting verification of correct and incorrect elementsin reasoning>"}Return ONLY this JSON object with no additional text.Figure 4: Evaluation prompt template used for assessing model responses to multiple choice physics questions.Multiple_Choice_Unstructured_ResponseYou are an expert physics teacher evaluating AI-generated responses to physics problems.Analyze the following with great care:Question: {question}Options: {options}Ground Truth:- Correct Option: {correct_option}- Correct Option Answer: {correct_option_answer}- Reasoning: {ground_truth_reasoning}Model Response:{model_response}Evaluation Instructions:Step!: Extract the model’s selected option from the Model Response text. Look for patterns like "ANSWER: [letter]", "The answer is[letter]", or clear indication of option selection.Step": Extract the model’s reasoning from the Model Response text. Look for sections marked "REASONING:" or explanatoryparagraphs.Step#: Verify if the extracted answer matches the correct option ({correct_option}).Step$: Carefully trace through the extracted reasoning step-by-step.Step%: Identify which parts of the reasoning are correct and which are incorrect.Step&: Categorize the response’s answer into one of these numeric categories:(!) Correct answer: Model’s selected option matches the correct option(’) Wrong answer: Model’s selected option does not match the correct optionStep(: Categorize the reasoning into one of these numeric categories:(") Fully correct reasoning: All physics principles, concepts, and logic are correct(!) Partially correct reasoning: Some correct physics principles but with errors or misconceptions(’) Incorrect reasoning: Fundamental)aws in the physics concepts, formulas, or approachStep*: For calculations, use these categories:(") No calculations required for this problem(!) Calculations required and performed correctly(’) Calculations required but performed incorrectly or with errorsVeri+cation Guidelines:Document what was extracted from the Model ResponseFor partially correct reasoning, identify both the correct reasoning elements and the speci+c errors or misconceptions.For incorrect reasoning, identify the fundamental)aws in the physics understanding.If no clear answer or reasoning can be extracted, categorize as’For calculation errors, specify exactly what went wrong in the mathematical steps.For numerical problems: approximations within!–"% of calculated values are reasonable.IMPORTANT: Your response MUST be in the following JSON format:{"answer": <1 for correct or 0 for wrong>,"reasoning": <2 for fully correct, 1 for partially correct, or 0 for incorrect>,"calculations": <2 for no calculations needed, 1 for correct calculations, 0 for incorrectcalculations>,"explanation": "<brief explanation highlighting verification of correct and incorrect elementsin reasoning>"}Return ONLY this JSON object with no additional text.Figure 5: Evaluation prompt template used for assessing unstructured responses to multiple choice physics questions.Open_EndedYou are an expert physics teacher evaluating AI-generated responses to open-ended physics problems. This question does not havea single correct answer, so evaluate whether the response adequately addresses the question.Question: {question}Expected Approach/ Reasoning: {ground_truth_reasoning}Topic Context:-P h y s i c sT o p i c :{ t o p i c }- Knowledge Type: {knowledge_dimension}- Cognitive Level: {cognitive_dimension}Model Response:{model_response}Evaluation Instructions:Step !: Determine if the model’s response actually addresses the question asked.Step ": Carefully trace through the model’s approach and reasoning.Step #: For performance tasks, check if all parts (Part A, B, C, etc.) are addressed.Step $: For numerical problems, verify calculations. For theoretical problems, verify concepts.Step %: Compare the model’s approach with the expected reasoning, but allow for valid alternative approaches.Step &: Categorize the response’s adequacy:(!) Adequate answer: Response appropriately addresses the question with valid physics(’) Inadequate answer: Response fails to address the question or contains major errorsStep (: Categorize the reasoning quality:(") Fully correct reasoning: Excellent physics understanding, complete and accurate(!) Partially correct reasoning: Good physics understanding with minor gaps or errors(’) Incorrect reasoning: Poor physics understanding or signi)cant errorsStep *: For calculations, use these categories:(") No calculations required for this problem(!) Calculations required and performed correctly(’) Calculations required but performed incorrectly or with errorsVeri)cation Guidelines:Accept valid alternative approaches that di+er from the expected reasoningFor experimental design, evaluate practicality and physics validityFor multi-part questions, assess completeness of coverageFocus on physics accuracy rather than exact match to expected answerFor numerical problems: approximations within !-"% of calculated values are reasonableIMPORTANT: Your response MUST be in the following JSON format:{"answer": <1 for adequate or 0 for inadequate>,"reasoning": <2 for fully correct, 1 for partially correct, or 0 for incorrect>,"calculations": <2 for no calculations needed, 1 for correct calculations, 0 for incorrectcalculations>,"explanation": "<analysis of how well the response addresses the question and physicsaccuracy>"}Return ONLY this JSON object with no additional text.Figure 6: Evaluation prompt template used for assessing model responses to open ended physics questions.Figure 7: SLM Performance Across Physics Topics. The heatmaps show: (a) Answer Accuracy, (b) Fully Correct
Reasoning Accuracy, (c) Weighted Reasoning Accuracy, and (d) Calculation Accuracy across physics topics.
Topics: 1: Introduction, 2: Mechanics, 3: Thermodynamics, 4: Waves and Acoustics, 5: Optics, 6: Electricity and
Magnetism, 7: Modern Physics.Figure 8: SLM Performance Across Bloom’s Taxonomy Cognitive Dimensions. The heatmaps show: (a) Answer
Accuracy, (b) Fully Correct Reasoning Accuracy, (c) Weighted Reasoning Accuracy, and (d) Calculation Accuracy
across cognitive dimensions. Cognitive dimensions: 1: Remember, 2: Understand, 3: Apply, 4: Analyze, 5:
Evaluate, 6: Create.Figure 9: SLM Performance Across Bloom’s Taxonomy Knowledge Dimensions. The heatmaps show: (a) Answer
Accuracy, (b) Fully Correct Reasoning Accuracy, (c) Weighted Reasoning Accuracy, and (d) Calculation Accuracy
across knowledge dimensions (Factual, Conceptual, Procedural).