HCQA-1.5 @ Ego4D EgoSchema Challenge 2025
Haoyu Zhang1 2, Yisen Feng1, Qiaohui Chu1 2, Meng Liu3, Weili Guan1,
Yaowei Wang1 2, Liqiang Nie1
1Harbin Institute of Technology (Shenzhen)2Pengcheng Laboratory
3Shandong Jianzhu University
{zhang.hy.2019, yisenfeng.hit, qiaohuichu8599, mengliu.sdu, honeyguan, nieliqiang }@gmail.com;
wangyw@pcl.ac.cn
Abstract
In this report, we present the method that achieves third
place for Ego4D EgoSchema Challenge in CVPR 2025. To
improve the reliability of answer prediction in egocentric
video question answering, we propose an effective exten-
sion to the previously proposed HCQA framework. Our
approach introduces a multi-source aggregation strategy
to generate diverse predictions, followed by a confidence-
based filtering mechanism that selects high-confidence an-
swers directly. For low-confidence cases, we incorporate a
fine-grained reasoning module that performs additional vi-
sual and contextual analysis to refine the predictions. Eval-
uated on the EgoSchema blind test set, our method achieves
77% accuracy on over 5,000 human-curated multiple-
choice questions, outperforming last year’s winning solu-
tion and the majority of participating teams. Our code will
be added at https://github.com/Hyu-Zhang/HCQA.
1. Introduction
Egocentric video understanding has become a key research
focus in the fields of embodied AI [23] and video-language
modeling [6, 20, 21], offering valuable insights into how
people interact with the world from a first-person perspec-
tive. Compared to third-person video, egocentric footage
captures fine-grained, context-rich visual signals, but it also
poses significant challenges [3, 4]. These include rapid
camera motion, limited field of view, and frequent oc-
clusions, all of which complicate downstream reasoning
tasks [25].
One such representative task is the EgoSchema [9] Chal-
lenge from the Ego4D benchmark [5], which focuses on
long-form video question answering. The goal is to se-
lect the correct answer from five multiple-choice options,
given a three-minute-long egocentric video and an associ-
ated question. Evaluation is conducted on the EgoSchemadataset [10], which includes over 5,000 human-curated
question–answer pairs spanning more than 250 hours of
real-world egocentric video. The dataset covers a wide
range of natural human activities and presents a particularly
challenging benchmark due to its long temporal context and
semantic ambiguity.
While recent progress in large language models (LLMs)
and vision-language models (VLMs) has advanced perfor-
mance on video question answering [16, 22], existing meth-
ods such as HCQA [24], often rely on a single LLM in the
final decision stage. This can lead to overconfidence in in-
correct predictions, especially when visual cues are sparse
or ambiguous. To address this, we propose a simple yet ef-
fective extension to the HCQA framework that improves an-
swer reliability through two key enhancements: (1) multi-
source aggregation, where multiple advanced LLMs are
used to produce diverse candidate answers; and (2) fine-
grained reasoning, where low-confidence outputs are selec-
tively reprocessed using visual and textual reasoning mod-
ules. This two-stage strategy enables more robust decision-
making in challenging scenarios without introducing signif-
icant architectural complexity.
By adopting this enhanced framework, our approach out-
performs last year’s winning solution as well as the majority
of participating teams in the current challenge, ultimately
securing third place in the final leaderboard. Specifically,
our pipeline achieves a notable improvement over HCQA,
increasing accuracy from 75% to 77%, as shown in Table 1.
2. Methodology
As illustrated in Figure 1, given a long egocentric video, we
first apply the previously proposed HCQA framework [24]
to obtain multiple diverse predictions. These initial outputs
serve as the basis for a two-stage decision-making process
aimed at improving the reliability of the final answer. In
the first stage, we perform multi-source aggregation to con-
solidate the results. High-confidence answers are directlyarXiv:2505.20644v1  [cs.CV]  27 May 2025Reason: The 
actions described 
in the captions involve concrete ……Answer: C is 
painting a picture on the desk.Confidence: 5HCQA
 Multi -source Aggregation 1
Reason: The actions 
described in the 
captions involve 
concrete ……Answer: C is 
painting a picture on the desk.
Confidence: 5
AnswerFine-grained Reasoning 2
High Conf.Vision Reasoning
Think Reasoning
High Conf.Low
Conf.Figure 1. An illustration of two-stage decision-making process.
selected as final outputs, while low-confidence ones are for-
warded to a fine-grained reasoning module for further re-
finement.
2.1. Multi-source Aggregation
To enhance prediction diversity, we replace the third-stage
LLM used in the original HCQA pipeline, such as GPT-4o,
with several state-of-the-art models, including Gemini-1.5-
Pro1, GPT-4.12, and Qwen2.5 [17]. The use of multiple
LLMs introduces complementary perspectives, which im-
proves robustness and coverage across different input cases.
To effectively integrate the outputs, we adopt a confidence-
based filtering mechanism. For each sample, predictions
with a confidence score higher than 4 (on a scale from 1 to
5) are considered reliable and retained as final answers. The
remaining low-confidence predictions are passed to the next
stage for additional processing.
2.2. Fine-grained Reasoning
For samples with low-confidence predictions, we introduce
a fine-grained reasoning module that includes two com-
plementary strategies: vision-based reasoning and thought-
based reasoning. The vision-based approach focuses on
re-analyzing visual evidence. Specifically, we extract 45
frames uniformly from the original video and input them
into the Qwen2.5-VL-72B model [1] with default settings
to generate refined predictions based on visual content.
In contrast, the thought-based approach emphasizes tex-
tual and contextual reasoning. It aggregates all available
textual information, including captions and summaries ex-
tracted by HCQA, as well as all predictions generated in the
multi-source aggregation stage. This information is then fed
into DeepSeek-R1 [7], which performs deeper thinking rea-
soning to produce an updated answer.
1https://deepmind.google/models/gemini/pro/ .
2https://openai.com/index/gpt-4-1/ .Table 1. Performance comparison of existing work and the top five
teams on the public leaderboard.
Method Rank Accuracy
mPLUG-Owl [18] - 0.31
LongViViT [11] - 0.33
InternVideo2 [15] - 0.41
LLoVi [19] - 0.50
VideoAgent [13] - 0.54
ProViQ [2] - 0.57
LifelongMemory [14] - 0.68
Gemini-1.5-Pro [12] - 0.71
GPT-4o [8] - 0.72
Qwen2.5-VL-72B [1] - 0.76
Noah’s Ark Lab 5 0.75
ccego 4 0.76
LPCIE (PCIE) 2 0.79
Reality Distortion 1 0.81
HCQA-1.5 (iLearn2.0) 3 0.77
Finally, we compare the outputs from both reasoning
strategies and select the one with the higher confidence
score as the final result. This combined approach allows
the system to recover reliable answers even from initially
uncertain predictions, improving the overall robustness and
accuracy of the framework.
3. Experiment
3.1. Performance Comparison
Table 1 presents the performance comparison of existing
methods and the top five teams on the public leaderboard
of the Ego4D EgoSchema Challenge. Our method, HCQA-
1.5 (iLearn2.0), achieves an accuracy of 77%, securing third
place among all submissions. It outperforms several strongSummary :Cputs buttons and dices onthetable, then arranges and picks
multiple cards .Clooks around and focuses onthetable and thecards while
picking andplacing cards continually .Man XandCinteract with thecards, with
Man Xpicking andplacing cards anddices periodically .Cconsistently counts
andarranges cards anddices onthetable. Cmoves around slightly, shuffling
andselecting cards repeatedly, while occasionally looking atthetable andthe
cards .Ccontinues toengage withthecards andother small items onthetable,
such aspellets, bottle tops, balls, andcloth pieces .Throughout theactivity, both
Cand Man Xcontinue picking, placing, and interacting with thecards and
playing theCatan game involving thedices .The process involves aroutine of
picking, shuffling, andarranging cards, with occasional interactions between C
andother individuals likeMan Kandperson J,based onputting andlooking at
cards .Finally, Ccontinues toarrange, pick, and putdown cards, aswell as
engage inplaying activities andorganizing items onthetable.Question :Describe themain objective and overall process depicted inthe
video, mentioning theprimary actions performed bycandtheperson.
option 0:Cispracticing magic tricks.
option 1:Cisplaying acard game withtheperson.
option 2:Currently, cisactively engaged inplaying astrategic board game .
option 3:Currently, cisdiligently sorting numerous playing cards .
option 4 : Currently, c is diligently constructing a delicate house of cards.
Reason :Throughout thevideo, Candother individuals repeatedly pick, arrange,
shuffle, andplace cards anddice onthetable. There areconsistent interactions
between Candatleast oneother person (Man X),including handing cards and
dice, looking atthecards, andactions specifically referencing playing the'Catan
game,' which isawell-known strategic board game that involves cards, dice,
andgame pieces .While there isclear organization andhandling ofcards, the
references toplaying Catan and theinteractions with dice and other people
indicate thatthemain objective isnotsimply sorting cards, performing magic, or
building ahouse ofcards, butrather playing astrategic board game, specifically
TheSettlers ofCatan .Therefore, thebest option isthatCisactively engaged in
playing astrategic board game.
Answer :option 2
Ground Truth :option 1… …
#C Cpicks a card… #C Cgives the card… #C Carranges cards…Caption
Figure 2. One failed example of our framework on EgoSchema subset.
baselines, including GPT-4o (72%) and Gemini-1.5-Pro
(71%), as well as other top teams such as Noah’s Ark Lab
(75%) and ccego (76%). The results demonstrate the effec-
tiveness of our multi-source aggregation and fine-grained
reasoning in handling complex, long-form egocentric video
question answering.
3.2. Ablation Study
Table 2 presents an ablation study assessing the contribu-
tions of different models at each stage of our HCQA-1.5
framework. In Stage 1, the individual models Gemini-1.5-
Pro, Qwen2.5, and GPT-4.1 yield varying levels of accu-
racy, with GPT-4.1 achieving the highest at 76.1%. Integrat-
ing their outputs by selecting the answer with the highest
confidence leads to further performance gains, demonstrat-
ing the advantage of model ensemble.
In Stage 2, Qwen2.5-VL-72B and DeepSeek-R1 per-
form similarly (76.8% vs. 76.6%) when applied to low-
confidence samples, confirming the value of fine-grained
reasoning in uncertain cases.
Combining both stages into the full pipeline results in
the highest overall accuracy of 77.3%, indicating that each
stage makes a positive contribution to the final performance.
3.3. Case Study
Figure 2 shows a failed example of our framework. The
model incorrectly selects option 2 (“C is actively engaged
in playing a strategic board game”) instead of the correct
option 1 (“C is playing a card game with the person”).
This mistake is mainly due to the model’s overreliance on
surface-level visual cues—such as the presence of dice and
cards—that are commonly associated with strategic board
games like “The Settlers of Catan”. However, the question
emphasizes not just the objects but the overall objective and
interaction between individuals.Table 2. Ablation study with different models of our framework.
Stage Model Accuracy
1Gemini-1.5-Pro 0.710
GPT-4.1 0.761
Qwen2.5 0.748
Integration 0.763
2Qwen2.5-VL-72B 0.768
DeepSeek-R1 0.766
- HCQA-1.5 0.773
While the summary clearly describes C and another per-
son repeatedly picking, handing, and arranging cards to-
gether, the model fails to capture this collaborative aspect
and instead focuses on categorizing the scene based on
game components. This suggests a weakness in understand-
ing social dynamics and intent, particularly when multiple
plausible interpretations share similar visual elements.
4. Conclusion
We propose an effective enhancement to the HCQA frame-
work for egocentric video question answering, integrat-
ing multi-source aggregation with fine-grained reasoning.
By selectively reasoning over low-confidence cases, our
method improves the reliability of final answers. Achieving
77% accuracy on the EgoSchema benchmark and ranking
third in the CVPR 2025 Challenge, our approach demon-
strates strong performance of our design.
References
[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin
Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, JunTang, et al. Qwen2. 5-vl technical report. arXiv preprint
arXiv:2502.13923 , 2025. 2
[2] Rohan Choudhury, Koichiro Niinuma, Kris M Kitani, and
L´aszl´o A Jeni. Zero-shot video question answering with pro-
cedural programs. arXiv preprint arXiv:2312.00937 , 2023.
2
[3] Yisen Feng, Haoyu Zhang, Yuquan Xie, Zaijing Li, Meng
Liu, and Liqiang Nie. Objectnlq@ ego4d episodic memory
challenge 2024. arXiv preprint arXiv:2406.15778 , 2024. 1
[4] Yisen Feng, Haoyu Zhang, Meng Liu, Weili Guan, and
Liqiang Nie. Object-shot enhanced grounding network for
egocentric video. arXiv preprint arXiv:2505.04270 , 2025. 1
[5] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d:
Around the world in 3,000 hours of egocentric video. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition , pages 18995–19012, 2022. 1
[6] Weili Guan, Xuemeng Song, Haoyu Zhang, Meng Liu,
Chung-Hsing Yeh, and Xiaojun Chang. Bi-directional het-
erogeneous graph hashing towards efficient outfit recom-
mendation. In Proceedings of the 30th ACM international
conference on multimedia , pages 268–276, 2022. 1
[7] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song,
Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi
Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning
capability in llms via reinforcement learning. arXiv preprint
arXiv:2501.12948 , 2025. 2
[8] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perel-
man, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Weli-
hinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.
arXiv preprint arXiv:2410.21276 , 2024. 2
[9] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra
Malik. Egoschema: A diagnostic benchmark for very long-
form video language understanding. Advances in Neural In-
formation Processing Systems , 36:46212–46244, 2023. 1
[10] Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra
Malik. Egoschema: A diagnostic benchmark for very long-
form video language understanding. Advances in Neural In-
formation Processing Systems , 36, 2024. 1
[11] Pinelopi Papalampidi, Skanda Koppula, Shreya Pathak,
Justin Chiu, Joe Heyward, Viorica Patraucean, Jiajun Shen,
Antoine Miech, Andrew Zisserman, and Aida Nematzdeh.
A simple recipe for contrastively pre-training video-first en-
coders beyond 16 frames. arXiv preprint arXiv:2312.07395 ,
2023. 2
[12] Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry
Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu
Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrit-
twieser, et al. Gemini 1.5: Unlocking multimodal under-
standing across millions of tokens of context. arXiv preprint
arXiv:2403.05530 , 2024. 2
[13] Xiaohan Wang, Yuhui Zhang, Orr Zohar, and Serena
Yeung-Levy. Videoagent: Long-form video understand-
ing with large language model as agent. arXiv preprint
arXiv:2403.10517 , 2024. 2[14] Ying Wang, Yanlai Yang, and Mengye Ren. Lifelongmem-
ory: Leveraging llms for answering queries in egocentric
videos. arXiv preprint arXiv:2312.05269 , 2023. 2
[15] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan
He, Guo Chen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun
Wang, et al. Internvideo2: Scaling video foundation mod-
els for multimodal video understanding. arXiv preprint
arXiv:2403.15377 , 2024. 2
[16] Yunxiao Wang, Meng Liu, Rui Shao, Haoyu Zhang, Bin
Wen, Fan Yang, Tingting Gao, Di Zhang, and Liqiang
Nie. Time: Temporal-sensitive multi-dimensional instruc-
tion tuning and benchmarking for video-llms. arXiv preprint
arXiv:2503.09994 , 2025. 1
[17] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang,
Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen
Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv
preprint arXiv:2505.09388 , 2025. 2
[18] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mplug-owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178 , 2023. 2
[19] Ce Zhang, Taixi Lu, Md Mohaiminul Islam, Ziyang Wang,
Shoubin Yu, Mohit Bansal, and Gedas Bertasius. A sim-
ple llm framework for long-range video question-answering.
arXiv preprint arXiv:2312.17235 , 2023. 2
[20] Haoyu Zhang, Meng Liu, Zan Gao, Xiaoqiang Lei, Yinglong
Wang, and Liqiang Nie. Multimodal dialog system: Rela-
tional graph-based context-aware question understanding. In
Proceedings of the 29th ACM international conference on
multimedia , pages 695–703, 2021. 1
[21] Haoyu Zhang, Meng Liu, Yuhong Li, Ming Yan, Zan Gao,
Xiaojun Chang, and Liqiang Nie. Attribute-guided collab-
orative learning for partial person re-identification. IEEE
Transactions on Pattern Analysis and Machine Intelligence ,
45(12):14144–14160, 2023. 1
[22] Haoyu Zhang, Meng Liu, Yaowei Wang, Da Cao, Weili
Guan, and Liqiang Nie. Uncovering hidden connections:
Iterative tracking and reasoning for video-grounded dialog.
arXiv preprint arXiv:2310.07259 , 2023. 1
[23] Haoyu Zhang, Meng Liu, Zixin Liu, Xuemeng Song, Yaowei
Wang, and Liqiang Nie. Multi-factor adaptive vision selec-
tion for egocentric video question answering. In Proceedings
of the 41st International Conference on Machine Learning ,
pages 59310–59328. PMLR, 2024. 1
[24] Haoyu Zhang, Yuquan Xie, Yisen Feng, Zaijing Li, Meng
Liu, and Liqiang Nie. Hcqa@ ego4d egoschema challenge
2024. arXiv preprint arXiv:2406.15771 , 2024. 1
[25] Haoyu Zhang, Qiaohui Chu, Meng Liu, Yunxiao Wang, Bin
Wen, Fan Yang, Tingting Gao, Di Zhang, Yaowei Wang,
and Liqiang Nie. Exo2ego: Exocentric knowledge guided
mllm for egocentric video understanding. arXiv preprint
arXiv:2503.09143 , 2025. 1