arXiv:2505.21396v1  [cs.CL]  27 May 2025Improving Research Idea Generation Through Data: An Empirical
Investigation in Social Science
Xiao Liu1, Xinyi Dong2, Xinyang Gao3, Yansong Feng1∗and Xun Pang4,5,6∗
1Wangxuan Institute of Computer Technology2Yuanpei College3School of Government
4School of International Studies5Institute for Carbon Neutrality
6Analytics Lab for Global Risk Politics
Peking University
{lxlisa,xinyang0614,fengyansong,xpang}@pku.edu.cn
dongxy@stu.pku.edu.cn
Abstract
Recent advancements in large language mod-
els (LLMs) have shown promise in generating
novel research ideas. However, these ideas of-
ten face challenges related to feasibility and
expected effectiveness. This paper explores
how augmenting LLMs with relevant data dur-
ing the idea generation process can enhance
the quality of generated ideas. We introduce
two ways of incorporating data: (1) providing
metadata during the idea generation stage to
guide LLMs toward feasible directions, and (2)
adding automatic validation during the idea se-
lection stage to assess the empirical plausibility
of hypotheses within ideas. We conduct experi-
ments in the social science domain, specifically
with climate negotiation topics, and find that
metadata improves the feasibility of generated
ideas by 20%, while automatic validation im-
proves the overall quality of selected ideas by
7%. A human study shows that LLM-generated
ideas, along with their related data and valida-
tion processes, inspire researchers to propose
research ideas with higher quality. Our work
highlights the potential of data-driven research
idea generation, and underscores the practical
utility of LLM-assisted ideation in real-world
academic settings.
1 Introduction
Recent advances in large language models (LLMs)
have demonstrated their potential to generate
domain-specific research ideas, with some stud-
ies suggesting that these ideas can exhibit greater
novelty than those proposed by human experts (Si
et al., 2024; Yamada et al., 2025). However, many
LLM-generated ideas suffer from practical limita-
tions: they may be infeasible to implement, lack
suitable datasets for validation, or have uncertain
effectiveness. For instance, an LLM might propose
investigating “the impact of diplomats’ childhood
*Corresponding authors.environmental experiences on their bargaining po-
sitions in UN climate negotiations” , which is an
interesting idea but lacks available data for empiri-
cal analysis.
Intuitively, if LLMs are provided with relevant
datasets, they could be better equipped to gener-
ate empirically grounded research ideas: those that
are not only novel but also feasible for experimen-
tation. Just as human researchers navigate trade-
offs between theoretical ambition and empirical
tractability when developing research ideas, LLMs
could benefit from this balancing act when data is
available. For example, if the LLM is aware of
the existence of records on climate conference at-
tendance, it might propose a more feasible study,
like“how the professional backgrounds of diplo-
mats influence their countries’ emission reduction
commitment ambitions. ”
In this paper, we investigate whether augmenting
LLMs with data during the research idea genera-
tion process can enhance the quality of generated
ideas . Data can not only help LLMs to generate
more feasible ideas, but also enable preliminary
validation of hypotheses within ideas. With ac-
cess to relevant datasets, LLMs can write code to
analyze the data and perform reasoning to assess
whether the hypotheses are supported by the avail-
able evidence. Although this validation is prelim-
inary and does not guarantee sound conclusions,
it provides valuable signals regarding whether the
ideas are likely to be effective .
As shown in Figure 1, the standard framework
for LLM ideation consists of three stages: liter-
ature search, idea generation, and idea selection.
Models first search related literature for a given
topic, then generate ideas based on the retrieved
literature, and finally rank and select the top ideas
as the output. We enhance this framework by in-
corporating data at two key stages: (1) During idea
generation, we provide metadata, such as dataset
descriptions, to guide models toward feasible re-Related 
LiteratureResearch 
Topic
Literature SearchResearch 
Question
Theory
HypothesesResearch Idea
Idea Generation Idea Selection
Top Ideas
Metadata ( §3)Automatic 
Validation ( §4)Inspire 
Researchers (§5)Figure 1: Overview of how we incorporate data into the research idea generation process.
search directions; and (2) during idea selection, we
integrate automatic validation to account for the
empirical plausibility of the proposed hypotheses
within ideas.
We conduct experiments in the domain of social
science, focusing specifically on topics related to
climate negotiations. To support the experiments,
we first collect and gather relevant datasets into
a unified CLIMATE DATABANK. To evaluate the
impact of metadata, we compare LLM-generated
ideas with and without access to the metadata of
CLIMATE DATABANK, and observe that incorpo-
rating metadata improves the feasibility by 20%
and the expected effectiveness by 18% in human
evaluation. Additionally, we find that automatic
validation improves the accuracy of idea ranking
by an average of 8%, and ideas selected with val-
idation are rated 7% higher in human evaluation
compared to those selected without validation.
Beyond assessing the quality of generated ideas,
we explore whether LLM-generated ideas, along
with their related data and validation processes,
can inspire human researchers to develop their own
ideas . In a study with 23 researchers, we find that
compared to traditional idea creation aided only by
the Internet, participants propose ideas of higher
quality when given the reference of LLM-generated
ideas. Feedback from participants indicates that
LLM-generated ideas and validation processes are
very helpful, with some researchers using them as
starting points for further refinement, which helps
broaden their thinking.
Our contributions are as follows: (1) We propose
two ways of integrating data into research idea gen-
eration: adding metadata in idea generation and
adding automatic validation in idea selection. (2)
Our experiments demonstrate that metadata and
automatic validation improve the quality of gener-
ated ideas, particularly in feasibility and expected
effectiveness. (3) The human study reveals thatLLM-generated ideas can inspire researchers to
propose higher-quality ideas. (4) We construct the
CLIMATE DATABANK to support future work in
data-driven research idea generation.
2 Data Collection
CLIMATE DATABANK Construction We first
collect data related to climate negotiations, con-
structing the CLIMATE DATABANK to facilitate the
following experiments. Our process begins with a
comprehensive literature review, identifying impor-
tant and commonly used datasets. We then collect
datasets of common variables from World Bank
Open Data *, and other datasets from their original
sources.
TheCLIMATE DATABANK is composed of three
primary types of data: (1) Textual data, which in-
cludes documents such as national communications
and high-level statements issued by various coun-
tries, enabling both qualitative analysis and text
mining. (2) Panel data, such as the Gross Domestic
Product (GDP) of each country over time, facili-
tating longitudinal analysis of trends over multiple
years. (3) Cross-sectional data, capturing static
attributes such as membership in the Alliance of
Small Island States (AOSIS), with all values stan-
dardized to the year 2025 for consistency.
CLIMATE DATABANK contains 22 datasets in
total, each stored in CSV format for ease of access.
The full list of datasets and corresponding data
descriptions is in Appendix Table 6.
Reference Paper Collection During the litera-
ture review, we also collect papers with clear hy-
potheses and replicable data. After manually re-
viewing 103 papers, we identify 8 papers that meet
these criteria, as shown in Appendix Table 7. These
papers, along with the corresponding data, are used
in §4, where models are asked to validate the hy-
*https://data.worldbank.org/potheses in these papers, and rank the ground-truth
ideas among LLM-generated ideas.
3Can Metadata Benefit Idea Generation?
This section explores the role of metadata in idea
generation. We first describe how social science
research ideas are structured and generated, then
explain how metadata is integrated into the gener-
ation process. Finally, we present both automatic
and human evaluation results.
3.1 Social Science Idea Generation
A typical social science research idea consists of
three components: a research question rq, a theory
th, and several hypotheses h(Powner, 2014; King
et al., 1994). As illustrated in the example of Fig-
ure 2 (right), the research question guides the study
by identifying the central issue to be explored. The
theory speculates on the answer to the research
question and explains why the proposed answer
is reasonable. The hypotheses identify observable
implications of the theory, i.e., things we would
observe if the theory is correct.
Given a research topic t, LLMs first conduct a
literature search and retrieve related literature L,
and then generate research ideas with the compo-
nents (rq, th,h)through the idea generation stage.
The generated ideas are then passed to the idea
selection stage to select the top-ranked ideas.
3.2 Incorporating Metadata into Idea
Generation
Figure 2 (left) shows how we incorporate meta-
data, which is concise dataset descriptions, into the
idea generation prompt along with the topic and
related literature. Each metadata entry summarizes
a dataset in CLIMATE DATABANK with one or two
sentences, including information like the meaning
of key variables, temporal coverage, and spatial
scope. The prompt informs LLMs that here are
existing data related to this topic , without strict
restriction on using the provided data. This ensures
that models can balance theoretical creativity with
empirical feasibility by themselves.
By exposing models to metadata early, we en-
courage data-informed ideation where the feasibil-
ity of measurement is considered. Note that in this
stage, we provide only the metadata, not the real
content of the data, avoiding models from conduct-
ing data dredging by finding patterns in data and
disguising them as hypotheses.3.3 Experimental Setup
Research Topics We generate 10 climate
negotiation-related research topics using GPT-
4o (Hurst et al., 2024), and manually verify them
to ensure their quality. The created topics are in
Appendix Table 8.
Methods We experiment with three prevalent re-
search idea generation methods: AI-Researcher (Si
et al., 2024), GPT-Researcher (Elovic, 2023), and
Chain-of-Ideas (Li et al., 2024a). Each method
first retrieves relevant literature and then generates
ideas, with detailed descriptions in Appendix C.
We preserve each method’s original design while
adding metadata to the idea generation prompt.
For each research topic, we generate 50 ideas per
method, then select the top 5 for evaluation using a
unified idea selection module.
Idea Selection As Si et al. (2024) demonstrate
that LLMs better assess ideas in pairwise ranking
than rating, we conduct a Swiss tournament for
idea selection. Over 5 rounds, ideas are paired by
similar accumulated scores, with LLMs ranking
each pair using the criteria below.
Idea Evaluation We assess idea quality using
four criteria motivated by previous works (Si et al.,
2024; Yang et al., 2024b): significance, novelty,
feasibility, and expected effectiveness (abbreviated
as exp. effectiveness). These criteria are used in
both idea selection and evaluation, with detailed
definitions in Appendix D. For automatic evalua-
tion, we conduct tournament ranking and compute
ELO scores following Idea Arena (Li et al., 2024a).
Implementation Details We use GPT-
4o ( gpt-4o-2024-08-06 ) for idea gener-
ation and selection, and use Gemini-1.5-
Pro (Team et al., 2024) ( gemini-1.5-pro-002 )
and Claude-3.5-Sonnet (Anthropic, 2024)
(claude-3-5-sonnet-20241022 ) as judge mod-
els.†More implementation details, prompts, and
human evaluation details are in Appendices E, H,
and I, respectively.
3.4 Results
Automatic Evaluation Figure 3 shows that meta-
data improves average ratings across all methods,
suggesting that incorporating metadata enhances
the overall quality of generated research ideas.
Expected effectiveness consistently benefits from
†These model versions are used throughout the paper.Related 
LiteratureResearch Topic: 
Adaptation vs. Mitigation Focus 
of Climate NegotiationsResearch Question : How does membership in international coalitions 
shape national adaptation -mitigation strategies in climate negotiations?Research Idea
Textual Data
1. National communications: National communications 
submitted by countries every four years or eight years, 
outlining their efforts to address climate change. 
2. …Metadata: ClimateDataBank
Panel Data
1. CO2 emissions: …Cross- Sectional Data
1. AOSIS member: …Theory : International coalitions play a significant role in shaping the 
climate negotiation strategies of member countries. For example, 
AOSIS members, being small island states, are more vulnerable to 
climate change and thus prioritize adaptation. …
Hypotheses : 
1. AOSIS members are more likely to emphasize adaptation in their 
national communications compared to non- members.
2. …Figure 2: An example of metadata provided during idea generation, alongside a generated research idea.
Average
Significance
Novelty FeasibilityExpected
Effectiveness
800900100011001200
Gemini-1.5-Pro as the Judge
Average
Significance
Novelty FeasibilityExpected
Effectiveness
800900100011001200
Claude-3.5-Sonnet as the JudgeAI-Researcher
GPT-ResearcherChain-of-Ideas
AI-Researcher w. MetadataGPT-Researcher w. Metadata
Chain-of-Ideas w. Metadata
Figure 3: Automatic evaluation results of ideas generated with (w.) and without metadata. A tabular version is in
Appendix Table 10.
metadata, with feasibility and significance also im-
proving in most cases, demonstrating metadata’s
role in generating more empirically grounded and
impactful ideas. However, novelty declines for AI-
Researcher and Chain-of-Ideas when evaluated by
Claude, indicating that data-aware generation may
limit the generation of highly unconventional ideas.
w. Metadata Tie w/o Metadata
Significance 38.8 22.4 38.8
Novelty 42.6 14.0 43.4
Feasibility 46.5 27.1 26.4
Exp. Effectiveness 51.2 16.3 32.5
Overall 43.4 14.7 41.9
Table 1: Human comparison results of ideas generated
by GPT-Researcher with and without metadata ( %).
Human Evaluation We perform human evalua-
tion on GPT-Researcher’s output, as this method
achieves high rankings in the automatic evaluation.
We recruit human annotators at the graduate levelor above, with academic backgrounds in social sci-
ence. For the 50 idea pairs (5 ideas per topic across
10 topics) generated by GPT-Researcher with and
without metadata, each pair is annotated by at least
two participants. In addition to the four evaluation
criteria, we also ask annotators to assess the overall
quality of research ideas.
As shown in Table 1, the human evaluation re-
sults align with the automatic evaluation trends.
The integration of metadata leads to substantial
improvements in feasibility (20%) and expected
effectiveness (18%), though novelty experiences
a modest decrease. The overall assessment score
increases by 1.5%, suggesting that while metadata
strengthens specific quality dimensions, the holis-
tic assessment of research ideas involves balancing
multiple criteria. Appendix G demonstrates an ex-
ample where incorporating metadata improves the
feasibility of generated idea, thus improving the
overall quality of the idea.Domain # Papers # Hypotheses Accuracy ( %)
Diverse 20 100 78.0
Climate 8 18 72.2
(a) Accuracy of the hypothesis validation results.Choice Ratio ( %)
Mostly Validate the Hypothesis 50
Partially Validate the Hypothesis 40
Does not Help Validating 10
(b) Human evaluation results of the validation processes.
Knowledge Recall Data Analysis Reasoning Code Generation Result Interpretation
30.0 63.3 30.0 33.3
(c) Error analysis of the validation processes. Numbers indicate the ratio of validation processes that encounter the error ( %).
Table 2: Pilot study results on LLM performance in automatic hypothesis validation. The LLM used is GPT-4o with
the code interpreter assistant.
4 Can Automatic Validation Benefit Idea
Selection?
This section investigates the role of automatic vali-
dation in idea selection through: (1) a pilot study
assessing the hypothesis validation capabilities of
LLMs, (2) a reference-based evaluation of idea se-
lection performance, and (3) a human evaluation
comparing ideas selected with and without the vali-
dation process.
4.1 Pilot Study: Can LLMs conduct
preliminary hypothesis validation?
We assess LLMs’ ability to validate hypotheses
from published papers.
Experimental Setup We extract 18 hypotheses
from the 8 domain-specific papers collected in §2,
of which 10 are supported and 8 are refuted. Hy-
potheses with insignificant or mixed evidence are
excluded. To expand the experimental scope, we
sample 50 hypotheses from DiscoveryBench (Ma-
jumder et al., 2024b), drawn from 20 papers across
diverse fields like humanities, sociology, and eco-
nomics. Since all these hypotheses are supported
in the original papers, we create 50 negative hy-
potheses by modifying their variables or relations
to balance the evaluation dataset.
We experiment with GPT-4o using the code in-
terpreter assistant, a built-in tool available in GPT
models. It achieves superior performance in quanti-
tative reasoning with data (Liu et al., 2024b), while
more advanced methods can also be employed in
the future. We input the hypotheses along with their
corresponding data into the model. The model en-
gages in multi-turn interactions to write and run
Python code in a sandbox environment, to validate
whether the hypotheses are supported.
Results We begin by evaluating whether the
LLM’s validation results align with the conclusionspresented in the original papers. As shown in Ta-
ble 2a, the model achieves over 70% accuracy on
both general-domain and domain-specific hypothe-
ses. To assess whether this performance stems
from memorization, we compare it to a memoriza-
tion baseline, where the LLM is asked to predict
whether the hypotheses are supported without ac-
cess to the data. Under this setting, the model
correctly predicts 65% of DiscoveryBench cases
and 55% of climate negotiation cases. Hypothesis
validation with data surpasses the memorization
baseline by a substantial margin ( ≥13%), suggest-
ing that the LLM exhibits a meaningful capacity
for hypothesis validation.
To evaluate the quality of the validation pro-
cesses, we conduct a human evaluation, asking
two domain experts to review the validation steps
for 15 hypotheses drawn from 6 sampled climate
negotiation papers, with the annotation interface
in Appendix Figure 6. As shown in Table 2b, half
of the validation processes mostly support the hy-
potheses with only minor flaws, while another 40%
partially align with the hypotheses but raise signifi-
cant concerns, such as insufficient control variables.
The error analysis in Table 2c reveals that data anal-
ysis, particularly involving textual data, is the most
challenging aspect for the model. Other common
issues, including knowledge recall, reasoning code
generation, and result interpretation, also occur in
approximately 30% cases.
Despite the imperfections, annotators note that
the automatic validation is helpful as an auxiliary
tool for exploratory research , which aligns well
with our intended use of the validation process: as
a reference during idea selection.Judge Model for Idea SelectionRaw Reasoning Trace … (data loading omitted) …Preliminary Automatic Validation
Feasibility CheckFeasibility: 
YesData Used: national_communications.csv, 
highlevel_statements.csv, aosis.csv
Output:
AOSIS
0.0 0.8741.0 0.885
Research QuestionResearch Idea
Hypotheses : 
1. AOSIS members are more likely to emphasize 
adaptation in their national communications compared to non -members.
2. …Theory
Summarization … Adaptation is frequently mentioned by both AOSIS 
and non- AOSIS members, slightly more so by AOSIS. Further statistical 
significance testing would be required for a robust conclusion.
Figure 4: An example of incorporating automatic validation into idea selection. We check the feasibility of
hypotheses in ideas, conduct automatic validation of feasible hypotheses, and provide the ideas together with the
summarized validation processes to the judge model.
4.2 Incorporating Automatic Validation into
Idea Selection
Figure 4 illustrates how we conduct automatic vali-
dation of generated ideas and incorporate the vali-
dation results into the idea selection process.
Feasibility Check For each idea, we first assess
the feasibility of validating its hypotheses. The
idea, together with metadata from all available
datasets, is provided to an LLM. The model is
prompted to determine whether the hypotheses can
be tested using the provided datasets and to identify
which datasets would be used for the validation.
Specifically, the datasets are indexed numer-
ically. If the model judges the hypotheses to
be testable, it outputs the indices of the selected
datasets along with a corresponding validation plan.
Given the difficulty for LLMs to handle complex
data analysis, the number of selected datasets is
limited to a maximum of three, with no more than
one being a textual dataset.
Hypothesis Validation If the idea is deemed
testable, the corresponding datasets are then pro-
vided to the LLM for validation. Similar to the
pilot study, the model generates a validation trace
consisting of code snippets, intermediate execution
outputs, and textual interpretations.
Validation Process Summarization The raw
reasoning traces may be verbose and sometimes
contain noise, such as trial-and-error in code ex-
ecution. To make the output more interpretable
and useful for downstream selection, we prompt
the LLM to summarize the full validation process
into concise natural language steps, including the
crucial reasoning process and results that lead to
the final conclusion. The summarized validationprocesses along with the ideas are then provided to
the judge model for idea selection.
We use GPT-4o for both the feasibility check
and validation process summarization, and GPT-4o
with the code interpreter assistant for hypothesis
validation. Implementation details and prompts
are in Appendices E and H. A detailed case of the
automatic validation process is in Appendix G.
4.3 Reference-based Automatic Evaluation
We evaluate the impact of validation on idea se-
lection performance, following the setup of Re-
searchBench (Liu et al., 2025). We prompt LLMs
to perform pairwise ranking between ground-truth
ideas (extracted from academic papers) and LLM-
generated ideas, and compare ranking accuracy
with and without access to validation processes.
For each of the 8 climate negotiation papers
we collected, we manually extract the research
topic and use GPT-Researcher to generate 10 ideas
on the same topic, provided with the correspond-
ing dataset description. We then perform auto-
matic validation on both the ground-truth and LLM-
generated ideas, and ask judge LLMs to pairwisely
compare the ground-truth ideas with the generated
ideas under the same topic. Accuracy is defined as
the proportion of comparisons where the ground-
truth idea is ranked higher. To mitigate position
bias, each pair is evaluated twice with reversed
positions, and the results are averaged.
We use Gemini-1.5-Pro and Claude-3.5-Sonnet
as judge LLMs. Results are shown in Table 3.
For both models, incorporating validation leads
to consistently higher average ranking accuracy
compared to the setting without validation. Im-
provements are particularly notable in the feasibil-
ity and expected effectiveness dimensions, whileJudge Model w. Validation Significance Novelty Feasibility Exp. Effectiveness Average
Gemini-1.5-Pro✗ 69.9 71.3 29.7 56.7 56.9
✓ 67.3 65.8 55.6 60.6 62.3
Claude-3.5-Sonnet✗ 89.4 82.5 20.1 83.8 69.0
✓ 88.1 86.9 46.9 93.6 78.9
Table 3: Accuracy of judge models in ranking ground-truth ideas among LLM-generated ideas ( %). Better results
of each judge model are in bold.
a slight decrease is observed in the judgment of
significance and novelty.
4.4 Human Evaluation
w. Validation Tie w/o Validation
Significance 37.5 27.5 35.0
Novelty 45.0 21.7 33.3
Feasibility 40.0 33.3 26.7
Exp. Effectiveness 43.3 27.5 29.2
Overall 42.5 21.7 35.8
Table 4: Human comparison results of ideas selected
by Claude-3.5-Sonnet with and without validation pro-
cesses ( %).
We then conduct a human evaluation comparing
LLM-generated ideas selected with and without
validation processes. For the 50 ideas generated by
GPT-Researcher in §3 on each research topic, we
use Claude-3.5-Sonnet, which performs better in
the reference-based evaluation, to select the top 5
ideas in two settings: (1) based on the idea content
alone, and (2) based on both the idea and its val-
idation process. Human annotators then perform
pairwise evaluations of the two sets, using the same
evaluation setup as described in §3.4.
As shown in Table 4, ideas selected with vali-
dation processes are ranked higher across all di-
mensions, with the largest improvement observed
in feasibility. This aligns with the reference-based
evaluation results and suggests that validation pro-
cesses provide a valuable signal for enhancing idea
selection.
5Human Study: Are the LLM Generated
Ideas Inspiring to Researchers?
Beyond evaluating idea quality, we are interested
in whether LLM-generated ideas can be useful in
real-world academic settings. We conduct a hu-
man study to investigate whether ideas generated
by LLMs, along with related data and validation
processes, can inspire researchers to formulate their
own research ideas.5.1 Experiment Design
We recruit 23 participants from a social science
course to take part in the study. Among them, 19
are undergraduate or graduate students, and the re-
maining 4 are more senior researchers holding a
PhD in a related field. Participants are presented
with four research topics related to climate nego-
tiations and asked to select two topics they are
personally interested in. For each selected topic,
they are asked to propose a research idea.
For one of the two topics, participants are pro-
vided with three reference ideas, accompanied by
data snippets used in automatic validation and
the validation processes. The reference ideas are
from the experiment of §4.4, generated by GPT-
Researcher with metadata and selected by Claude-
3.5-Sonnet based on validation processes. For each
idea, we present the first 10 lines of the datasets
used during validation. Both the raw validation
traces and the summarized versions are provided,
and participants may choose which format to con-
sult. For the other topic, participants are not given
any references. They are allowed to browse the
Internet and search for literature but are not permit-
ted to use LLMs. Additional details including the
experiment interface are provided in Appendix I.
5.2 Results
Quality of Ideas We ask human experts to evalu-
ate the quality of the ideas proposed by participants,
using the same evaluation setup as §3.4. Since the
number of ideas proposed with and without ref-
erences may differ for a given topic, we first pair
ideas from both settings one-to-one. For any excess
ideas in one setting, we randomly sample additional
ideas from the other to complete the set of pairs.
As shown in Table 5a, ideas proposed with refer-
ences demonstrate higher overall quality. Specifi-
cally, improvements are observed in novelty, feasi-
bility, and expected effectiveness.
Feedback from Participants To understand
whether participants find the references helpfulw. Reference Tie w/o Reference
Significance 39.1 21.7 39.1
Novelty 43.5 23.9 32.6
Feasibility 50.0 17.4 32.6
Exp. Effectiveness 39.1 28.3 32.6
Overall 39.1 28.3 32.6
(a) Human comparison results of ideas proposed by partici-
pants with vs. without access to references (including LLM-
generated ideas, related data and validation processes).Helpfulness High Medium Not Helpful
Reference Ideas 61.1 33.3 5.6
Data Segments 33.3 50.0 16.7
Validation Processes 55.5 38.9 5.6
(b) Participant feedback on the helpfulness of reference
ideas, data segments, and validation processes for generat-
ing their own research ideas. High and medium helpfulness
correspond to the very helpful andsomewhat helpful
options, respectively.
Table 5: Human study results on the inspirational value of LLM-generated ideas. Numbers are in percentages ( %).
and how they use them, we collect self-reported
feedback. Participants are asked to rate the help-
fulness of the reference ideas, data segments, and
validation processes separately, using a three-point
scale: very helpful ,somewhat helpful , and
not helpful .
As shown in Table 5b, all three components are
generally found helpful by most participants. Ref-
erence ideas and validation processes are rated as
very helpful by more than half of the participants.
The data segments receive relatively lower ratings,
likely because raw data often requires additional
interpretation or context to be fully understood,
whereas ideas and validation outputs provide more
immediately actionable guidance.
Several participants provide detailed feedback.
One student notes they build their own idea by ex-
tending the most interesting reference idea , while
another mentions that the concepts and measure-
ments in the references help refine their own re-
search direction . A professor also remarks that
the references served as useful shortcuts and they
can revise upon them . These insights highlight
how LLM-generated references support researchers
based on their background and research stage.
6 Related Work
Research Idea Generation There is growing in-
terest in leveraging LLMs for research idea gener-
ation, either as a standalone task (Si et al., 2024;
Baek et al., 2024) or as part of an end-to-end auto-
mated research pipeline (Li et al., 2024b; Lu et al.,
2024; Jansen et al., 2025). The former line of
work focuses on enhancing the literature search and
idea generation stages, typically generating ideas
grounded in prior literature (Wang et al., 2024;
Yang et al., 2024a). The latter line of work pro-
poses more comprehensive frameworks that extend
to later stages, such as experiment design, execu-
tion, and paper writing.Our work aligns more closely with the first line,
but novelly incorporates data into the idea gener-
ation process. While our work also involves code
generation and execution for hypothesis validation,
this is not intended as rigorous experimental verifi-
cation, but serves as a preliminary signal to support
idea selection.
Hypothesis Generation A related but distinct
line of work focuses on hypothesis generation,
where models generate hypotheses to explain phe-
nomena given access to data, like inducing rules
from observations (Zhong et al., 2023; Qiu et al.,
2024). Studies in this area explore data-driven
methods (Majumder et al., 2024a; Zhou et al.,
2024) or integrate literature with data (Liu et al.,
2024a), but their goal is to uncover patterns in ex-
isting datasets, contrasting with our objective of
generating high-quality research ideas.
7 Conclusion and Discussion
Our study shows that incorporating data into re-
search idea generation, through metadata and au-
tomatic validation, improves the overall quality of
research ideas generated by LLMs. By guiding
idea generation with dataset descriptions and se-
lecting ideas given automatic validation processes,
LLMs are able to propose ideas that are more feasi-
ble and more likely to be effective. Beyond quality
improvements, we find that these LLM-generated
ideas, along with their validation traces, can serve
as valuable inspiration for human researchers.
In discussing this work with social sciences
researchers, we encounter thoughtful reflections
on the value of LLM-generated ideas. Some
researchers question whether ideas proposed by
LLMs truly matter if they do not originate from
human “care” or intention. These conversations
raise deeper questions about the nature of research:
What distinguishes a good idea from a valuable
idea? How could LLM-generated ideas contributeto real-world research in ways that augment human
creativity? While we provide a preliminary case
study of such use in §5, these questions remain
open and worth future exploration.
Limitations
Task Scope While our experiments focus on top-
ics related to climate negotiations, the proposed
method could be applied to other quantitative so-
cial science research areas. We believe that incor-
porating data could also enhance the generation of
research ideas in other domains, such as computer
science, but this would need further development
of the method.
Exploration of LLMs and Validation Methods
Due to the high cost of human evaluation, our ex-
periments focus on a single LLM and a specific
automatic validation method. Future studies could
systematically evaluate how different models and
validation methods impact idea quality.
Trade-off between Novelty and Feasibility The
introduction of metadata improves feasibility but
leads to a modest decline in novelty. This suggests
that although LLMs are not explicitly restricted to
the provided data, the metadata implicitly narrows
their scope of imagination. Future works could
broaden the data scope from existing data to data
that can be collected, or better integrate literature
with data to maintain a balance between creativity
and feasibility.
Ethical Considerations
Research ideas generated by LLMs may reflect
biases present in their training data and could unin-
tentionally resemble existing work without proper
citation. Therefore, these ideas should not be
adopted for practical use without thorough vali-
dation. Furthermore, any use of LLM-generated
ideas should be disclosed transparently to ensure
ethical integrity.
References
Anthropic. 2024. Introducing claude 3.5 son-
net. https://www.anthropic.com/news/
claude-3-5-sonnet .
Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan,
and Sung Ju Hwang. 2024. Researchagent: Iter-
ative research idea generation over scientific liter-
ature with large language models. arXiv preprint
arXiv:2404.07738 .Benjamin E Bagozzi. 2015. The multifaceted nature of
global climate change negotiations. The Review of
International Organizations , 10:439–464.
Daria Blinova, Rakesh Emuru, and Benjamin E Bagozzi.
2024. Individual attendance data for over 30 years
of international climate change talks. Scientific Data ,
11(1):1134.
Tobias Böhmelt. 2013. A closer look at the informa-
tion provision rationale: Civil society participation
in states’ delegations at the unfccc. The Review of
International Organizations , 8(1):55–80.
Paula Castro and Marlene Kammerer. 2021. The institu-
tionalization of a cleavage: how differential treatment
affects state behavior in the climate negotiations. In-
ternational studies quarterly , 65(3):683–698.
Chen Chen, Ian Noble, Jessica Hellmann, Joyce Coffee,
Martin Murillo, and Nitesh Chawla. 2015. University
of notre dame global adaptation index. University of
Notre Dame .
Assaf Elovic. 2023. gpt-researcher. https://github.
com/assafelovic/gpt-researcher .
Federica Genovese. 2019. Sectors, pollution, and trade:
How industrial interests shape domestic positions
on global climate agreements. International Studies
Quarterly , 63(4):819–836.
Federica Genovese, Richard J McAlexander, and Jo-
hannes Urpelainen. 2023. Institutional roots of in-
ternational alliances: Party groupings and position
similarity at global climate negotiations. The Review
of International Organizations , 18(2):329–359.
Aaron Hurst, Adam Lerer, Adam P Goucher, Adam
Perelman, Aditya Ramesh, Aidan Clark, AJ Os-
trow, Akila Welihinda, Alan Hayes, Alec Radford,
et al. 2024. Gpt-4o system card. ArXiv preprint ,
abs/2410.21276.
Peter Jansen, Oyvind Tafjord, Marissa Radensky, Pao
Siangliulue, Tom Hope, Bhavana Dalvi Mishra, Bod-
hisattwa Prasad Majumder, Daniel S Weld, and Pe-
ter Clark. 2025. Codescientist: End-to-end semi-
automated scientific discovery with code-based ex-
perimentation. arXiv preprint arXiv:2503.22708 .
Ayse Kaya and Lynne Steuerle Schofield. 2020. Which
countries send more delegates to climate change con-
ferences? analysis of unfccc cops, 1995–2015. For-
eign Policy Analysis , 16(3):478–491.
Gary King, Robert O Keohane, and Sidney Verba. 1994.
Designing Social Inquiry: Scientific Inference in
Qualitative Research . Princeton University Press.
Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao,
Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yum-
ing Jiang, Yifei Xin, Ronghao Dang, et al. 2024a.
Chain of ideas: Revolutionizing research via novel
idea development with llm agents. arXiv preprint
arXiv:2410.13185 .Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya
Du. 2024b. Mlr-copilot: Autonomous machine learn-
ing research based on large language models agents.
arXiv preprint arXiv:2408.14033 .
Haokun Liu, Yangqiaoyu Zhou, Mingxuan Li, Chenfei
Yuan, and Chenhao Tan. 2024a. Literature meets
data: A synergistic approach to hypothesis genera-
tion. arXiv preprint arXiv:2410.17309 .
Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei
Chang, and Yansong Feng. 2024b. Are llms capable
of data-based statistical and causal reasoning? bench-
marking advanced quantitative reasoning with data.
InFindings of the Association for Computational
Linguistics ACL 2024 , pages 9215–9235.
Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben
Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang,
Erik Cambria, and Dongzhan Zhou. 2025. Research-
bench: Benchmarking llms in scientific discovery via
inspiration-based task decomposition. arXiv preprint
arXiv:2503.21248 .
Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foer-
ster, Jeff Clune, and David Ha. 2024. The ai scientist:
Towards fully automated open-ended scientific dis-
covery. arXiv preprint arXiv:2408.06292 .
Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv
Agarwal, Sanchaita Hazra, Ashish Sabharwal,
and Peter Clark. 2024a. Data-driven discovery
with large generative models. arXiv preprint
arXiv:2402.13610 .
Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv
Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh
Meena, Aryan Prakhar, Tirth V ora, Tushar Khot,
Ashish Sabharwal, and Peter Clark. 2024b. Discov-
erybench: Towards data-driven discovery with large
language models. arXiv preprint arXiv:2407.01725 .
Monty G Marshall, Ted Robert Gurr, and Keith Jaggers.
2014. Polity iv project: Political regime characteris-
tics and transitions, 1800–2013. Center for Systemic
Peace , 5.
Leanne C Powner. 2014. Empirical research and writ-
ing: A political science student’s practical guide . CQ
Press.
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar,
Valentina Pyatkin, Chandra Bhagavatula, Bailin
Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al.
2024. Phenomenal yet puzzling: Testing inductive
reasoning capabilities of language models with hy-
pothesis refinement. In The Twelfth International
Conference on Learning Representations .
Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024.
Can llms generate novel research ideas? a large-
scale human study with 100+ nlp researchers. arXiv
preprint arXiv:2409.04109 .Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan
Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,
Damien Vincent, Zhufeng Pan, Shibo Wang, et al.
2024. Gemini 1.5: Unlocking multimodal under-
standing across millions of tokens of context. ArXiv
preprint , abs/2403.05530.
Vegard Tørstad, Håkon Sælen, and Live Standal Bøyum.
2020. The domestic politics of international climate
commitments: which factors explain cross-country
variation in ndc ambition? Environmental Research
Letters , 15(2):024021.
Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope.
2024. Scimon: Scientific inspiration machines opti-
mized for novelty. In Proceedings of the 62nd Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 279–299.
Torsten Welle and Joern Birkmann. 2015. The world
risk index–an approach to assess risk and vulnera-
bility on a global scale. Journal of Extreme Events ,
2(01):1550003.
Sarah Judith Wright, Anne Sietsma, Stefanie Korswa-
gen, Ioannis N Athanasiadis, and Robbert Biesbroek.
2023. How do countries frame climate change? a
global comparison of adaptation and mitigation in
unfccc national communications. Regional Environ-
mental Change , 23(4):129.
Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shen-
gran Hu, Chris Lu, Jakob Foerster, Jeff Clune, and
David Ha. 2025. The ai scientist-v2: Workshop-level
automated scientific discovery via agentic tree search.
arXiv preprint arXiv:2504.08066 .
Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Sou-
janya Poria, and Erik Cambria. 2024a. Large lan-
guage models for automated open-domain scientific
hypotheses discovery. In Findings of the Associa-
tion for Computational Linguistics ACL 2024 , pages
13545–13565.
Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie,
Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik
Cambria, and Dongzhan Zhou. 2024b. Moose-
chem: Large language models for rediscovering un-
seen chemistry scientific hypotheses. arXiv preprint
arXiv:2410.07076 .
Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan
Klein, and Jacob Steinhardt. 2023. Goal driven dis-
covery of distributional differences via language de-
scriptions. Advances in Neural Information Process-
ing Systems , 36:40204–40237.
Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava,
Hongyuan Mei, and Chenhao Tan. 2024. Hypoth-
esis generation with large language models. arXiv
preprint arXiv:2404.04326 .A Data Collection
Table 6 presents the full list of datasets in CLI-
MATE DATABANK and corresponding data descrip-
tions. Table 7 demonstrates the climate negotiation
papers we collect for the automatic validation ex-
periments.
National communications, highlevel statements,
and business statements are collected from the UN-
FCCC website‡, which allows free download and
copy. Earth negotiation bulletins are collected from
the ENB website§. Meeting attendance records
are from Blinova et al. (2024) under the CC0 1.0
license. Democracy index is from Marshall et al.
(2014). World risk index is from Welle and Birk-
mann (2015) under the CC BY license. ND-GAIN
vulnerability index is from Chen et al. (2015). All
other data in CLIMATE DATABANK are from World
Bank Open Data under the CC-BY 4.0 license.
B Research Topics
We generate 10 climate negotiation-related re-
search topics using GPT-4o, with the prompt:
Could you propose 10 research topics related to cli-
mate negotiation? The topics should be important
for social science researchers, like in the commu-
nity of political science and climate policies. The
output should be in JSON format, with the key being
the topic name and the value being the explanation.
Each topic name should be within five words.
The created topics are listed in Table 8, and their
quality has been reviewed and verified by human
experts. All the research topics and generated ideas
throughout this paper are in English.
C Research Idea Generation Methods
We experiment with three prevalent research idea
generation methods in §3:
•AI-Researcher (Si et al., 2024): This method
first retrieves papers related to the given re-
search topic from Semantic Scholar, uses the
retrieved papers to ground idea generation,
produces a large number of candidate ideas,
and then ranks them to identify the best ones.
•GPT-Researcher (Elovic, 2023): This method
builds a multi-agent framework consisting of
planner, executor, and publisher agents. The
planner generates plans, while the executor
‡https://unfccc.int/
§https://enb.iisd.org/gathers relevant information. The publisher
aggregates all information and generates the
research ideas.
•Chain-of-Ideas (Li et al., 2024a): This method
enhances the literature search module by orga-
nizing relevant literature in a chain structure
to effectively mirror the progressive research
development.
To ensure a fair comparison, each method is uni-
formly tasked with generating 50 candidate ideas
for each research topic. We then use the same idea
selection module to rank and select the top ideas.
D Evaluation Criteria
The ideas are evaluated according to the following
four criteria:
•Significance: Whether the research idea is
impactful to the researchers and the broader
public.
•Novelty: Whether the idea contributes fresh
insights and perspectives to the existing body
of knowledge.
•Feasibility: Whether the study can be done
with available resources, time, and technol-
ogy, typically within a one-year scope for a
political science PhD student.
•Expected Effectiveness: How likely the pro-
posed idea will successfully achieve its in-
tended outcomes, i.e., how likely the theory
will be supported by empirical evidence.
A more detailed version of the criteria is shown
in Table 9. This is provided to LLMs during idea
selection and automatic evaluation, as well as to
human annotators for reference.
E Implementation Details
For the research idea generation methods, we ad-
here to their original hyperparameters but modify
the idea generation prompts to include instructions
related to idea formats, and add the metadata. Since
in social science research, policy implications are
frequently invoked to demonstrate a study’s broader
relevance and impact, we also ask LLMs to explain
the policy implications of generated ideas in the
idea generation step. Note that this is only for self-
awareness and is excluded from subsequent idea
selection and evaluation.Name Description
Textual Data
National communications National communications submitted by countries every four years (Annex I Parties) or
eight years (Non-Annex I Parties), outlining their efforts to address climate change.
Highlevel statements Highlevel climate change conference speeches, covering the formal statements made by
country-representatives at COPs (2010-2023).
Earth negotiation bulletins Reports summarizing the negotiation process and main outputs of UNFCCC meetings,
including both daily reports and summary reports (1995-2024).
Business statements UNFCCC statements of business associations in the span of eight years (2007-2014).
Panel Data
Meeting attendance records Attendee records from all UNFCCC COP meetings (1995-2023), including their delegation,
job, gender, and so on.
GDP The sum of gross value added by all resident producers in the economy plus any product
taxes and minus any subsidies not included in the value of the products (in US$).
GDP per capita Gross domestic product divided by midyear population (in US$).
Population The population of the country, which counts all residents regardless of legal status or
citizenship.
Foreign direct investment Direct investment equity flows in the reporting economy, which is the sum of equity capital,
reinvestment of earnings, and other capital (in US$).
Life expectancy at birth The number of years a newborn infant would live if prevailing patterns of mortality at the
time of its birth were to stay the same throughout its life.
Gender parity index The ratio of girls to boys enrolled at primary and secondary levels in public and private
schools.
CO2 emissions per capita Carbon dioxide (CO2) emissions excluding LULUCF per capita.
Forest area Land under natural or planted stands of trees of at least 5 meters in situ (sq. km).
Natural resources rent Rents from coal, oil and natural gas production (% of GDP).
Trade openness index Sum of exports and imports of goods and services, divided by gross domestic product,
expressed as a percentage.
Democracy index The country’s level of democracy, ranging from -10 to 10 (fully democratic).
World risk index Higher scores indicate higher vulnerability to climate change.
ND-GAIN vulnerability index Higher scores indicate higher vulnerability to climate change.
Cross-Sectional Data (in 2025)
Member of AOSIS Whether the country is a member of the Alliance of Small Island States (AOSIS).
Member of OPEC Whether the country is a member of the Organization of the Petroleum Exporting Countries
(OPEC).
Member of G20 Whether the country is a member of G20.
Annex I country Whether the country is an Annex I country.
Table 6: List of datasets and the corresponding data descriptions in C LIMATE DATABANK.
ID Paper Title
1 The Multifaceted Nature of Global Climate Change Negotiations (Bagozzi, 2015)
2 A Closer Look at the Information Provision Rationale: Civil Society Participation in States’ Delegations at the
UNFCCC (Böhmelt, 2013)
3 Sectors, Pollution, and Trade: How Industrial Interests Shape Domestic Positions on Global Climate Agreements (Gen-
ovese, 2019)
4 The domestic politics of international climate commitments: which factors explain cross-country variation in NDC
ambition? (Tørstad et al., 2020)
5 Which Countries Send More Delegates to Climate Change Conferences? Analysis of UNFCCC COPs,
1995–2015 (Kaya and Schofield, 2020)
6 The Institutionalization of a Cleavage: How Differential Treatment Affects State Behavior in the Climate Negotia-
tions (Castro and Kammerer, 2021)
7 How Do Countries Frame Climate Change? A Global Comparison of Adaptation and Mitigation in UNFCCC National
Communications (Wright et al., 2023)
8 Institutional Roots of International Alliances: Party Groupings and Position Similarity at Global Climate Negotia-
tions (Genovese et al., 2023)
Table 7: List of reference papers used in automatic validation experiments.
For idea selection in §3, we follow AI-
Researcher’s tournament ranking method but adapt
it by having the model rank idea pairs based on thefour evaluation aspects separately. The idea that
wins in more aspects is considered the winner, and
a tie occurs if the two ideas win an equal numberTopic Description
Adaptation vs. Mitigation FocusStudy the negotiation dynamics and policy priorities between adaptation and mit-
igation efforts, and the factors influencing their prominence in different countries’
strategies.
Climate Finance PoliticsExamine the political challenges and negotiations around climate finance, including
funding commitments, allocation mechanisms, and equity in financial support for
adaptation and mitigation.
Climate Justice and EquityInvestigate how principles of justice and equity are integrated into climate negotiations
and their impacts on policy outcomes for different countries and communities.
Compliance and Monitoring Mecha-
nismsFocus on the systems in place for ensuring adherence to international climate agree-
ments, and the effectiveness of these mechanisms in promoting accountability.
Impacts of Domestic PoliciesExplore how domestic climate policies of influential nations affect their negotiation
positions and the overall dynamics in international climate agreements.
Historical Responsibility DebatesAnalyze discussions around historical responsibility for climate change and how these
debates shape fairness principles and burden-sharing in negotiations.
Negotiation Strategies and TacticsAnalyze the negotiation strategies employed by countries or blocs in climate negotia-
tions, including coalition-building, bargaining tactics, and compromise-making.
Role of Non-State ActorsStudy the influence and participation of non-state actors, such as NGOs, private sector,
and indigenous groups, in shaping climate negotiation agendas and outcomes.
Power Dynamics and InfluenceExamine the roles of different countries, especially major emitters versus vulnerable
states, and their influence in shaping international climate agreements and commit-
ments.
Technology Transfer and Collabora-
tionExplore the negotiations related to technology transfer, the barriers to effective
collaboration, and how they impact developing countries’ abilities to meet climate
goals.
Table 8: Climate negotiation research topics used in this paper.
of aspects.
The temperature is set to 0 for all steps after
idea generation. The maximum number of output
tokens is set to 1024 for the feasibility check, idea
selection, and automatic evaluation. Experiments
are conducted on 8 NVIDIA A800 GPUs.
F Experimental Results
Table 10 presents the ELO scores for research idea
generation methods with and without metadata,
serving as the tabular counterpart to Figure 3.
G Case Study
Table 11 presents an example of ideas generated by
GPT-Researcher under the same topic. Idea 1, gen-
erated without metadata, contains undefined terms
such as inclusivity and high-quality data, while
Idea 2, which is guided by metadata, introduces
clear and measurable hypotheses. The integration
of metadata makes the research idea more action-
able, increasing the likelihood of meaningful find-
ings and improving overall quality.
Table 12 showcases an example of the automatic
validation process. Based on an idea generated by
GPT-Researcher under the topic Role of Non-State
Actors , the LLM first conducts a feasibility check
and selects three datasets from the CLIMATE DATA-
BANK. It then performs hypothesis validation andsummarizes the validation process in natural lan-
guage.
H Prompts
Table 13 presents the prompt for idea generation
using AI-Researcher. The same instructions regard-
ing idea format, example ideas, and metadata are
provided to GPT-Researcher and Chain-of-Ideas.
The example ideas are drawn from existing aca-
demic papers.
Table 14 shows the prompt used for both idea
selection and automatic evaluation in §3. The dif-
ference is that idea selection is conducted by the
LLM for idea generation, whereas in automatic
evaluation, other LLMs are used to reduce bias.
Tables 15 through 17 display the prompts for the
automatic validation process, including feasibility
checks, hypothesis validation, and validation pro-
cess summarization. Table 18 outlines the prompt
for idea selection in §4, which differs from Table 14
by incorporating the validation process.
I Annotation Details
Figures 5 and 6 show the annotation interfaces for
human evaluations of idea pairs (in §3 - §5) and
hypothesis validation processes (in §4.1), respec-
tively. All annotators are fairly paid with more than
$10 per hour.Significance
1. Impact on the Field:
- Does the research have the potential to influence future work in the field significantly?
- Will it change the way scholars and practitioners think about a particular issue or problem?
2. Relevance to Current Problems:
- Does the research tackle urgent or pressing issues faced by society today?
- How does it contribute to solving real-world problems or advancing public policy?
3. Advancement of Theoretical or Practical Understanding:
- Does it deepen our theoretical insights or provide new frameworks for understanding?
- Can the findings be translated into practical applications or technologies that benefit society?
Novelty
1. Originality:
- Is the research question unique or a significant departure from existing studies?
- Does the theory offer a new perspective or challenge prevailing paradigms?
2. Innovation in Approach:
- Are there novel methodologies or analytical techniques proposed?
- Does it introduce new datasets or sources of evidence?
3. Contribution to Knowledge:
- Does the idea fill a significant gap in the literature?
- How does it expand or refine existing theories or models?
Feasibility
1. Resource Availability:
- Can the necessary data or materials be accessed or acquired with reasonable effort?
- Are funding, human resources, and technical support sufficient?
2. Timeline Appropriateness:
- Can the study be realistically completed within one year?
- Does the research have clear stages with achievable milestones?
3. Technical and Methodological Soundness:
- Are the proposed methodologies practical and well-founded?
Expected Effectiveness
1. Theoretical Rigor:
- Is the theory logically sound with well-defined constructs and relationships?
- How well are the hypotheses grounded in existing literature and theory?
2. Empirical Evidence Potential:
- How robust is the potential for empirical evidence to support the theory?
- Are the proposed indicators measurable and likely to yield clear data?
Table 9: Detailed criteria for evaluating ideas.
In the human study of §5, participants are given
20 minutes to propose one research idea for each
research topic they select. The experiment interface
for this task is shown in Figure 7.Method w. Metadata Significance Novelty Feasibility Exp. Effectiveness Average
Gemini-1.5-Pro as the Judge
AI-Researcher✗ 902 933 1047 951 958
✓ 938 931 1098 997 991
GPT-Researcher✗ 1019 1000 1045 1015 1020
✓ 1073 1021 1183 1134 1103
Chain-of-Ideas✗ 974 1025 822 915 934
✓ 1094 1091 805 988 995
Claude-3.5-Sonnet as the Judge
AI-Researcher✗ 870 968 1060 881 945
✓ 855 859 1152 918 946
GPT-Researcher✗ 931 972 1076 928 977
✓ 1000 903 1228 1085 1054
Chain-of-Ideas✗ 1066 1118 768 1012 991
✓ 1278 1180 716 1176 1088
Table 10: ELO scores for research idea generation methods with and without metadata. Better results of each
method are in bold.
Topic Compliance and Monitoring Mechanisms
Idea 1: Generated without Metadata
Research QuestionHow does the Global Stocktake process under the Paris Agreement influence collective progress
toward climate goals, and what factors enhance its effectiveness?
TheoryThe theory posits that the Global Stocktake serves as a critical feedback mechanism that fosters
collective action by identifying gaps, sharing best practices, and increasing ambition. Its effectiveness
depends on the inclusivity of the process, the quality of data used, and the willingness of states to
act on its findings. The theory also suggests that the Global Stocktake is more effective when it
incorporates inputs from non-party stakeholders and aligns with domestic political priorities.
Hypotheses1. The inclusivity of the Global Stocktake process (e.g., participation of non-party stakeholders)
positively correlates with its effectiveness in fostering collective action.
2. High-quality data used in the Global Stocktake increases its impact on state ambition and compli-
ance.
3. States that align their domestic policies with Global Stocktake findings are more likely to achieve
their climate goals.
4. The Global Stocktake is less effective in fostering collective action when major emitters (e.g., the
US, China) fail to participate meaningfully.
Idea 2: Generated with Metadata
Research QuestionHow do domestic political institutions influence the effectiveness of compliance monitoring in
international climate negotiations?
TheoryDomestic political institutions play a critical role in shaping a country’s commitment and ability
to comply with international climate agreements. Democratic institutions, with their emphasis on
transparency and accountability, are theorized to facilitate better compliance monitoring. Conversely,
autocratic regimes may prioritize short-term economic gains over long-term climate commitments,
leading to weaker compliance. Additionally, institutional mechanisms such as independent regulatory
agencies, judicial oversight, and civil society engagement may enhance the credibility and accuracy
of compliance monitoring.
Hypotheses1. Countries with higher Democracy Index scores are more likely to submit accurate and timely
compliance reports to international climate bodies.
2. The presence of independent regulatory agencies positively correlates with the quality of compliance
monitoring in climate negotiations.
3. Civil society engagement, as measured by the number of environmental NGOs per capita, enhances
the accuracy of compliance reporting.
4. Autocratic regimes are more likely to underreport their emissions compared to democratic regimes,
controlling for economic and environmental factors.
Table 11: Examples of ideas generated by GPT-Researcher with and without metadata.Topic Role of Non-State Actors
Idea Generated
Research QuestionHow do non-state actors influence the ambition levels of national climate commitments under the
Paris Agreement?
TheoryNon-state actors (NSAs), such as businesses, civil society organizations (CSOs), and research in-
stitutions, play a critical role in driving climate ambition by pressuring governments to adopt more
stringent climate policies. This influence stems from their ability to mobilize public opinion, pro-
vide technical expertise, and create accountability mechanisms. The theory posits that NSAs are
particularly effective in democracies, where governments are more responsive to public pressure, and
in countries with high trade openness, where businesses are incentivized to align with international
climate norms to maintain competitiveness.
Hypotheses1. Countries with higher levels of NSA participation in UNFCCC meetings will exhibit greater
increases in the ambition of their Nationally Determined Contributions (NDCs) over time.
2. The impact of NSA participation on NDC ambition will be stronger in democracies compared to
autocracies.
3. Trade openness moderates the relationship between NSA participation and NDC ambition, with
more open economies showing a stronger positive relationship.
Feasibility Check
Feasibility Yes
Data Selected 1 (national communications), 5 (meeting attendance records), 16 (democracy index)
Hypothesis Validation
(raw traces omitted due to space limit)
Validation Process Summarization
1. The datasets were successfully loaded, containing information on national communications, attendance at UNFCCC
meetings, and democracy index scores. The approach involves testing three hypotheses: 1) NSA attendance correlates with
increased NDC ambition, 2) this relationship differs between democratic and autocratic countries, and 3) trade openness
influences the relationship between NSA participation and NDC ambition.
2. NSA participation was quantified by counting attendees per country and year. Ambition levels in national communica-
tions were inferred by counting occurrences of keywords like ’increase’, ’commitment’, and ’target’.
3. Ambition scores were calculated based on keyword occurrences in national communications. The data was integrated
with NSA participation and democracy index data, aligning them by country and year.
4. The integrated dataset was prepared, containing NSA participation, ambition scores, and democracy index scores. This
dataset is ready for hypothesis testing.
5. Hypothesis 1 was tested, showing a statistically significant positive correlation between NSA participation and NDC
ambition scores, supporting the hypothesis.
6. Hypothesis 1 is supported, indicating a positive correlation between NSA participation and NDC ambition.
7. Hypothesis 2 was tested by segmenting data into democracies and autocracies based on democracy index scores. The
relationship was not significantly stronger in democracies, so the hypothesis is not supported.
8. Hypothesis 2 is not supported, as the relationship between NSA participation and NDC ambition is not significantly
stronger in democracies compared to autocracies.
9. Hypothesis 3 could not be tested due to the absence of trade openness data. The final results are: Hypothesis 1 is
supported, Hypothesis 2 is not supported, and Hypothesis 3 needs more data.
Table 12: Examples of the automatic validation process.Prompt for Idea Generation
You are an expert researcher in political science. Now I want you to help me brainstorm some new research ideas on the topic
of {research topic}.
Here are some relevant papers on this topic just for your background knowledge:
{titles and abstracts of related literature}
The above papers are only for inspiration and you should not cite them and just make some incremental modifications.
Instead, you should make sure your ideas are novel and distinct from the prior literature.
Here are existing data related to this topic:
Textual data:
1. National communications: National communications submitted by countries every four years (Annex I Parties) or eight
years (Non-Annex I Parties), outlining their efforts to address climate change.
2. Highlevel statements: ...[omitted]...
Panel data:
5. Meeting attendance records: ...[omitted]...
Cross-sectional data:
19. Member of AOSIS: ...[omitted]...
You should generate {number of ideas to generate} different ideas on this topic. Try to be creative and diverse in idea
generation, and do not repeat any similar ideas.
You should aim for research that can be published in top political science journals. Good research should contribute to
theoretical value and/or policy implications.
Each idea should be described as:
(1) Research Question: Clearly propose a research question, which should be closely related to the topic. Research questions
can delve into issues of what, why, how, when, and so forth. Interesting research questions are those that intellectually appeal
to political scientists, address concerns of a broad population and decision makers, and where the answers are not obvious.
(2) Theory: Develop a theory that reasonably speculates on the answer to the research question, including a statement
about why the proposed answer is correct. A theory is a system of concepts and relationships between those concepts, that
collectively presents a logical, systematic, and coherent explanation of a phenomenon of interest.
(3) Hypotheses: Propose 1-5 hypotheses derived from the theory. The hypotheses identify observable implications of the
theory, i.e., things we would observe if the theory is correct, and make predictions about relationships between measurable
indicators of the theory’s concepts.
(4) Policy Implication: Explain how the research could help policymakers to adjust their decisions, or implement policy more
effectively or justly.
Here are examples of research ideas on other topics.
{content of two example ideas}
You should make sure to come up with your own novel and different ideas for the specified topic: {research topic}. You
should make each idea standalone and not dependent on the other ideas.
You should avoid repeating generating ideas with the following existing research questions, and try to be different and diverse:
{existing ideas generated}
Please write down your {number of ideas to generate} ideas. Output the ideas in json format as a dictionary, where the key
is ’ideas’, and the value is a list of ideas. Each idea has keys ’Research Question’, ’Theory’, ’Hypotheses’, and ’Policy
Implication’. The value of ’Hypotheses’ is a list of strings, and the value of other keys is a string.
Table 13: Example prompt for idea generation with AI-Researcher. The same idea format instructions, example
ideas, and metadata are also provided to GPT-Researcher and Chain-of-Ideas.Prompt for Both Idea Selection and Automatic Evaluation in §3
You are an expert researcher in political science. You are given two research ideas related to the topic {research topic}.
Your task is to identify which idea is better from the following four dimensions ’Significance’, ’Novelty’, ’Feasibility’, and
’Expected Effectiveness’.
Each research idea comprises the following three parts.
Research Question: A specific question about a behavior, event, or phenomenon of interest that the researcher wishes to seek
answers for in the research.
Theory: Reasonably speculate on the answer to the research question, including a statement about why the proposed answer
is correct.
Hypotheses: Identify observable implications of the theory, i.e., things we would observe if the theory is correct, and make
predictions about relationships between measurable indicators of the theory’s concepts.
Evaluation Criteria:
{detailed content of the evaluation criteria}
Note: Please make your decision based on the weighted assessment of sub-criteria to avoid subjective bias. Avoid any position
biases and ensure that the order of the two ideas does not influence your decision. DO NOT allow the LENGTH of the ideas
to influence your evaluation. Be as objective as possible.
Here are the two research ideas for you to assess:
Idea 1:
{content of idea 1}
Idea 2:
{content of idea 2}
Please provide an explanation supporting your assessment. At the last line of your response, format your assessment in JSON
with the keys: ’Significance’, ’Novelty’, ’Feasibility’, and ’Expected Effectiveness’. The value of each key is an integer
ranging from 0 to 2. 0 means a tie, 1 means idea 1 is better, and 2 means idea 2 is better.
Table 14: Example prompt for both idea selection and automatic evaluation in §3.
Prompt for Feasibility Check
You are an expert researcher in political science. Given a research idea with the components of ’Research Question’, ’Theory’,
’Hypotheses’, along with descriptions of existing data, please determine the feasibility of validating the hypotheses using the
provided data.
Here is the research idea:
{content of the idea}
Here is the existing data:
{content of the metadata in C LIMATE DATABANK }
Your task is as follows:
1. Feasibility Assessment:
- Evaluate whether it is possible to validate the hypotheses with the given data.
- If feasible, provide a validation plan and specify the data that will be used by their numbers. A hypothesis is considered
feasible to validate if the concepts in the hypothesis can be measured with existing data.
- If not feasible, output ’Feasibility’ as ’No’. Note that the theory provides an answer and explanation to the research question,
and the hypotheses identify observable implications of the theory.
2. Output Requirements:
- Format your response in JSON with the keys: ’Feasibility’, ’Validation Plan’, and ’Data Used’.
- ’Feasibility’: This can take values from [’Yes’, ’No’]. It indicates whether the hypotheses can be validated with the existing
data.
- ’Validation Plan’: A string detailing the plan to validate the hypotheses.
- ’Data Used’: A list of numbers denoting which data are utilized in the validation process, keep the number of them within 3.
As textual data is hard to handle, please only select necessary textual data, and keep the number of them within 1.
- If the hypotheses are infeasible to validate, only include ’Feasibility’ in the JSON output.
Table 15: Example prompt for feasibility check.Prompt for Hypothesis Validation
Please write code to validate the following hypotheses using the provided data.
Hypotheses:
{hypotheses within the idea}
Data:
{metadata of datasets selected}
The last line of your output should be the final answer, in the JSON format like {’Hypothesis 1’: ’Supported’, ...}.
The value for each hypothesis should be ’Supported’ or ’Not supported’. If the evidence for the hypothesis is insignifi-
cant/mixed/limited/partial, the hypothesis is also classified as not supported.
Table 16: Example prompt for hypothesis validation.
Prompt for Validation Process Summarization
Here is the validation process of several hypotheses. It contains steps in both text and code formats. For steps in text format,
the step contains keys ’type’ and ’content’. For steps in code format, the step contains keys ’type’, ’content’, and ’output’ or
’error’.
Please summarize the validation process in natural language, removing unnecessary steps and errors. Only keep the crucial
reasoning process and results that lead to the final conclusion.
Your output should be a list in json structure. Each item in the list is a dict with keys ’type’ and ’summarization’. The value of
’type’ is ’text’ or ’code’, and the value of ’summarization’ is a string describing the step. Limit the output into 1000 tokens.
Original Validation Steps:
{raw validation traces}
Output:
Table 17: Example prompt for validation process summarization.
Prompt for Idea Selection in §4
You are an expert researcher in political science. You are given two research ideas related to the topic {research topic}.
Your task is to identify which idea is better from the following four dimensions ’Significance’, ’Novelty’, ’Feasibility’, and
’Expected Effectiveness’.
Each research idea comprises the following four parts.
Research Question: A specific question about a behavior, event, or phenomenon of interest that the researcher wishes to seek
answers for in the research.
Theory: Reasonably speculate on the answer to the research question, including a statement about why the proposed answer
is correct.
Hypotheses: Identify observable implications of the theory, i.e., things we would observe if the theory is correct, and make
predictions about relationships between measurable indicators of the theory’s concepts.
Preliminary Validation: Summarization of the preliminary validation process of the hypotheses.
Evaluation Criteria:
{detailed content of the evaluation criteria}
Note: Please make your decision based on the weighted assessment of sub-criteria to avoid subjective bias. Avoid any position
biases and ensure that the order of the two ideas does not influence your decision. DO NOT allow the LENGTH of the ideas
to influence your evaluation. Be as objective as possible.
Here are the two research ideas for you to assess:
Idea 1:
{content of idea 1, containing the summarized validation process }
Idea 2:
{content of idea 2, containing the summarized validation process }
Please provide an explanation supporting your assessment. At the last line of your response, format your assessment in JSON
with the keys: ’Significance’, ’Novelty’, ’Feasibility’, and ’Expected Effectiveness’. The value of each key is an integer
ranging from 0 to 2. 0 means a tie, 1 means idea 1 is better, and 2 means idea 2 is better.
Table 18: Example prompt for idea selection in §4, which differs from Table 14 in adding the validation results.Figure 5: Annotation interface for human evaluation of idea pairs.Figure 6: Annotation interface for human evaluation of hypothesis validation processes.Figure 7: Experiment interface for the human study of proposing research ideas.