arXiv:2505.21393v1  [cs.LG]  27 May 2025Leveraging the Power of Conversations: Optimal Key Term
Selection in Conversational Contextual Bandits
Maoli Liu
The Chinese University of Hong Kong
Hong Kong, China
mlliu@cse.cuhk.edu.hkZhuohua Liâˆ—â€ 
Guangzhou Institute of Technology, Xidian University
Guangzhou, Guangdong, China
zhli@cse.cuhk.edu.hk
Xiangxiang Dai
The Chinese University of Hong Kong
Hong Kong, China
xxdai23@cse.cuhk.edu.hkJohn C.S. Lui
The Chinese University of Hong Kong
Hong Kong, China
cslui@cse.cuhk.edu.hk
Abstract
Conversational recommender systems proactively query users with
relevant â€œ key terms â€ and leverage the feedback to elicit usersâ€™ pref-
erences for personalized recommendations. Conversational con-
textual bandits, a prevalent approach in this domain, aim to op-
timize preference learning by balancing exploitation and explo-
ration. However, several limitations hinder their effectiveness in
real-world scenarios. First, existing algorithms employ key term
selection strategies with insufficient exploration, often failing to
thoroughly probe usersâ€™ preferences and resulting in suboptimal
preference estimation. Second, current algorithms typically rely
on deterministic rules to initiate conversations, causing unneces-
sary interactions when preferences are well-understood and missed
opportunities when preferences are uncertain. To address these
limitations, we propose three novel algorithms: CLiSK, CLiME,
and CLiSK-ME. CLiSK introduces smoothed key term contexts to
enhance exploration in preference learning, CLiME adaptively initi-
ates conversations based on preference uncertainty, and CLiSK-ME
integrates both techniques. We theoretically prove that all three
algorithms achieve a tighter regret upper bound of O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡)
with respect to the time horizon ğ‘‡, improving upon existing meth-
ods. Additionally, we provide a matching lower bound Î©(âˆš
ğ‘‘ğ‘‡)
for conversational bandits, demonstrating that our algorithms are
nearly minimax optimal. Extensive evaluations on both synthetic
and real-world datasets show that our approaches achieve at least
a 14.6% improvement in cumulative regret.
CCS Concepts
â€¢Information systems â†’Recommender systems ;â€¢Theory of
computationâ†’Online learning algorithms ;Online learning
theory .
âˆ—Zhuohua Li is the corresponding author.
â€ Also with The Chinese University of Hong Kong.
This work is licensed under a Creative Commons Attribution 4.0 International License.
KDD â€™25, Toronto, ON, Canada
Â©2025 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-1454-2/2025/08
https://doi.org/10.1145/3711896.3737025Keywords
Conversational Recommendation, Preference Learning, Contextual
Bandits, Online Learning
ACM Reference Format:
Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui. 2025. Leveraging
the Power of Conversations: Optimal Key Term Selection in Conversational
Contextual Bandits. In Proceedings of the 31st ACM SIGKDD Conference
on Knowledge Discovery and Data Mining V.2 (KDD â€™25), August 3â€“7, 2025,
Toronto, ON, Canada. ACM, New York, NY, USA, 17 pages. https://doi.org/
10.1145/3711896.3737025
KDD Availability Link:
The source code of this paper has been made publicly available at https:
//doi.org/10.5281/zenodo.15490021.
1 Introduction
Recommender systems play a crucial role in applications like movie
recommendations, online advertising, and personalized news feeds,
where providing relevant and engaging content is essential for user
satisfaction. To cater to diverse user interests, recommender sys-
tems are designed to interact with users and continuously learn
from their feedback. For instance, in product and news recommen-
dations, the system can monitor usersâ€™ real-time click rates and
accordingly refine its recommendations. Modern recommender sys-
tems incorporate advanced online learning techniques to adapt in
real time and uncover previously unknown user preferences.
A fundamental challenge in recommender systems is the trade-
off between exploration (i.e., recommending new items to uncover
usersâ€™ unknown preferences) and exploitation (i.e., recommending
items that align with usersâ€™ historical preferences). Contextual ban-
dits [ 15] address this trade-off by enabling the system to learn from
user interactions continuously while optimizing recommendations
without compromising the user experience. In this framework, each
item to be recommended is treated as an â€œ armâ€, represented by a fea-
ture vector. At each round, the agent (i.e., the recommender system)
recommends an arm to the user based on historical interactions and
the context of each arm, and then receives feedback/rewards (e.g.,
clicks). The objective of the algorithm executed by the agent is to
design an arm recommendation strategy that maximizes cumulative
reward (or equivalently, minimizes cumulative regret) over time.
Another major challenge in recommender systems is the â€œ cold
startâ€ problem, where the system initially lacks sufficient data aboutKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
def phase_elimination(num_arms, num_rounds, 
arms_means):
    remaining_arms = list(range(num_arms))
    rounds_per_phase = num_rounds // 
               int(np.log2(num_rounds))
    for phase in range(int(np.log2(num_rounds))):
        for arm in remaining_arms:
    . . .
    return remaining_arms[0]
    . . .
print(f"The best arm identified is: {best_arm}")python
class PhaseElimination:
    def _ _init_ _(self, num_arms, num_rounds, 
arms_means):
     . . .
          self.remaining_arms = list(range(num_arms))
    def run(self):
           rounds_per_phase = self.num_rounds //                                                                
int(np.log2(self.num_rounds))
    . . .
           return self.remaining_arms[0]
    . . .
print(f"The best arm identified is: {best_arm}")python
Response 1
Phase elimination algorithm is implemented as a function:
Response 2
Phase elimination algorithm is implemented as a class:Which response do you prefer?
Your choice will help make ChatGPT better.
Figure 1: Illustration of conversational recommendation by
ChatGPT, where users select their preferred response from
presented options.
new usersâ€™ preferences, making accurate recommendations difficult.
Conversational recommender systems (CRSs) [ 5,11,23,30] have
emerged as a promising solution. Unlike traditional systems that
rely solely on feedback from recommended items, CRSs can actively
initiate queries with users to collect richer feedback and quickly
infer their preferences. For example, as shown in Figure 1, platforms
like ChatGPT occasionally present users with multiple response
options and allow them to select their preferred one. Through these
interactions, ChatGPT can refine its understanding and improve fu-
ture responses to better align with user preferences. To model these
interactions, conversational contextual bandits [ 29] are proposed
as a natural extension of contextual bandits. In this framework,
besides recommending items (arms) and observing arm-level feed-
back, the agent can proactively prompt users with questions about
key terms and receive key term-level feedback. The key terms are
related to a subset of arms, providing valuable insights into usersâ€™
preferences and improving recommendation quality.
Despite recent advances in conversational contextual bandits [ 25,
27, 28], existing approaches still face the following limitations:
â€¢Insufficient Exploration in Key Term Selection : Existing
studies about conversational bandits fail to sufficiently explore
key terms, limiting their effectiveness in preference learning.
Zhang et al . [29] introduce the ConUCB algorithm with a re-
gret upper bound of O(ğ‘‘âˆš
ğ‘‡logğ‘‡), whereğ‘‘is the dimension
andğ‘‡is the number of rounds. However, despite incorporating
additional queries about key terms, the method does not yield
substantial improvement over non-conversational approaches.
Since then, improving regret through conversational interactions
has remained an open problem in the field. Wang et al . [25] and
Yang et al . [28] introduce an additional assumption that the key
term set spans Rğ‘‘and propose the ConLinUCB-BS and ConDuel
algorithms, respectively. The two algorithms reduce aâˆšï¸
logğ‘‡
term in the regret, but worsen the dependence on ğ‘‘(as discussed
in Section 4.4), resulting in a suboptimal regret bound. To achieve
optimal regret, more explorative key term selection strategies
are needed to efficiently gather informative user feedback and
improve learning efficiency.
â€¢Inflexible Conversation Mechanism : Existing conversational
bandit algorithms [ 27,29] often use a deterministic function tocontrol the frequency of conversations. Specifically, the agent
can only initiate ğ‘„conversations at once per ğ‘ƒrounds, where ğ‘ƒ
andğ‘„are fixed integers. However, this rigid approach is imprac-
tical and insufficient in real-world scenarios. For example, in a
music streaming service, a fixed-frequency approach may cause
unnecessary interactions when usersâ€™ preferences are already
well-understood, disrupting the listening experience. Conversely,
it may fail to collect feedback when the uncertainty is high, lead-
ing to suboptimal recommendations. To address these limitations,
a more adaptive conversation mechanism is needed to adjust the
interaction frequency based on the preference uncertainty.
Motivated by these observations, we develop three algorithms
aimed at improving conversational contextual bandits. To start, we
introduce the concept of â€œ smoothed key term contexts â€, inspired by
the smoothed analysis for contextual bandits [ 13], and propose
theConversational LinUCB with Smoothed Key terms (CLiSK)
algorithm. Specifically, CLiSK launches conversations at a fixed
frequency, similar to Zhang et al . [29] , but greedily selects key
terms that are slightly perturbed by Gaussian noise. For example, in
movie recommendations, instead of asking directly about a genre
like â€œcomedyâ€ or â€œdramaâ€, CLiSK blends elements of related genres,
such as â€œcomedy-dramaâ€ or â€œdark comedyâ€. This approach helps
the system explore usersâ€™ preferences in a more nuanced manner.
We will show that these small perturbations have strong theoretical
implications , allowing the agent to explore the feature space more
effectively and speed up the learning process.
We next develop the Conversational LinUCB with Minimum
Eigenvalues (CLiME) algorithm, which introduces an adaptive con-
versational mechanism driven by preference uncertainty. Unlike
the fixed-frequency approach of Wang et al . [25] , Zhang et al . [29] ,
CLiME assesses preference uncertainty and initiates conversations
only when the uncertainty is high, thereby maximizing information
gain while avoiding unnecessary interactions. When a conversa-
tion is triggered, CLiME selects key terms that target the areas
of highest uncertainty within the feature space, rapidly refining
user preferences. This adaptive approach not only ensures that
conversations are timely and relevant, but also improves the user
experience. Additionally, we design a family of uncertainty check-
ing functions to determine when to assess the uncertainty, offering
greater flexibility and better alignment with diverse applications.
The smoothed key term contexts approach in CLiSK and the
adaptive conversation technique in CLiME are orthogonal, allow-
ing them to be applied independently or in combination. Therefore,
we further propose the CLiSK-ME algorithm, which integrates both
techniques to maximize exploration efficiency and adaptively ad-
just user interactions. By leveraging the strengths of both methods,
CLiSK-ME enhances exploration efficiency and optimizes user in-
teractions for improved preference learning.
Our algorithms introduce advanced key term selection strategies,
significantly enhancing the efficiency of conversational contextual
bandits. Theoretically, we prove that CLiSK achieves a regret upper
bound ofO(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡+ğ‘‘), while CLiME and CLiSK-ME achieve a
regret upper bound of O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡). Notably, all three algorithms
reduce the dependence on ğ‘‡by a factor ofâˆš
ğ‘‘compared to prior
studies. To the best of our knowledge, our work is the first to
achieve the eO(âˆš
ğ‘‘ğ‘‡)regret in the conversational bandit literature. InLeveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
addition, we establish a matching lower bound of Î©(âˆš
ğ‘‘ğ‘‡), showing
that our algorithms are minimax optimal up to logarithmic factors.
In summary, our contributions are listed as follows.
â€¢We propose three novel conversational bandit algorithms: CLiSK
with smoothed key term contexts, CLiME with an adaptive con-
versation mechanism, and CLiSK-ME, which integrates both for
improved preference learning.
â€¢We establish the minimax optimality of our algorithms by prov-
ing regret upper bounds of O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡+ğ‘‘)for CLiSK and
O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡)for CLiME and CLiSK-ME, along with a matching
lower bound of Î©(âˆš
ğ‘‘ğ‘‡). These results underscore the theoretical
advancements achieved by our methods.
â€¢We conduct extensive evaluations on both synthetic and real-
world datasets, showing that our algorithms reduce regret by
over 14.6% compared to baselines.
2 Problem Formulation
In conversational contextual bandits, an agent interacts with a user
overğ‘‡âˆˆN+rounds. The userâ€™s preferences are represented by
a fixed but unknown vector ğœ½âˆ—âˆˆRğ‘‘, whereğ‘‘is the dimension.
The agentâ€™s goal is to learn ğœ½âˆ—to recommend items that align with
the userâ€™s preferences. There exists a finite arm set denoted by A,
where each arm ğ‘âˆˆA represents an item and is associated with
a feature vector ğ’™ğ‘âˆˆRğ‘‘. We denote[ğ‘‡]={1,2,...,ğ‘‡}. At each
roundğ‘¡âˆˆ[ğ‘‡], the agent is given a subset of arms Ağ‘¡âŠ†A . The
agent then selects an arm ğ‘ğ‘¡âˆˆAğ‘¡and receives a reward ğ‘Ÿğ‘ğ‘¡,ğ‘¡. The
reward is assumed to be linearly related to the preference vector
and the feature vector of the arm, i.e., ğ‘Ÿğ‘ğ‘¡,ğ‘¡=ğ’™âŠ¤ğ‘ğ‘¡ğœ½âˆ—+ğœ‚ğ‘¡, whereğœ‚ğ‘¡
is a random noise term.
Letğ‘âˆ—
ğ‘¡be the optimal arm at round ğ‘¡, i.e.,ğ‘âˆ—
ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ’™âŠ¤ğ‘ğœ½âˆ—.
The agentâ€™s objective is to minimize the cumulative regret, which is
defined as the total difference between the rewards of the optimal
arms and the rewards obtained by the agent, i.e.,
R(ğ‘‡)=ğ‘‡âˆ‘ï¸
ğ‘¡=1
ğ’™âŠ¤
ğ‘âˆ—
ğ‘¡ğœ½âˆ—âˆ’ğ’™âŠ¤
ğ‘ğ‘¡ğœ½âˆ—
.
Beyond observing the userâ€™s preference information through
arm recommendations, the agent can gather additional feedback
by launching conversations involving key terms. Specifically, a
â€œkey termâ€ represents a category or keyword associated with a
subset of arms. For example, in movie recommendations, key terms
might include genres like â€œcomedyâ€ or â€œthrillerâ€, and themes such as
â€œromanceâ€ or â€œsci-fiâ€. Let Kdenote the finite set of key terms, where
each key term ğ‘˜âˆˆK corresponds to a context vector Ëœğ’™ğ‘˜âˆˆRğ‘‘.
At roundğ‘¡, if a conversation is initiated, the agent selects a key
termğ‘˜âˆˆK, queries the user, and receives key-term level feedback
Ëœğ‘Ÿğ‘˜,ğ‘¡. We follow the formulation of Wang et al . [25] that the userâ€™s
preference vector ğœ½âˆ—remains consistent across both arms and key
terms. The relationship between key terms and the userâ€™s preference
is also linear, i.e., Ëœğ‘Ÿğ‘˜,ğ‘¡=Ëœğ’™âŠ¤
ğ‘˜ğœ½âˆ—+Ëœğœ‚ğ‘¡, where Ëœğœ‚ğ‘¡is a random noise term.
We list and explain our assumptions as follows. Both Assump-
tions 1 and 2 are consistent with previous works on conversational
contextual bandits [25, 29] and linear contextual bandits [1, 15].
Assumption 1. We assume that the feature vectors for both arms
and key terms are normalized, i.e., âˆ¥ğ’™ğ‘âˆ¥2=1andâˆ¥Ëœğ’™ğ‘˜âˆ¥2=1for allğ‘âˆˆA andğ‘˜âˆˆK. We also assume the unknown preference vector
ğœ½âˆ—is bounded, i.e.,âˆ¥ğœ½âˆ—âˆ¥2â‰¤1.
Assumption 2. We assume the noise terms ğœ‚ğ‘¡,Ëœğœ‚ğ‘¡are conditionally
independent and 1-sub-Gaussian across ğ‘‡rounds.
3 Algorithm Design
In this section, we introduce our proposed algorithms, outlining
their key components and implementation details.
3.1 CLiSK Algorithm
To enhance the exploration of usersâ€™ preferences, we introduce the
smoothed key term contexts and propose the CLiSK algorithm, de-
tailed in Algorithm 1. The algorithm consists of two main modules:
key term selection (Lines 4 to 10) and arm selection (Lines 11 to 16).
Specifically, in each round ğ‘¡, the agent first determines whether to
initiate a conversation based on a predefined query budget (Lines 2
and 3). If a conversation is initiated, the agent selects a key term
ğ‘˜(Line 5) and queries the user about it. Subsequently, the agent
updates its estimate of the preference vector ğœ½ğ‘¡(Line 11) and selects
an armğ‘ğ‘¡for recommendation (Line 12). The strategies for key
term selection and arm selection are elaborated as follows.
Algorithm 1: CLiSK
Input:A,K,ğ‘(ğ‘¡),ğœ†,{ğ›¼ğ‘¡}ğ‘¡>0
Initialization: ğ‘´1=ğœ†ğ‘°ğ‘‘,ğ’ƒ1=0ğ‘‘
1forğ‘¡=1,...,ğ‘‡ do
2ğ‘ğ‘¡=âŒŠğ‘(ğ‘¡)âŒ‹âˆ’âŒŠğ‘(ğ‘¡âˆ’1)âŒ‹
3 whileğ‘ğ‘¡>0do
4 Smooth the key term contexts to get {ËœËœğ’™ğ‘˜}ğ‘˜âˆˆK
5 Select a key term ğ‘˜=arg maxğ‘˜âˆˆKËœËœğ’™âŠ¤
ğ‘˜ğœ½ğ‘¡
6 Query the userâ€™s feedback for ğ‘˜
7 Receive the key term-level feedback Ëœğ‘Ÿğ‘˜,ğ‘¡
8 ğ‘´ğ‘¡=ğ‘´ğ‘¡+ËœËœğ’™ğ‘˜,ğ‘¡ËœËœğ’™âŠ¤
ğ‘˜,ğ‘¡
9 ğ’ƒğ‘¡=ğ’ƒğ‘¡+Ëœğ‘Ÿğ‘˜,ğ‘¡ËœËœğ’™ğ‘˜,ğ‘¡
10ğ‘ğ‘¡=ğ‘ğ‘¡âˆ’1
11 ğœ½ğ‘¡=ğ‘´âˆ’1
ğ‘¡ğ’ƒğ‘¡
12 Selectğ‘ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ’™âŠ¤ğ‘ğœ½ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘€âˆ’1
ğ‘¡
13 Ask the userâ€™s preference for arm ğ‘ğ‘¡
14 Observe the reward ğ‘Ÿğ‘ğ‘¡,ğ‘¡
15 ğ‘´ğ‘¡+1=ğ‘´ğ‘¡+ğ’™ğ‘ğ‘¡ğ’™âŠ¤ğ‘ğ‘¡
16 ğ’ƒğ‘¡+1=ğ’ƒğ‘¡+ğ‘Ÿğ‘ğ‘¡,ğ‘¡ğ’™ğ‘ğ‘¡
3.1.1 Intuition Overview. Building on insights from Kannan et al .
[13]and Raghavan et al . [20] , we add small perturbations to the key
term contexts to deepen the exploration of usersâ€™ preferences. These
perturbations increase data diversity and help uncover preferences
that might be overlooked when selecting key terms directly. For
instance, instead of using â€œcomedyâ€ alone, variations like â€œromantic
comedyâ€ or â€œdark comedyâ€ can reveal more specific preferences.
Below is the formal definition of smoothed key term contexts,
where the perturbations are modeled as Gaussian noise.KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
Definition 1 (Smoothed Key Term Contexts) .Given a key term set
K, the smoothed key term contexts are defined as {ËœËœğ’™ğ‘˜}ğ‘˜âˆˆK, where
ËœËœğ’™ğ‘˜=Ëœğ’™ğ‘˜+ğœºğ‘˜for eachğ‘˜âˆˆK. The noise vector ğœºğ‘˜is independently
drawn from a truncated multivariate Gaussian distribution N(0,ğœŒ2Â·
ğ‘°ğ‘‘), where ğ‘°ğ‘‘is theğ‘‘-dimensional identity matrix and ğœŒ2controls
the level of perturbations. Each dimension of ğœºğ‘˜is truncated within
[âˆ’ğ‘…,ğ‘…]for someğ‘…>0, i.e.,|(ğœºğ‘˜)ğ‘—|â‰¤ğ‘…,âˆ€ğ‘—âˆˆ[ğ‘‘].
3.1.2 Key Term Selection. When initiating conversations, the agent
no longer selects key terms directly based on their original contexts.
Instead, the agent applies a small random perturbation to each key
termâ€™s context, as defined in Definition 1 (Line 4). It then greedily
selects the key term with the highest value under the perturbed
contexts, i.e., ğ‘˜=arg maxğ‘˜âˆˆKËœËœğ’™âŠ¤
ğ‘˜ğœ½ğ‘¡(Line 5).
Remark 1.Note that the smoothed key term contexts are re-generated
foreach conversation . For notational consistency, we use the same
notation{ËœËœğ’™ğ‘˜}ğ‘˜âˆˆKto represent the smoothed key term contexts
across different conversations.
3.1.3 Conversation Frequency. Following Zhang et al . [29] , CLiSK
uses a deterministic function ğ‘(ğ‘¡)to regulate the frequency of con-
versation initiation. The function ğ‘(ğ‘¡)is monotonically increasing
regardingğ‘¡and satisfies ğ‘(0)=0. At roundğ‘¡, the agent initiates
ğ‘(ğ‘¡)=âŒŠğ‘(ğ‘¡)âŒ‹âˆ’âŒŠğ‘(ğ‘¡âˆ’1)âŒ‹conversations if ğ‘(ğ‘¡)>0; otherwise, no
conversation is conducted.
3.1.4 Arm Selection. CLiSK uses the Upper Confidence Bound
(UCB) strategy for arm selection, a prevalent method in linear ban-
dits. At round ğ‘¡, the agent updates its estimated preference vector
ğœ½ğ‘¡based on both arm-level and key term-level feedback. This esti-
mation follows a ridge regression framework with regularization
parameterğœ†, i.e., ğœ½ğ‘¡=ğ‘´âˆ’1
ğ‘¡ğ’ƒğ‘¡, with ğ‘´ğ‘¡andğ’ƒğ‘¡defined as
ğ‘´ğ‘¡=ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğ’™âŠ¤
ğ‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜+ğœ†ğ‘°ğ‘‘,
ğ’ƒğ‘¡=ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ‘Ÿğ‘ğ‘ ,ğ‘ ğ’™ğ‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ Ëœğ‘Ÿğ‘˜,ğ‘ ËœËœğ’™ğ‘˜,
whereKğ‘ is the set of key terms selected at round ğ‘ .ğ‘´ğ‘¡is commonly
referred to as the covariance matrix.
After the update, the agent selects the arm with the highest
UCB value, i.e., ğ‘ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ’™âŠ¤ğ‘ğœ½ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡, whereâˆ¥ğ’™âˆ¥ğ‘´
denotes the Mahalanobis normâˆš
ğ’™âŠ¤ğ‘´ğ’™and{ğ›¼ğ‘¡}ğ‘¡>0are parameters
designed to balance the exploration-exploitation trade-off.
3.2 CLiME Algorithm
To enable more adaptive and flexible conversation initiation, we in-
troduce the CLiME algorithm, detailed in Algorithm 2. The CLiME
adopts the same arm selection strategy as CLiSK, but it introduces
key innovations in determining when to initiate conversations and
which key terms to select. Unlike CLiSK, which follows a determin-
istic function ğ‘(ğ‘¡)for scheduling conversations, CLiME adaptively
determines when to conduct a conversation based on the uncer-
tainty in the preference estimation.Algorithm 2: CLiME
Input:A,K,ğœ†,ğ›¼,{ğ›¼ğ‘¡}ğ‘¡>0
Initialization: ğ‘´1=ğœ†ğ‘°ğ‘‘,ğ’ƒ1=0ğ‘‘
1forğ‘¡=1,...,ğ‘‡ do
2 ifUncertaintyChecking( ğ‘¡)then
3 Diagonalize ğ‘´ğ‘¡=Ãğ‘‘
ğ‘–=1ğœ†ğ’—ğ‘–ğ’—ğ‘–ğ’—ğ‘–âŠ¤
4 foreachğœ†ğ’—ğ‘–<ğ›¼ğ‘¡do
5 ğ‘˜=arg maxğ‘˜âˆˆK|Ëœğ’™âŠ¤
ğ‘˜ğ’—ğ‘–|
6 ğ‘›ğ‘˜=âŒˆ(ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–)/ğ‘2
0âŒ‰
7 Scheduleğ‘›ğ‘˜conversations about the key term ğ‘˜
before next uncertainty checking
8 Update ğ‘´ğ‘¡andğ’ƒğ‘¡accordingly
9 ğœ½ğ‘¡=ğ‘´âˆ’1
ğ‘¡ğ’ƒğ‘¡
10 Selectğ‘ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ’™âŠ¤ğ‘ğœ½ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘€âˆ’1
ğ‘¡
11 Ask the userâ€™s preference for arm ğ‘ğ‘¡
12 Observe the reward ğ‘Ÿğ‘ğ‘¡,ğ‘¡
13 ğ‘´ğ‘¡+1=ğ‘´ğ‘¡+ğ’™ğ‘ğ‘¡ğ’™âŠ¤ğ‘ğ‘¡
14 ğ’ƒğ‘¡+1=ğ’ƒğ‘¡+ğ‘Ÿğ‘ğ‘¡,ğ‘¡ğ’™ğ‘ğ‘¡
3.2.1 Intuition Overview. The main idea behind CLiME is to adap-
tively initiate conversations based on the current level of uncer-
tainty in the estimated preference and use key terms to explore
the uncertain directions effectively. Specifically, the covariance ma-
trixğ‘´ğ‘¡encodes information about the feature space, where its
eigenvectors represent the principal directions within the space,
and the corresponding eigenvalues indicate the level of uncertainty
along these directions. A smaller eigenvalue indicates a higher
uncertainty in the associated direction. Therefore, by guiding the
agent to explore such high-uncertainty directions, the agent can
reduce uncertainty and improve learning efficiency. If the minimum
eigenvalue of ğ‘´ğ‘¡remains above a certain value, the agent ensures
sufficient exploration of the feature space. To facilitate exploration,
we introduce the following assumption.
Assumption 3. We assume that the elements in the key term set K
are sufficiently rich and diverse, such that for any ğ’™âˆˆRğ‘‘satisfying
âˆ¥ğ’™âˆ¥2=1, there exists a key term ğ‘˜âˆˆK such that|Ëœğ’™âŠ¤
ğ‘˜ğ’™|â‰¥ğ‘0,
whereğ‘0is some constant close to 1.
This mild assumption ensures that the key term set Kis com-
prehensive enough to cover all relevant directions in the feature
space. In other words, for any direction ğ’™that the agent might need
to explore, there exists a key term ğ‘˜âˆˆK whose context Ëœğ’™ğ‘˜aligns
sufficiently well with ğ’™. This diversity allows the agent to effec-
tively reduce uncertainty by exploring underrepresented directions,
thereby improving preference learning.
3.2.2 Conversation Initiation and Key Term Selection. In CLiME,
conversation initiation and key term selection are designed to max-
imize the information gained from user interactions. As shown
in Algorithm 2, the agent first evaluates the eigenvalues of the
covariance matrix ğ‘´ğ‘¡(Line 3). If any eigenvalue ğœ†ğ’—ğ‘–falls below a
certain threshold (derived from Section 4.2), i.e., ğœ†ğ’—ğ‘–<ğ›¼ğ‘¡(Line 4),
the agent prompts ğ‘›ğ‘˜=âŒˆ(ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–)/ğ‘2
0âŒ‰conversations by selectingLeveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
key terms that most closely align with the corresponding eigen-
vector ğ’—ğ‘–(Lines 5 to 7). Here, ğ›¼âˆˆ(0,ğ‘2
0)is an exploration control
parameter that regulates the exploration level. Note that the agent
can distribute these ğ‘›ğ‘˜conversations across multiple rounds before
re-evaluating the eigenvalues of the covariance matrix.
To further enhance flexibility and accommodate diverse real-
world applications, we design an uncertainty checking function
UncertaintyChecking( ğ‘¡)(Line 2). This function determines when
to assess uncertainty and potentially trigger conversations. Exam-
ples of such checking functions are given as follows.
â€¢Continuous Checking : The agent assesses uncertainty at every
round and initiates conversations as needed.
â€¢Fixed Interval Checking : The agent assesses uncertainty every
ğ‘ƒrounds, where ğ‘ƒis a fixed integer.
â€¢Exponential Phase Checking : The agent evaluates uncertainty
at exponentially increasing intervals of 2ğ‘–, whereğ‘–=1,2,....
Remark 2.The uncertainty checking functions in CLiME differ
fundamentally from the frequency function ğ‘(ğ‘¡)in ConUCB [ 29].
Specifically, these checking functions regulate how often uncer-
tainty is assessed but do not directly dictate conversation initiation.
In contrast, ğ‘(ğ‘¡)deterministically controls both the timing and
number of conversations. CLiME and ConUCB also differ in how
they select key terms, further distinguishing the two approaches.
Remark 3.It is worth noting that the smoothed key term contexts
approach in CLiSK and the adaptive conversation technique in
CLiME are orthogonal. The two strategies can operate indepen-
dently or be integrated to enhance learning efficiency further. To
this end, we introduce the CLiSK-ME algorithm, detailed in Ap-
pendix A.1, which integrates both approaches to leverage their
complementary strengths.
4 Theoretical Analysis
This section presents the theoretical results of our algorithms,
which employ analytical techniques that differ from standard linear
bandit methods. Detailed proofs of all lemmas and theorems are
provided in the Appendices.
4.1 Regret Analysis of CLiSK Algorithm
Following Zhang et al . [29] and Wang et al . [25] , we assume ğ‘(ğ‘¡)=
ğ‘ğ‘¡for someğ‘âˆˆ(0,1). We start with Lemma 1, which bounds the
difference between the estimated and true rewards for each arm.
Lemma 1. Under Assumptions 1 and 2, for CLiSK, for any round
ğ‘¡âˆˆ[ğ‘‡]and any arm ğ‘âˆˆA, with probability at least 1âˆ’ğ›¿for some
ğ›¿âˆˆ(0,1), we have
ğ’™âŠ¤
ğ‘ğœ½ğ‘¡âˆ’ğ’™âŠ¤
ğ‘ğœ½âˆ—â‰¤ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡,
whereğ›¼ğ‘¡=vt
2 log(1
ğ›¿)+ğ‘‘log 
1+ğ‘¡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘¡
ğœ†ğ‘‘!
+âˆš
ğœ†.
Next, we examine the smoothed key term contexts and their
impact on exploring the feature space.
Lemma 2. For any round ğ‘¡âˆˆ [ğ‘‡], with the smoothed key term
contexts in Definition 1, CLiSK has the following lower bound on theminimum eigenvalue of the matrix E[ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜]for anyğ‘˜âˆˆKğ‘¡, i.e.,
ğœ†min
E[ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜]
â‰¥ğ‘1ğœŒ2
log|K|â‰œğœ†K,
whereğ‘1âˆˆ(0,1)is some constant.
Lemma 2 provides a lower bound on the minimum eigenvalue of
the expected outer product of the selected key term. Intuitively, this
implies that under smoothed contexts, the selected key terms exhibit
sufficient diversity in the feature space, ensuring that each query
contributes meaningful information about the userâ€™s preferences.
Lemma 3. For CLiSK, with probability at least 1âˆ’ğ›¿for someğ›¿âˆˆ
(0,1), ifğ‘¡â‰¥ğ‘‡0â‰œ8(1+âˆš
ğ‘‘ğ‘…)2
ğ‘ğœ†Klog
ğ‘‘
ğ›¿
, we have
ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ÂªÂ®
Â¬â‰¥ğœ†Kğ‘ğ‘¡
2.
Lemma 3 establishes a lower bound on the minimum eigenvalue
of the Gram matrix that grows linearly with time ğ‘¡. This guarantees
that CLiSK accumulates enough statistical information to effectively
estimate the userâ€™s preference vector through ridge regression. Fol-
lowing these results, we bound âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡in Lemma 4 and derive a
high-probability regret upper bound for CLiSK in Theorem 1.
Lemma 4. For CLiSK, for any ğ‘âˆˆA, ifğ‘¡â‰¥ğ‘‡0â‰œ8(1+âˆš
ğ‘‘ğ‘…)2
ğ‘ğœ†Klog
ğ‘‘
ğ›¿
,
with probability at least 1âˆ’ğ›¿for someğ›¿âˆˆ(0,1),âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
2
ğœ†Kğ‘ğ‘¡.
Theorem 1 (Regret of CLiSK) .With probability at least 1âˆ’ğ›¿for
someğ›¿âˆˆ(0,1), the regret upper bound of CLiSK satisfies
R(ğ‘‡)â‰¤8(1+âˆš
ğ‘‘ğ‘…)2log(|K|)
ğ‘1ğœŒ2ğ‘logğ‘‘
ğ›¿
+4âˆšï¸„
2ğ‘1ğœŒ2ğ‘‡
ğ‘log(|K|)Â·
Â©Â­Â­Â­
Â«vuuuut
2 log1
ğ›¿
+ğ‘‘logÂ©Â­Â­
Â«1+ğ‘‡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘‡
ğœ†ğ‘‘ÂªÂ®Â®
Â¬+âˆš
ğœ†ÂªÂ®Â®Â®
Â¬
=O(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)+ğ‘‘),
whereğ‘…andğœŒ2are constants in Definition 1.
4.2 Regret Analysis of CLiME Algorithm
We begin with Lemma 5, which closely parallels Lemma 1.
Lemma 5. Letğœ½ğ‘¡be the estimated preference vector at round ğ‘¡and
ğœ½âˆ—be the true preference vector. Under Assumptions 1, 2 and 3, for
CLiME, at round ğ‘¡, for any arm ğ‘âˆˆA, with probability at least 1âˆ’ğ›¿
(ğ›¿âˆˆ(0,1)), we haveğ’™âŠ¤
ğ‘ğœ½ğ‘¡âˆ’ğ’™âŠ¤
ğ‘ğœ½âˆ—â‰¤ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡,
whereğ›¼ğ‘¡=âˆšï¸„
2 log(1
ğ›¿)+ğ‘‘log
1+ğ‘¡+ğ›¼ğ‘‘ğ‘¡
ğœ†ğ‘‘ğ‘2
0
+âˆš
ğœ†,ğ›¼is an exploration
control factor in Algorithm 2, and ğ‘0is a constant in Assumption 3.
Since conversations are initiated adaptively in CLiME, the num-
ber of conversations conducted up to each round ğ‘¡is not determin-
istic. A key challenge to prove Lemma 5 is to bound this quantity.
Then, we present Lemma 6, which bounds âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡.KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
Lemma 6. For CLiME, for any arm ğ‘âˆˆA, with probability at least
1âˆ’ğ›¿for someğ›¿âˆˆ(0,1), at roundğ‘¡â‰¥2ğ‘ƒ, we haveâˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
2
ğ›¼ğ‘¡,
whereğ‘ƒis a fixed integer.
The proof of Lemma 6 relies on establishing a lower bound on the
minimum eigenvalue of ğ‘´ğ‘¡, i.e.,ğœ†min(ğ‘´ğ‘¡)â‰¥ğ›¼ğ‘¡, which involves a
delicate analysis of covariance matrix eigenvalues. The condition
ğ‘¡â‰¥2ğ‘ƒis introduced to generalize all three checking functions.
Building on this, we derive the following theorem for CLiME.
Theorem 2 (Regret of CLiME) .With probability at least 1âˆ’ğ›¿for
someğ›¿âˆˆ(0,1), the regret upper bound of CLiME satisfies
R(ğ‘‡)â‰¤4âˆšï¸‚
2ğ‘‡
ğ›¼Â©Â­
Â«vut
2 log(1
ğ›¿)+ğ‘‘log 
1+ğ‘‡+ğ›¼ğ‘‘ğ‘‡
ğœ†ğ‘‘ğ‘2
0!
+âˆš
ğœ†ÂªÂ®
Â¬+2ğ‘ƒ
=Oâˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)
.
Remark 4.Note that Theorem 2 applies to all three uncertainty
checking functions discussed in CLiME algorithm, which under-
scores the generality of our methods.
CLiSK-ME combines the advantages of both smoothed key term
contexts and adaptive conversation techniques, ensuring efficient
exploration while adaptively adjusting conversation frequency
based on uncertainty. As a result, we derive the following corollary.
Corollary 1. With probability at least 1âˆ’ğ›¿for someğ›¿âˆˆ(0,1), the
regret upper bound of CLiSK-ME satisfies R(ğ‘‡)=O(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)).
4.3 Lower Bound for Conversational Bandits
We establish a regret lower bound for conversational bandits with
finite andtime-varying arm sets. Our result is novel because the well-
known lower bound Î©(âˆš
ğ‘‘ğ‘‡)by Chu et al . [6] does not consider
conversational information and thus cannot be directly applied to
our setting. Additionally, the existing lower bound for federated
conversational bandits [ 19] is also inapplicable, as it assumes a fixed
arm set. The detailed proof is given in Appendix A.11.
Theorem 3 (Regret lower bound) .For any policy that chooses
at most one key term per time step, there exists an instance of the
conversational bandit problem such that the expected regret is at least
Î©(âˆš
ğ‘‘ğ‘‡). Furthermore, for any ğ‘‡=2ğ‘šwithğ‘šâˆˆ[ğ‘‘], the regret is at
leastÎ©(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)).
4.4 Discussion on Optimality
To the best of our knowledge, we are the first to propose algorithms
for conversational contextual bandits that achieve the optimal regret
bound of order eO(âˆš
ğ‘‘ğ‘‡). We summarize the regret bounds of our
proposed algorithms and related algorithms in Table 1 and discuss
the theoretical improvements over existing methods.
The regret upper bound of LinUCB [ 1] isO(ğ‘‘âˆš
ğ‘‡logğ‘‡), which
serves as a standard benchmark in contextual linear bandits. The
first algorithm for conversational bandits, ConUCB [ 29], offers the
same regret upper bound as LinUCB, indicating that it does not offer
a substantial theoretical improvement over the non-conversational
algorithms. Since then, improving regret through conversational
interactions has remained an open problem in the field. UnderTable 1: Comparison of theoretical regret bounds.
Algorithm Conversational Regret
LinUCB [1] âœ— O(ğ‘‘âˆš
ğ‘‡logğ‘‡)
ConUCB [29], ConLinUCB-MCR [25] âœ“ O(ğ‘‘âˆš
ğ‘‡logğ‘‡)
ConLinUCB-BS [25] âœ“ At leastO(ğ‘‘âˆšï¸
ğ‘‡logğ‘‡)*
CLiSK (Ours, Theorem 1) âœ“ O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡+ğ‘‘)
CLiME (Ours, Theorem 2) âœ“ O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡)
CLiSK-ME (Ours, Corollary 1) âœ“ O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡)
*The original paper claims a regret of O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡)but its analysis is flawed.
the assumption that the key term set Kspans Rğ‘‘, ConLinUCB-
BS [25] achieves a regret upper bound of O(1âˆš
ğœ†Bâˆšï¸
ğ‘‘ğ‘‡logğ‘‡), where
ğœ†Bâ‰”ğœ†min
Eğ‘˜âˆˆunif(B)h
Ëœğ’™ğ‘˜Ëœğ’™âŠ¤
ğ‘˜i
andBis the barycentric spanner of
K. The authors assume ğœ†Bis a constant, leading to a regret bound of
O(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡). However, this assumption is incorrect as ğœ†Bdepends
on the dimension ğ‘‘and is not a constant. Specifically, denoting
ğ‘¿â‰”Eğ‘˜âˆˆunif(B)h
Ëœğ’™ğ‘˜Ëœğ’™âŠ¤
ğ‘˜i
and{ğœ†ğ‘–}ğ‘‘
ğ‘–=1as its eigenvalues, we use the
fact thatâˆ¥Ëœğ’™ğ‘˜âˆ¥=1and obtain Tr(ğ‘¿)=Eğ‘˜âˆˆunif(B)h
Tr
Ëœğ’™ğ‘˜Ëœğ’™âŠ¤
ğ‘˜i
=1,
thusğœ†Bâ‰¤Ãğ‘‘
ğ‘–=1ğœ†ğ‘–
ğ‘‘=Tr(ğ‘¿)
ğ‘‘=1
ğ‘‘. Consequently, by plugging
this result back into the regret expression, the regret bound of
ConLinUCB-BS cannot be better than O(ğ‘‘âˆšï¸
ğ‘‡logğ‘‡). These previ-
ous attempts underscore the significance of our work. In contrast,
with the smoothed key term context technique and with the adap-
tive conversation technique, our algorithms achieve a better regret
bound ofO(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡+ğ‘‘)andO(âˆšï¸
ğ‘‘ğ‘‡logğ‘‡), respectively. These
improvements successfully match the lower bound (Theorem 3) up
to logarithmic factors in their dependence on the time horizon ğ‘‡.
5 Evaluation
In this section, we evaluate the performance of our algorithms on
both synthetic and real-world datasets. All the experiments were
conducted on a machine equipped with a 3.70 GHz Intel Xeon
E5-1630 v4 CPU and 32GB RAM.
5.1 Experiment Setups
5.1.1 Datasets. Consistent with existing studies, we generate a
synthetic dataset and use three real-world datasets: MovieLens-
25M [12], Last.fm [4], and Yelp1.
For the synthetic dataset, we set the dimension ğ‘‘=50, the
number of users ğ‘=200, the number of arms |A|=5,000,
and the number of key terms |K|=1,000. We generate it fol-
lowing Zhang et al . [29] . First, for each key term ğ‘˜âˆˆK, we sample
a pseudo feature vector Â¤ğ’™ğ‘˜with each dimension drawn from a
uniform distribution U(âˆ’ 1,1). For each arm ğ‘–âˆˆA, we randomly
select an integer ğ‘›ğ‘–âˆˆ{1,2,..., 5}and uniformly sample a subset
of key termsKğ‘–âŠ‚K with|Kğ‘–|=ğ‘›ğ‘–. The weight is defined as
ğ‘¤ğ‘–,ğ‘˜=1/ğ‘›ğ‘–for eachğ‘˜âˆˆKğ‘–. For each arm ğ‘–, the feature vector
ğ’™ğ‘–is drawn from a multivariate Gaussian N(Ã
ğ‘—âˆˆKğ‘–Â¤ğ’™ğ‘—/ğ‘›ğ‘–,ğ‘°). The
feature vector for each key term ğ‘˜, denoted by Ëœğ’™ğ‘˜, is computed
asËœğ’™ğ‘˜=Ã
ğ‘–âˆˆAğ‘¤ğ‘–,ğ‘˜Ã
ğ‘—âˆˆAğ‘¤ğ‘—,ğ‘˜ğ’™ğ‘–. Finally, each userâ€™s preference vector
1https://www.yelp.com/datasetLeveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
ğœ½ğ‘¢âˆˆRğ‘‘is generated by sampling each dimension from U(âˆ’ 1,1)
and normalizing it to unit length.
For the real-world datasets, we regard movies/artists/businesses
as arms. To exclude unrepresentative or insufficiently informative
data (such as users who have not submitted any reviews or movies
with only a few reviews), we extract a subset of |A|=5,000arms
with the highest number of user-assigned ratings/tags, and a subset
ofğ‘=200users who have assigned the most ratings/tags. Key
terms are identified by using the associated movie genres, busi-
ness categories, or tag IDs in the MovieLens, Yelp, and Last.fm
datasets, respectively. For example, each movie is associated with a
list of genres, such as â€œactionâ€ or â€œcomedyâ€, and each business (e.g.,
restaurant) is categorized by terms such as â€œMexicanâ€ or â€œBurgersâ€.
Using the data extracted above, we create a feedback matrix ğ‘¹of
sizeğ‘Ã—|A| , where each element ğ‘¹ğ‘–,ğ‘—represents the user ğ‘–â€™s feed-
back to arm ğ‘—. We assume that the userâ€™s feedback is binary. For the
MovieLens and Yelp datasets, a userâ€™s feedback for a movie/business
is 1 if the userâ€™s rating is higher than 3; otherwise, the feedback is 0.
For the Last.fm dataset, a userâ€™s feedback for an artist is 1 if the user
assigns a tag to the artist. Next, we generate the feature vectors for
arms ğ’™ğ‘–and the preference vectors for users ğœ½ğ‘¢. Following existing
works, we decompose the feedback matrix ğ‘¹using truncated Sin-
gular Value Decomposition (SVD) as ğ‘¹â‰ˆğš¯ğ‘ºğ‘¨âŠ¤, where ğš¯âˆˆRğ‘Ã—ğ‘‘
andğ‘¨âˆˆR|A|Ã—ğ‘‘contain the top- ğ‘‘left and right singular vectors,
andğ‘ºâˆˆRğ‘‘Ã—ğ‘‘is a diagonal matrix with the corresponding top- ğ‘‘
singular values. Then each ğœ½âŠ¤ğ‘¢corresponds to the ğ‘¢-th row of ğš¯ğ‘º
for allğ‘¢âˆˆ[ğ‘], and each ğ’™âŠ¤
ğ‘–corresponds to the ğ‘–-th row of ğ‘¨for
allğ‘–âˆˆA. The feature vectors for key terms are generated similarly
to those in the synthetic dataset, by assigning equal weights for all
key terms corresponding to each arm.
5.1.2 Baseline Algorithms. We select the following baseline algo-
rithms from existing studies: (1) LinUCB [ 1]: The standard linear
contextual bandit algorithm, which does not consider the conversa-
tional setting and only has arm-level feedback. (2) Arm-Con [ 5]: An
extension of LinUCB that initiates conversations directly from arm
sets. (3) ConUCB [ 29]: The first algorithm proposed for conversa-
tional contextual bandits that queries key terms when conversations
are allowed. (4) ConLinUCB [ 25]: It consists of three algorithms
with different key term selection strategies. ConLinUCB-BS com-
putes the barycentric spanner of key terms as an exploration basis.
ConLinUCB-MCR selects key terms with the largest confidence
radius. ConLinUCB-UCB chooses key terms with the largest upper
confidence bounds. Since ConLinUCB-BS and ConLinUCB-MCR
demonstrate superior performance, we focus our comparisons on
these two variants.
5.2 Evaluation Results
5.2.1 Cumulative Regret. First, we compare our algorithms against
all baseline algorithms in terms of cumulative regret over ğ‘‡=6,000
rounds. In each round, we randomly select |A|=200arms from
each dataset. For the baseline algorithms, we adopt the conver-
sation frequency function ğ‘(ğ‘¡)=5âŒŠlog(ğ‘‡)âŒ‹, as specified in their
original papers. We present the results for all three checking func-
tions â€œContinuousâ€, â€œFixed Intervalâ€, and "Exponential Phaseâ€, for
both CLiME and CLiSK-ME. For the â€œFixed Intervalâ€ function,
UncertaintyChecking is triggered every 100 rounds, whereas forthe "Exponential Phaseâ€ it is triggered whenever ğ‘¡is a power of
2. For CLiSK, both the perturbation level ğœŒ2and the truncation
limitğ‘…are set to 1. The results are averaged over 20 trials, and the
resulting confidence intervals are included in the figures. Under
the â€œContinuous Checkingâ€ function, as shown in Figure 2, our
three algorithms consistently achieve the best performance (low-
est regret) with an improvement of over 14.6% compared to the
best baseline. Similar performance trends hold under the other two
checking functions, as illustrated in Figures 3 and 4. These results
confirm the validity of our theoretical advancements.
0 2000 4000 6000
Round
(a) Synthetic dataset0.00.51.01.5Regret1e3
0 2000 4000 6000
Round
(b) MovieLens dataset0.00.51.01.5Regret1e3
0 2000 4000 6000
Round
(c) Yelp dataset012Regret1e3
0 2000 4000 6000
Round
(d) Last.fm dataset024Regret1e3
CLiSK-ME
CLiMECLiSK
ConUCBLinUCB
Arm-ConConLinUCB-MCR
ConLinUCB-BS
Figure 2: Comparison of cumulative regret where CLiME and
CLiSK-ME use the â€œContinuous Checkingâ€ function.
0 2000 4000 6000
Round
(a) Synthetic dataset0.00.51.01.5Regret1e3
0 2000 4000 6000
Round
(b) MovieLens dataset0.00.51.01.5Regret1e3
0 2000 4000 6000
Round
(c) Yelp dataset012Regret1e3
0 2000 4000 6000
Round
(d) Last.fm dataset024Regret1e3
CLiSK-ME
CLiMECLiSK
ConUCBLinUCB
Arm-ConConLinUCB-MCR
ConLinUCB-BS
Figure 3: Comparison of cumulative regret where CLiME and
CLiSK-ME use the â€œFixed Intervalâ€ function.KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
0 2000 4000 6000
Round
(a) Synthetic dataset0.00.51.01.5Regret1e3
0 2000 4000 6000
Round
(b) MovieLens dataset0.00.51.01.5Regret1e3
0 2000 4000 6000
Round
(c) Yelp dataset012Regret1e3
0 2000 4000 6000
Round
(d) Last.fm dataset024Regret1e3
CLiSK-ME
CLiMECLiSK
ConUCBLinUCB
Arm-ConConLinUCB-MCR
ConLinUCB-BS
Figure 4: Comparison of cumulative regret where CLiME and
CLiSK-ME use the â€œExponential Phaseâ€ function.
5.2.2 Precision of Estimated Preference Vectors. To assess how ac-
curately each algorithm learns the userâ€™s preferences over time,
we measure the average distance between the estimated vector
bğœ½ğ‘¡and the ground truth ğœ½âˆ—for all algorithms over 1000 rounds.
We present the results for the â€œContinuous Checkingâ€ function of
CLiME and CLiSK-ME, with results for other functions provided
in Appendix A.2. As shown in Figure 5, all algorithms exhibit a de-
creasing estimation error over time. However, our three algorithms
consistently achieve the lowest estimation error in all datasets. This
is because they leverage our novel conversational mechanism to
gather more informative feedback, significantly accelerating the
reduction of estimation error. As a result, our algorithms estimate
the userâ€™s preference vector more quickly and accurately than the
baseline methods.
5.2.3 Number of Conversations. Next, we evaluate the number of
conversations initiated by CLiME. Since CLiSK and all baseline
algorithms initiate conversations based on a deterministic function
ğ‘(ğ‘¡), their results are consistent across all datasets. Therefore, we
plots the scenarios for ğ‘(ğ‘¡)=5âŒŠlog(ğ‘¡)âŒ‹andğ‘(ğ‘¡)=âŒŠğ‘¡/50âŒ‹as in
prior studies. It is also important to note that although some exist-
ing studies employ a logarithmic ğ‘(ğ‘¡)in their experiments, their
theoretical results require a linear ğ‘(ğ‘¡)to hold. In contrast to the
baselines, our algorithm CLiME adaptively initiates conversations
depending on the current uncertainty of user preferences, providing
greater flexibility and enhancing the user experience. We plot the
number of conversations initiated by CLiME with different uncer-
tainty checking functions across 4 datasets. As shown in Figure 6,
the number of conversations increases only logarithmically with
the number of rounds.
5.2.4 Running Time. To evaluate the computational efficiency, we
compare the running times of our algorithms with other conver-
sational methods using the MovieLens dataset across ğ‘‡=6,000
rounds. We separately report the total running times, as well as the
100 300 500 700 900 1100
Round
(a) Synthetic dataset0.000.050.100.150.20k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(b) MovieLens dataset0.00.10.20.3k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(c) Yelp dataset0.000.050.100.150.20k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(d) Last.fm dataset0.00.10.20.3k^ÂµtÂ¡ÂµÂ¤k2
CLiSK-ME
CLiMECLiSK
ConUCBLinUCB
Arm-ConConLinUCB-MCR
ConLinUCB-BSFigure 5: Comparison of estimation precision where CLiME
and CLiSK-ME use the â€œContinuous Checkingâ€ function.
(a) Continuous Checking (every round)
02550
(b) Fixed Interval Checking (every 100 rounds)
02550
0 1000 2000 3000 4000 5000 6000(c) Exponential Phase Checking (when t is a power of 2)
02550
RoundNumber of Conversations
Synthetic
MovieLensYelp
Last.fm5blog(t)c
bt=50c
Figure 6: Number of conversations initiated by deterministic
approaches and our adaptive approach CLiME with different
uncertainty checking functions.
times for picking arms and key terms. The results are averaged over
20 runs. As shown in Table 2, our three algorithms show substan-
tial improvements compared to ConUCB and exhibit performance
comparable to the ConLinUCB family of algorithms. For CLiME
and CLiSK-ME, while matrix operations and eigenvalue compu-
tation introduce slight overhead, the algorithms remain efficient,
particularly with interval and exponential checking strategies.Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
Table 2: Comparison of Running Times for Conversational
Bandit Algorithms Using the Movielens dataset.
AlgorithmsRunning Time (s)
Key terms Arms Total
CLiSK-MEContinuous 1.169 3.443 4.651
Interval 0.332 3.361 3.723
Exponential 0.352 3.344 3.724
CLiMEContinuous 0.803 3.371 4.205
Interval 0.021 3.341 3.390
Exponential 0.014 3.334 3.375
CLiSK 0.490 3.339 3.857
ConUCB 0.011 8.362 8.403
ConLinUCBUCB 0.009 3.354 3.392
MCR 0.007 3.337 3.371
BS 0.006 3.334 3.366
5.2.5 Ablation Study. We conduct an ablation study evaluating
the effect of the truncation limit ğ‘…. Specifically, we analyze how
different values of ğ‘…affect algorithm performance by comparing
the cumulative regrets at round 6,000 across all datasets, as shown
in Figure 7. The results indicate that increasing ğ‘…from 0.1 to 3.1
leads to a decrease in regret, with performance stabilizing when
ğ‘…>2. For the perturbation level ğœŒ2, we observe that varying it
from 0.1 to 3 results in no significant change in regret. Therefore,
we do not include a separate figure for this parameter.
0.10.30.50.70.91.11.31.51.71.92.12.32.52.72.93.1
Truncation Limit R
(a) Synthetic dataset0.000.250.500.751.00Regret1e3
0.10.30.50.70.91.11.31.51.71.92.12.32.52.72.93.1
Truncation Limit R
(b) MovieLens dataset0.000.250.500.751.001.25Regret1e3
0.10.30.50.70.91.11.31.51.71.92.12.32.52.72.93.1
Truncation Limit R
(c) Yelp dataset0.00.51.01.5Regret1e3
0.10.30.50.70.91.11.31.51.71.92.12.32.52.72.93.1
Truncation Limit R
(d) Last.fm dataset0.00.51.01.52.02.5Regret1e3
Figure 7: Effect of the truncation limit ğ‘….
6 Related Work
Our research is closely aligned with studies on conversational con-
textual bandits, particularly focusing on the problem of key term
selection within this framework.
Contextual bandits serve as a fundamental framework for online
sequential decision-making problems, covering applications likerecommender systems [ 6,15] and computer networking [ 10]. Con-
textual bandit algorithms aim to maximize the cumulative reward
in the long run while making the trade-off between exploitation
and exploration. Prominent algorithms include LinUCB [ 1] and
Thompson Sampling (TS) [2].
To address the cold start problem, conversational recommender
systems (CRSs) [ 5,23,30] are proposed to engage users in con-
versations to learn their preferences more effectively. Zhang et al .
[29] extend the standard contextual bandits to model conversa-
tional interactions, and the pioneering ConUCB algorithm with a
regret upper bound O(ğ‘‘âˆš
ğ‘‡logğ‘‡). Following the foundational work
of Zhang et al . [29] , a branch of research has advanced this field. Li
et al. [16] design the first TS-type algorithm ConTS. Wu et al . [26]
propose a clustering-based algorithm to automatically generate key
terms. Zuo et al . [32] propose Hier-UCB and Hier-LinUCB, leverag-
ing the hierarchical structures between key terms and items. Xie
et al. [27] introduce a comparison-based conversation framework
and propose RelativeConUCB. Zhao et al . [31] integrate knowl-
edge graphs into conversational bandits. Li et al . [19] investigate
federated conversational bandits. Dai et al . [7,8]study the conver-
sational bandits with misspecified/corrupted models. To enhance
learning efficiency, Dai et al . [9]consider multi-agent LLM response
identification with a fixed arm set. Wang et al . [25] and Yang et al .
[28] investigate the key term selection strategies and propose the
ConLinUCB-BS and ConDuel algorithms, respectively. Both algo-
rithms uniformly select key terms from the barycentric spanner of
the key term set.
The smoothed analysis for contextual bandits has been widely
studied recently [ 13,17,18,20â€“22]. The smoothed setting bridges
i.i.d. distributional and adversarial contexts. Kannan et al . [13] first
introduce the smoothed analysis for linear contextual bandits, show-
ing that small perturbations can lead to sublinear regret with a
greedy algorithm. Raghavan et al . [21] and Raghavan et al . [20]
show that the greedy algorithm achieves the best possible Bayesian
regret in this setting. Sivakumar et al . [22] extend the smoothed
analysis to structured linear bandits. Building on these insights, we
apply the smoothed key term contexts in conversational contextual
bandits.
7 Conclusion
In this paper, we studied key term selection strategies for conversa-
tional contextual bandits and introduced three novel algorithms:
CLiSK, CLiME, and CLiSK-ME. CLiSK leverages smoothed key term
contexts to enhance exploration, while CLiME adaptively initiates
conversations with key terms that minimize uncertainty in the
feature space. CLiSK-ME integrates both techniques, further im-
proving learning efficiency. We proved that all three algorithms
achieve tighter regret bounds than prior studies. Extensive evalua-
tions showed that our algorithms outperform other conversational
bandit algorithms.
Acknowledgments
The work of John C.S. Lui was supported in part by the RGC GRF-
14202923.KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
References
[1]Yasin Abbasi-Yadkori, DÃ¡vid PÃ¡l, and Csaba SzepesvÃ¡ri. 2011. Improved algo-
rithms for linear stochastic bandits. Advances in neural information processing
systems 24 (2011).
[2]Shipra Agrawal and Navin Goyal. 2012. Analysis of thompson sampling for the
multi-armed bandit problem. In Conference on learning theory . JMLR Workshop
and Conference Proceedings, 39â€“1.
[3]J. Bretagnolle and C. Huber. 1978. Estimation des densitÃ©s : Risque minimax.
InSÃ©minaire de ProbabilitÃ©s XII , C. Dellacherie, P. A. Meyer, and M. Weil (Eds.).
Springer Berlin Heidelberg, Berlin, Heidelberg, 342â€“363.
[4]Ivan Cantador, Peter Brusilovsky, and Tsvi Kuflik. 2011. Second Workshop on
Information Heterogeneity and Fusion in Recommender Systems (HetRec2011).
InProceedings of the Fifth ACM Conference on Recommender Systems (RecSys â€™11) .
387â€“388.
[5]Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016. Towards
conversational recommender systems. In Proceedings of the 22nd ACM SIGKDD
international conference on knowledge discovery and data mining . 815â€“824.
[6]Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. 2011. Contextual bandits
with linear payoff functions. In Proceedings of the Fourteenth International Con-
ference on Artificial Intelligence and Statistics . JMLR Workshop and Conference
Proceedings, 208â€“214.
[7]Xiangxiang Dai, Zhiyong Wang, Jize Xie, Xutong Liu, and John CS Lui. 2024.
Conversational Recommendation with Online Learning and Clustering on Mis-
specified Users. IEEE Transactions on Knowledge and Data Engineering 36, 12
(2024), 7825â€“7838.
[8]Xiangxiang Dai, Zhiyong Wang, Jize Xie, Tong Yu, and John CS Lui. 2024. Online
Learning and Detecting Corrupted Users for Conversational Recommendation
Systems. IEEE Transactions on Knowledge and Data Engineering 36, 12 (2024),
8939â€“8953.
[9]Xiangxiang Dai, Yuejin Xie, Maoli Liu, Xuchuang Wang, Zhuohua Li, Huanyu
Wang, and John C. S. Lui. 2025. Multi-Agent Conversational Online Learning for
Adaptive LLM Response Identification. arXiv:2501.01849 [cs.HC]
[10] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain. 2012. Combinatorial network
optimization with unknown variables: Multi-armed bandits with linear rewards
and individual observations. IEEE/ACM Transactions on Networking 20, 5 (2012),
1466â€“1478.
[11] Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, and Tat-Seng
Chua. 2021. Advances and challenges in conversational recommender systems:
A survey. AI open 2 (2021), 100â€“126.
[12] F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History
and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (dec 2015), 19 pages.
[13] Sampath Kannan, Jamie H Morgenstern, Aaron Roth, Bo Waggoner, and Zhi-
wei Steven Wu. 2018. A smoothed analysis of the greedy algorithm for the linear
contextual bandit problem. Advances in neural information processing systems 31
(2018).
[14] Tor Lattimore and Csaba SzepesvÃ¡ri. 2020. Bandit Algorithms . Cambridge Uni-
versity Press.
[15] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-
bandit approach to personalized news article recommendation. In Proceedings of
the 19th international conference on World wide web . 661â€“670.
[16] Shijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan He, Peng Jiang, and Tat-Seng
Chua. 2021. Seamlessly unifying attributes and items: Conversational recommen-
dation for cold-start users. ACM Transactions on Information Systems (TOIS) 39, 4
(2021), 1â€“29.
[17] Zhuohua Li, Maoli Liu, Xiangxiang Dai, and John C.S. Lui. 2025. Demystify-
ing Online Clustering of Bandits: Enhanced Exploration Under Stochastic and
Smoothed Adversarial Contexts. In The Thirteenth International Conference on
Learning Representations .
[18] Zhuohua Li, Maoli Liu, Xiangxiang Dai, and John C.S. Lui. 2025. Towards Efficient
Conversational Recommendations: Expected Value of Information Meets Bandit
Learning. In Proceedings of the ACM on Web Conference 2025 (Sydney NSW,
Australia) (WWW â€™25) . 4226â€“4238.
[19] Zhuohua Li, Maoli Liu, and John C. S. Lui. 2024. FedConPE: Efficient Federated
Conversational Bandits with Heterogeneous Clients. In Proceedings of the Thirty-
Third International Joint Conference on Artificial Intelligence, IJCAI-24 . 4533â€“4541.
[20] Manish Raghavan, Aleksandrs Slivkins, Jennifer Wortman Vaughan, and Zhi-
wei Steven Wu. 2023. Greedy algorithm almost dominates in smoothed contextual
bandits. SIAM J. Comput. 52, 2 (2023), 487â€“524.
[21] Manish Raghavan, Aleksandrs Slivkins, Jennifer Vaughan Wortman, and Zhi-
wei Steven Wu. 2018. The externalities of exploration and how data diversity
helps exploitation. In Conference on Learning Theory . PMLR, 1724â€“1738.
[22] Vidyashankar Sivakumar, Steven Wu, and Arindam Banerjee. 2020. Structured lin-
ear contextual bandits: A sharp and geometric smoothed analysis. In International
Conference on Machine Learning . PMLR, 9026â€“9035.
[23] Yueming Sun and Yi Zhang. 2018. Conversational recommender system. In The
41st international acm sigir conference on research & development in information
retrieval . 235â€“244.[24] Joel A. Tropp. 2011. User-Friendly Tail Bounds for Sums of Random Matrices.
Foundations of Computational Mathematics 12, 4 (Aug. 2011), 389â€“434.
[25] Zhiyong Wang, Xutong Liu, Shuai Li, and John CS Lui. 2023. Efficient explorative
key-term selection strategies for conversational contextual bandits. In Proceedings
of the AAAI Conference on Artificial Intelligence , Vol. 37. 10288â€“10295.
[26] Junda Wu, Canzhe Zhao, Tong Yu, Jingyang Li, and Shuai Li. 2021. Clustering
of conversational bandits for user preference learning and elicitation. In Pro-
ceedings of the 30th ACM International Conference on Information & Knowledge
Management . 2129â€“2139.
[27] Zhihui Xie, Tong Yu, Canzhe Zhao, and Shuai Li. 2021. Comparison-based
conversational recommender system with relative bandit feedback. In Proceedings
of the 44th International ACM SIGIR Conference on Research and Development in
Information Retrieval . 1400â€“1409.
[28] Shuhua Yang, Hui Yuan, Xiaoying Zhang, Mengdi Wang, Hong Zhang, and
Huazheng Wang. 2024. Conversational Dueling Bandits in Generalized Linear
Models. arXiv preprint arXiv:2407.18488 (2024).
[29] Xiaoying Zhang, Hong Xie, Hang Li, and John CS Lui. 2020. Conversational
contextual bandit: Algorithm and application. In Proceedings of the web conference
2020. 662â€“672.
[30] Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W Bruce Croft. 2018.
Towards conversational search and recommendation: System ask, user respond. In
Proceedings of the 27th acm international conference on information and knowledge
management . 177â€“186.
[31] Canzhe Zhao, Tong Yu, Zhihui Xie, and Shuai Li. 2022. Knowledge-aware conver-
sational preference elicitation with bandit feedback. In Proceedings of the ACM
Web Conference 2022 . 483â€“492.
[32] Jinhang Zuo, Songwen Hu, Tong Yu, Shuai Li, Handong Zhao, and Carlee Joe-
Wong. 2022. Hierarchical conversational preference elicitation with bandit feed-
back. In Proceedings of the 31st ACM International Conference on Information &
Knowledge Management . 2827â€“2836.
A Appendix
A.1 CLiSK-ME Algorithm
In this section, we present the details of the CLiSK-ME algorithm
(Algorithm 3), which integrates the smoothed key term contexts
and the adaptive conversation technique.
Algorithm 3: CLiSK-ME
Input:A,K,ğ‘(ğ‘¡),ğœ†,{ğ›¼ğ‘¡}ğ‘¡>0
Initialization: ğ‘´1=ğœ†ğ‘°ğ‘‘,ğ’ƒ1=0ğ‘‘
1forğ‘¡=1,...,ğ‘‡ do
2 ifUncertaintyChecking( ğ‘¡)then
3 Diagonalize ğ‘´ğ‘¡=Ãğ‘‘
ğ‘–=1ğœ†ğ’—ğ‘–ğ’—ğ‘–ğ’—ğ‘–âŠ¤
4 foreachğœ†ğ’—ğ‘–<ğ›¼ğ‘¡do
5 ğ‘›ğ’—ğ‘–=âŒˆ(ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–)/ğ‘2
0âŒ‰
6 forğ‘›ğ’—ğ‘–>0do
7 Smooth the key term contexts to get
{ËœËœğ’™ğ‘˜}ğ‘˜âˆˆK
8 ğ‘˜=arg maxğ‘˜âˆˆK|ËœËœğ’™âŠ¤
ğ‘˜ğ’—ğ‘–|
9 Receive the key term-level feedback Ëœğ‘Ÿğ‘˜,ğ‘¡
10 ğ‘´ğ‘¡=ğ‘´ğ‘¡+ËœËœğ’™ğ‘˜,ğ‘¡ËœËœğ’™âŠ¤
ğ‘˜,ğ‘¡
11 ğ’ƒğ‘¡=ğ’ƒğ‘¡+Ëœğ‘Ÿğ‘˜,ğ‘¡ËœËœğ’™ğ‘˜,ğ‘¡
12 ğ‘›ğ’—ğ‘–=ğ‘›ğ’—ğ‘–âˆ’1
13 ğœ½ğ‘¡=ğ‘´âˆ’1
ğ‘¡ğ’ƒğ‘¡
14 Selectğ‘ğ‘¡=arg maxğ‘âˆˆAğ‘¡ğ’™âŠ¤ğ‘ğœ½ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘€âˆ’1
ğ‘¡
15 Ask the userâ€™s preference for arm ğ‘ğ‘¡
16 Observe the reward ğ‘Ÿğ‘ğ‘¡,ğ‘¡
17 ğ‘´ğ‘¡+1=ğ‘´ğ‘¡+ğ’™ğ‘ğ‘¡ğ’™âŠ¤ğ‘ğ‘¡
18 ğ’ƒğ‘¡+1=ğ’ƒğ‘¡+ğ‘Ÿğ‘ğ‘¡,ğ‘¡ğ’™ğ‘ğ‘¡Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
A.2 Supplementary Experiment Results
We compare the estimation precision for the â€œFixed Intervalâ€ and
â€œExponential Phaseâ€ uncertainty checking functions of CLiME in
Figures 8 and 9. In the former, UncertaintyChecking is triggered
every 100 rounds while in the latter it is triggered when ğ‘¡is a
power of 2. Combined with the results presented in the evaluation
results section, the experiments demonstrate that our algorithms
consistently outperform the baselines.
100 300 500 700 900 1100
Round
(a) Synthetic dataset0.000.050.100.150.20k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(b) MovieLens dataset0.00.10.20.3k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(c) Yelp dataset0.000.050.100.150.20k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(d) Last.fm dataset0.00.10.20.3k^ÂµtÂ¡ÂµÂ¤k2
CLiSK-ME
CLiMECLiSK
ConUCBLinUCB
Arm-ConConLinUCB-MCR
ConLinUCB-BS
Figure 8: Comparison of estimation precision where CLiME
and CLiSK-ME use the â€œFixed Intervalâ€ function.
100 300 500 700 900 1100
Round
(a) Synthetic dataset0.000.050.100.150.20k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(b) MovieLens dataset0.00.10.20.3k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(c) Yelp dataset0.000.050.100.150.20k^ÂµtÂ¡ÂµÂ¤k2
100 300 500 700 900 1100
Round
(d) Last.fm dataset0.00.10.20.3k^ÂµtÂ¡ÂµÂ¤k2
CLiSK-ME
CLiMECLiSK
ConUCBLinUCB
Arm-ConConLinUCB-MCR
ConLinUCB-BS
Figure 9: Comparison of estimation precision where CLiME
and CLiSK-ME use the â€œExponential Phaseâ€ function.
A.3 Proof of Lemma 1
Lemma 1. Under Assumptions 1 and 2, for CLiSK, for any round
ğ‘¡âˆˆ[ğ‘‡]and any arm ğ‘âˆˆA, with probability at least 1âˆ’ğ›¿for someğ›¿âˆˆ(0,1), we haveğ’™âŠ¤
ğ‘ğœ½ğ‘¡âˆ’ğ’™âŠ¤
ğ‘ğœ½âˆ—â‰¤ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡,
whereğ›¼ğ‘¡=vt
2 log(1
ğ›¿)+ğ‘‘log 
1+ğ‘¡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘¡
ğœ†ğ‘‘!
+âˆš
ğœ†.
Proof. For any arm ğ‘âˆˆA, from the definition of ğ‘´ğ‘¡andğ’ƒğ‘¡,
andğœ½ğ‘¡=ğ‘´âˆ’1
ğ‘¡ğ’ƒğ‘¡, we have
ğ’™âŠ¤
ğ‘ ğœ½ğ‘¡âˆ’ğœ½âˆ—=ğ’™âŠ¤
ğ‘
ğ‘´âˆ’1
ğ‘¡ğ’ƒğ‘¡âˆ’ğœ½âˆ—
=ğ’™âŠ¤
ğ‘Â©Â­
Â«ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ‘Ÿğ‘ğ‘ ,ğ‘ ğ’™ğ‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ Ëœğ‘Ÿğ‘˜,ğ‘ ËœËœğ’™ğ‘˜ÂªÂ®
Â¬âˆ’ğœ½âˆ—ÂªÂ®
Â¬
=ğ’™âŠ¤
ğ‘Â©Â­
Â«ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ 
ğ’™âŠ¤
ğ‘ğ‘ ğœ½âˆ—+ğœ‚ğ‘ 
+ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜
ËœËœğ’™âŠ¤
ğ‘˜ğœ½âˆ—+Ëœğœ‚ğ‘ ÂªÂ®
Â¬âˆ’ğœ½âˆ—ÂªÂ®
Â¬
=ğ’™âŠ¤
ğ‘Â©Â­
Â«ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğ’™âŠ¤
ğ‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜+ğœ†ğ‘°ğ‘‘âˆ’ğœ†ğ‘°ğ‘‘ÂªÂ®
Â¬ğœ½âˆ—âˆ’ğœ½âˆ—ÂªÂ®
Â¬
+ğ’™âŠ¤
ğ‘Â©Â­
Â«ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğœ‚ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜Ëœğœ‚ğ‘ ÂªÂ®
Â¬ÂªÂ®
Â¬
=ğœ†ğ’™âŠ¤
ğ‘ğ‘´âˆ’1
ğ‘¡ğœ½âˆ—+ğ’™âŠ¤
ğ‘Â©Â­
Â«ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğœ‚ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜Ëœğœ‚ğ‘ ÂªÂ®
Â¬ÂªÂ®
Â¬.
By the Cauchy-Schwarz inequality, we haveğ’™âŠ¤
ğ‘ ğœ½ğ‘¡âˆ’ğœ½âˆ—â‰¤ğœ†âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡âˆ¥ğœ½âˆ—âˆ¥ğ‘´âˆ’1
ğ‘¡
+âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğœ‚ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜Ëœğœ‚ğ‘ ğ‘´âˆ’1
ğ‘¡.(1)
For the first term, by the fact that ğœ†min(ğ‘´ğ‘¡)â‰¥ğœ†, and by the
property of the Rayleigh quotient, we have
âˆ¥ğœ½âˆ—âˆ¥2
ğ‘´âˆ’1
ğ‘¡
âˆ¥ğœ½âˆ—âˆ¥2
2=ğœ½âˆ—âŠ¤ğ‘´âˆ’1
ğ‘¡ğœ½âˆ—
ğœ½âˆ—âŠ¤ğœ½âˆ—â‰¤ğœ†max(ğ‘´âˆ’1
ğ‘¡)â‰¤1
ğœ†min(ğ‘´ğ‘¡)â‰¤1
ğœ†.
Therefore, we have
ğœ†âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡ğœ½âˆ—ğ‘´âˆ’1
ğ‘¡â‰¤ğœ†âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡ğœ½âˆ—2
â‰¤ğœ†âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡âˆšï¸‚
1
ğœ†=âˆš
ğœ†âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡. (2)
For the second term, from Theorem 1 in Abbasi-Yadkori et al .
[1], for anyğ›¿âˆˆ(0,1), with probability at least 1âˆ’ğ›¿, for allğ‘¡â‰¥1,
we haveğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğœ‚ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜Ëœğœ‚ğ‘ ğ‘´âˆ’1
ğ‘¡â‰¤vut
2 log 
det(ğ‘´ğ‘¡)1
2det(ğœ†ğ‘°ğ‘‘)âˆ’1
2
ğ›¿!
.
(3)
By adopting the determinant-trace inequality (Lemma 12), we
have
Tr(ğ‘´ğ‘¡)â‰¤ğ‘‘ğœ†+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1Tr(ğ’™ğ‘ğ‘ ğ’™âŠ¤
ğ‘ğ‘ )+ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ Tr(ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜)KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
â‰¤ğ‘‘ğœ†+ğ‘¡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘¡,
which is obtained because there are at most ğ‘ğ‘¡key terms selected
by roundğ‘¡andâˆ¥ËœËœğ’™ğ‘˜âˆ¥â‰¤1+âˆš
ğ‘‘ğ‘…for allğ‘˜âˆˆK, and therefore,
det(ğ‘´ğ‘¡)â‰¤Tr(ğ‘´ğ‘¡)
ğ‘‘ğ‘‘
â‰¤Â©Â­Â­
Â«ğ‘‘ğœ†+ğ‘¡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘¡
ğ‘‘ÂªÂ®Â®
Â¬ğ‘‘
, (4)
where Tr(ğ‘¿)denotes the trace of matrix ğ‘¿.
By substituting Equation (4) into Equation (3), we haveğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğœ‚ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜Ëœğœ‚ğ‘ ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸„
2 log1
ğ›¿
+logdet(ğ‘´ğ‘¡)
det(ğœ†ğ‘°ğ‘‘)
â‰¤vuuuut
2 log1
ğ›¿
+ğ‘‘logÂ©Â­Â­
Â«1+ğ‘¡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘¡
ğœ†ğ‘‘ÂªÂ®Â®
Â¬. (5)
Plugging Equation (2) and Equation (5) into Equation (1), we
haveğ’™âŠ¤
ğ‘ ğœ½ğ‘¡âˆ’ğœ½âˆ—
â‰¤âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«âˆš
ğœ†+vut
2 log1
ğ›¿
+log 
1+ğ‘¡+(1+âˆš
ğ‘‘ğ‘…)ğ‘ğ‘¡
ğœ†ğ‘‘!
ÂªÂ®
Â¬.(6)
which completes the proof. â–¡
A.4 Proof of Lemma 2
Lemma 2. For any round ğ‘¡âˆˆ [ğ‘‡], with the smoothed key term
contexts in Definition 1, CLiSK has the following lower bound on the
minimum eigenvalue of the matrix E[ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜]for anyğ‘˜âˆˆKğ‘¡, i.e.,
ğœ†min
E[ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜]
â‰¥ğ‘1ğœŒ2
log|K|â‰œğœ†K,
whereğ‘1âˆˆ(0,1)is some constant.
Proof. Fix a timeğ‘¡, and denote the key term selected at this
time asğ‘˜ğ‘¡. Although multiple key terms may be selected at each
time step, they all satisfy the properties of this lemma. Therefore,
we do not distinguish between them and use only a single subscript
ğ‘¡. Let ğ‘¸be a unitary matrix that rotates the estimated preference
vector ğœ½ğ‘¡to align it with the ğ‘¥-axis, maintaining its length but
zeroing out all components except the first component, i.e., ğ‘¸ğœ½ğ‘¡=
(âˆ¥ğœ½ğ‘¡âˆ¥,0,0,..., 0). Note that such ğ‘¸always exists because it just
rotates the space. According to CLiSKâ€™s key term selection strategy
ËœËœğ’™ğ‘˜ğ‘¡=arg maxğ‘˜âˆˆKğœ½âŠ¤
ğ‘¡ËœËœğ’™ğ‘˜, we have
ğœ†min
Eh
ËœËœğ’™ğ‘˜ğ‘¡ËœËœğ’™âŠ¤
ğ‘˜ğ‘¡i
=ğœ†min 
E"
ğ’™ğ’™âŠ¤ğ’™=arg max
ğ‘˜âˆˆKğœ½âŠ¤
ğ‘¡ËœËœğ’™ğ‘˜#!
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1ğ’˜âŠ¤E"
ğ’™ğ’™âŠ¤ğ’™=arg max
ğ‘˜âˆˆKğœ½âŠ¤
ğ‘¡ËœËœğ’™ğ‘˜#
ğ’˜
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1E"
(ğ’˜âŠ¤ğ’™)2ğ’™=arg max
ğ‘˜âˆˆKğœ½âŠ¤
ğ‘¡ËœËœğ’™ğ‘˜#
â‰¥min
ğ’˜:âˆ¥ğ’˜âˆ¥=1Var"
ğ’˜âŠ¤ğ’™ğ’™=arg max
ğ‘˜âˆˆKğœ½âŠ¤
ğ‘¡ËœËœğ’™ğ‘˜#=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1Var"
(ğ‘¸ğ’˜)âŠ¤ğ‘¸ğ’™ğ’™=arg max
ğ‘˜âˆˆK(ğ‘¸ğœ½ğ‘¡)âŠ¤ğ‘¸ËœËœğ’™ğ‘˜#
(7)
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1Var"
ğ’˜âŠ¤ğ‘¸ğ’™ğ’™=arg max
ğ‘˜âˆˆKâˆ¥ğœ½ğ‘¡âˆ¥(ğ‘¸ËœËœğ’™ğ‘˜)1#
(8)
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1Var"
ğ’˜âŠ¤ğ‘¸ğœºğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK(ğ‘¸Ëœğ’™ğ‘˜+ğ‘¸ğœºğ‘˜)1#
(9)
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1Var"
ğ’˜âŠ¤ğœºğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK(ğ‘¸Ëœğ’™ğ‘˜+ğœºğ‘˜)1#
(10)
where Equation (7) uses the property of unitary matrices: ğ‘¸âŠ¤ğ‘¸=ğ‘°ğ‘‘.
Equation (8) applies matrix ğ‘¸so only the first component is non-
zero and we use the fact that minimizing over ğ‘¸ğ’˜is equivalent to
overğ’˜. Equation (9) follows because each smoothed key term ËœËœğ’™ğ‘˜=
Ëœğ’™ğ‘˜+ğœºğ‘˜by definition, and adding a constant a to a random variable
does not change its variance. Equation (10) is due to the rotation
invariance of symmetrically truncated Gaussian distributions.
Since ğœºğ‘˜âˆ¼N( 0,ğœŒ2Â·ğ‘°ğ‘‘)conditioned on|(ğœºğ‘˜)ğ‘—|â‰¤ğ‘…,âˆ€ğ‘—âˆˆ[ğ‘‘], by
the property of (truncated) multivariate Gaussian distributions, the
components of ğœºğ‘¡,ğ‘–can be equivalently regarded as ğ‘‘independent
samples from a (truncated) univariate Gaussian distribution, i.e.,
(ğœºğ‘˜)ğ‘—âˆ¼N( 0,ğœŒ2)conditioned on|(ğœºğ‘˜)ğ‘—|â‰¤ğ‘…,âˆ€ğ‘—âˆˆ[ğ‘‘]. Therefore,
we have
Var
ğ’˜âŠ¤ğœº
=Var"ğ‘‘âˆ‘ï¸
ğ‘–=1ğ’˜ğ‘–ğœºğ‘–#
=ğ‘‘âˆ‘ï¸
ğ‘–=1ğ’˜2
ğ‘–Var[ğœºğ‘–],
where the exchanging of variance and summation is due to the
independence of ğœºğ‘–. Therefore, we can write
min
ğ’˜:âˆ¥ğ’˜âˆ¥=1Var"
ğ’˜âŠ¤ğœºğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1ğ‘‘âˆ‘ï¸
ğ‘—=1ğ’˜2
ğ‘—Var"
(ğœº)ğ‘—ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1(
ğ’˜2
1Var"
(ğœº)1ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
+ğ‘‘âˆ‘ï¸
ğ‘—=2ğ’˜2
ğ‘—Var"
(ğœº)ğ‘—ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#ï£¼ï£´ï£´ ï£½
ï£´ï£´ï£¾
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1(
ğ’˜2
1Var"
(ğœº)1ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
+ğ‘‘âˆ‘ï¸
ğ‘—=2ğ’˜2
ğ‘—Var
(ğœº)ğ‘—ï£¼ï£´ï£´ ï£½
ï£´ï£´ï£¾
=min
ğ’˜:âˆ¥ğ’˜âˆ¥=1(
ğ’˜2
1Var"
(ğœº)1ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
+(1âˆ’ğ’˜2
1)ğœŒ2)
=min(
Var"
(ğœº)1ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
,ğœŒ2)
â‰¥ğ‘1ğœŒ2
log|K|,Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
where in the last inequality, we use Lemma 15 and Lemma 14 in
Sivakumar et al. [22] and get
Var"
(ğœº)1ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK((ğœºğ‘˜)1+(ğ‘¸Ëœğ’™ğ‘˜)1)#
â‰¥Var"
(ğœº)1ğœº=arg max
ğœºğ‘˜:ğ‘˜âˆˆK(ğœºğ‘˜)1#
â‰¥ğ‘1ğœŒ2
log|K|.â–¡
A.5 Proof of Lemma 3
Lemma 3. For CLiSK, with probability at least 1âˆ’ğ›¿for someğ›¿âˆˆ
(0,1), ifğ‘¡â‰¥ğ‘‡0â‰œ8(1+âˆš
ğ‘‘ğ‘…)2
ğ‘ğœ†Klog
ğ‘‘
ğ›¿
, we have
ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ÂªÂ®
Â¬â‰¥ğœ†Kğ‘ğ‘¡
2.
Proof. To apply the matrix Chernoff bound (Lemma 11), we
first verify the required two conditions for the self-adjoint matrices
ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜for anyğ‘˜âˆˆKğ‘ andğ‘ âˆˆ[ğ‘¡]. First, ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜is obviously positive
semi-definite. Second, by the Courant-Fischer theorem,
ğœ†max(ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜)=max
ğ’˜:âˆ¥ğ’˜âˆ¥=1ğ’˜âŠ¤ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ğ’˜=max
ğ’˜:âˆ¥ğ’˜âˆ¥=1(ğ’˜âŠ¤ËœËœğ’™ğ‘˜)2
â‰¤max
ğ’˜:âˆ¥ğ’˜âˆ¥=1âˆ¥ğ’˜âˆ¥2âˆ¥ËœËœğ’™ğ‘˜âˆ¥2â‰¤(1+âˆš
ğ‘‘ğ‘…)2.
Next, by Lemma 2 and the super-additivity of the minimum eigen-
value (due to Weylâ€™s inequality), we have
ğœ‡min=ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ Eh
ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜iÂªÂ®
Â¬â‰¥ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ğœ†min
Eh
ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜i
â‰¥ğœ†Kğ‘ğ‘¡,
where the last inequality is because there are at most ğ‘ğ‘¡key terms
selected by round ğ‘¡, so the summation has at most ğ‘ğ‘¡terms. So by
Lemma 11, we have for any ğœ€âˆˆ(0,1),
Prï£®ï£¯ï£¯ï£¯ï£¯ï£°ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ÂªÂ®
Â¬â‰¤(1âˆ’ğœ€)ğœ†Kğ‘ğ‘¡ï£¹ï£ºï£ºï£ºï£ºï£»
â‰¤Prï£®ï£¯ï£¯ï£¯ï£¯ï£°ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ÂªÂ®
Â¬â‰¤(1âˆ’ğœ€)ğœ‡minï£¹ï£ºï£ºï£ºï£ºï£»
â‰¤ğ‘‘ğ‘’âˆ’ğœ€
(1âˆ’ğœ€)1âˆ’ğœ€ğœ‡min/(1+âˆš
ğ‘‘ğ‘…)2
â‰¤ğ‘‘ğ‘’âˆ’ğœ€
(1âˆ’ğœ€)1âˆ’ğœ€ğœ†Kğ‘ğ‘¡
(1+âˆš
ğ‘‘ğ‘…)2
,
where the last inequality is because ğ‘’âˆ’ğ‘¥is decreasing. Choosing
ğœ€=1
2, we get
Prï£®ï£¯ï£¯ï£¯ï£¯ï£°ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ÂªÂ®
Â¬â‰¤ğœ†Kğ‘ğ‘¡
2ï£¹ï£ºï£ºï£ºï£ºï£»â‰¤ğ‘‘âˆš
2ğ‘’âˆ’1
2ğœ†Kğ‘ğ‘¡
(1+âˆš
ğ‘‘ğ‘…)2.
Letting the RHS be ğ›¿, we getğ‘¡=2(1+âˆš
ğ‘‘ğ‘…)2log(ğ‘‘
ğ›¿)
ğœ†Kğ‘(1âˆ’log(2))â‰¤8(1+âˆš
ğ‘‘ğ‘…)2
ğœ†Kğ‘log
ğ‘‘
ğ›¿
.
Therefore,ğœ†minÃğ‘¡
ğ‘ =1Ã
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜
â‰¥ğœ†Kğ‘ğ‘¡
2holds with probability
at least 1âˆ’ğ›¿whenğ‘¡â‰¥8(1+âˆš
ğ‘‘ğ‘…)2
ğœ†Kğ‘log
ğ‘‘
ğ›¿
. â–¡A.6 Proof of Lemma 4
Lemma 4. For CLiSK, for any ğ‘âˆˆA, ifğ‘¡â‰¥ğ‘‡0â‰œ8(1+âˆš
ğ‘‘ğ‘…)2
ğ‘ğœ†Klog
ğ‘‘
ğ›¿
,
with probability at least 1âˆ’ğ›¿for someğ›¿âˆˆ(0,1),âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
2
ğœ†Kğ‘ğ‘¡.
Proof.
âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡=âˆšï¸ƒ
ğ’™âŠ¤ğ‘ğ‘´âˆ’1
ğ‘¡ğ’™ğ‘â‰¤âˆšï¸ƒ
ğœ†max(ğ‘´âˆ’1
ğ‘¡)ğ’™âŠ¤ğ‘ğ’™ğ‘=âˆšï¸‚1
ğœ†min(ğ‘´ğ‘¡),
(11)
where the first inequality is due to the property of the Rayleigh
quotient, and the second inequality is due to the fact that ğ’™âŠ¤ğ‘ğ’™ğ‘=1.
By the definition of ğ‘´ğ‘¡, we have
ğœ†min(ğ‘´ğ‘¡)=ğœ†minÂ©Â­
Â«ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1ğ’™ğ‘ğ‘ ğ’™âŠ¤
ğ‘ğ‘ +ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜+ğœ†ğ‘°ğ‘‘ÂªÂ®
Â¬
â‰¥ğœ†minÂ©Â­
Â«ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ ËœËœğ’™ğ‘˜ËœËœğ’™âŠ¤
ğ‘˜ÂªÂ®
Â¬
â‰¥ğœ†Kğ‘ğ‘¡
2, (12)
where the first inequality follows the property of Loewner order
that if ğ‘¨âª°ğ‘©thenğœ†min(ğ‘¨) â‰¥ğœ†min(ğ‘©), and the last inequality
follows from Lemma 3 conditioned on ğ‘¡â‰¥ğ‘‡0.
Therefore, by plugging Equation (12) into Equation (11), we have
âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
2
ğœ†Kğ‘ğ‘¡. â–¡
A.7 Proof of Lemma 5
Lemma 5. Letğœ½ğ‘¡be the estimated preference vector at round ğ‘¡and
ğœ½âˆ—be the true preference vector. Under Assumptions 1, 2 and 3, for
CLiME, at round ğ‘¡, for any arm ğ‘âˆˆA, with probability at least 1âˆ’ğ›¿
(ğ›¿âˆˆ(0,1)), we haveğ’™âŠ¤
ğ‘ğœ½ğ‘¡âˆ’ğ’™âŠ¤
ğ‘ğœ½âˆ—â‰¤ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡,
whereğ›¼ğ‘¡=âˆšï¸„
2 log(1
ğ›¿)+ğ‘‘log
1+ğ‘¡+ğ›¼ğ‘‘ğ‘¡
ğœ†ğ‘‘ğ‘2
0
+âˆš
ğœ†,ğ›¼is an exploration
control factor in Algorithm 2, and ğ‘0is a constant in Assumption 3.
Proof. The proof of Lemma 5 is similar to that of Lemma 1. The
only difference is the trace and determinant of the matrix ğ‘´ğ‘¡.
We first show that by round ğ‘¡, at mostğ›¼ğ‘‘ğ‘¡
ğ‘2
0key terms have
been selected since the beginning of the algorithm for all three
uncertainty checking functions.
Consider the case where CLiME uses the â€œContinuous Checkingâ€
function, i.e., the agent checks the eigenvalues of the matrix ğ‘´ğ‘¡
at each round. We first denote the covariance matrix before select-
ing key terms at round ğ‘¡byğ‘´â€²
ğ‘¡, i.e., ğ‘´ğ‘¡=ğ‘´â€²
ğ‘¡+Ã
ğ‘˜âˆˆKğ‘¡ğ’™ğ‘˜ğ’™âŠ¤
ğ‘˜. For
ğ‘´â€²
ğ‘¡, denote its eigenvectors by {ğ’—ğ‘–}ğ‘‘
ğ‘–=1and corresponding eigen-
values by{ğœ†ğ’—ğ‘–}ğ‘‘
ğ‘–=1. If some key term ğ‘˜is selected at round ğ‘¡, then
there must exist an eigenvector ğ’—ğ‘–such thatğœ†ğ’—ğ‘–<ğ›¼ğ‘¡, and the
corresponding key term context Ëœğ’™ğ‘˜is close to ğ’—ğ‘–, i.e., Ëœğ’™âŠ¤
ğ‘˜ğ’—ğ‘–â‰¥ğ‘0.
We can write the vector Ëœğ’™ğ‘˜=Ãğ‘‘
ğ‘–=1ğ›¾ğ‘–ğ’—ğ‘–for some coefficients
{ğ›¾ğ‘–}ğ‘‘
ğ‘–=1. Then, for ğ‘—âˆˆ [ğ‘‘], Denote ğ’›ğ‘—=Ãğ‘‘
ğ‘–=1,ğ‘–â‰ ğ‘—ğ›¾ğ‘–ğ’—ğ‘–. For any
ğ‘—âˆˆ [ğ‘‘], we have Ëœğ’™âŠ¤
ğ‘˜ğ’—ğ‘—=Ãğ‘‘
ğ‘–=1ğ›¾ğ‘–ğ’—âŠ¤
ğ‘–ğ’—ğ‘—=ğ›¾ğ‘—â‰¥ğ‘0and Ëœğ’™ğ‘˜Ëœğ’™âŠ¤
ğ‘˜=KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
(ğ›¾ğ‘—ğ’—ğ‘—+ğ’›ğ‘—)(ğ›¾ğ‘—ğ’—ğ‘—+ğ’›ğ‘—)âŠ¤=ğ›¾2
ğ‘—ğ’—ğ‘—ğ’—âŠ¤
ğ‘—+ğ’›ğ‘—ğ’›âŠ¤
ğ‘—, and then, we have the
following:
ğ‘´â€²
ğ‘¡+âˆ‘ï¸
ğ‘˜âˆˆKğ‘¡ğ’™ğ‘˜ğ’™âŠ¤
ğ‘˜=ğ‘´â€²
ğ‘¡+âˆ‘ï¸
ğ‘–âˆˆ[ğ‘‘]:ğœ†ğ’—ğ‘–â‰¤ğ›¼ğ‘¡&
ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–
ğ‘2
0'
(ğ›¾2
ğ‘–ğ’—ğ‘–ğ’—âŠ¤
ğ‘–+ğ’›ğ‘–ğ’›âŠ¤
ğ‘–)
âª°ğ‘‘âˆ‘ï¸
ğ‘–=1ğœ†ğ’—ğ‘–ğ’—ğ‘–ğ’—âŠ¤
ğ‘–+âˆ‘ï¸
ğ‘–âˆˆ[ğ‘‘]:ğœ†ğ’—ğ‘–â‰¤ğ›¼ğ‘¡ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–
ğ‘2
0(ğ›¾2
ğ‘–ğ’—ğ‘–ğ’—âŠ¤
ğ‘–+ğ’›ğ‘–ğ’›âŠ¤
ğ‘–)
âª°ğ‘‘âˆ‘ï¸
ğ‘–=1ğœ†ğ’—ğ‘–ğ’—ğ‘–ğ’—âŠ¤
ğ‘–+âˆ‘ï¸
ğ‘–âˆˆ[ğ‘‘]:ğœ†ğ’—ğ‘–â‰¤ğ›¼ğ‘¡ ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–ğ’—ğ‘–ğ’—âŠ¤
ğ‘–
=âˆ‘ï¸
ğ‘–âˆˆ[ğ‘‘]:ğœ†ğ’—ğ‘–<ğ›¼ğ‘¡(ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–+ğœ†ğ’—ğ‘–)ğ’—ğ‘–ğ’—âŠ¤
ğ‘–+âˆ‘ï¸
ğ‘–âˆˆ[ğ‘‘]:ğœ†ğ’—ğ‘–>ğ›¼ğ‘¡ğœ†ğ’—ğ‘–ğ’—ğ‘–ğ’—âŠ¤
ğ‘–
âª°ğ‘‘âˆ‘ï¸
ğ‘–=1ğ›¼ğ‘¡ğ’—ğ‘–ğ’—âŠ¤
ğ‘–. (13)
Following from Equation (13), we have
ğœ†min(ğ‘´ğ‘¡)â‰¥ğ›¼ğ‘¡. (14)
Denote the number of key terms selected at round ğ‘¡asğ¾ğ‘¡. We
haveğ¾ğ‘¡=Ãğ‘‘
ğ‘–=1ğ›¼ğ‘¡âˆ’ğœ†ğ’—ğ‘–
ğ‘2
0. Sinceğœ†ğ’—ğ‘–â‰¥ğ›¼(ğ‘¡âˆ’1),âˆ€ğ‘–âˆˆ[ğ‘‘]according to
Equation (14), we have ğ¾ğ‘¡â‰¤ğ›¼ğ‘‘
ğ‘2
0, and thenÃğ‘¡
ğ‘ =1ğ¾ğ‘ â‰¤ğ›¼ğ‘‘ğ‘¡
ğ‘2
0.
For the â€œFixed Interval Checkingâ€ function, at each uncertainty
checking point ğ‘¡ğ‘—=ğ‘—ğ‘ƒwhereğ‘—âˆˆ{1,2,...,âŒŠğ‘‡
ğ‘ƒâŒ‹}, we haveğœ†min(ğ‘´ğ‘¡ğ‘—)â‰¥
ğ›¼ğ‘¡ğ‘—. For theğ‘—-th checking, there areÃğ‘‘
ğ‘–=1ğ›¼ğ‘¡ğ‘—âˆ’ğœ†ğ’—ğ‘–
ğ‘2
0â‰¤Ãğ‘‘
ğ‘–=1ğ›¼ğ‘¡ğ‘—âˆ’ğ›¼ğ‘¡ğ‘—âˆ’1
ğ‘2
0â‰¤
ğ›¼ğ‘‘ğ‘ƒ
ğ‘2
0conversations to be launched. Thus, by round ğ‘¡, the number
of total conversations satisfiesÃâŒŠğ‘¡
ğ‘ƒâŒ‹
ğ‘—=1ğ›¼ğ‘‘ğ‘ƒ
ğ‘2
0â‰¤ğ›¼ğ‘‘ğ‘¡
ğ‘2
0.
For the â€œExponential Phase Checkingâ€ function, by round ğ‘¡, there
areâŒŠlog2(ğ‘¡)âŒ‹uncertainty checking points. For the ğ‘—-th checking,
there areÃğ‘‘
ğ‘–=1ğ›¼ğ‘¡ğ‘—âˆ’ğœ†ğ’—ğ‘–
ğ‘2
0â‰¤Ãğ‘‘
ğ‘–=1ğ›¼ğ‘¡ğ‘—âˆ’ğ›¼ğ‘¡ğ‘—âˆ’1
ğ‘2
0â‰¤ğ›¼ğ‘‘2ğ‘—âˆ’1
ğ‘2
0conversations
to be launched. By round ğ‘¡, the number of total conversations
satisfiesÃâŒŠlog2(ğ‘¡)âŒ‹
ğ‘—=1ğ›¼ğ‘‘2ğ‘—âˆ’1
ğ‘2
0â‰¤ğ›¼ğ‘‘ğ‘¡
ğ‘2
0.
Therefore, we have
Tr(ğ‘´ğ‘¡)â‰¤ğ‘‘ğœ†+ğ‘¡âˆ’1âˆ‘ï¸
ğ‘ =1Tr(ğ’™ğ‘ğ‘ ğ’™âŠ¤
ğ‘ğ‘ )+ğ‘¡âˆ‘ï¸
ğ‘ =1âˆ‘ï¸
ğ‘˜âˆˆKğ‘ Tr(Ëœğ’™ğ‘˜Ëœğ’™âŠ¤
ğ‘˜)â‰¤ğ‘‘ğœ†+ğ‘¡+ğ›¼ğ‘‘ğ‘¡
ğ‘2
0,
and
det(ğ‘´ğ‘¡)â‰¤Tr(ğ‘´ğ‘¡)
ğ‘‘ğ‘‘
â‰¤Â©Â­
Â«ğ‘‘ğœ†+ğ‘¡+ğ›¼ğ‘‘ğ‘¡
ğ‘2
0
ğ‘‘ÂªÂ®
Â¬ğ‘‘
â‰¤ 
ğœ†+ğ‘¡+ğ›¼ğ‘‘ğ‘¡
ğ‘2
0ğ‘‘!ğ‘‘
,
where the last inequality is obtained by the fact that ğ‘0<1.
Following the same steps as in the proof of Lemma 1, we can
obtain that
ğ’™âŠ¤
ğ‘ ğœ½ğ‘¡âˆ’ğœ½âˆ—â‰¤âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡Â©Â­
Â«âˆš
ğœ†+vut
2 log1
ğ›¿
+ğ‘‘log 
1+ğ‘¡+ğ›¼ğ‘‘ğ‘¡
ğœ†ğ‘‘ğ‘2
0!
ÂªÂ®
Â¬,
which concludes the proof. â–¡A.8 Proof of Lemma 6
Lemma 6. For CLiME, for any arm ğ‘âˆˆA, with probability at least
1âˆ’ğ›¿for someğ›¿âˆˆ(0,1), at roundğ‘¡â‰¥2ğ‘ƒ, we haveâˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
2
ğ›¼ğ‘¡,
whereğ‘ƒis a fixed integer.
Proof. We first consider the case where CLiME uses the â€œCon-
tinuous Checkingâ€ function, i.e., the agent checks the eigenval-
ues of the matrix ğ‘´ğ‘¡at each round. By Equation (14), we have
ğœ†min(ğ‘´ğ‘¡)â‰¥ğ›¼ğ‘¡. Then, following from Equation (11) in the proof of
Lemma 4, we can obtain that âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
1
ğ›¼ğ‘¡.
Next, we consider the case for the â€œFixed Interval Checkingâ€
function. In this case, the agent only checks the eigenvalues of the
matrix ğ‘´ğ‘¡everyğ‘ƒrounds. For the rounds ğ‘¡when the agent checks
the uncertainty, we have the same results as ğœ†min(ğ‘´ğ‘¡)â‰¥ğ›¼ğ‘¡; For the
roundsğ‘¡when the agent does not check it, we have ğœ†min(ğ‘´ğ‘¡)â‰¥ğ›¼ğ‘¡â€²
whereğ‘¡â€²is the last round that the agent conducts the check and
ğ‘¡âˆ’ğ‘¡â€²â‰¤ğ‘ƒ. Whenğ‘¡â‰¥2ğ‘ƒ,ğ‘¡â€²â‰¥ğ‘¡âˆ’ğ‘ƒâ‰¥ğ‘¡
2, we can obtain that
âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
1
ğ›¼ğ‘¡â€²â‰¤âˆšï¸ƒ
2
ğ›¼ğ‘¡.
Finally, we consider the â€œExponential Phase Checkingâ€ function.
At roundsğ‘¡satisfying 2ğ‘–â‰¤ğ‘¡<2ğ‘–+1forğ‘–=1,2,..., the last checking
pointğ‘¡â€²=2ğ‘–, then we have ğœ†min(ğ‘´ğ‘¡)â‰¥ğ›¼Â·2ğ‘–. Whenğ‘¡â‰¥2, we
haveğ‘¡
2â‰¤2ğ‘–, and thenâˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
1
ğ›¼2ğ‘–â‰¤âˆšï¸ƒ
2
ğ›¼ğ‘¡.
Therefore, to generalize the bound, we can conclude that when
ğ‘¡â‰¥2ğ‘ƒ,âˆ¥ğ’™ğ‘âˆ¥ğ‘´âˆ’1
ğ‘¡â‰¤âˆšï¸ƒ
2
ğ›¼ğ‘¡for all three checking functions. â–¡
A.9 Proof of Theorem 1
Theorem 1 (Regret of CLiSK) .With probability at least 1âˆ’ğ›¿for
someğ›¿âˆˆ(0,1), the regret upper bound of CLiSK satisfies
R(ğ‘‡)â‰¤8(1+âˆš
ğ‘‘ğ‘…)2log(|K|)
ğ‘1ğœŒ2ğ‘logğ‘‘
ğ›¿
+4âˆšï¸„
2ğ‘1ğœŒ2ğ‘‡
ğ‘log(|K|)Â·
Â©Â­Â­Â­
Â«vuuuut
2 log1
ğ›¿
+ğ‘‘logÂ©Â­Â­
Â«1+ğ‘‡+
1+âˆš
ğ‘‘ğ‘…
ğ‘ğ‘‡
ğœ†ğ‘‘ÂªÂ®Â®
Â¬+âˆš
ğœ†ÂªÂ®Â®Â®
Â¬
=O(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)+ğ‘‘),
whereğ‘…andğœŒ2are constants in Definition 1.
Proof. Denote the instantaneous regret at round ğ‘¡byregğ‘¡. We
first decompose it as follows:
regğ‘¡=(ğ’™âŠ¤
ğ‘âˆ—
ğ‘¡ğœ½âˆ—+ğœ‚ğ‘¡)âˆ’(ğ’™âŠ¤
ğ‘ğ‘¡ğœ½âˆ—+ğœ‚ğ‘¡)
=ğ’™âŠ¤
ğ‘âˆ—
ğ‘¡(ğœ½âˆ—âˆ’ğœ½ğ‘¡)+(ğ’™âŠ¤
ğ‘âˆ—
ğ‘¡ğœ½ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ—
ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡)âˆ’(ğ’™âŠ¤
ğ‘ğ‘¡ğœ½ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡)
+ğ’™âŠ¤
ğ‘ğ‘¡(ğœ½ğ‘¡âˆ’ğœ½âˆ—)âˆ’ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ—
ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡
â‰¤ğ’™âŠ¤
ğ‘âˆ—
ğ‘¡(ğœ½âˆ—âˆ’ğœ½ğ‘¡)+ğ’™âŠ¤
ğ‘ğ‘¡(ğœ½ğ‘¡âˆ’ğœ½âˆ—)âˆ’ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ—
ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡(15)
â‰¤ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ—
ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡âˆ’ğ›¼ğ‘¡âˆ¥ğ’™ğ‘âˆ—
ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡+ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡(16)
â‰¤2ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡,
where Equation (15) follows from the UCB strategy for arm selec-
tion, and Equation (16) follows from Lemma 1. Next, we have
R(ğ‘‡)=ğ‘‡0âˆ‘ï¸
ğ‘¡=1regğ‘¡+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘‡0+1regğ‘¡Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
â‰¤ğ‘‡0+ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘‡0+12ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡(17)
â‰¤ğ‘‡0+2ğ‘‡âˆ‘ï¸
ğ‘¡=ğ‘‡0+1ğ›¼ğ‘¡âˆšï¸„
2
ğœ†Kğ‘ğ‘¡(18)
â‰¤ğ‘‡0+4ğ›¼ğ‘‡âˆšï¸„
2ğ‘‡
ğœ†Kğ‘(19)
where Equation (17) is because the instantaneous regret regğ‘¡â‰¤1
by Assumption 1, Equation (18) follows from Lemma 4, and Equa-
tion (19) is because ğ›¼ğ‘¡is non-decreasing andÃğ‘‡
ğ‘¡=11âˆšğ‘¡â‰¤2âˆš
ğ‘‡.
Recall the definition of ğ‘‡0â‰œ8(1+âˆš
ğ‘‘ğ‘…)2
ğ‘ğœ†Klog
ğ‘‘
ğ›¿
in Lemma 3 and
the definition of ğ›¼ğ‘¡in Lemma 1. Plugging ğ‘‡0andğ›¼ğ‘¡into Equa-
tion (19), we can obtain the regret bound. â–¡
A.10 Proof of Theorem 2
Theorem 2 (Regret of CLiME) .With probability at least 1âˆ’ğ›¿for
someğ›¿âˆˆ(0,1), the regret upper bound of CLiME satisfies
R(ğ‘‡)â‰¤4âˆšï¸‚
2ğ‘‡
ğ›¼Â©Â­
Â«vut
2 log(1
ğ›¿)+ğ‘‘log 
1+ğ‘‡+ğ›¼ğ‘‘ğ‘‡
ğœ†ğ‘‘ğ‘2
0!
+âˆš
ğœ†ÂªÂ®
Â¬+2ğ‘ƒ
=Oâˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)
.
Proof. With the same decomposition as in the proof of Theo-
rem 1, we have
R(ğ‘‡)=2ğ‘ƒâˆ‘ï¸
ğ‘¡=1regğ‘¡+ğ‘‡âˆ‘ï¸
ğ‘¡=2ğ‘ƒ+1regğ‘¡â‰¤2ğ‘ƒ+2ğ‘‡âˆ‘ï¸
ğ‘¡=2ğ‘ƒ+1ğ›¼ğ‘¡âˆ¥ğ’™ğ‘ğ‘¡âˆ¥ğ‘´âˆ’1
ğ‘¡
â‰¤2ğ‘ƒ+2ğ‘‡âˆ‘ï¸
ğ‘¡=2ğ‘ƒ+1ğ›¼ğ‘¡âˆšï¸‚
2
ğ›¼ğ‘¡(20)
â‰¤2ğ‘ƒ+4ğ›¼ğ‘‡âˆšï¸‚
2ğ‘‡
ğ›¼. (21)
=2ğ‘ƒ+4âˆšï¸‚
2ğ‘‡
ğ›¼Â©Â­
Â«vut
2 log1
ğ›¿
+ğ‘‘log 
1+ğ‘‡+ğ›¼ğ‘‘ğ‘‡
ğœ†ğ‘‘ğ‘2
0!
+âˆš
ğœ†ÂªÂ®
Â¬,(22)
where Equations (20) and (21) follow from Lemma 5 and analogous
steps in Theorem 1. Note that ğ‘ƒ>1is a given constant for the â€œFixed
Interval Checkingâ€ function. Plugging ğ›¼ğ‘‡into the inequality, we
can obtain the result and conclude that R(ğ‘‡)=O(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)).â–¡
A.11 Proof of Theorem 3
Since any algorithms for conversational bandits must select both
arms and key terms, we model a policy ğœ‹as a tuple consisting of
two components ğœ‹=(ğœ‹arm,ğœ‹key), whereğœ‹armselects arms and
ğœ‹keyselects key terms. We assume that at each time step, the policy
can select at most one key term; otherwise, the number of key terms
could exceed the number of arms, which is impractical. Let Hğ‘¡=
{ğ‘1,ğ‘¥1,ğ‘˜1,eğ‘¥1,...,ğ‘ğ‘¡,ğ‘¥ğ‘¡,ğ‘˜ğ‘¡,eğ‘¥ğ‘¡}denote the history of interactions
between the policy and the environment up to time ğ‘¡. We note that
the presence of key terms at every time step in Hğ‘¡is without lossof generality because we allow ğ‘˜ğ‘¡to be empty if no conversation
is initiated at round ğ‘¡. The noise terms associated with both arm-
level and key term-level feedback, denoted by ğœ‚ğ‘¡andeğœ‚ğ‘¡, follow the
standard Gaussian distribution N(0,1). We also denote the feature
vectors of selected arm and key term as random variables ğ‘¨ğ‘¡,ğ‘²ğ‘¡âˆˆ
Rğ‘‘, and the arm-level and key term-level rewards ğ‘‹ğ‘¡=âŸ¨ğ‘¨ğ‘¡,ğœ½âŸ©+ğœ‚ğ‘¡
andeğ‘‹ğ‘¡=âŸ¨ğ‘²ğ‘¡,ğœ½âŸ©+eğœ‚ğ‘¡, followN(âŸ¨ğ‘¨ğ‘¡,ğœ½âŸ©,1)andN(âŸ¨ğ‘²ğ‘¡,ğœ½âŸ©,1),
respectively. We denote by Pğœ½the probability measure induced by
the environment ğœ½and policyğœ‹, and by Eğœ½the expectation under
Pğœ½. With these definitions, we present the following lemma.
Lemma 7. Letğ·(ğ‘ƒâˆ¥ğ‘„)denote the KL divergence between distri-
butionsğ‘ƒandğ‘„, and let ğœ½,ğœ½â€²be two environments, then we have
ğ·(Pğœ½âˆ¥Pğœ½â€²)=1
2ğ‘‡âˆ‘ï¸
ğ‘¡=1
Eğœ½h
ğ‘¨ğ‘¡,ğœ½âˆ’ğœ½â€²2i
+Eğœ½h
ğ‘²ğ‘¡,ğœ½âˆ’ğœ½â€²2i
.
Proof. Given a bandit instance with parameter ğœ½and a policy
ğœ‹, according to Section 4.6 of Lattimore and SzepesvÃ¡ri [14], we
construct the canonical bandit model of our setting as follows. Let
(Î©,F,Pğœ½)be a probability space and Abe the set of all possible
arms, where Î©=(AÃ—R)ğ‘‡,F=B(Î©), and the density function
of the probability measure Pğœ½is defined by ğ‘ğœ½,ğœ‹:Î©â†’R:
ğ‘ğœ½(Hğ‘‡)=ğ‘‡Ã–
ğ‘¡=1ğœ‹arm
ğ‘¡(ğ‘ğ‘¡|Hğ‘¡âˆ’1)ğ‘ğ‘ğ‘¡(ğ‘¥ğ‘¡)Â·ğœ‹key
ğ‘¡(ğ‘˜ğ‘¡|Hğ‘¡âˆ’1)eğ‘ğ‘˜ğ‘¡(eğ‘¥ğ‘¡),
whereğ‘ğ‘ğ‘¡andeğ‘ğ‘˜ğ‘¡are the density functions of arm-level and key
term-level reward distributions ğ‘ƒğ‘ğ‘¡andeğ‘ƒğ‘˜ğ‘¡, respectively. The def-
inition of Pğœ½â€²is identical except that ğ‘ğ‘ğ‘¡,eğ‘ğ‘˜ğ‘¡are replaced by ğ‘â€²ğ‘ğ‘¡,
eğ‘â€²
ğ‘˜ğ‘¡andğ‘ƒğ‘ğ‘¡,eğ‘ƒğ‘˜ğ‘¡are replaced by ğ‘ƒâ€²ğ‘ğ‘¡,eğ‘ƒâ€²
ğ‘˜ğ‘¡.
By the definition of KL divergence ğ·(ğ‘ƒâˆ¥ğ‘„)=âˆ«
Î©log
dğ‘ƒ
dğ‘„
dğ‘ƒ,
ğ·(Pğœ½âˆ¥Pğœ½â€²)=âˆ«
Î©logdPğœ½
dPğœ½â€²
dPğœ½=Eğœ½
logdPğœ½
dPğœ½â€²
.
Note that
logdPğœ½
dPğœ½â€²(Hğ‘‡)
=logğ‘ğœ½,ğœ‹(Hğ‘‡)
ğ‘ğœ½â€²,ğœ‹(Hğ‘‡)(23)
=logÃğ‘‡
ğ‘¡=1ğœ‹arm
ğ‘¡(ğ‘ğ‘¡|Hğ‘¡âˆ’1)ğ‘ğ‘ğ‘¡(ğ‘¥ğ‘¡)Â·ğœ‹key
ğ‘¡(ğ‘˜ğ‘¡|Hğ‘¡âˆ’1)eğ‘ğ‘˜ğ‘¡(eğ‘¥ğ‘¡)
Ãğ‘‡
ğ‘¡=1ğœ‹arm
ğ‘¡(ğ‘ğ‘¡|Hğ‘¡âˆ’1)ğ‘â€²ğ‘ğ‘¡(ğ‘¥ğ‘¡)Â·ğœ‹key
ğ‘¡(ğ‘˜ğ‘¡|Hğ‘¡âˆ’1)eğ‘â€²
ğ‘˜ğ‘¡(eğ‘¥ğ‘¡)
=ğ‘‡âˆ‘ï¸
ğ‘¡=1 
logğ‘ğ‘ğ‘¡(ğ‘¥ğ‘¡)
ğ‘â€²ğ‘ğ‘¡(ğ‘¥ğ‘¡)+logeğ‘ğ‘˜ğ‘¡(eğ‘¥ğ‘¡)
eğ‘â€²
ğ‘˜ğ‘¡(eğ‘¥ğ‘¡)!
.
where in Equation 23 we used the chain rule for Radonâ€“Nikodym
derivatives, and in the last equality, all the terms involving the
policyğœ‹cancel. Therefore,
ğ·(Pğœ½âˆ¥Pğœ½â€²)
=ğ‘‡âˆ‘ï¸
ğ‘¡=1 
Eğœ½"
logğ‘ğ´ğ‘¡(ğ‘‹ğ‘¡)
ğ‘â€²
ğ´ğ‘¡(ğ‘‹ğ‘¡)#
+Eğœ½"
logeğ‘ğ¾ğ‘¡(eğ‘‹ğ‘¡)
eğ‘â€²
ğ¾ğ‘¡(eğ‘‹ğ‘¡)#!
=ğ‘‡âˆ‘ï¸
ğ‘¡=1 
Eğœ½"
Eğœ½"
logğ‘ğ´ğ‘¡(ğ‘‹ğ‘¡)
ğ‘â€²
ğ´ğ‘¡(ğ‘‹ğ‘¡)ğ´ğ‘¡##
+Eğœ½"
Eğœ½"
logeğ‘ğ¾ğ‘¡(eğ‘‹ğ‘¡)
eğ‘â€²
ğ¾ğ‘¡(eğ‘‹ğ‘¡)ğ¾ğ‘¡##!
=ğ‘‡âˆ‘ï¸
ğ‘¡=1
Eğœ½h
ğ·(ğ‘ƒğ´ğ‘¡âˆ¥ğ‘ƒâ€²
ğ´ğ‘¡)i
+Eğœ½h
ğ·(eğ‘ƒğ¾ğ‘¡âˆ¥eğ‘ƒâ€²
ğ¾ğ‘¡)iKDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada Maoli Liu, Zhuohua Li, Xiangxiang Dai, and John C.S. Lui
=1
2ğ‘‡âˆ‘ï¸
ğ‘¡=1
Eğœ½h
ğ‘¨ğ‘¡,ğœ½âˆ’ğœ½â€²2i
+Eğœ½h
ğ‘²ğ‘¡,ğœ½âˆ’ğœ½â€²2i
.
where the last equation uses Lemma 10 and the fact that ğ‘ƒğ´ğ‘¡âˆ¼
N(âŸ¨ğ´ğ‘¡,ğœ½âŸ©,1),ğ‘ƒâ€²
ğ´ğ‘¡âˆ¼ N(âŸ¨ğ´ğ‘¡,ğœ½â€²âŸ©,1),eğ‘ƒğ¾ğ‘¡âˆ¼ N(âŸ¨ğ¾ğ‘¡,ğœ½âŸ©,1), and
eğ‘ƒâ€²
ğ¾ğ‘¡âˆ¼N(âŸ¨ğ¾ğ‘¡,ğœ½â€²âŸ©,1), respectively. â–¡
Next, we present a lower bound for conversational bandits but
without imposing the constraint that the number of arms is ğ¾.
Lemma 8. Let the arm set and the key term set A=K=[âˆ’1,1]ğ‘‘
andÎ˜=
Â±âˆšï¸ƒ
1
ğ‘‡ğ‘‘
, then for any policy, there exists an environ-
ment ğœ½âˆˆÎ˜such that the expected regret satisfies: Eğœ½[ğ‘…(ğ‘‡)]â‰¥
exp(âˆ’4)
4ğ‘‘âˆš
ğ‘‡.
Proof. For anyğ‘–âˆˆ[ğ‘‘]andğœ½âˆˆÎ˜, defineEğœ½,ğ‘–as the event that
the sign of the ğ‘–-th coordinate of at least half of {ğ‘¨ğ‘¡}ğ‘‡
ğ‘¡=1does not
agree with ğœ½:Eğœ½,ğ‘–=Ãğ‘‡
ğ‘¡=1I{sign(ğ‘¨ğ‘¡ğ‘–)â‰ sign(ğœ½ğ‘–)}â‰¥ğ‘‡
2	
.
Letğ‘ğœ½,ğ‘–=Pğœ½
Eğœ½,ğ‘–andğœ½â€²=(ğœ½1,...,ğœ½ğ‘–âˆ’1,âˆ’ğœ½ğ‘–,ğœ½ğ‘–+1,...,ğœ½ğ‘‘)âŠ¤,
i.e.,ğœ½â€²is the same as ğœ½except that the ğ‘–-th coordinate is negated.
It is easy to verify that Eğ‘
ğœ½,ğ‘–=Eğœ½â€²,ğ‘–. Thus, applying Lemma 9 and
Lemma 7, we obtain
ğ‘ğœ½,ğ‘–+ğ‘ğœ½â€²,ğ‘–â‰¥1
2exp(ğ·(Pğœ½âˆ¥Pğœ½â€²))
=1
2exp 
âˆ’1
2ğ‘‡âˆ‘ï¸
ğ‘¡=1
Eğœ½h
ğ‘¨ğ‘¡,ğœ½âˆ’ğœ½â€²2i
+Eğœ½h
ğ‘²ğ‘¡,ğœ½âˆ’ğœ½â€²2i!
=1
2exp(âˆ’4),
where the last equality follows from a straightforward calculation
showing thatâŸ¨ğ‘¨ğ‘¡,ğœ½âˆ’ğœ½â€²âŸ©=âŸ¨ğ‘¨ğ‘¡,ğœ½âˆ’ğœ½â€²âŸ©=4/ğ‘‡.
Since|Î˜|=2ğ‘‘, we have
âˆ‘ï¸
ğœ½âˆˆÎ˜1
|Î˜|ğ‘‘âˆ‘ï¸
ğ‘–=1ğ‘ğœ½,ğ‘–=1
|Î˜|ğ‘‘âˆ‘ï¸
ğ‘–=1âˆ‘ï¸
ğœ½âˆˆÎ˜ğ‘ğœ½,ğ‘–
=1
2ğ‘‘Â·ğ‘‘Â·2ğ‘‘
2Â·1
2exp(âˆ’4)=ğ‘‘
4exp(âˆ’4).
This implies the existence of some ğœ½âˆ—âˆˆÎ˜such that
ğ‘‘âˆ‘ï¸
ğ‘–=1ğ‘ğœ½âˆ—,ğ‘–â‰¥ğ‘‘
4exp(âˆ’4). (24)
Choosing this ğœ½âˆ—and defining the optimal arm ğ’‚âˆ—as:
ğ’‚âˆ—=arg max
ğ’‚âˆˆA
ğ’‚,ğœ½âˆ—
=arg max
ğ’‚âˆˆAğ‘‘âˆ‘ï¸
ğ‘–=1ğ’‚âˆ—
ğ‘–ğœ½âˆ—
ğ‘–.
It is easy to verify that to maximizeÃğ‘‘
ğ‘–=1ğ’‚âˆ—
ğ‘–ğœ½âˆ—
ğ‘–, we must have ğ‘âˆ—
ğ‘–=
sign(ğœ½âˆ—
ğ‘–)for allğ‘–âˆˆ[ğ‘‘]. Therefore, the expected regret is at least
Eğœ½âˆ—[ğ‘…(ğ‘‡)]=Eğœ½âˆ—"ğ‘‡âˆ‘ï¸
ğ‘¡=1
ğ’‚âˆ—âˆ’ğ‘¨ğ‘¡,ğœ½âˆ—#
=Eğœ½âˆ—"ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘‘âˆ‘ï¸
ğ‘–=1(ğ’‚âˆ—
ğ‘–âˆ’ğ‘¨ğ‘¡ğ‘–)ğœ½âˆ—
ğ‘–#=Eğœ½âˆ—"ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘‘âˆ‘ï¸
ğ‘–=1(sign(ğœ½âˆ—
ğ‘–)âˆ’ğ‘¨ğ‘¡ğ‘–)ğœ½âˆ—
ğ‘–#
=Eğœ½âˆ—"ğ‘‡âˆ‘ï¸
ğ‘¡=1ğ‘‘âˆ‘ï¸
ğ‘–=12I
sign(ğ‘¨ğ‘¡ğ‘–)â‰ sign(ğœ½âˆ—
ğ‘–)	âˆšï¸‚
1
ğ‘‡#
=2âˆšï¸‚
1
ğ‘‡ğ‘‘âˆ‘ï¸
ğ‘–=1Eğœ½âˆ—"ğ‘‡âˆ‘ï¸
ğ‘¡=1I
sign(ğ‘¨ğ‘¡ğ‘–)â‰ sign(ğœ½âˆ—
ğ‘–)	#
â‰¥âˆš
ğ‘‡ğ‘‘âˆ‘ï¸
ğ‘–=1Pğœ½âˆ—"ğ‘‡âˆ‘ï¸
ğ‘¡=1I
sign(ğ‘¨ğ‘¡ğ‘–)â‰ sign(ğœ½âˆ—
ğ‘–)	
â‰¥ğ‘‡/2#
(25)
=âˆš
ğ‘‡ğ‘‘âˆ‘ï¸
ğ‘–=1ğ‘ğœ½âˆ—,ğ‘–â‰¥exp(âˆ’4)
4ğ‘‘âˆš
ğ‘‡,
where Equation (25) uses Markovâ€™s inequality, and the last inequal-
ity follows from Equation (24). â–¡
Theorem 3 (Regret lower bound) .For any policy that chooses
at most one key term per time step, there exists an instance of the
conversational bandit problem such that the expected regret is at least
Î©(âˆš
ğ‘‘ğ‘‡). Furthermore, for any ğ‘‡=2ğ‘šwithğ‘šâˆˆ[ğ‘‘], the regret is at
leastÎ©(âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)).
Proof. Suppose we have ğ›½=ğ‘‘
ğ‘šsmaller problem instances
ğ¼1.ğ¼2,...,ğ¼ğ›½, each corresponding to an ğ‘š-dimensional, ğ¾-armed
bandit instance with a horizon of ğ‘‡/ğ›½and we assume they have
preference vectors ğœ½1,...,ğœ½ğ›½âˆˆRğ‘š, respectively. We denote the
arm set for instance ğ¼ğ‘—asAğ¼ğ‘—âŠ‚Rğ‘š, and the regret incurred
by instance ğ¼under policy ğœ‹asğ‘…ğœ‹
ğ¼(ğ‘‡). Next, we construct a ğ‘‘-
dimensional instance ğ¼=(ğ¼1,ğ¼2,...,ğ¼ğ›½)by leting the unknown
preference vector for instance ğ¼beğœ½=(ğœ½âŠ¤
1,...,ğœ½âŠ¤
ğ›½)âŠ¤, and dividing
the time horizon ğ‘‡intoğ›½consecutive periods, each of length ğ‘‡/ğ›½.
For each time step ğ‘¡âˆˆ [ğ‘‡], the feature vectors of arms Ağ‘¡are
constructed from instance ğ¼ğ‘—, whereğ‘—=âŒˆğ‘¡ğ›½/ğ‘‡âŒ‰. Specifically,Ağ‘¡=
(0âŠ¤,..., ğ’™âŠ¤,..., 0âŠ¤)âŠ¤	
ğ’™âˆˆAğ¼ğ‘—, where the non-zero entry is located
at theğ‘—-th block. This means that at time ğ‘¡, the learner can only
get information about the ğ‘—-th block of the preference vector ğœ½.
Therefore for any policy ğœ‹, there exists policies ğœ‹1,...,ğœ‹ğ›½such that
ğ‘…ğœ‹
ğ¼(ğ‘‡)=Ãğ›½
ğ‘—=1ğ‘…ğœ‹ğ‘—
ğ¼ğ‘—(ğ‘‡
ğ›½). Applying Lemma 8, we can always find
instancesğ¼1,ğ¼2,...,ğ¼ğ›½such that
ğ‘…ğœ‹
ğ¼(ğ‘‡)=ğ›½âˆ‘ï¸
ğ‘—=1ğ‘…ğœ‹ğ‘—
ğ¼ğ‘—(ğ‘‡
ğ›½)â‰¥ğ›½âˆ‘ï¸
ğ‘—=1Î© 
ğ‘šâˆšï¸„
ğ‘‡
ğ›½!
=Î©
ğ‘šâˆšï¸
ğ‘‡ğ›½
=Î© 
ğ‘šâˆšï¸‚
ğ‘‡ğ‘‘
ğ‘š!
=Î©âˆš
ğ‘‘ğ‘‡ğ‘š
=Î©âˆšï¸
ğ‘‘ğ‘‡log(ğ‘‡)
.â–¡
A.12 Technical Inequalities
We present the technical inequalities used throughout the proofs.
We provide detailed references for readersâ€™ convenience.
Lemma 9 (Bretagnolle and Huber [3]).Letğ‘ƒandğ‘„be probability
measures on the same measurable space (Î©,F), and letğ´âˆˆF be an
arbitrary event. Then,
ğ‘ƒ(ğ´)+ğ‘„(ğ´ğ‘)â‰¥1
2exp(âˆ’ğ·(ğ‘ƒâˆ¥ğ‘„)),Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits KDD â€™25, August 3â€“7, 2025, Toronto, ON, Canada
whereğ·(ğ‘ƒâˆ¥ğ‘„)=âˆ«
Î©log
dğ‘ƒ
dğ‘„
dğ‘ƒ=Eğ‘ƒh
logdğ‘ƒ
dğ‘„i
is the KL diver-
gence between ğ‘ƒandğ‘„.ğ´ğ‘=Î©\ğ´is the complement of ğ´.
Lemma 10 (KL divergence between Gaussian distributions) .If
ğ‘ƒâˆ¼N(ğœ‡1,ğœ2)andğ‘„âˆ¼N(ğœ‡2,ğœ2), then
ğ·(ğ‘ƒâˆ¥ğ‘„)=(ğœ‡1âˆ’ğœ‡2)2
2ğœ2.
Lemma 11 (Matrix Chernoff, Corollary 5.2 in Tropp [24]).Consider
a finite sequence{ğ‘¿ğ‘˜}of independent, random, self-adjoint matrices
with dimension ğ‘‘. Assume that each random matrix satisfies
ğ‘¿ğ‘˜âª°0andğœ†max(ğ‘¿ğ‘˜)â‰¤ğ‘…almost surely.
Define
ğ’€:=âˆ‘ï¸
ğ‘˜ğ‘¿ğ‘˜andğœ‡min:=ğœ†min(E[ğ’€])=ğœ†min âˆ‘ï¸
ğ‘˜E[ğ‘¿ğ‘˜]!
.Then, for any ğ›¿âˆˆ(0,1),
Pr"
ğœ†min âˆ‘ï¸
ğ‘˜ğ‘¿ğ‘˜!
â‰¤(1âˆ’ğ›¿)ğœ‡min#
â‰¤ğ‘‘"
ğ‘’âˆ’ğ›¿
(1âˆ’ğ›¿)1âˆ’ğ›¿#ğœ‡min/ğ‘…
.
Lemma 12 (Determinant-trace inequality, Lemma 10 in Abbasi-Yad-
kori et al . [1]).Suppose ğ‘¿1,ğ‘¿2,..., ğ‘¿ğ‘¡âˆˆRğ‘‘and for any 1â‰¤ğ‘ â‰¤ğ‘¡,
âˆ¥ğ‘¿ğ‘ âˆ¥2â‰¤ğ¿. Letğ‘½ğ‘¡=ğœ†ğ‘°+Ãğ‘¡
ğ‘ =1ğ‘¿ğ‘ ğ‘¿âŠ¤ğ‘ for someğœ†>0. Then,
det(ğ‘½ğ‘¡)â‰¤
ğœ†+ğ‘¡ğ¿2
ğ‘‘ğ‘‘
.