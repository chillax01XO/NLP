arXiv:2505.21391v1  [cs.LG]  27 May 2025Finite Sample Analysis of Linear Temporal Difference
Learning with Arbitrary Features
Zixuan Xieâˆ—
University of Virginia
xie.zixuan@email.virginia.eduXinyu Liuâˆ—
University of Virginia
xinyuliu@virginia.edu
Rohan Chandra
University of Virginia
rohanchandra@virginia.eduShangtong Zhang
University of Virginia
shangtong@virginia.edu
Abstract
Linear TD( Î») is one of the most fundamental reinforcement learning algorithms
for policy evaluation. Previously, convergence rates are typically established under
the assumption of linearly independent features, which does not hold in many
practical scenarios. This paper instead establishes the first L2convergence rates for
linear TD( Î») operating under arbitrary features, without making any algorithmic
modification or additional assumptions. Our results apply to both the discounted
and average-reward settings. To address the potential non-uniqueness of solutions
resulting from arbitrary features, we develop a novel stochastic approximation
result featuring convergence rates to the solution set instead of a single point.
1 Introduction
Temporal difference learning (TD, Sutton [1988]) is a fundamental algorithm in reinforcement
learning (RL, Sutton and Barto [2018]), enabling efficient policy evaluation by combining dynamic
programming [Bellman, 1966] with stochastic approximation (SA, Benveniste et al. [1990], Kushner
and Yin [2003], Borkar [2009]). Its linear variant, linear TD( Î») [Sutton, 1988], emerges as a practical
extension, employing linear function approximation to tackle large or continuous state spaces where
tabular representations become impractical. Linear TD( Î») takes the dot product between features and
weights to compute the approximated value. Establishing theoretical guarantees for linear TD( Î»),
particularly convergence rates, has been a major focus of research. Most existing works (Table 1),
however, require the features used in linear TD to be linearly independent. As argued in Wang and
Zhang [2024], this assumption is impractical in many scenarios. For example, in continual learning
with sequentially arriving data [Ring, 1994, Khetarpal et al., 2022, Abel et al., 2023], there is no way
to rigorously verify whether the features are independent or not. See Wang and Zhang [2024] for more
discussion on the restrictions of the feature independence assumptions. Furthermore, Peter [1992],
Tsitsiklis and Roy [1996, 1999] also outline the elimination of the linear independence assumption as
a future research direction.
While efforts have been made to eliminate the linear independence assumption [Wang and Zhang,
2024], they only provide asymptotic (almost sure) convergence guarantees in the discounted setting.
By contrast, this paper establishes the first L2convergence rates for linear TD( Î») with arbitrary
features in both discounted and average-reward settings . This success is enabled by a novel
stochastic approximation result (Theorem 3) concerning the convergence rates to a solution set instead
of a single point, driven by a novel Lyapunov function. This new result provides a unified approach
âˆ—Equal contribution
Preprint.applicable to both discounted (Theorem 1) and average-reward (Theorem 2) settings. Notably, we
do not make any algorithmic modification and do not introduce any additional assumptions. Table 1
provides a detailed comparison of existing theoretical analyses for linear TD( Î»), contextualizing our
contributions within the landscape of prior work.
Setting FeaturesNoise
TypeRate
Tsitsiklis and Roy [1996] Î³ <1 Independent Markovian
Bhandari et al. [2018] Î³ <1 Independent Markovian âœ“
Lakshminarayanan and SzepesvÃ¡ri [2018] Î³ <1 Independent i.i.d. âœ“
Srikant and Ying [2019] Î³ <1 Independent Markovian âœ“
Chen et al. [2023b] Î³ <1 Independent i.i.d. âœ“
Wang and Zhang [2024] Î³ <1 Arbitrary Markovian
Mitra [2025] Î³ <1 Independent Markovian âœ“
Theorem 1 Î³ <1 Arbitrary Markovian âœ“
Tsitsiklis and Roy [1999] Î³= 1 Independent Markovian
Zhang et al. [2021c] Î³= 1 Independent Markovian âœ“
Chen et al. [2025] Î³= 1 Independent Markovian âœ“
Theorem 2 Î³= 1 Arbitrary Markovian âœ“
Table 1: Comparison of finite-sample analyses for linear TD( Î»). â€œSettingâ€ indicates the problem
setting: Î³ <1stands for the discounted setting and Î³= 1 stands for the average reward setting.
â€œFeaturesâ€ describes assumptions on the features. â€œIndependentâ€ indicates linear independence is
assumed. â€œArbitraryâ€ indicates no assumption is made on features. â€œNoise Typeâ€ indicates the data
generation process: Markovian samples or independent and identically distributed (i.i.d.) samples.
â€œRateâ€ is checked if a convergence rate is provided.
2 Background
Notations. We use âŸ¨x, yâŸ©.=xâŠ¤yto denote the standard inner product in Euclidean spaces and âˆ¥Â·âˆ¥
to denote the â„“2norm for vectors and the associated induced operator norm (i.e., the spectral norm)
for matrices, unless stated otherwise. A function fis said to be L-smooth (w.r.t. âˆ¥Â·âˆ¥) ifâˆ€w, wâ€²,
f(wâ€²)â‰¤f(w)+âŸ¨âˆ‡f(w), wâ€²âˆ’wâŸ©+L
2âˆ¥wâ€²âˆ’wâˆ¥2. For a matrix A,col(A)denotes its column space,
ker(A)denotes its kernel, and Aâ€ denotes its Moore-Penrose inverse. When xis a point and Uis a
set, we denote d(x, U).= inf yâˆˆUâˆ¥xâˆ’yâˆ¥as the Euclidean distance from xtoU. For sets U, V , their
Minkowski sum is U+V.={u+v|uâˆˆU, vâˆˆV}; andUâŠ¥denotes the orthogonal complement of
U. We use 0and1to denote the zero vector and the all-ones vector respectively, where the dimension
is clear from context. For any square matrix AâˆˆRdÃ—d(not necessarily symmetric), we say Ais
negative definite (n.d.) if there exists a Î¾ >0such that xâŠ¤Axâ‰¤ âˆ’Î¾âˆ¥xâˆ¥2âˆ€xâˆˆRd. For any set
EâŠ†Rd, we say Aisn.d. on Eif there exists a Î¾ >0such that xâŠ¤Axâ‰¤ âˆ’Î¾âˆ¥xâˆ¥2âˆ€xâˆˆE. A is
negative semidefinite (n.s.d.) if Î¾= 0in the above definition.
Markov Decision Processes. We consider an infinite horizon Markov Decision Process (MDP,
Bellman [1957]) defined by a tuple (S,A, p, r, p 0), where Sis a finite set of states, Ais a finite set
of actions, p:S Ã— S Ã— A â†’ [0,1]is the transition probability function, r:S Ã— A â†’ Ris the
reward function, and p0:S â†’ [0,1]denotes the initial distribution. In this paper, we focus on the
policy evaluation problem, where the goal is to estimate the value function of an arbitrary policy
Ï€:A Ã— S â†’ [0,1]. At the time step 0, an initial state S0is sampled from p0. At each subsequent
time step t, the agent observes state Stâˆˆ S, executes an action Atâˆ¼Ï€(Â·|St), receives reward
Rt+1.=r(St, At), and transitions to the next state St+1âˆ¼p(Â·|St, At). We use PÏ€to denote the state
transition matrix induced by the policy Ï€, i.e., PÏ€[s, sâ€²] =P
aâˆˆAÏ€(a|s)p(sâ€²|s, a). LetdÏ€âˆˆR|S|be
the stationary distribution of the Markov chain induced by the policy Ï€. We use DÏ€to denote the
diagonal matrix whose diagonal is dÏ€.
Linear Function Approximation. In this paper, we use linear function approximation to approximate
value functions vÏ€:S â†’R(to be defined shortly). We consider a feature mapping x:S â†’Rdand
a weight vector wâˆˆRd. We then approximate vÏ€(s)withx(s)âŠ¤w. We use XâˆˆR|S|Ã—dto denote
2the feature matrix, where the s-th row of Xisx(s)âŠ¤. The approximated state-value function across
all states can then be represented as the vector XwâˆˆR|S|. The goal is thus to find a wsuch that Xw
closely approximates vÏ€.
Discounted Setting. In the discounted setting, we introduce a discount factor Î³âˆˆ[0,1). The (dis-
counted) value function vÏ€:S â†’Rfor policy Ï€is defined as vÏ€(s).=EPâˆ
i=0Î³iRt+i+1St=s
.
We define the Bellman operator T:R|S|â†’R|S|asTv.=rÏ€+Î³PÏ€v, where rÏ€âˆˆR|S|is the
vector of expected immediate rewards under Ï€, with components rÏ€(s) =P
aÏ€(a|s)r(s, a). With a
Î»âˆˆ[0,1], theÎ»-weighted Bellman operator TÎ»is defined as TÎ»v.= (1âˆ’Î»)Pâˆ
m=0Î»mTm+1v=
rÎ»+Î³PÎ»v, where
rÎ»=Pâˆ
k=0(Î»Î³)kPk
Ï€rÏ€= (1âˆ’Î³Î»P Ï€)âˆ’1rÏ€,
PÎ»=(1âˆ’Î»)Pâˆ
m=0(Î»Î³)mPm+1
Ï€ = (1âˆ’Î»)(1âˆ’Î³Î»P Ï€)âˆ’1PÏ€.
This represents a weighted average of multi-step applications of T. It is well-known that vÏ€is the
unique fixed point of TÎ»[Bertsekas and Tsitsiklis, 1996]. Linear TD( Î») is a family of TD learning
algorithms that use eligibility traces to estimate vÏ€(s)of the fixed policy Ï€with linear function
approximation. The algorithm maintains a weight vector wtâˆˆRdand an eligibility trace vector
etâˆˆRd, with the following update rules:
wt+1=wt+Î±t(Rt+1+Î³x(St+1)âŠ¤wtâˆ’x(St)âŠ¤wt)et,
et=Î³Î»etâˆ’1+x(St), eâˆ’1=0. (Discounted TD)
Here,{Î±t}is the learning rate. The eligibility trace ettracks recently visited states, assigning credit
for the prediction error to multiple preceding states. Let
A.=XâŠ¤DÏ€(Î³PÎ»âˆ’I)X, b.=XâŠ¤DÏ€rÎ», Wâˆ—.={w|Aw+b=0}.
IfXhas a full column rank, Tsitsiklis and Roy [1996] proves that Wâˆ—is a singleton and {wt}
converge to âˆ’Aâˆ’1balmost surely. A key result used by Tsitsiklis and Roy [1996] is that the matrix
DÏ€(Î³PÎ»âˆ’I)is n.d. [Sutton, 1988]. As a result, the Amatrix is also n.d. when Xhas a full column
rank. Wang and Zhang [2024] prove, without making any assumption on X, that Wâˆ—is always
nonempty and the {wt}converges to Wâˆ—almost surely. A key challenge there is that without making
assumptions on X,Ais only n.s.d.
Average-Reward Setting. In the average-reward setting, the overall performance of a policy Ï€is
measured by the average reward JÏ€.= lim Tâ†’âˆ1
TEhPTâˆ’1
t=0Rti
. The corresponding (differential)
value function is defined as vÏ€(s) = lim Tâ†’âˆ1
TPTâˆ’1
i=0E[(r(St+i, At+i)âˆ’JÏ€)|St=s]. We define
the Bellman operator T:R|S|â†’R|S|asTv.=rÏ€âˆ’JÏ€1+PÏ€v. Similarly, the Î»-weighted
counterpart TÎ»is defined as TÎ»v.=rÎ»âˆ’JÏ€
1âˆ’Î»1+PÎ»v. Although vÏ€is a fixed point of TÎ», it is not
the unique fixed point. In fact,
{vÏ€+c1|câˆˆR} (2)
are all the fixed points of TÎ»[Puterman, 2014]. Linear average-reward TD( Î») is an algorithm for
estimating both JÏ€andvÏ€using linear function approximation and eligibility traces. The update
rules are
et=Î»etâˆ’1+x(St), eâˆ’1=0,
wt+1=wt+Î±t(Rt+1âˆ’Ë†Jt+x(St+1)âŠ¤wtâˆ’x(St)âŠ¤wt)et,
Ë†Jt+1=Ë†Jt+Î²t(Rt+1âˆ’Ë†Jt), (Average Reward TD)
where {Î±t}and{Î²t}are learning rates. Let
A.=XâŠ¤DÏ€(PÎ»âˆ’I)X,b.=XâŠ¤DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1),Wâˆ—.=
w|Aw+b=0	
. (4)
IfXhas a full column rank and 1/âˆˆcol(X), Tsitsiklis and Roy [1999] proves that Wâˆ—is a singleton
and{wt}converge to âˆ’Aâˆ’1balmost surely. This is made possible by an important fact from the
Perron-Frobenius theorem (see, e.g., Seneta [2006]) that
w|wâŠ¤DÏ€(PÎ»âˆ’I)w= 0	
={c1|câˆˆR}. (5)
Zhang et al. [2021c] further provides a convergence rate, still assuming Xhas a full column rank but
without assuming 1/âˆˆcol(X). When Xdoes not have a full column rank, to our knowledge, it is
even not clear whether Wâˆ—is always nonempty or not, much less the behavior of {wt}.
33 Main Results
We start with our assumptions. As promised, we do not make any assumption on X.
Assumption 3.1. The Markov chain associated with PÏ€is irreducible and aperiodic.
Assumption LR. The learning rates are Î±t=Î±
(t+t0)Î¾andÎ²t=cÎ²Î±t, where Î¾âˆˆ(0.5,1],Î± >0,
t0>0, and cÎ²>0are constants.
Discounted Setting. Wang and Zhang [2024] proves the almost sure convergence of
(Discounted TD) with arbitrary features by using âˆ¥wâˆ’wâˆ—âˆ¥2with an arbitrary and fixed wâˆ—âˆˆWâˆ—as
a Lyapunov function and analyzing the property of the ODEdw(t)
dt=Aw(t). Since Ais only n.s.d.,
Wang and Zhang [2024] conducts their analysis in the complex number field. In this work, instead
of following the ODE-based analysis originating from Tsitsiklis and Roy [1996], Borkar and Meyn
[2000], we extend Srikant and Ying [2019] to obtain convergence rates by using d(w, W âˆ—)2as the
Lyapunov function. To our knowledge, this is the first time that such distance function to a set is used
as the Lyapunov function to analyze RL algorithms, which is our key technical contribution from the
methodology aspect. According to Theorem 1 of Wang and Zhang [2024], Wâˆ—is nonempty, and ap-
parently convex and closed.2LetÎ“(w).= arg min wâˆ—âˆˆWâˆ—âˆ¥wâˆ’wâˆ—âˆ¥be the orthogonal projection to
Wâˆ—. We then define L(w).=1
2d(w, W âˆ—)2=1
2âˆ¥wâˆ’Î“(w)âˆ¥2. Two important and highly non-trivial
observations are
(i)âˆ‡L(w) =wâˆ’Î“(w)(Example 3.31 of Beck [2017]),
(ii)L(w)is 1-smooth w.r.t. âˆ¥Â·âˆ¥(Example 5.5 of Beck [2017]).
Both (i) and (ii) result from the fact that Wâˆ—is nonempty, closed, and convex. Using L(w)as the
Lyapunov function together with more characterization of âˆ‡L(w)(Section 5.2), we obtain
Theorem 1. Let Assumptions 3.1and LRhold and Î»âˆˆ[0,1]. Then for sufficiently large t0
andÎ±, there exist some constants CThm1 andÎº1>1such that the iterates {wt}generated
by(Discounted TD) satisfy for all t
E
d(wt, Wâˆ—)2
â‰¤CThm1 t0
tâŒŠÎº1âŒ‹d(w0, wâˆ—)2+
ln(t+t0)
(t+t0)min(2 Î¾âˆ’1,âŒŠÎº1âŒ‹âˆ’1)
.
The proof is in Section 5.2. Notably, Lemma 3 of Wang and Zhang [2024] states that for any
wâˆ—, wâˆ—âˆ—âˆˆWâˆ—, it holds that Xwâˆ—=Xwâˆ—âˆ—. We then define
Ë†vÏ€.=Xwâˆ— (6)
for any wâˆ—âˆˆWâˆ—. Theorem 1 then also gives the L2convergence rate of the value estimate, i.e., the
rate at which Xwtconverges to Ë†vÏ€. The value estimate Ë†vÏ€is the unique fixed point of a projected
Bellman equation. See Wang and Zhang [2024] for more discussion on the property of Ë†vÏ€.
Average Reward Setting. Characterizing Wâˆ—is much more challenging. We first present a novel
decomposition of the feature matrix X. To this end, define m.= rank( X)â‰¤min{|S|, d}. Ifm= 0,
all the results in this work are trivial and we thus discuss only the case mâ‰¥1.
Lemma 1. There exist matrices X1, X2such that X=X1+X2with the following properties (1)
rank( X1) =mâˆ’I1âˆˆcol(X)and1/âˆˆX1(2)X2=1Î¸âŠ¤withÎ¸âˆˆRd.
The proof is in Section B.1 with Ibeing the indicator function. Essentially, X2is a rank one matrix
with identical rows Î¸(i.e., the i-th column of X2isÎ¸i1). To our knowledge, this is the first time
that such decomposition is used to analyze average-reward RL algorithms, which is our second
technical contribution from the methodology aspect. This decomposition is useful in three aspects.
First, we have A=XâŠ¤
1DÏ€(PÎ»âˆ’I)X1(Lemma 14). Second, this decomposition is the key to prove
thatWâˆ—is nonempty (Lemma 15). Third, this decomposition is the key to characterize Wâˆ—in that
Wâˆ—={wâˆ—}+ ker( X1)withwâˆ—being any vector in Wâˆ—(Lemma 16). To better understand this
characterization, we note that ker(X1) ={w|Xw=c1, câˆˆR}(Lemma 16). As a result, adding
2This theorem only discusses the case of Î»= 0. The proof for a general Î»âˆˆ[0,1]is exactly the same up to
change of notations.
4anyw0âˆˆker(X1)to a weight vector wchanges the resulting value function Xw only by c1. Two
values v1andv2can be considered â€œduplicationâ€ if v1âˆ’v2=c1(cf.(2)). So intuitively, ker(X1)is
the source of the â€œduplicationâ€. With the help of this novel decomposition, we obtain
Theorem 2. Let Assumptions 3.1and LRhold and Î»âˆˆ[0,1). Then for sufficiently large Î±,
t0andcÎ², there exist some constants CThm2 andÎº2>1such that the iterates {wt}generated
by(Average Reward TD) satisfy for all t
Eh
(Ë†Jtâˆ’JÏ€)2+d(wt,Wâˆ—)2i
â‰¤CThm2 t0
tâŒŠÎº2âŒ‹h
(Ë†J0âˆ’JÏ€)2+d(w0,Wâˆ—)2i
+CThm2
ln(t+t0)
(t+t0)min(2 Î¾âˆ’1,âŒŠÎº2âŒ‹âˆ’1)
.
The proof is in Section 5.3.
Stochastic Approximation. We now present a general stochastic approximation result to prove
Theorems 1and2. The notations in this part are independent of the rest of the paper. We consider
a general iterative update rule for a weight vector wâˆˆRd, driven by a time-homogeneous Markov
chain{Yt}evolving in a possibly infinite space Y:
wt+1=wt+Î±tH(wt, Yt+1), (SA)
where H:RdÃ— Y â†’ Rddefines the incremental update.
Assumption A1. There exists a constant CA1such that supyâˆˆYâˆ¥H(0, y)âˆ¥<âˆ,
âˆ¥H(w1, y)âˆ’H(w2, y)âˆ¥ â‰¤CA1âˆ¥w1âˆ’w2âˆ¥ âˆ€w1, w2, y.
Assumption A2. {Yt}has a unique stationary distribution dY.
Leth(w).=Eyâˆ¼dY[H(w, y)]. Assumption A1 then immediately implies that
âˆ¥h(w1)âˆ’h(w2)âˆ¥ â‰¤CA1âˆ¥w1âˆ’w2âˆ¥ âˆ€w1, w2.
In many existing works about stochastic approximation [Borkar and Meyn, 2000, Chen et al., 2021b,
Borkar et al., 2021, Qian et al., 2024], it is assumed that h(w) = 0 adopts a unique solution. To work
with the challenges of linear TD with arbitrary features, we relax this assumption and consider a
setWâˆ—. Importantly, Wâˆ—does not need to contain all solutions to h(w) = 0 . Instead, we make the
following assumptions on Wâˆ—.
Assumption A3. Wâˆ—is nonempty, closed, and convex.
Notably, Wâˆ—does not need to be bounded. Assumption A3ensures that the orthogonal projec-
tion to Wâˆ—is well defined, allowing us to define Î“(w).= arg min wâˆ—âˆˆWâˆ—âˆ¥wâˆ’wâˆ—âˆ¥, L(w).=
1
2âˆ¥wâˆ’Î“(w)âˆ¥2. As discussed before, Assumption A3ensures that âˆ‡L(w) =wâˆ’Î“(w)andL
is 1-smooth w.r.t. âˆ¥Â·âˆ¥[Beck, 2017]. We further assume that the expected update h(wt)decreases
L(wt)in the following sense, making L(w)a candidate Lyapunov function.
Assumption A4. There exists a constant CA4>0such that almost surely,
âŸ¨âˆ‡L(wt), h(wt)âŸ© â‰¤ âˆ’ CA4L(wt).
Lastly, we make the most â€œunnaturalâ€ assumption of Wâˆ—.
Assumption A5. There exists a matrix Xand constants CA5andÏ„âˆˆ[0,1)such that (1) âˆ€wâˆ—âˆˆWâˆ—,
âˆ¥Xwâˆ—âˆ¥ â‰¤CA5; (2)âˆ€w, y,âˆ¥H(w, y)âˆ¥ â‰¤CA5(âˆ¥Xwâˆ¥+ 1) ; (3) For any nâ‰¥1:
âˆ¥h(w)âˆ’E[H(w, Y t+n)|Yt]âˆ¥ â‰¤CA5Ï„n(âˆ¥Xwâˆ¥+ 1) (7)
This assumption is technically motivated but trivially holds in our analyses of (Discounted TD)
and(Average Reward TD) . Specifically, Assumption A1immediately leads to at-most-linear growth
âˆ¥H(w, y)âˆ¥ â‰¤CA1,1(âˆ¥wâˆ¥+ 1) for some constant CA1,1. However, this bound is insufficient for
our analysis because âˆ¥wâˆ¥ â‰¤ âˆ¥ wâˆ’Î“(w)âˆ¥+âˆ¥Î“(w)âˆ¥butÎ“(w)âˆˆWâˆ—can be unbounded. By
Assumption A5, we can have âˆ¥Xwâˆ¥ â‰¤ âˆ¥ Xwâˆ’XÎ“(w)âˆ¥+âˆ¥XÎ“(w)âˆ¥ â‰¤ âˆ¥ Xâˆ¥âˆ¥wâˆ’Î“(w)âˆ¥+CA5.
The inequality (7)is related to geometrical mixing of the chain and we additionally include Xw in
the bound for the same reason. We now present our general results regarding the convergence rate
of (SA) to Wâˆ—.
5Theorem 3. Let Assumptions A1-A5and LRhold. Denote Îº.=Î±C A4, then there exist some
constants t0andCThm3, such that the iterates {wt}generated by (SA) satisfy for all t
E[L(wt)]â‰¤CThm3,1 t0
tâŒŠÎºâŒ‹L(w0) +CThm3,2
ln(t+t0)
(t+t0)min(2 Î¾âˆ’1,âŒŠÎºâŒ‹âˆ’1)
.
The proof is in Section 5.1.
4 Related Works
Most prior works regarding the convergence of linear TD summarized in Table 1 rely on having
linearly independent features. In fact, the reliance on feature independence goes beyond linear TD
and exists in almost all previous analyses of RL algorithms with linear function approximation, see,
e.g., Sutton et al. [2008, 2009], Maei [2011], Hackman [2013], Yu [2015, 2016], Zou et al. [2019],
Yang et al. [2019], Zhang et al. [2020b], Bo et al. [2020], Xu et al. [2020a], Zhang et al. [2020a], Xu
et al. [2020b], Wu et al. [2020], Chen et al. [2021a], Long et al. [2021], Qiu et al. [2021], Zhang et al.
[2021a,b], Xu et al. [2021], Zhang et al. [2022], Zhang and Whiteson [2022], Zhang et al. [2023],
Chen et al. [2023a], NicolÃ² et al. [2024], Shaan and Siva [2024], Yue et al. [2024], Swetha et al.
[2024], Liu et al. [2025a], Qian and Zhang [2025], Sreejeet and Aritra [2025], Yang et al. [2025],
Chen et al. [2025], Liu et al. [2025b]. But as argued by Peter [1992], Tsitsiklis and Roy [1996,
1999], Wang and Zhang [2024], relaxing this assumption is an important research direction. This
work can be viewed as an extension of Wang and Zhang [2024], Zhang et al. [2021c]. In terms
of(Discounted TD) , we extend Wang and Zhang [2024] by proving a finite sample analysis. Though
we rely on the characterization of Wâˆ—from Wang and Zhang [2024], the techniques we use for finite
sample analysis are entirely different from the techniques of Wang and Zhang [2024] for almost
sure asymptotic convergence. In terms of (Average Reward TD) , we extend Zhang et al. [2021c]
by allowing Xto be arbitrary. Essentially, key to Zhang et al. [2021c] is their proof that Ais n.d.
on a subspace E, assuming Xhas a full column rank. We extend Zhang et al. [2021c] in that we
give a finer and more detailed characterization of the counterparts of their Ethrough the novel
decomposition of the features (Lemma 1) and establish the n.d. property under weaker conditions
(i.e., without assuming Xhas a full column rank). Our improvements are made possible by the novel
Lyapunov function L(w)and we argue that this Lyapunov function can be used to analyze many
other linear RL algorithms with arbitrary features.
In terms of stochastic approximation, our Theorem 3is novel in that it allows convergence to
a possibly unbounded set. By contrast, most prior works about stochastic approximation study
convergence to a point [Borkar and Meyn, 2000, Borkar et al., 2021, Chen et al., 2020, 2021b, Zhang
et al., 2022, Chen et al., 2023b, Qian et al., 2024, Liu et al., 2025a]. In the case of convergence to a
set, most prior works require the set to be bounded [Kushner and Yin, 2003, Borkar, 2009, Liu et al.,
2025a]. Only a few prior works allow stochastic approximation to converge to an unbounded set, see,
e.g., Bravo and Cominetti [2022], Chen [2025], Blaser and Zhang [2025], which apply to only tabular
RL algorithms.
5 Proofs of the Main Results
5.1 Proof of Theorem 3
Proof. From the 1-smoothness of L(w)and (SA), we can get
L(wt+1)â‰¤L(wt) +Î±tâŸ¨wtâˆ’Î“(wt), h(wt)âŸ©
+Î±tâŸ¨wtâˆ’Î“(wt), H(wt, Yt)âˆ’h(wt)âŸ©+1
2Î±2
tâˆ¥H(wt, Yt)âˆ¥2. (8)
We then bound the RHS one by one. âŸ¨wâˆ’Î“(w), h(w)âŸ©is already bounded in Assumption A4.
Lemma 2. There exists a positive constant C2, such that for any w,
âˆ¥Xwâˆ¥ â‰¤C2(âˆ¥wâˆ’Î“(w)âˆ¥+ 1).
The proof is in Section C.1. With Lemma 2and Assumption A5, the last term in (8)can be bounded
easily.
6Lemma 3. There exists a constant C3such that âˆ¥H(wt, Yt)âˆ¥2â‰¤C3(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1) .
The proof is in Section C.2. To bound âŸ¨wtâˆ’Î“(wt), H(wt, Yt)âˆ’h(wt)âŸ©, leveraging (7), we define
Ï„Î±.= min {nâ‰¥0|CA5Ï„nâ‰¤Î±} (9)
as the number of steps that the Markov chain needs to mix to an accuracy Î±. In addition, we denote a
shorthand Î±t1,t2.=Pt2
i=t1Î±i. Then with techniques from Srikant and Ying [2019], we obtain
Lemma 4. There exists a constant C4such that
E[âŸ¨wtâˆ’Î“(wt), H(wt, Yt)âˆ’h(wt)âŸ©]â‰¤C4Î±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1).
The proof is in Section C.3. Plugging all the bounds back to (8), we obtain
Lemma 5. There exists some Dt=O(Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1), such that
E[L(wt+1)]â‰¤(1âˆ’CA4Î±t)E[L(wt)] +Dt.
The proof is in Section C.4. Recursively applying Lemma 5 then completes the proof of Theorem 3
(See Section C.5 for details).
In the following sections, we first map the general update (SA) to(Discounted TD)
and(Average Reward TD) by defining H(w, y),h(w), and L(w)properly. Then we bound the
remaining term âŸ¨âˆ‡L(wt), h(wt)âŸ©to complete the proof.
5.2 Proof of Theorem 1
Proof. We first rewrite (Discounted TD) in the form of (SA) . To this end, we define Yt+1.=
(St, At, St+1, et), which evolves in an infinite space Y.=S Ã— A Ã— S Ã— { e| âˆ¥eâˆ¥ â‰¤Ce}with
Ce.=maxsâˆ¥x(s)âˆ¥
1âˆ’Î³Î»being the straightforward bound of suptâˆ¥etâˆ¥. We define the incremental update
H:RdÃ— Y â†’ Rdas
H(w, y) = (r(s, a) +Î³x(sâ€²)âŠ¤wâˆ’x(s)âŠ¤w)e, (10)
using shorthand y= (s, a, sâ€², e). We now proceed to verifying the assumptions of Theorem 3.
Assumption A1 is verified by the following lemma.
Lemma 6. There exists some finite C6such that
âˆ¥H(w1, y)âˆ’H(w2, y)âˆ¥ â‰¤C6âˆ¥w1âˆ’w2âˆ¥ âˆ€w1, w2, y.
Moreover, supyâˆˆYâˆ¥H(0, y)âˆ¥<âˆ.
The proof is in Section D.1.
For Assumption A2, Theorem 3.2 of Yu [2012] confirms that {Yt}has a unique stationary distribution
dY. Yu [2012] also computes that
h(w).=Eyâˆ¼dY[H(w, y)] =Aw+b.
Assumption A3 trivially holds by the definition of Wâˆ—.
For Assumption A4, the key observation is that AÎ“(w) +b= 0always holds because Î“(w)âˆˆWâˆ—.
Then we have h(w) =Aw+b= (Aw+b)âˆ’(AÎ“(w) +b) =A(wâˆ’Î“(w)). Thus the term
âŸ¨âˆ‡L(w), h(w)âŸ©can be written as (wâˆ’Î“(w))âŠ¤A(wâˆ’Î“(w)). We now prove that for whatever X, it
always holds that Ais n.d. on ker(A)âŠ¥.
Lemma 7. There exists a constant C7>0such that for âˆ€wâˆˆker(A)âŠ¥,wâŠ¤Awâ‰¤ âˆ’C7âˆ¥wâˆ¥2.
Furthermore, for any wâˆˆRd, it holds that wâˆ’Î“(w)âˆˆker(A)âŠ¥.
The proof is in Section D.3. We then have
âŸ¨wtâˆ’Î“(wt), A(wtâˆ’Î“(wt))âŸ© â‰¤ âˆ’ C7âˆ¥wtâˆ’Î“(wt)âˆ¥2,
which satisfies Assumption A4.
For Assumption A5,(6)verifies Assumption A5(1). Assumption A5(2) is verified by the following
lemma.
7Lemma 8. There exists a constant C8such that for âˆ€w, y,âˆ¥H(w, y)âˆ¥ â‰¤C8(âˆ¥Xwâˆ¥+ 1) .
The proof is in Section D.4. Assumption A5(3) is verified following a similar procedure as Lemma 6.7
in Bertsekas and Tsitsiklis [1996] (Lemma 18). Invoking Theorem 3 then completes the proof.
5.3 Proof of Theorem 2
Proof. We recall that in view of Lemma 1,ker(X1)creates â€œduplicationâ€ in value estimation. We,
therefore, define the projection matrix Î âˆˆRdÃ—dthat projects a vector into the orthogonal comple-
ment of ker(X1), i.e., Î w.= arg min wâ€²âˆˆker(X1)âŠ¥âˆ¥wâˆ’wâ€²âˆ¥. It can be computed that Î  = Xâ€ 
1X1.
We now examine the sequence {Î wt}with{wt}being the iterates of (Average Reward TD) and
consider the combined parameter vector ewt.=Ë†Jt
Î wt
âˆˆR1+d. The following lemma characterizes
the evolution of ewt. Let Yt= (St, At, St+1, et)âˆˆ S Ã— A Ã— S Ã—n
eâˆˆRd| âˆ¥eâˆ¥ â‰¤maxsâˆ¥x(s)âˆ¥
1âˆ’Î»o
,
then
Lemma 9. ewt+1=ewt+Î±t(eA(Yt)ewt+eb(Yt)), where we have, with y= (s, a, sâ€², e),
eA(y) =âˆ’cÎ² 0
âˆ’Î eÎ e(x(sâ€²)âŠ¤âˆ’x(s)âŠ¤)
,eb(y) =
cÎ²r(s, a)
r(s, a)Î e
.
This view is inspired by Zhang et al. [2021c] and the proof is in Section E.1. We now apply Theorem 3
to{ewt}.
The verification of Assumptions A1andA2is identical to that in Section 5.2and is thus omitted.
For Assumption A3, we define fWâˆ—.=
JÏ€
Î wwâˆˆWâˆ—
. It is apparently nonempty, closed, and
convex.
For Assumption A4, we define eA.=Eyâˆ¼dYh
eA(y)i
andeb.=Eyâˆ¼dYh
eb(y)i
and therefore realize
thehin(SA) ash(ew) =eAew+eb. Noticing that eAÎ“(ew) +eb=0(Lemma 19), we then have
h(ew) =eA(ewâˆ’Î“(ew)). The term âŸ¨âˆ‡L(ew), h(ew)âŸ©can thus be written as (ewâˆ’Î“(ew))âŠ¤eA(ewâˆ’Î“(ew)).
Next, we prove that when cÎ²is large enough, eAis n.d. on RÃ—ker(X1)âŠ¥.
Lemma 10. LetcÎ²be sufficiently large. Then there exists a constant C10>0such that âˆ€zâˆˆ
RÃ—ker(X1)âŠ¥,zTeAzâ‰¤ âˆ’C10âˆ¥zâˆ¥2.
The proof is in Section E.3. By definition, we have ewtâˆˆRÃ—ker(X1)âŠ¥andÎ“(ew)âˆˆRÃ—ker(X1)âŠ¥.
Soewâˆ’Î“(ew)âˆˆRÃ—ker(X1)âŠ¥, yielding
âŸ¨ewtâˆ’Î“(ewt),eA(ewtâˆ’Î“(ewt))âŸ© â‰¤ âˆ’ C10âˆ¥ewtâˆ’Î“(ewt)âˆ¥2,
which verifies Assumption A4.
For Assumption A5, we define eX=
10âŠ¤
0X
. Assumption A5(1) is verified below.
Lemma 11. There exists a positive constant C11, such that for any ewâˆˆfWâˆ—,eXew=C11.
The proof is in Section E.4. With H(ew, y) =eA(y)ew+eb(y), the verification of Assumption A5(2)
and (3) is similar to Lemmas 8and18and is thus omitted. Invoking Theorem 3 then yields the
convergence rate of E[L(ewt)], i.e., the convergence rate of d(ewt,fWâˆ—)2by the definition of L. The
next key observation is that d(ewt,fWâˆ—)2= (Ë†Jtâˆ’JÏ€)2+d(wt,Wâˆ—)2(Lemma 20), which completes
the proof.
86 Experiments
We now empirically examine linear TD with linearly dependent features. Following the practice of
Sutton and Barto [2018], we use constant learning rates Î±andÎ²instead of Î±tandÎ²tto facilitate
experiments. We use a variant of Boyanâ€™s chain [Boyan, 1999] with 15 states ( |S|= 15 ) and 5 actions
(|A|= 5) under a uniform policy Ï€(a|s) = 1 /|A|, where the feature matrix XâˆˆR15Ã—5is designed
to be of rank 3 (more details in Section F). The weight convergence to a set is indeed observed. It is
within expectation that different Î»requires different Î±, Î².
0.00.51.01.5
StepsÃ—106246=0.9
d(wt,W*)=0.1
=0.005
=0.01
0.00.51.01.5
StepsÃ—106510=0.5
=0.005
=0.01
0.00.51.01.5
StepsÃ—1061020=0.9
=0.005
=0.01
Figure 1: Convergence of (Discounted TD) withÎ³= 0.9, Î±âˆˆ {0.005,0.01}. Curves are averaged
over 10 runs with shaded regions (too small to be visible) indicating standard errors.
0.00.51.01.5
StepsÃ—1061.01.21.4d(wt,W*)=0.1, =0.01
=0.01
=0.02
=0.1
0.00.51.01.5
StepsÃ—1060.250.500.75=0.5, =0.01
=0.01
=0.02
=0.1
0.00.51.01.5
StepsÃ—106123=0.9, =0.01
=0.01
=0.02
=0.1
Figure 2: Convergence of (Average Reward TD) withÎ²= 0.01, Î±âˆˆ {0.01,0.02,0.1}. Curves are
averaged over 10 runs with shaded regions (too small to be visible) indicating standard errors.
7 Conclusion
This paper provides the first finite sample analysis of linear TD with arbitrary features in both
discounted and average reward settings, fulfilling the long standing desiderata of Peter [1992],
Tsitsiklis and Roy [1996, 1999], enabled by a novel stochastic approximation result concerning the
convergence rate to a set. The key methodology contributions include a novel Lyapunov function
based on the distance to a set and a novel decomposition of the feature matrix for the average-reward
setting. We envision the techniques developed in this work can easily transfer to the analyses of
other linear RL algorithms. That being said, one limitation of the work is its focus on linear function
approximation. Extension to neural networks with neural tangent kernels (cf. Cai et al. [2019]) is a
possible future work. Another limitation is that this work considers only L2convergence rates but
the convergence mode of random variables are versatile. Establishing almost sure convergence rates,
Lpconvergence rates, and high probability concentration bounds (cf. Qian et al. [2024]) is also a
possible future work.
Acknowledgments and Disclosure of Funding
This work is supported in part by the US National Science Foundation under grants III-2128019 and
SLES-2331904.
9References
David Abel, AndrÃ© Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and Satinder Singh. A
definition of continual reinforcement learning. ArXiv Preprint, 2023.
Amir Beck. First-order methods inoptimization. SIAM, 2017.
Richard Bellman. A markovian decision process. Journal ofmathematics andmechanics, 1957.
Richard Bellman. Dynamic programming. Science, 1966.
Albert Benveniste, Michel MÃ©tivier, and Pierre Priouret. Adaptive Algorithms andStochastic Approximations .
Springer, 1990.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-Dynamic Programming . Athena Scientific Belmont, MA,
1996.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Proceedings oftheConference onLearning Theory, 2018.
Ethan Blaser and Shangtong Zhang. Asymptotic and finite sample analysis of nonexpansive stochastic approxi-
mations with markovian noise. ArXiv Preprint, 2025.
Liu Bo, Liu Ji, Ghavamzadeh Mohammad, Mahadevan Sridhar, and Petrik Marek. Finite-sample analysis of
proximal gradient td algorithms. ArXiv Preprint, 2020.
Vivek Borkar, Shuhang Chen, Adithya Devraj, Ioannis Kontoyiannis, and Sean Meyn. The ode method for
asymptotic statistics in stochastic approximation and reinforcement learning. ArXiv Preprint, 2021.
Vivek S Borkar. Stochastic approximation: adynamical systems viewpoint. Springer, 2009.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and reinforce-
ment learning. SIAM Journal onControl andOptimization, 2000.
Justin A. Boyan. Least-squares temporal difference learning. In Proceedings oftheInternational Conference on
Machine Learning, 1999.
Mario Bravo and Roberto Cominetti. Stochastic fixed-point iterations for nonexpansive maps: Convergence and
error bounds. ArXiv Preprint, 2022.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference and q-learning provably
converge to global optima. ArXiv Preprint, 2019.
Xuyang Chen, Jingliang Duan, Yingbin Liang, and Lin Zhao. Global convergence of two-timescale actor-critic
for solving linear quadratic regulator. Proceedings oftheAAAI Conference onArtificial Intelligence , 2023a.
Zaiwei Chen. Non-asymptotic guarantees for average-reward q-learning with adaptive stepsizes. ArXiv Preprint ,
2025.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample analysis of
contractive stochastic approximation using smooth convex envelopes. ArXiv Preprint, 2020.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample analysis of
off-policy td-learning via generalized bellman operators. ArXiv Preprint, 2021a.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A lyapunov theory for
finite-sample guarantees of asynchronous q-learning and td-learning variants. ArXiv Preprint, 2021b.
Zaiwei Chen, Siva Theja Maguluri, and Martin Zubeldia. Concentration of contractive stochastic approximation:
Additive and multiplicative noise. ArXiv Preprint, 2023b.
Zaiwei Chen, Sheng Zhang, Zhe Zhang, Shaan Ul Haque, and Siva Theja Maguluri. A non-asymptotic theory of
seminorm lyapunov stability: From deterministic to stochastic iterative algorithms. ArXiv Preprint, 2025.
Leah M Hackman. Faster gradient-td algorithms, 2013.
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement learning:
A review and perspectives. Journal ofArtificial Intelligence Research, 2022.
Harold Kushner and G George Yin. Stochastic approximation andrecursive algorithms andapplications .
Springer Science & Business Media, 2003.
10Chandrashekar Lakshminarayanan and Csaba SzepesvÃ¡ri. Linear stochastic approximation: How far does
constant step-size and iterate averaging go? In Proceedings oftheInternational Conference onArtificial
Intelligence andStatistics, 2018.
Shuze Liu, Shuhang Chen, and Shangtong Zhang. The ODE method for stochastic approximation and reinforce-
ment learning with markovian noise. Journal ofMachine Learning Research, 2025a.
Xinyu Liu, Zixuan Xie, and Shangtong Zhang. Linear q-learning does not diverge in l2: Convergence rates to a
bounded set. In Proceedings oftheInternational Conference onMachine Learning, 2025b.
Yang Long, Zheng Gang, Zhang Yu, Zheng Qian, Li Pengfei, and Pan Gang. On convergence of gradient
expected sarsa( Î»). In Proceedings oftheAAAI Conference onArtificial Intelligence, 2021.
Hamid Reza Maei. Gradient temporal-difference learning algorithms. PhD thesis, University of Alberta, 2011.
Aritra Mitra. A simple finite-time analysis of td learning with linear function approximation. IEEE Transactions
onAutomatic Control, 2025.
Dal Fabbro NicolÃ², Adibi Arman, Mitra Aritra, and J. Pappas George. Finite-time analysis of asynchronous
multi-agent td learning. ArXiv Preprint, 2024.
Dayan Peter. The convergence of td( Î») for general Î».Machine Learning, 1992.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley & Sons,
2014.
Xiaochi Qian and Shangtong Zhang. Revisiting a design choice in gradient temporal difference learning. In
Proceedings oftheInternational Conference onLearning Representations, 2025.
Xiaochi Qian, Zixuan Xie, Xinyu Liu, and Shangtong Zhang. Almost sure convergence rates and concentration
of stochastic approximation and reinforcement learning with markovian noise. ArXiv Preprint, 2024.
Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of actor-critic algorithm.
IEEE Journal onSelected Areas inInformation Theory, 2021.
Mark Bishop Ring. Continual learning inreinforcement environments . PhD thesis, The University of Texas at
Austin, 1994.
Eugene Seneta. Non-negative matrices andMarkov chains. Springer Science & Business Media, 2006.
Ul Haque Shaan and Theja Maguluri Siva. Stochastic approximation with unbounded markovian noise: A
general-purpose theorem. ArXiv Preprint, 2024.
Maity Sreejeet and Mitra Aritra. Adversarially-robust td learning with markovian data: Finite-time rates and
fundamental limits. ArXiv Preprint, 2025.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning.
InProceedings oftheConference onLearning Theory, 2019.
Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: AnIntroduction (2nd Edition) . MIT press,
2018.
Richard S. Sutton, Csaba SzepesvÃ¡ri, and Hamid Reza Maei. A convergent o(n) temporal-difference algorithm
for off-policy learning with linear function approximation. In Advances inNeural Information Processing
Systems, 2008.
Richard S. Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba SzepesvÃ¡ri,
and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function
approximation. In Proceedings oftheInternational Conference onMachine Learning, 2009.
Ganesh Swetha, Uddin Mondal Washim, and Aggarwal Vaneet. Order-optimal global convergence for average
reward reinforcement learning via actor-critic approach. ArXiv Preprint, 2024.
John N. Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation.
InIEEE Transactions onAutomatic Control, 1996.
John N. Tsitsiklis and Benjamin Van Roy. Average cost temporal-difference learning. Automatica, 1999.
11Jiuqi Wang and Shangtong Zhang. Almost sure convergence of linear temporal difference learning with arbitrary
features. ArXiv Preprint, 2024.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic
methods. In Advances inNeural Information Processing Systems, 2020.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural) actor-critic
algorithms. In Advances inNeural Information Processing Systems, 2020a.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale (natural)
actor-critic algorithms. ArXiv Preprint, 2020b.
Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang. Doubly robust off-policy actor-critic: Conver-
gence and optimality. ArXiv Preprint, 2021.
Peng Yang, Jin Kaicheng, Zhang Liangyu, and Zhang Zhihua. Finite sample analysis of distributional td learning
with linear function approximation. ArXiv Preprint, 2025.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably global convergence of actor-critic: A
case for linear quadratic regulator with ergodic cost. In Advances inNeural Information Processing Systems ,
2019.
Huizhen Yu. Least squares temporal difference methods: An analysis under general conditions. SIAM Journal
onControl andOptimization, 2012.
Huizhen Yu. On convergence of emphatic temporal-difference learning. In Proceedings oftheConference on
Learning Theory, 2015.
Huizhen Yu. Weak convergence properties of constrained emphatic temporal-difference learning with constant
and slowly diminishing stepsize. Journal ofMachine Learning Research, 2016.
Wang Yue, Zhou Yi, and Zou Shaofeng. Finite-time error bounds for greedy-gq. Machine Learning, 2024.
Shangtong Zhang and Shimon Whiteson. Truncated emphatic temporal difference methods for prediction and
control. Journal ofMachine Learning Research, 2022.
Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized offline estimation of
stationary values. In Proceedings oftheInternational Conference onMachine Learning, 2020a.
Shangtong Zhang, Bo Liu, Hengshuai Yao, and Shimon Whiteson. Provably convergent two-timescale off-
policy actor-critic with function approximation. In Proceedings oftheInternational Conference onMachine
Learning, 2020b.
Shangtong Zhang, Yi Wan, Richard S. Sutton, and Shimon Whiteson. Average-reward off-policy policy
evaluation with function approximation. In Proceedings oftheInternational Conference onMachine Learning ,
2021a.
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target network. In
Proceedings oftheInternational Conference onMachine Learning, 2021b.
Shangtong Zhang, Remi Tachet, and Romain Laroche. Global optimality and finite sample analysis of softmax
off-policy actor critic under state distribution mismatch. Journal ofMachine Learning Research, 2022.
Shangtong Zhang, Remi Tachet Des Combes, and Romain Laroche. On the convergence of sarsa with linear
function approximation. In Proceedings oftheInternational Conference onMachine Learning, 2023.
Sheng Zhang, Zhe Zhang, and Siva Theja Maguluri. Finite sample analysis of average-reward td learning and
q-learning. In Advances inNeural Information Processing Systems, 2021c.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approxima-
tion. In Advances inNeural Information Processing Systems, 2019.
12A Auxiliary Lemmas and Notations
Lemma 12 (Discrete Gronwall Inequality, Lemma 8 in Section 11.2 of Borkar [2009]) .For non-
negative real sequences {xn, nâ‰¥0}and{an, nâ‰¥0}and scalar Lâ‰¥0, it holds
xn+1â‰¤C+LPn
i=0aixiâˆ€n=â‡’xn+1â‰¤(C+x0) exp( LPn
i=0ai)âˆ€n.
Lemma 13 (Lemma 11 of Zhang et al. [2022]) .For sufficiently large t0, it holds that
Ï„Î±t=O(log(t+t0)), Î± tâˆ’Ï„Î±t,tâˆ’1=Olog(t+t0)
(t+t0)Î¾
.
Lemma 13 ensures that there exists some t >0(depending on t0) such that for all tâ‰¥t, it holds
thattâ‰¥Ï„Î±t. Also, it ensures that for sufficiently large t0, we have Î±tâˆ’Ï„Î±t,tâˆ’1<1. Throughout the
appendix, we always assume t0is sufficiently large and tâ‰¥t. We will refine (i.e., increase) talong
the proof when necessary.
B Proofs in Section 3
B.1 Proof of Lemma 1
Proof. LetxiâˆˆRddenote the i-th column of X. Without loss of generity, let the first mcolumns be
linearly independent.
Case 1: When 1âˆˆcol(X), there must exist mscalars {ci}such thatPm
i=1cixi=1. Apparently, at
least one of {ci}must be nonzero. Without loss of generity, let xmÌ¸= 0. We then have
xm=1
cm(1âˆ’mâˆ’1X
i=1cixi).
In other words, xmcan be expressed as linear combination of {x1, . . . , x mâˆ’1}and1. Since X
has a column rank m, we are able to express {xm+1, . . . , x d}by linear combination {x1, . . . , x m}
and thus further by linear combination of {x1, . . . , x mâˆ’1}and1. Let Z1.= [x1, . . . , x mâˆ’1]be the
firstmâˆ’1columns of XandZ2.= [xm, . . . , x d]be the rest. We now know that there exists some
CâˆˆR(mâˆ’1)Ã—(dâˆ’m+1)(i.e., coefficients of the lienar combination) such that
Z2=Z1C+ [Î¸m1, . . . , Î¸ d1],
where Î¸m, . . . Î¸ dare scalars (i.e., â€œcoordinatesâ€ along the 1-axis), e.g., Î¸m=1
cm. This means that
we can express Xas
X= [Z1Z1C] + [Î¸11, . . . , Î¸ d1] (11)
withÎ¸1=Â·Â·Â·=Î¸mâˆ’1= 0. Now define
X1.= [Z1Z1C], X2.= [Î¸11, . . . , Î¸ d1].
We note that 1/âˆˆcol(Z1). Otherwise, there would exist scalars {câ€²
i}such thatPmâˆ’1
i=1câ€²
ixi=1.
Then we getPmâˆ’1
i=1(ciâˆ’câ€²
i)xi+cmxm= 0, which is impossible because {xi}i=1,Â·Â·Â·,mare linearly
independent. Since col(X1) = col( Z1), we then have 1/âˆˆcol(X1).
Case 2: When 1/âˆˆcol(X), we can trivially define X1=XandX2= 0. Additionally, we can still
further decompose X1as
X1= [Z1Z1C], (12)
where Z1is now the first mcolumns of X. Apparently, we still have 1/âˆˆcol(X1).
Lemma 14. Let Assumption 3.1 hold. Then A=X1DÏ€(PÎ»âˆ’I)X1,b=XâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1).
Proof. Apply the decomposition shown in Lemma 1, we can get
A=(X1+X2)âŠ¤DÏ€(PÎ»âˆ’I)(X1+X2)
13=XâŠ¤
1DÏ€(PÎ»âˆ’I)X1+XâŠ¤
2DÏ€(PÎ»âˆ’I)X1+XâŠ¤
1DÏ€(PÎ»âˆ’I)X2+XâŠ¤
2DÏ€(PÎ»âˆ’I)X2
=XâŠ¤
1DÏ€(PÎ»âˆ’I)X1,
where the last equality holds because (PÎ»âˆ’I)1= 0 and1âŠ¤DÏ€(PÎ»âˆ’I) =dâŠ¤
Ï€PÎ»âˆ’dâŠ¤
Ï€= 0.
Similarly, for bwe can obtain
b=(X1+X2)âŠ¤DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)
=XâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1) +XâŠ¤
2DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)
=XâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1) +Î¸âŠ¤(dâŠ¤
Ï€(Iâˆ’Î»PÏ€)âˆ’1rÏ€âˆ’JÏ€
1âˆ’Î»)
=XâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1) +Î¸âŠ¤(1
1âˆ’Î»dâŠ¤
Ï€rÏ€âˆ’JÏ€
1âˆ’Î»)
=XâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1).
Here, the fourth inequality holds because dâŠ¤
Ï€(Iâˆ’Î»PÏ€) = (1 âˆ’Î»)dâŠ¤
Ï€, which gives us dâŠ¤
Ï€=
(1âˆ’Î»)dâŠ¤
Ï€(Iâˆ’Î»PÏ€)âˆ’1. The last inequality holds since JÏ€=dâŠ¤
Ï€rÏ€. This completes the proof.
Lemma 15. Let Assumption 3.1 hold. Then Wâˆ—is nonempty.
Proof. In view of (11) and(12), we have X1= [Z1Z1C]. Notably, Z1has a full column rank
and1/âˆˆcol(Z1). Decompose w.=
w1
w2
accordingly and recall (4)and Lemma 14, we can rewrite
Aw+b= 0as

ZâŠ¤
1
(Z1C)âŠ¤
DÏ€(PÎ»âˆ’I)[Z1Z1C]
w1
w2
=âˆ’ZâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)
âˆ’(Z1C)âŠ¤DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)
,
which thus gives us the following simultaneous equations
(
ZâŠ¤
1DÏ€(PÎ»âˆ’I)Z1w1+ZâŠ¤
1DÏ€(PÎ»âˆ’I)Z1Cw2=âˆ’ZâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)
(Z1C)âŠ¤DÏ€(PÎ»âˆ’I)Z1w1+ (Z1C)âŠ¤DÏ€(PÎ»âˆ’I)Z1Cw2=âˆ’(Z1C)âŠ¤DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1).
We now prove the claim by constructing a solution. Choose any w2âˆˆker(Z1C)(e.g., w2= 0), the
equations then become
(
ZâŠ¤
1DÏ€(PÎ»âˆ’I)Z1w1=âˆ’ZâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)
CâŠ¤ZâŠ¤
1DÏ€(PÎ»âˆ’I)Z1w1=âˆ’CâŠ¤ZâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1).
Since Z1is full rank and 1/âˆˆZ1, Lemma 7 of Tsitsiklis and Roy [1999] shows ZâŠ¤
1DÏ€(PÎ»âˆ’I)Z1is
n.d. and thus invertible. Choose w1=âˆ’(ZâŠ¤
1DÏ€(PÎ»âˆ’I)Z1)âˆ’1ZâŠ¤
1DÏ€(rÎ»âˆ’JÏ€
1âˆ’Î»1)then satisfies
the equations. This completes the proof.
Lemma 16. Let Assumption 3.1 hold. Then
Wâˆ—={wâˆ—}+ ker( X1)andker(X1) ={w|Xw=c1, câˆˆR}.
Proof. For any solution wâˆ—, wâˆ—âˆ—âˆˆWâˆ—, according to the definition of Wâˆ—in(4), we have Awâˆ—+b=
0andAwâˆ—âˆ—+b=0. That is A(wâˆ—âˆ’wâˆ—âˆ—) =0. By multiplying (wâˆ—âˆ’wâˆ—âˆ—)âŠ¤on both side we can
get
(wâˆ—âˆ’wâˆ—âˆ—)âŠ¤XâŠ¤DÏ€(PÎ»âˆ’I)X(wâˆ—âˆ’wâˆ—âˆ—) = 0 .
According to the Perron-Frobenius theorem with Assumption 3.1, vâŠ¤DÏ€(PÎ»âˆ’I)v= 0if and only
ifv=c1for some câˆˆR. Therefore, we must have X(wâˆ—âˆ’wâˆ—âˆ—) =c1for some câˆˆR. That is,
(X1+X2)(wâˆ—âˆ’wâˆ—âˆ—) =c1. Recall the definition of X2in(11), we have X2(wâˆ—âˆ’wâˆ—âˆ—) = (Î¸âŠ¤(wâˆ—âˆ’
14wâˆ—âˆ—))1. This means X1(wâˆ—âˆ’wâˆ—âˆ—) =câ€²1withcâ€²=câˆ’Î¸âŠ¤(wâˆ—âˆ’wâˆ—âˆ—). Since 1/âˆˆcol(X1), we must
havecâ€²= 0. That is, wâˆ—âˆ’wâˆ—âˆ—âˆˆker(X1). Thus, we have established that Wâˆ—={wâˆ—}+ ker( X1).
Furthermore, if wâˆˆker(X1), we have Xw= (X1+X2)w= (Î¸âŠ¤w)1. IfXw=c1, we have
X1w=c1âˆ’X2w= (câˆ’Î¸âŠ¤w)1. But 1/âˆˆcol(X1). So we must have câˆ’Î¸âŠ¤w= 0, i.e.,
wâˆˆker(X1). This completes the proof of ker(X1) ={w|Xw=c1, câˆˆR}.
C Proofs in Section 5.1
Lemma 17. For sufficiently large t0, there exists a constant C17such that the following statement
holds. For any tâ‰¥tand any iâˆˆ[tâˆ’Ï„Î±t, t], it holds thatwiâˆ’wtâˆ’Ï„Î±tâ‰¤C17Î±tâˆ’Ï„Î±t,iâˆ’1(âˆ¥wiâˆ’Î“(wi)âˆ¥+ 1).
Proof. In this proof, to simplify notations, we define shorthand t1.=tâˆ’Ï„Î±tandCx.= max sâˆ¥x(s)âˆ¥.
Given Lemma 13, we can select a sufficiently large t0such that for any tâ‰¥t,
exp 
CA5CxÎ±tâˆ’Ï„Î±ttâˆ’1
<3,
CA5CxÎ±tâˆ’Ï„Î±ttâˆ’1<1
6.
We then bound âˆ¥wiâˆ’wt1âˆ¥as
âˆ¥wiâˆ’wt1âˆ¥ â‰¤iâˆ’1X
k=t1âˆ¥Î±kH(wk, Yk+1)âˆ¥
â‰¤iâˆ’1X
k=t1Î±kCA5(âˆ¥Xwkâˆ’Xwt1âˆ¥+âˆ¥Xwt1âˆ¥+ 1) (Assumption A5)
â‰¤iâˆ’1X
k=t1Î±kCA5(âˆ¥Xwt1âˆ¥+ 1) +iâˆ’1X
k=t1Î±kCA5(âˆ¥Xwkâˆ’Xwt1âˆ¥)
â‰¤iâˆ’1X
k=t1Î±kCA5(âˆ¥Xwt1âˆ¥+ 1) +iâˆ’1X
k=t1Î±kC17,1(âˆ¥wkâˆ’wt1âˆ¥)
â‰¤CA5Î±t1,iâˆ’1(âˆ¥Xwt1âˆ¥+ 1) exp( C17,1Î±t1,tâˆ’1),(Lemma 12)
where C17,1.=CA5Cx. We then have
âˆ¥wiâˆ’wt1âˆ¥
â‰¤CA5Î±t1,iâˆ’1(âˆ¥Xwiâˆ’Xwt1âˆ¥+âˆ¥Xwiâˆ¥+ 1) exp( C17,1Î±t1,tâˆ’1)
â‰¤CA5Cxexp(C17,1Î±t1,tâˆ’1)Î±t1,iâˆ’1âˆ¥wiâˆ’wt1âˆ¥+ exp( C17,1Î±t1,tâˆ’1)(âˆ¥Xwiâˆ¥+ 1)CA5Î±t1,iâˆ’1
â‰¤1
2âˆ¥wiâˆ’wt1âˆ¥+C17,2Î±t1,iâˆ’1(âˆ¥Xwiâˆ¥+ 1),
where C17,2.= 3CA5. Thus, we have
âˆ¥wiâˆ’wt1âˆ¥ â‰¤2C17,2Î±t1,iâˆ’1(âˆ¥Xwiâˆ¥+ 1)
â‰¤2C17,2Î±t1,iâˆ’1(C2(âˆ¥wiâˆ’Î“(wi)âˆ¥+ 1) + 1)
â‰¤C17Î±t1,iâˆ’1(âˆ¥wiâˆ’Î“(wi)âˆ¥+ 1),
where C17.= 2C17,2(C2+ 1) . This completes the proof.
C.1 Proof of Lemma 2
Proof.
âˆ¥Xwâˆ¥=âˆ¥Xwâˆ’XÎ“(w) +XÎ“(w)âˆ¥
â‰¤ âˆ¥X(wâˆ’Î“(w))âˆ¥+âˆ¥XÎ“(w)âˆ¥
â‰¤ âˆ¥Xâˆ¥âˆ¥wâˆ’Î“(w)âˆ¥+CA5 (Assumption A5)
15C.2 Proof of Lemma 3
Proof. According to the definition of H(wt, Yt)in (10),
âˆ¥H(wt, Yt)âˆ¥2
â‰¤C2
A5(âˆ¥Xwtâˆ¥+ 1)2(By Assumption A5)
â‰¤2C2
A5(âˆ¥Xwtâˆ¥2+ 1)
â‰¤2C2
A5(C2
2(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1)2+ 1)
â‰¤2C2
A5(2C2
2(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1) + 1)
â‰¤C3(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1),
where C3.= 2C2
A5(2C2
2+ 1) . This completes the proof.
C.3 Proof of Lemma 4
Proof. We first decompose âŸ¨wtâˆ’Î“(wt), H(wt, Yt)âˆ’h(wt)âŸ©into three components similarly to
Srikant and Ying [2019] as
âŸ¨wtâˆ’Î“(wt), H(wt, Yt)âˆ’h(wt)âŸ©
=âŸ¨(wtâˆ’Î“(wt))âˆ’(wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t)), H(wt, Yt)âˆ’h(wt)âŸ©
| {z }
T1
+âŸ¨wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t), H(wt, Yt)âˆ’H(wtâˆ’Ï„Î±t, Yt) +h(wtâˆ’Ï„Î±t)âˆ’h(wt)âŸ©
| {z }
T2
+âŸ¨wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t), H(wtâˆ’Ï„Î±t, Yt)âˆ’h(wtâˆ’Ï„Î±t)âŸ©
| {z }
T3.
We leverage Lemma 2 and (9) to bound them one by one as follows.
Bounding T1:
T1â‰¤(wtâˆ’Î“(wt))âˆ’(wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t))
| {z }
T11Â·âˆ¥H(wt, Yt)âˆ’h(wt)âˆ¥| {z }
T12.
For the first term, we have
T11=wtâˆ’Î“(wt)âˆ’wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t)
â‰¤wtâˆ’wtâˆ’Ï„Î±t+Î“(wt)âˆ’Î“(wtâˆ’Ï„Î±t)
â‰¤2wtâˆ’wtâˆ’Ï„Î±t(Since Wâˆ—is convex)
â‰¤2C17Î±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1) (Lemma 17)
For the second term, we have
T12â‰¤CA5(âˆ¥Xwtâˆ¥+ 1) + CA5(âˆ¥Xwtâˆ¥+ 1)
â‰¤2CA5(C2(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1) + 1)
â‰¤C4,1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1),
where C4,1.= 2CA5(C2+ 1) . Therefore, we can get
T1â‰¤2C17C4,1Î±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1)2.
Choosing C4,a.= 4C17C4,1then yields the bound
T1â‰¤C4,aÎ±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1).
Bounding T2:
T2=âŸ¨wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t), H(wt, Yt)âˆ’H(wtâˆ’Ï„Î±t, Yt) +h(wtâˆ’Ï„Î±t)âˆ’h(wt)âŸ©
â‰¤wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t)
| {z }
T21Â·H(wt, Yt)âˆ’H(wtâˆ’Ï„Î±t, Yt) +h(wtâˆ’Ï„Î±t)âˆ’h(wt)âŸ©
| {z }
T22.
16For the first term, we have:
T21=(wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t))âˆ’(Î“(wt)âˆ’Î“(wt))
â‰¤wtâˆ’Ï„Î±tâˆ’Î“(wt)+Î“(wt)âˆ’Î“(wtâˆ’Ï„Î±t)
â‰¤wtâˆ’Ï„Î±tâˆ’Î“(wt)+wtâˆ’wtâˆ’Ï„Î±t
â‰¤wtâˆ’Î“(wt) +wtâˆ’Ï„Î±tâˆ’wt+wtâˆ’wtâˆ’Ï„Î±t
â‰¤ âˆ¥wtâˆ’Î“(wt)âˆ¥+ 2wtâˆ’wtâˆ’Ï„Î±t
â‰¤ âˆ¥wtâˆ’Î“(wt)âˆ¥+ 2C17Î±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1) (Lemma 17)
â‰¤C4,2(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1).(Lemma 13) (13)
For the second term, we have:
T22â‰¤H(wt, Yt)âˆ’H(wtâˆ’Ï„Î±t, Yt)+h(wt)âˆ’h(wtâˆ’Ï„Î±t)
â‰¤2CA1wtâˆ’Ï„Î±tâˆ’wt
â‰¤C4,3C17Î±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1).(Lemma 17) (14)
Combine the result in (13) and (14), we have:
T2â‰¤C4,2C4,3C17Î±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1)2.
Choosing C4,b.= 2C4,2C4,3C17then yield the bound
T2â‰¤C4,bÎ±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1).
Bounding T3:
T3=
wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t), H(wtâˆ’Ï„Î±t, Yt)âˆ’h(wtâˆ’Ï„Î±t)
.
Take expectation on both sides, we can get
E[T3] =E
âŸ¨wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t), H(wtâˆ’Ï„Î±t, Yt)âˆ’h(wtâˆ’Ï„Î±t)âŸ©
=Eh
E
âŸ¨wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t), H(wtâˆ’Ï„Î±t, Yt)âˆ’h(wtâˆ’Ï„Î±t)âŸ©wtâˆ’Ï„Î±t
Ytâˆ’Ï„Î±ti
=Eh
âŸ¨wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t),Eh
H(wtâˆ’Ï„Î±t, Yt)âˆ’h(wtâˆ’Ï„Î±t)wtâˆ’Ï„Î±t
Ytâˆ’Ï„Î±ti
âŸ©i
â‰¤Eï£®
ï£¯ï£¯ï£°wtâˆ’Ï„Î±tâˆ’Î“(wtâˆ’Ï„Î±t)
| {z }
T31Â·Eh
H(wtâˆ’Ï„Î±t, Yt)âˆ’h(wtâˆ’Ï„Î±t)|wtâˆ’Ï„Î±t
Ytâˆ’Ï„Î±ti
| {z }
T32ï£¹
ï£ºï£ºï£».
We have
T32â‰¤Î±t(Xwtâˆ’Ï„Î±t+ 1) (By (7) and (9))
â‰¤Î±t(Xwtâˆ’Ï„Î±tâˆ’Xwt+âˆ¥Xwtâˆ¥+ 1)
â‰¤Î±t(Xwtâˆ’Ï„Î±tâˆ’Xwt+C2(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1) + 1)
â‰¤Î±t(C17CxÎ±tâˆ’Ï„Î±t,tâˆ’1(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1) + C2âˆ¥wtâˆ’Î“(wt)âˆ¥+C2+ 1)
â‰¤C4,4Î±t(âˆ¥wtâˆ’Î“(wt)âˆ¥+ 1).
Thus, together with (13), we obtain
E[T3]â‰¤C4,cÎ±t(âˆ¥wtâˆ’Î“(wt)âˆ¥2+ 1),
where C4,c.=C4,2C4,4. Finally, denote C4.=C4,a+C4,b+C4,cthen completes the proof.
17C.4 Proof of Lemma 5
Proof. We recall that
âˆ¥wtâˆ’Î“(wt)âˆ¥2= 2L(wt).
Aligning Assumption A4, Lemmas 3 and 4 with (8), we get
E[L(wt+1)]
â‰¤(1âˆ’CA4Î±t)E[L(wt)] +C4Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1(2E[L(wt)] + 1) +C3
2Î±2
t+C3Î±2
tE[L(wt)]
â‰¤ 
1âˆ’2CA4Î±t+ 2C4Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1+C3Î±2
t
E[L(wt)] +C4Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1+C3
2Î±2
t.
Furthermore, we aim to derive an upper bound for E[L(wt)]that depends on the initial expected loss
E[L(w0)]and decreases over time. First, letâ€™s denote the coefficients as CtandDt:
Ct.= 1âˆ’2CA4Î±t+ 2C4Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1+C3Î±2
t,
Dt.=C4Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1+C3
2Î±2
t.
For sufficiently large t0andtâ‰¥t, we obtain 4C4Î±tâˆ’Ï„Î±t,tâˆ’1+C3Î±t< C A4. Thus, the recursive
inequality further becomes:
E[L(wt+1)]â‰¤(1âˆ’CA4Î±t)E[L(wt)] +Dt,
where Dt=O(Î±tÎ±tâˆ’Ï„Î±t,tâˆ’1).
C.5 Proof of Theorem 3
Proof. To express E[L(wt)]in terms of E[L(w0)], we recursively apply the inequality:
E[L(wt)]â‰¤tY
i=t(1âˆ’CA4Î±i)E[L(wt)] +tX
j=tï£«
ï£­tY
i=j+1(1âˆ’CA4Î±i)ï£¶
ï£¸Dj.
Denote E1.=Qt
i=t(1âˆ’CA4Î±i)E[L(wt)],E2.=Pt
j=tQt
i=j+1(1âˆ’CA4Î±i)
ln(j+t0)
(j+t0)2Î¾, and Îº=
CA4Î±. Recall we have Î±t=Î±
(t+t0)Î¾. For E1, sett0> Îº=CA4Î±, we have
tY
i=t(1âˆ’CA4Î±i)E[L(wt)] =tY
i=t
1âˆ’CA4Î±
(i+t0)Î¾
E[L(wt)]
â‰¤tY
i=t
1âˆ’Îº
i+t0
E[L(wt)]
=E[L(wt)]tY
i=ti+t0âˆ’Îº
i+t0
â‰¤E[L(wt)]t+t0
t+t0âˆ’ÎºâŒŠÎºâŒ‹
.
ForE2, we have
E2=tX
j=tï£«
ï£­tY
i=j+1i+t0âˆ’Îº
i+t0ï£¶
ï£¸ln(j+t0)
(j+t0)2Î¾
=tâˆ’âŒŠÎºâŒ‹X
j=tï£«
ï£­tY
i=j+1i+t0âˆ’Îº
i+t0ï£¶
ï£¸ln(j+t0)
(j+t0)2Î¾+tX
j=tâˆ’âŒŠÎºâŒ‹+1ï£«
ï£­tY
i=j+1i+t0âˆ’Îº
i+t0ï£¶
ï£¸ln(j+t0)
(j+t0)2Î¾
18â‰¤tâˆ’âŒŠÎºâŒ‹X
j=tj+ 1 + t0
t+t0âˆ’ÎºâŒŠÎºâŒ‹ln(j+t0)
(j+t0)2Î¾+âŒŠÎºâŒ‹ln(t+t0)
(tâˆ’ âŒŠÎºâŒ‹+ 1 + t0)2Î¾
â‰¤ln(t+t0)
(t+t0âˆ’Îº)âŒŠÎºâŒ‹CThm3,1tâˆ’âŒŠÎºâŒ‹X
j=t(j+t0)âŒŠÎºâŒ‹âˆ’2Î¾+âŒŠÎºâŒ‹ln(t+t0)
(tâˆ’Îº+ 1 + t0)2Î¾
Case 1: âŒŠÎºâŒ‹ âˆ’2Î¾ >0
E2â‰¤ln(t+t0)
(t+t0âˆ’Îº)âŒŠÎºâŒ‹CThm3,2 (tâˆ’ âŒŠÎºâŒ‹+t0)âŒŠÎºâŒ‹âˆ’2Î¾+1+âŒŠÎºâŒ‹ln(t+t0)
(tâˆ’Îº+ 1 + t0)2Î¾
â‰¤ln(t+t0)
(t+t0âˆ’Îº)2Î¾âˆ’1CThm3,3 +âŒŠÎºâŒ‹ln(t+t0)
(tâˆ’Îº+ 1 + t0)2Î¾
â‰¤CThm3,4ln(t+t0)
(t+t0)2Î¾âˆ’1
.
Case 2: âŒŠÎºâŒ‹ âˆ’2Î¾â‰¤0
E2â‰¤ln(t+t0)
(t+t0âˆ’Îº)âŒŠÎºâŒ‹CThm3,1 (tâˆ’ âŒŠÎºâŒ‹+ 1) + âŒŠÎºâŒ‹ln(t+t0)
(tâˆ’Îº+ 1 + t0)2Î¾
â‰¤ln(t+t0)
(t+t0âˆ’Îº)âŒŠÎºâŒ‹âˆ’1CThm3,5 +âŒŠÎºâŒ‹ln(t+t0)
(tâˆ’Îº+ 1 + t0)2Î¾
â‰¤CThm3,6ln(t+t0)
(t+t0)âŒŠÎºâŒ‹âˆ’1
.
Starting from the update of wt+1, we have
âˆ¥wt+1âˆ¥ â‰¤ âˆ¥ wtâˆ¥+Î±tâˆ¥H(wt, Yt+1)âˆ¥ â‰¤ âˆ¥ wtâˆ¥+Î±tCA1(âˆ¥wtâˆ¥+ 1).
That is, âˆ¥wt+1âˆ¥ â‰¤Î±0CA1+Pt
i=0(Î±0CA1+ 1)âˆ¥wiâˆ¥. Applying discrete Gronwall inequality, we
obtain âˆ¥wtâˆ¥ â‰¤(CA1+âˆ¥w0âˆ¥) expPtâˆ’1
t=0(1 +Î±0CA1)
= (CA1+âˆ¥w0âˆ¥) exp 
t+tÎ±0CA1
.
Denoting CThm3,1.= exp 
2t+ 2tÎ±0CA1
andCThm3,2.= 2 max( CThm3,4 , CThm3,6 )then completes the
proof.
D Proofs in Section 5.2
D.1 Proof of Lemma 6
Proof. Lety= (s, a, sâ€², e)âˆˆ Y andCx.= max sâˆ¥x(s)âˆ¥. We have
âˆ¥H(w, y)âˆ’H(wâ€², y)âˆ¥=e(Î³x(sâ€²)âŠ¤âˆ’x(s)âŠ¤)(wâˆ’wâ€²)â‰¤2CxCeâˆ¥wâˆ’wâ€²âˆ¥.
Furthermore,
sup
yâˆˆYâˆ¥H(0, y)âˆ¥= sup
yâˆˆYâˆ¥r(s, a)eâˆ¥ â‰¤max
s,a|r(s, a)|Ce,
which completes the proof.
D.2 Proof of Lemma 18
Lemma 18. There exist a constant C18andÏ„âˆˆ[0,1)such that âˆ€w
âˆ¥E[H(w, Y t+n)|Yt]âˆ’h(w)âˆ¥ â‰¤C18Ï„n(âˆ¥Xwâˆ¥+ 1).
Proof. Given the Markov property, we only need to prove the case of t= 1. Recall that we use
y= (s, a, sâ€², e). Define shorthand
Î´((s, a, sâ€²), w).=r(s, a) +Î³x(sâ€²)âŠ¤wâˆ’x(s)âŠ¤w,
19Î´n+1(w).=Î´((Sn, An, Sn+1), w).
By (10), we can get
H(w, Y n+1) =Î´n+1(w)en.
By expanding en, we get
E[H(w, Y n+1)|Y1]
=E[Î´n+1(w)en|Y1]
=E"
Î´n+1(w)nX
k=0(Î³Î»)nâˆ’kx(Sk)|S0#
.
Now define a two-sided Markov chainÂ¯St,Â¯At	
t=...,âˆ’2,âˆ’1,0,1,2,...such that Pr Â¯St=s
=
dÏ€(s),Pr Â¯At=a|Â¯St=s
=Ï€(a|s), i.e., the new chain always stay in the stationary distribution of
the original chain. Similarly, define
Â¯Î´n+1(w).=Î´((Â¯Sn,Â¯An,Â¯Sn+1), w).
We then have
E"
Î´n+1(w)nX
k=0(Î³Î»)nâˆ’kx(Sk)|S0#
=E"
Â¯Î´n+1(w)nX
k=âˆ’âˆ(Î³Î»)nâˆ’kx(Â¯Sk)#
| {z }
f0(n)
+E"
Î´n+1(w)nX
k=0(Î³Î»)nâˆ’kx(Sk)|S0#
âˆ’E"
Â¯Î´n+1(w)nX
k=0(Î³Î»)nâˆ’kx(Â¯Sk)#
| {z }
f1(n)
âˆ’E"
Â¯Î´n+1(w)âˆ’1X
k=âˆ’âˆ(Î³Î»)nâˆ’kx(Â¯Sk)#
| {z }
f2(n).
In the proof of Lemma 6.7 of Bertsekas and Tsitsiklis [1996], it is proved that
f0(n) =Aw+b,
which coincides with h(w). Thus the rest of the proof is dedicated to proving that f1(n)andf2(n)
decay geometrically. For f2(n), we haveÂ¯Î´n+1(w)x(Â¯Sk)â‰¤C18,1(âˆ¥Xwâˆ¥+ 1) for some C18,1
(cf. (16)). We then have
âˆ¥f2(n)âˆ¥ â‰¤C18,1(âˆ¥Xwâˆ¥+ 1)âˆ’1X
k=âˆ’âˆ(Î³Î»)nâˆ’k
=C18,1(âˆ¥Xwâˆ¥+ 1)( Î³Î»)nâˆX
k=1(Î³Î»)k.
Forf1(n), since {St}adopts geometric mixing, there exists some Ï„1âˆˆ[0,1)andC18,2>0such that
X
sPr(Sk=s)âˆ’Pr Â¯Sk=sâ‰¤C18,2Ï„k
1.
Then we have
E[Î´n+1(w)x(Sk)|S0]âˆ’EÂ¯Î´n+1(w)x(Â¯Sk)
=X
sPr(Sk=s|S0)x(Sk)E[Î´n+1(w)|Sk=s]âˆ’X
sdÏ€(s)x(Â¯Sk)EÂ¯Î´n+1(w)|Â¯Sk=s
.
20Noticing that E[Î´n+1(w)|Sk=s] =EÂ¯Î´n+1(w)|Â¯Sk=s
due to the Markov property, we obtain
E[Î´n+1(w)x(Sk)|S0]âˆ’EÂ¯Î´n+1(w)x(Â¯Sk)â‰¤C18,2Ï„k
1C18,1(âˆ¥Xwâˆ¥+ 1).
This means
âˆ¥f2(n)âˆ¥ â‰¤C18,2C18,1(âˆ¥Xwâˆ¥+ 1)nX
k=0(Î³Î»)nâˆ’kÏ„k
1.
Noticing that
nX
k=0(Î³Î»)nâˆ’kÏ„k
1â‰¤nmax{Î³Î», Ï„ 1}n
then completes the proof.
D.3 Proof of Lemma 7
Proof. We start with proving âˆ€wâˆˆker(A)âŠ¥, wâŠ¤Awâ‰¤ âˆ’C7âˆ¥wâˆ¥2. This is apparently true if
w=0. Now fix any wâˆˆker(A)âŠ¥andwÌ¸=0, which implies that AwÌ¸=0. Now we prove by
contradiction that wâŠ¤AwÌ¸= 0. Otherwise, if wâŠ¤Aw= 0, we have wâŠ¤XâŠ¤DÏ€(Î³PÎ»âˆ’I)Xw= 0.
Since DÏ€(Î³PÎ»âˆ’I)is n.d., we then get Xw=0, further implying Aw=0, which is a contradiction.
We have now proved that wâŠ¤AwÌ¸= 0. We next prove that wâŠ¤Aw < 0. This is from the fact that A
is n.d., i.e., for âˆ€zâˆˆRd, zâŠ¤Azâ‰¤0. But wâŠ¤AwÌ¸= 0. So we must have wâŠ¤Aw < 0. Finally, we
use an extreme theorem argument to complete the proof. Define Z.=
w|wâˆˆker(A)âŠ¥,âˆ¥wâˆ¥= 1	
.
Because zâˆˆZimplies zâˆˆker(A)âŠ¥andzÌ¸= 0, we have âˆ€zâˆˆZ, zâŠ¤Az < 0. Since Zis clearly
compact, the extreme value theorem confirms that the function z7â†’zâŠ¤Azobtains its minimum value
inZ, denoted as âˆ’C7<0, i.e., we have
âˆ€zâˆˆZ, zâŠ¤Azâ‰¤ âˆ’C7. (15)
For any wâˆˆker(A)âŠ¥andwÌ¸=0, we havew
âˆ¥wâˆ¥âˆˆZ, sowâŠ¤Awâ‰¤ âˆ’C7âˆ¥wâˆ¥2, which completes the
proof of the first part.
We now prove that âˆ€wâˆˆRd, wâˆ’Î“(w)âˆˆker(A)âŠ¥. We recall that Î“is the orthogonal projection to
Wâˆ—={w|Aw+b= 0}. Since Î“is the orthogonal projection to Wâˆ—, we know wâˆ’Î“(w)âˆˆWâŠ¥
âˆ—.
Fix any wâˆ—âˆˆWâˆ—and let zâˆˆker(A), we then have A(wâˆ—+z) +b=0sowâˆ—+zâˆˆWâˆ—. We then
have
âŸ¨wâˆ’Î“(w), zâŸ©=âŸ¨wâˆ’Î“(w), wâˆ—+zâŸ© âˆ’ âŸ¨wâˆ’Î“(w), wâˆ—âŸ©= 0âˆ’0 = 0 ,
confirming that wâˆ’Î“(w)âˆˆker(A)âŠ¥, which completes the proof.
D.4 Proof of Lemma 8
Proof. Lety= (s, a, sâ€², e)âˆˆ Y, sincex(s)âŠ¤wâ‰¤max sâˆˆSx(s)âŠ¤wâ‰¤ âˆ¥Xwâˆ¥, according to (10),
we have
âˆ¥H(w, y)âˆ¥=e 
r(s, a) +Î³x(sâ€²)âŠ¤wâˆ’x(s)âŠ¤w (16)
â‰¤Ce(|r(s, a)|+Î³x(sâ€²)âŠ¤w+x(s)âŠ¤w)
â‰¤Ce(CR+ (Î³+ 1)âˆ¥Xwâˆ¥)
â‰¤C8(âˆ¥Xwâˆ¥+ 1),
where C8.=Ce(CR+Î³+ 1) . Forâˆ¥h(w)âˆ¥, we have
âˆ¥h(w)âˆ¥=âˆ¥Eyâˆ¼dY[H(w, y)]âˆ¥ â‰¤Eyâˆ¼dY[âˆ¥H(w, y)âˆ¥]â‰¤C8(âˆ¥Xwâˆ¥+ 1),
which completes the proof.
21E Proofs in Section 5.3
E.1 Proof of Lemma 9
Proof. The update ton
Ë†Jto
in (Average Reward TD) is
Ë†Jt+1=Ë†Jt+Î±t
cÎ²Rt+1âˆ’cÎ²Ë†Jt
.
This matches the first row of
eA(Yt)ewt+eb(Yt) =âˆ’cÎ² 0
âˆ’Î etÎ et(x(St+1)âŠ¤âˆ’x(St)âŠ¤)Ë†Jt
Î wt
+
cÎ²Rt+1
Rt+1Î et
.
Now consider the update for wt
wt+1=wt+Î±t
Rt+1âˆ’Ë†Jt+x(St+1)âŠ¤wtâˆ’x(St)âŠ¤wt
et.
Applying the projection matrix Î on both sides yields
Î wt+1âˆ’Î wt=Î±tÎ 
Rt+1âˆ’Ë†Jt+x(St+1)âŠ¤wtâˆ’x(St)âŠ¤wt
et
=
Rt+1âˆ’Ë†Jt+x(St+1)âŠ¤wtâˆ’x(St)âŠ¤wt
Î et
=
Rt+1âˆ’Ë†Jt+x(St+1)âŠ¤Î wtâˆ’x(St)âŠ¤Î wt
Î et.
To see the last equality, we recall Lemma 1 and recall Î  =Xâ€ 
1X1. We then have
XÎ w=X1Î w+1Î¸âŠ¤Î w
=X1w+1Î¸âŠ¤Î w.
This means that
x(sâ€²)âŠ¤Î wâˆ’x(s)âŠ¤Î w=x1(sâ€²)âŠ¤wâˆ’x1(s)âŠ¤w,
where we use x1(s)to denote the s-th row of X1. We also have
x(sâ€²)âŠ¤wâˆ’x(s)âŠ¤w=(x1(sâ€²) +Î¸)âŠ¤wâˆ’(x(s) +Î¸)âŠ¤w
=x1(sâ€²)âŠ¤wâˆ’x1(s)âŠ¤w,
which confirms the last equality and then completes the proof.
E.2 Proof of Lemma 19
Lemma 19. eAÎ“(ew) +eb=0
Proof. According to the definition of Î“(ew),Î“(ew)âˆˆfWâˆ—.=
JÏ€
Î wwâˆˆWâˆ—
. We have
eA=Eyâˆ¼dYh
eA(y)i
=E(s,a,sâ€²,e)âˆ¼dYâˆ’cÎ² 0
âˆ’Î eÎ  
e(x(sâ€²)âŠ¤âˆ’x(s)âŠ¤)
=âˆ’cÎ² 0
âˆ’Î EdY[e] Î A
,
eb=Eyâˆ¼dYh
eb(y)i
=E(s,a,sâ€²,e)âˆ¼dY
cÎ²r(s, a)
r(s, a)Î e
=cÎ²JÏ€
Î EdY[e]JÏ€+ Î b
. (17)
Therefore, for the first row of eAÎ“(ew) +eb, we get cÎ²(JÏ€âˆ’JÏ€) = 0 . For the second row, we can get
âˆ’Î EdY[e]JÏ€+ Î AÎ w+ Î EdY[e]JÏ€+ Î b
=Î (AÎ w+b)
=Î (XâŠ¤
1DÏ€(PÎ»âˆ’I)X1Î w+b)
=Î (XâŠ¤
1DÏ€(PÎ»âˆ’I)X1w+b)
=Î (Aw+b)
=0,
where the second equality comes with the definition of Î . This completes the proof.
22E.3 Proof of Lemma 10
Proof. Ifz= 0, the lemma trivially holds. So now let Let z=
z1
z2
âˆˆRÃ—ker(X1)âŠ¥,zÌ¸= 0. With
(17), we have
eA=âˆ’cÎ² 0
âˆ’Î E(s,a,sâ€²,e)âˆ¼dY[e] Î A
=âˆ’cÎ² 0
âˆ’Î EdY[e] Î XâŠ¤
1DÏ€(PÎ»âˆ’I)X1
(Lemma 14) .
For simplicity, define q.=EdY[e], B.=XâŠ¤
1DÏ€(PÎ»âˆ’I)X1. We then have
zâŠ¤eAz=
z1zâŠ¤
2
âˆ’cÎ²z1
Î (âˆ’qz1+Bz2)
=âˆ’cÎ²z2
1+zâŠ¤
2Î (âˆ’qz1+Bz2).
Recall that Î  =Xâ€ 
1X1and it is symmetric, we can get
zâŠ¤
2Î (âˆ’qz1+Bz2) = (Î  z2)âŠ¤(âˆ’qz1+Bz2) =zâŠ¤
2(âˆ’qz1+Bz2),
where the last equality holds because z2âˆˆker(XâŠ¥
1). Thus,
zâŠ¤eAz=âˆ’cÎ²z2
1âˆ’zâŠ¤
2qz1+zâŠ¤
2Bz2.
We now characterize zâŠ¤
2Bz2. Apparently, zâŠ¤
2Bz2â‰¤0always holds because DÏ€(PÎ»âˆ’I)is n.s.d.
In view of (5), the equality holds only if X1z2=c1. But 1/âˆˆcol(X1)andz2âˆˆker(X1)âŠ¥. So
the equality holds only when z2= 0. Now we have proved that âˆ€z2âˆˆker(X1)âŠ¥, z2Ì¸= 0, it holds
thatzâŠ¤
2Bz2<0. Using the normalization trick and the extreme value theorem again (cf. (15)), we
confirm that there exists some constant C10,1>0such that âˆ€z2âˆˆker(X1)âŠ¥,
zâŠ¤
2Bz2â‰¤ âˆ’C10,1âˆ¥z2âˆ¥2.
Since zÌ¸= 0, we now discuss two cases.
Case 1: z1= 0, z2Ì¸= 0.In this case, we have zâŠ¤eAz=zâŠ¤
2Bz2<0.
Case 2: z1Ì¸= 0.In this case, we have
zâŠ¤eAz=âˆ’cÎ²z2
1+z1zâŠ¤
2q+zâŠ¤
2Bz2â‰¤ âˆ’cÎ²z2
1+|z1|âˆ¥z2âˆ¥âˆ¥qâˆ¥ âˆ’C10,1âˆ¥z2âˆ¥2.
By completing squares, it is easy to see that when cÎ²is sufficiently large (depending on âˆ¥qâˆ¥and
C10,1), it holds zâŠ¤eAz < 0because z1Ì¸= 0.
Combining both cases, we have proved that âˆ€zâˆˆRÃ—ker(X1)âŠ¥, zÌ¸=0, it holds that
zâŠ¤eAz < 0.
Using the normalization trick and the extreme value theorem again (cf. (15)) then completes the
proof.
E.4 Proof of Lemma 11
Proof. By definition, fWâˆ—=
JÏ€
Î wwâˆˆWâˆ—
. In view of Lemma 16, let wâˆ—be any fixed vector
inWâˆ—. Then any ewâˆ—âˆˆfWâˆ—can be written as
ewâˆ—=
JÏ€
Î (wâˆ—+w0)
with some w0âˆˆker(X1). We then have
eXewâˆ—=
JÏ€
XÎ (wâˆ—+w0)
=
JÏ€
XÎ wâˆ—
,
where the last equality holds because Î is the orthogonal projection to ker(X1)âŠ¥. This means that
eXewâˆ—is a constant regardless of ewâˆ—, which completes the proof.
23E.5 Proof of Lemma 20
Lemma 20. (Ë†Jtâˆ’JÏ€)2+d(wt,Wâˆ—)2=d(ewt,fWâˆ—)2.
Proof. We recall that Î is the orthogonal projection to ker(X1)âŠ¥. LetÎ â€²be the orthogonal projection
toker(X1). We recall from Lemma 16 that Wâˆ—={wâˆ—}+ ker( X1)withwâˆ—being any fixed point in
Wâˆ—. Thus for any wâˆ—âˆˆWâˆ—, we can write it as wâˆ—+w0with some w0âˆˆker(X1). Then for any
wâˆˆRd, we have
d(w,Wâˆ—)2= inf
wâˆ—âˆˆWâˆ—âˆ¥wâˆ’wâˆ—âˆ¥2
= inf
w0âˆˆker(X1)âˆ¥wâˆ’wâˆ—âˆ’w0âˆ¥2
= inf
w0âˆˆker(X1)âˆ¥Î w+ Î â€²wâˆ’Î wâˆ—âˆ’Î â€²wâˆ—âˆ’w0âˆ¥2
= inf
w0âˆˆker(X1)âˆ¥Î wâˆ’Î wâˆ—âˆ¥2+âˆ¥Î â€²wâˆ’Î â€²wâˆ—âˆ’w0âˆ¥2
=âˆ¥Î wâˆ’Î wâˆ—âˆ¥2,
where the last equality holds because we can select w0= Î â€²wâˆ’Î â€²wâˆ—. Define Î Wâˆ—.=
Î w|wâˆˆWâˆ—	
. Then we have
d(Î w,Î Wâˆ—) = inf
wâˆ—âˆˆWâˆ—âˆ¥Î wâˆ’Î wâˆ—âˆ¥
= inf
w0âˆˆker(X1)âˆ¥Î wâˆ’Î (wâˆ—+w0)âˆ¥
=âˆ¥Î wâˆ’Î wâˆ—âˆ¥,
where the last equality holds because w0âˆˆker(X1)andÎ is the projection to ker(X1)âŠ¥soÎ w0= 0.
We now have âˆ€w, d(w,Wâˆ—) =d(Î w,Î Wâˆ—). Then we have
d(ewt,fWâˆ—)2
=(Ë†Jtâˆ’JÏ€)2+d(ewt,Î Wâˆ—)2
=(Ë†Jtâˆ’JÏ€)2+d(Î wt,Î Wâˆ—)2
=(Ë†Jtâˆ’JÏ€)2+d(wt,Wâˆ—)2,
which completes the proof.
F Details of Experiments
We use a variant of Boyanâ€™s chain [Boyan, 1999] with 15 states ( s0, s1, . . . , s 14) and 5 actions
(a0, . . . , a 4). The chain has deterministic transitions. For s2, . . . , s 14, the action a0goes to siâˆ’1and
the actions a1toa4go to siâˆ’2;s1always transitions to s0;s0transitions uniformly randomly to any
state. The reward function is
r(s, a) =1ifs=s0
0otherwise.
24We use a uniform random policy Ï€(a|s) = 0 .5. The feature matrix XâˆˆR15Ã—5is designed to be of
rank 3.
X=ï£®
ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£¯ï£°0.07 0 .11 0 .18 0 .14 0 .61
0.13 0 .19 0 .32 0 .26 0 .45
0.11 0 .17 0 .28 0 .22 0 .39
0.24 0 .36 0 .60 0 .48 0 .84
0.18 0 .28 0 .46 0 .36 1 .00
0.20 0 .30 0 .50 0 .40 1 .06
0.31 0 .47 0 .78 0 .62 1 .45
0.29 0 .45 0 .74 0 .58 1 .39
0.42 0 .64 1 .06 0 .84 1 .84
0.40 0 .62 1 .02 0 .80 1 .78
0.47 0 .73 1 .20 0 .94 2 .39
0.53 0 .81 1 .34 1 .06 2 .23
0.58 0 .9 1 .48 1 .16 2 .78
0.60 0 .92 1 .52 1 .20 2 .84
0.67 1 .03 1 .70 1 .34 3 .45ï£¹
ï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£ºï£»
Each experiment runs for 1.5Ã—106steps, averaged over 10runs. These experiments were con-
ducted on a server equipped with an AMD EPYC 9534 64-Core Processor, with each run taking
approximately 1minute to complete. Memory requirements are negligible.
25