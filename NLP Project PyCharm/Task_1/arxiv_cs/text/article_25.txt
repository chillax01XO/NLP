arXiv:2505.21391v1  [cs.LG]  27 May 2025Finite Sample Analysis of Linear Temporal Difference
Learning with Arbitrary Features
Zixuan Xie∗
University of Virginia
xie.zixuan@email.virginia.eduXinyu Liu∗
University of Virginia
xinyuliu@virginia.edu
Rohan Chandra
University of Virginia
rohanchandra@virginia.eduShangtong Zhang
University of Virginia
shangtong@virginia.edu
Abstract
Linear TD( λ) is one of the most fundamental reinforcement learning algorithms
for policy evaluation. Previously, convergence rates are typically established under
the assumption of linearly independent features, which does not hold in many
practical scenarios. This paper instead establishes the first L2convergence rates for
linear TD( λ) operating under arbitrary features, without making any algorithmic
modification or additional assumptions. Our results apply to both the discounted
and average-reward settings. To address the potential non-uniqueness of solutions
resulting from arbitrary features, we develop a novel stochastic approximation
result featuring convergence rates to the solution set instead of a single point.
1 Introduction
Temporal difference learning (TD, Sutton [1988]) is a fundamental algorithm in reinforcement
learning (RL, Sutton and Barto [2018]), enabling efficient policy evaluation by combining dynamic
programming [Bellman, 1966] with stochastic approximation (SA, Benveniste et al. [1990], Kushner
and Yin [2003], Borkar [2009]). Its linear variant, linear TD( λ) [Sutton, 1988], emerges as a practical
extension, employing linear function approximation to tackle large or continuous state spaces where
tabular representations become impractical. Linear TD( λ) takes the dot product between features and
weights to compute the approximated value. Establishing theoretical guarantees for linear TD( λ),
particularly convergence rates, has been a major focus of research. Most existing works (Table 1),
however, require the features used in linear TD to be linearly independent. As argued in Wang and
Zhang [2024], this assumption is impractical in many scenarios. For example, in continual learning
with sequentially arriving data [Ring, 1994, Khetarpal et al., 2022, Abel et al., 2023], there is no way
to rigorously verify whether the features are independent or not. See Wang and Zhang [2024] for more
discussion on the restrictions of the feature independence assumptions. Furthermore, Peter [1992],
Tsitsiklis and Roy [1996, 1999] also outline the elimination of the linear independence assumption as
a future research direction.
While efforts have been made to eliminate the linear independence assumption [Wang and Zhang,
2024], they only provide asymptotic (almost sure) convergence guarantees in the discounted setting.
By contrast, this paper establishes the first L2convergence rates for linear TD( λ) with arbitrary
features in both discounted and average-reward settings . This success is enabled by a novel
stochastic approximation result (Theorem 3) concerning the convergence rates to a solution set instead
of a single point, driven by a novel Lyapunov function. This new result provides a unified approach
∗Equal contribution
Preprint.applicable to both discounted (Theorem 1) and average-reward (Theorem 2) settings. Notably, we
do not make any algorithmic modification and do not introduce any additional assumptions. Table 1
provides a detailed comparison of existing theoretical analyses for linear TD( λ), contextualizing our
contributions within the landscape of prior work.
Setting FeaturesNoise
TypeRate
Tsitsiklis and Roy [1996] γ <1 Independent Markovian
Bhandari et al. [2018] γ <1 Independent Markovian ✓
Lakshminarayanan and Szepesvári [2018] γ <1 Independent i.i.d. ✓
Srikant and Ying [2019] γ <1 Independent Markovian ✓
Chen et al. [2023b] γ <1 Independent i.i.d. ✓
Wang and Zhang [2024] γ <1 Arbitrary Markovian
Mitra [2025] γ <1 Independent Markovian ✓
Theorem 1 γ <1 Arbitrary Markovian ✓
Tsitsiklis and Roy [1999] γ= 1 Independent Markovian
Zhang et al. [2021c] γ= 1 Independent Markovian ✓
Chen et al. [2025] γ= 1 Independent Markovian ✓
Theorem 2 γ= 1 Arbitrary Markovian ✓
Table 1: Comparison of finite-sample analyses for linear TD( λ). “Setting” indicates the problem
setting: γ <1stands for the discounted setting and γ= 1 stands for the average reward setting.
“Features” describes assumptions on the features. “Independent” indicates linear independence is
assumed. “Arbitrary” indicates no assumption is made on features. “Noise Type” indicates the data
generation process: Markovian samples or independent and identically distributed (i.i.d.) samples.
“Rate” is checked if a convergence rate is provided.
2 Background
Notations. We use ⟨x, y⟩.=x⊤yto denote the standard inner product in Euclidean spaces and ∥·∥
to denote the ℓ2norm for vectors and the associated induced operator norm (i.e., the spectral norm)
for matrices, unless stated otherwise. A function fis said to be L-smooth (w.r.t. ∥·∥) if∀w, w′,
f(w′)≤f(w)+⟨∇f(w), w′−w⟩+L
2∥w′−w∥2. For a matrix A,col(A)denotes its column space,
ker(A)denotes its kernel, and A†denotes its Moore-Penrose inverse. When xis a point and Uis a
set, we denote d(x, U).= inf y∈U∥x−y∥as the Euclidean distance from xtoU. For sets U, V , their
Minkowski sum is U+V.={u+v|u∈U, v∈V}; andU⊥denotes the orthogonal complement of
U. We use 0and1to denote the zero vector and the all-ones vector respectively, where the dimension
is clear from context. For any square matrix A∈Rd×d(not necessarily symmetric), we say Ais
negative definite (n.d.) if there exists a ξ >0such that x⊤Ax≤ −ξ∥x∥2∀x∈Rd. For any set
E⊆Rd, we say Aisn.d. on Eif there exists a ξ >0such that x⊤Ax≤ −ξ∥x∥2∀x∈E. A is
negative semidefinite (n.s.d.) if ξ= 0in the above definition.
Markov Decision Processes. We consider an infinite horizon Markov Decision Process (MDP,
Bellman [1957]) defined by a tuple (S,A, p, r, p 0), where Sis a finite set of states, Ais a finite set
of actions, p:S × S × A → [0,1]is the transition probability function, r:S × A → Ris the
reward function, and p0:S → [0,1]denotes the initial distribution. In this paper, we focus on the
policy evaluation problem, where the goal is to estimate the value function of an arbitrary policy
π:A × S → [0,1]. At the time step 0, an initial state S0is sampled from p0. At each subsequent
time step t, the agent observes state St∈ S, executes an action At∼π(·|St), receives reward
Rt+1.=r(St, At), and transitions to the next state St+1∼p(·|St, At). We use Pπto denote the state
transition matrix induced by the policy π, i.e., Pπ[s, s′] =P
a∈Aπ(a|s)p(s′|s, a). Letdπ∈R|S|be
the stationary distribution of the Markov chain induced by the policy π. We use Dπto denote the
diagonal matrix whose diagonal is dπ.
Linear Function Approximation. In this paper, we use linear function approximation to approximate
value functions vπ:S →R(to be defined shortly). We consider a feature mapping x:S →Rdand
a weight vector w∈Rd. We then approximate vπ(s)withx(s)⊤w. We use X∈R|S|×dto denote
2the feature matrix, where the s-th row of Xisx(s)⊤. The approximated state-value function across
all states can then be represented as the vector Xw∈R|S|. The goal is thus to find a wsuch that Xw
closely approximates vπ.
Discounted Setting. In the discounted setting, we introduce a discount factor γ∈[0,1). The (dis-
counted) value function vπ:S →Rfor policy πis defined as vπ(s).=EP∞
i=0γiRt+i+1St=s
.
We define the Bellman operator T:R|S|→R|S|asTv.=rπ+γPπv, where rπ∈R|S|is the
vector of expected immediate rewards under π, with components rπ(s) =P
aπ(a|s)r(s, a). With a
λ∈[0,1], theλ-weighted Bellman operator Tλis defined as Tλv.= (1−λ)P∞
m=0λmTm+1v=
rλ+γPλv, where
rλ=P∞
k=0(λγ)kPk
πrπ= (1−γλP π)−1rπ,
Pλ=(1−λ)P∞
m=0(λγ)mPm+1
π = (1−λ)(1−γλP π)−1Pπ.
This represents a weighted average of multi-step applications of T. It is well-known that vπis the
unique fixed point of Tλ[Bertsekas and Tsitsiklis, 1996]. Linear TD( λ) is a family of TD learning
algorithms that use eligibility traces to estimate vπ(s)of the fixed policy πwith linear function
approximation. The algorithm maintains a weight vector wt∈Rdand an eligibility trace vector
et∈Rd, with the following update rules:
wt+1=wt+αt(Rt+1+γx(St+1)⊤wt−x(St)⊤wt)et,
et=γλet−1+x(St), e−1=0. (Discounted TD)
Here,{αt}is the learning rate. The eligibility trace ettracks recently visited states, assigning credit
for the prediction error to multiple preceding states. Let
A.=X⊤Dπ(γPλ−I)X, b.=X⊤Dπrλ, W∗.={w|Aw+b=0}.
IfXhas a full column rank, Tsitsiklis and Roy [1996] proves that W∗is a singleton and {wt}
converge to −A−1balmost surely. A key result used by Tsitsiklis and Roy [1996] is that the matrix
Dπ(γPλ−I)is n.d. [Sutton, 1988]. As a result, the Amatrix is also n.d. when Xhas a full column
rank. Wang and Zhang [2024] prove, without making any assumption on X, that W∗is always
nonempty and the {wt}converges to W∗almost surely. A key challenge there is that without making
assumptions on X,Ais only n.s.d.
Average-Reward Setting. In the average-reward setting, the overall performance of a policy πis
measured by the average reward Jπ.= lim T→∞1
TEhPT−1
t=0Rti
. The corresponding (differential)
value function is defined as vπ(s) = lim T→∞1
TPT−1
i=0E[(r(St+i, At+i)−Jπ)|St=s]. We define
the Bellman operator T:R|S|→R|S|asTv.=rπ−Jπ1+Pπv. Similarly, the λ-weighted
counterpart Tλis defined as Tλv.=rλ−Jπ
1−λ1+Pλv. Although vπis a fixed point of Tλ, it is not
the unique fixed point. In fact,
{vπ+c1|c∈R} (2)
are all the fixed points of Tλ[Puterman, 2014]. Linear average-reward TD( λ) is an algorithm for
estimating both Jπandvπusing linear function approximation and eligibility traces. The update
rules are
et=λet−1+x(St), e−1=0,
wt+1=wt+αt(Rt+1−ˆJt+x(St+1)⊤wt−x(St)⊤wt)et,
ˆJt+1=ˆJt+βt(Rt+1−ˆJt), (Average Reward TD)
where {αt}and{βt}are learning rates. Let
A.=X⊤Dπ(Pλ−I)X,b.=X⊤Dπ(rλ−Jπ
1−λ1),W∗.=
w|Aw+b=0	
. (4)
IfXhas a full column rank and 1/∈col(X), Tsitsiklis and Roy [1999] proves that W∗is a singleton
and{wt}converge to −A−1balmost surely. This is made possible by an important fact from the
Perron-Frobenius theorem (see, e.g., Seneta [2006]) that
w|w⊤Dπ(Pλ−I)w= 0	
={c1|c∈R}. (5)
Zhang et al. [2021c] further provides a convergence rate, still assuming Xhas a full column rank but
without assuming 1/∈col(X). When Xdoes not have a full column rank, to our knowledge, it is
even not clear whether W∗is always nonempty or not, much less the behavior of {wt}.
33 Main Results
We start with our assumptions. As promised, we do not make any assumption on X.
Assumption 3.1. The Markov chain associated with Pπis irreducible and aperiodic.
Assumption LR. The learning rates are αt=α
(t+t0)ξandβt=cβαt, where ξ∈(0.5,1],α >0,
t0>0, and cβ>0are constants.
Discounted Setting. Wang and Zhang [2024] proves the almost sure convergence of
(Discounted TD) with arbitrary features by using ∥w−w∗∥2with an arbitrary and fixed w∗∈W∗as
a Lyapunov function and analyzing the property of the ODEdw(t)
dt=Aw(t). Since Ais only n.s.d.,
Wang and Zhang [2024] conducts their analysis in the complex number field. In this work, instead
of following the ODE-based analysis originating from Tsitsiklis and Roy [1996], Borkar and Meyn
[2000], we extend Srikant and Ying [2019] to obtain convergence rates by using d(w, W ∗)2as the
Lyapunov function. To our knowledge, this is the first time that such distance function to a set is used
as the Lyapunov function to analyze RL algorithms, which is our key technical contribution from the
methodology aspect. According to Theorem 1 of Wang and Zhang [2024], W∗is nonempty, and ap-
parently convex and closed.2LetΓ(w).= arg min w∗∈W∗∥w−w∗∥be the orthogonal projection to
W∗. We then define L(w).=1
2d(w, W ∗)2=1
2∥w−Γ(w)∥2. Two important and highly non-trivial
observations are
(i)∇L(w) =w−Γ(w)(Example 3.31 of Beck [2017]),
(ii)L(w)is 1-smooth w.r.t. ∥·∥(Example 5.5 of Beck [2017]).
Both (i) and (ii) result from the fact that W∗is nonempty, closed, and convex. Using L(w)as the
Lyapunov function together with more characterization of ∇L(w)(Section 5.2), we obtain
Theorem 1. Let Assumptions 3.1and LRhold and λ∈[0,1]. Then for sufficiently large t0
andα, there exist some constants CThm1 andκ1>1such that the iterates {wt}generated
by(Discounted TD) satisfy for all t
E
d(wt, W∗)2
≤CThm1 t0
t⌊κ1⌋d(w0, w∗)2+
ln(t+t0)
(t+t0)min(2 ξ−1,⌊κ1⌋−1)
.
The proof is in Section 5.2. Notably, Lemma 3 of Wang and Zhang [2024] states that for any
w∗, w∗∗∈W∗, it holds that Xw∗=Xw∗∗. We then define
ˆvπ.=Xw∗ (6)
for any w∗∈W∗. Theorem 1 then also gives the L2convergence rate of the value estimate, i.e., the
rate at which Xwtconverges to ˆvπ. The value estimate ˆvπis the unique fixed point of a projected
Bellman equation. See Wang and Zhang [2024] for more discussion on the property of ˆvπ.
Average Reward Setting. Characterizing W∗is much more challenging. We first present a novel
decomposition of the feature matrix X. To this end, define m.= rank( X)≤min{|S|, d}. Ifm= 0,
all the results in this work are trivial and we thus discuss only the case m≥1.
Lemma 1. There exist matrices X1, X2such that X=X1+X2with the following properties (1)
rank( X1) =m−I1∈col(X)and1/∈X1(2)X2=1θ⊤withθ∈Rd.
The proof is in Section B.1 with Ibeing the indicator function. Essentially, X2is a rank one matrix
with identical rows θ(i.e., the i-th column of X2isθi1). To our knowledge, this is the first time
that such decomposition is used to analyze average-reward RL algorithms, which is our second
technical contribution from the methodology aspect. This decomposition is useful in three aspects.
First, we have A=X⊤
1Dπ(Pλ−I)X1(Lemma 14). Second, this decomposition is the key to prove
thatW∗is nonempty (Lemma 15). Third, this decomposition is the key to characterize W∗in that
W∗={w∗}+ ker( X1)withw∗being any vector in W∗(Lemma 16). To better understand this
characterization, we note that ker(X1) ={w|Xw=c1, c∈R}(Lemma 16). As a result, adding
2This theorem only discusses the case of λ= 0. The proof for a general λ∈[0,1]is exactly the same up to
change of notations.
4anyw0∈ker(X1)to a weight vector wchanges the resulting value function Xw only by c1. Two
values v1andv2can be considered “duplication” if v1−v2=c1(cf.(2)). So intuitively, ker(X1)is
the source of the “duplication”. With the help of this novel decomposition, we obtain
Theorem 2. Let Assumptions 3.1and LRhold and λ∈[0,1). Then for sufficiently large α,
t0andcβ, there exist some constants CThm2 andκ2>1such that the iterates {wt}generated
by(Average Reward TD) satisfy for all t
Eh
(ˆJt−Jπ)2+d(wt,W∗)2i
≤CThm2 t0
t⌊κ2⌋h
(ˆJ0−Jπ)2+d(w0,W∗)2i
+CThm2
ln(t+t0)
(t+t0)min(2 ξ−1,⌊κ2⌋−1)
.
The proof is in Section 5.3.
Stochastic Approximation. We now present a general stochastic approximation result to prove
Theorems 1and2. The notations in this part are independent of the rest of the paper. We consider
a general iterative update rule for a weight vector w∈Rd, driven by a time-homogeneous Markov
chain{Yt}evolving in a possibly infinite space Y:
wt+1=wt+αtH(wt, Yt+1), (SA)
where H:Rd× Y → Rddefines the incremental update.
Assumption A1. There exists a constant CA1such that supy∈Y∥H(0, y)∥<∞,
∥H(w1, y)−H(w2, y)∥ ≤CA1∥w1−w2∥ ∀w1, w2, y.
Assumption A2. {Yt}has a unique stationary distribution dY.
Leth(w).=Ey∼dY[H(w, y)]. Assumption A1 then immediately implies that
∥h(w1)−h(w2)∥ ≤CA1∥w1−w2∥ ∀w1, w2.
In many existing works about stochastic approximation [Borkar and Meyn, 2000, Chen et al., 2021b,
Borkar et al., 2021, Qian et al., 2024], it is assumed that h(w) = 0 adopts a unique solution. To work
with the challenges of linear TD with arbitrary features, we relax this assumption and consider a
setW∗. Importantly, W∗does not need to contain all solutions to h(w) = 0 . Instead, we make the
following assumptions on W∗.
Assumption A3. W∗is nonempty, closed, and convex.
Notably, W∗does not need to be bounded. Assumption A3ensures that the orthogonal projec-
tion to W∗is well defined, allowing us to define Γ(w).= arg min w∗∈W∗∥w−w∗∥, L(w).=
1
2∥w−Γ(w)∥2. As discussed before, Assumption A3ensures that ∇L(w) =w−Γ(w)andL
is 1-smooth w.r.t. ∥·∥[Beck, 2017]. We further assume that the expected update h(wt)decreases
L(wt)in the following sense, making L(w)a candidate Lyapunov function.
Assumption A4. There exists a constant CA4>0such that almost surely,
⟨∇L(wt), h(wt)⟩ ≤ − CA4L(wt).
Lastly, we make the most “unnatural” assumption of W∗.
Assumption A5. There exists a matrix Xand constants CA5andτ∈[0,1)such that (1) ∀w∗∈W∗,
∥Xw∗∥ ≤CA5; (2)∀w, y,∥H(w, y)∥ ≤CA5(∥Xw∥+ 1) ; (3) For any n≥1:
∥h(w)−E[H(w, Y t+n)|Yt]∥ ≤CA5τn(∥Xw∥+ 1) (7)
This assumption is technically motivated but trivially holds in our analyses of (Discounted TD)
and(Average Reward TD) . Specifically, Assumption A1immediately leads to at-most-linear growth
∥H(w, y)∥ ≤CA1,1(∥w∥+ 1) for some constant CA1,1. However, this bound is insufficient for
our analysis because ∥w∥ ≤ ∥ w−Γ(w)∥+∥Γ(w)∥butΓ(w)∈W∗can be unbounded. By
Assumption A5, we can have ∥Xw∥ ≤ ∥ Xw−XΓ(w)∥+∥XΓ(w)∥ ≤ ∥ X∥∥w−Γ(w)∥+CA5.
The inequality (7)is related to geometrical mixing of the chain and we additionally include Xw in
the bound for the same reason. We now present our general results regarding the convergence rate
of (SA) to W∗.
5Theorem 3. Let Assumptions A1-A5and LRhold. Denote κ.=αC A4, then there exist some
constants t0andCThm3, such that the iterates {wt}generated by (SA) satisfy for all t
E[L(wt)]≤CThm3,1 t0
t⌊κ⌋L(w0) +CThm3,2
ln(t+t0)
(t+t0)min(2 ξ−1,⌊κ⌋−1)
.
The proof is in Section 5.1.
4 Related Works
Most prior works regarding the convergence of linear TD summarized in Table 1 rely on having
linearly independent features. In fact, the reliance on feature independence goes beyond linear TD
and exists in almost all previous analyses of RL algorithms with linear function approximation, see,
e.g., Sutton et al. [2008, 2009], Maei [2011], Hackman [2013], Yu [2015, 2016], Zou et al. [2019],
Yang et al. [2019], Zhang et al. [2020b], Bo et al. [2020], Xu et al. [2020a], Zhang et al. [2020a], Xu
et al. [2020b], Wu et al. [2020], Chen et al. [2021a], Long et al. [2021], Qiu et al. [2021], Zhang et al.
[2021a,b], Xu et al. [2021], Zhang et al. [2022], Zhang and Whiteson [2022], Zhang et al. [2023],
Chen et al. [2023a], Nicolò et al. [2024], Shaan and Siva [2024], Yue et al. [2024], Swetha et al.
[2024], Liu et al. [2025a], Qian and Zhang [2025], Sreejeet and Aritra [2025], Yang et al. [2025],
Chen et al. [2025], Liu et al. [2025b]. But as argued by Peter [1992], Tsitsiklis and Roy [1996,
1999], Wang and Zhang [2024], relaxing this assumption is an important research direction. This
work can be viewed as an extension of Wang and Zhang [2024], Zhang et al. [2021c]. In terms
of(Discounted TD) , we extend Wang and Zhang [2024] by proving a finite sample analysis. Though
we rely on the characterization of W∗from Wang and Zhang [2024], the techniques we use for finite
sample analysis are entirely different from the techniques of Wang and Zhang [2024] for almost
sure asymptotic convergence. In terms of (Average Reward TD) , we extend Zhang et al. [2021c]
by allowing Xto be arbitrary. Essentially, key to Zhang et al. [2021c] is their proof that Ais n.d.
on a subspace E, assuming Xhas a full column rank. We extend Zhang et al. [2021c] in that we
give a finer and more detailed characterization of the counterparts of their Ethrough the novel
decomposition of the features (Lemma 1) and establish the n.d. property under weaker conditions
(i.e., without assuming Xhas a full column rank). Our improvements are made possible by the novel
Lyapunov function L(w)and we argue that this Lyapunov function can be used to analyze many
other linear RL algorithms with arbitrary features.
In terms of stochastic approximation, our Theorem 3is novel in that it allows convergence to
a possibly unbounded set. By contrast, most prior works about stochastic approximation study
convergence to a point [Borkar and Meyn, 2000, Borkar et al., 2021, Chen et al., 2020, 2021b, Zhang
et al., 2022, Chen et al., 2023b, Qian et al., 2024, Liu et al., 2025a]. In the case of convergence to a
set, most prior works require the set to be bounded [Kushner and Yin, 2003, Borkar, 2009, Liu et al.,
2025a]. Only a few prior works allow stochastic approximation to converge to an unbounded set, see,
e.g., Bravo and Cominetti [2022], Chen [2025], Blaser and Zhang [2025], which apply to only tabular
RL algorithms.
5 Proofs of the Main Results
5.1 Proof of Theorem 3
Proof. From the 1-smoothness of L(w)and (SA), we can get
L(wt+1)≤L(wt) +αt⟨wt−Γ(wt), h(wt)⟩
+αt⟨wt−Γ(wt), H(wt, Yt)−h(wt)⟩+1
2α2
t∥H(wt, Yt)∥2. (8)
We then bound the RHS one by one. ⟨w−Γ(w), h(w)⟩is already bounded in Assumption A4.
Lemma 2. There exists a positive constant C2, such that for any w,
∥Xw∥ ≤C2(∥w−Γ(w)∥+ 1).
The proof is in Section C.1. With Lemma 2and Assumption A5, the last term in (8)can be bounded
easily.
6Lemma 3. There exists a constant C3such that ∥H(wt, Yt)∥2≤C3(∥wt−Γ(wt)∥2+ 1) .
The proof is in Section C.2. To bound ⟨wt−Γ(wt), H(wt, Yt)−h(wt)⟩, leveraging (7), we define
τα.= min {n≥0|CA5τn≤α} (9)
as the number of steps that the Markov chain needs to mix to an accuracy α. In addition, we denote a
shorthand αt1,t2.=Pt2
i=t1αi. Then with techniques from Srikant and Ying [2019], we obtain
Lemma 4. There exists a constant C4such that
E[⟨wt−Γ(wt), H(wt, Yt)−h(wt)⟩]≤C4αt−ταt,t−1(∥wt−Γ(wt)∥2+ 1).
The proof is in Section C.3. Plugging all the bounds back to (8), we obtain
Lemma 5. There exists some Dt=O(αtαt−ταt,t−1), such that
E[L(wt+1)]≤(1−CA4αt)E[L(wt)] +Dt.
The proof is in Section C.4. Recursively applying Lemma 5 then completes the proof of Theorem 3
(See Section C.5 for details).
In the following sections, we first map the general update (SA) to(Discounted TD)
and(Average Reward TD) by defining H(w, y),h(w), and L(w)properly. Then we bound the
remaining term ⟨∇L(wt), h(wt)⟩to complete the proof.
5.2 Proof of Theorem 1
Proof. We first rewrite (Discounted TD) in the form of (SA) . To this end, we define Yt+1.=
(St, At, St+1, et), which evolves in an infinite space Y.=S × A × S × { e| ∥e∥ ≤Ce}with
Ce.=maxs∥x(s)∥
1−γλbeing the straightforward bound of supt∥et∥. We define the incremental update
H:Rd× Y → Rdas
H(w, y) = (r(s, a) +γx(s′)⊤w−x(s)⊤w)e, (10)
using shorthand y= (s, a, s′, e). We now proceed to verifying the assumptions of Theorem 3.
Assumption A1 is verified by the following lemma.
Lemma 6. There exists some finite C6such that
∥H(w1, y)−H(w2, y)∥ ≤C6∥w1−w2∥ ∀w1, w2, y.
Moreover, supy∈Y∥H(0, y)∥<∞.
The proof is in Section D.1.
For Assumption A2, Theorem 3.2 of Yu [2012] confirms that {Yt}has a unique stationary distribution
dY. Yu [2012] also computes that
h(w).=Ey∼dY[H(w, y)] =Aw+b.
Assumption A3 trivially holds by the definition of W∗.
For Assumption A4, the key observation is that AΓ(w) +b= 0always holds because Γ(w)∈W∗.
Then we have h(w) =Aw+b= (Aw+b)−(AΓ(w) +b) =A(w−Γ(w)). Thus the term
⟨∇L(w), h(w)⟩can be written as (w−Γ(w))⊤A(w−Γ(w)). We now prove that for whatever X, it
always holds that Ais n.d. on ker(A)⊥.
Lemma 7. There exists a constant C7>0such that for ∀w∈ker(A)⊥,w⊤Aw≤ −C7∥w∥2.
Furthermore, for any w∈Rd, it holds that w−Γ(w)∈ker(A)⊥.
The proof is in Section D.3. We then have
⟨wt−Γ(wt), A(wt−Γ(wt))⟩ ≤ − C7∥wt−Γ(wt)∥2,
which satisfies Assumption A4.
For Assumption A5,(6)verifies Assumption A5(1). Assumption A5(2) is verified by the following
lemma.
7Lemma 8. There exists a constant C8such that for ∀w, y,∥H(w, y)∥ ≤C8(∥Xw∥+ 1) .
The proof is in Section D.4. Assumption A5(3) is verified following a similar procedure as Lemma 6.7
in Bertsekas and Tsitsiklis [1996] (Lemma 18). Invoking Theorem 3 then completes the proof.
5.3 Proof of Theorem 2
Proof. We recall that in view of Lemma 1,ker(X1)creates “duplication” in value estimation. We,
therefore, define the projection matrix Π∈Rd×dthat projects a vector into the orthogonal comple-
ment of ker(X1), i.e., Πw.= arg min w′∈ker(X1)⊥∥w−w′∥. It can be computed that Π = X†
1X1.
We now examine the sequence {Πwt}with{wt}being the iterates of (Average Reward TD) and
consider the combined parameter vector ewt.=ˆJt
Πwt
∈R1+d. The following lemma characterizes
the evolution of ewt. Let Yt= (St, At, St+1, et)∈ S × A × S ×n
e∈Rd| ∥e∥ ≤maxs∥x(s)∥
1−λo
,
then
Lemma 9. ewt+1=ewt+αt(eA(Yt)ewt+eb(Yt)), where we have, with y= (s, a, s′, e),
eA(y) =−cβ 0
−ΠeΠe(x(s′)⊤−x(s)⊤)
,eb(y) =
cβr(s, a)
r(s, a)Πe
.
This view is inspired by Zhang et al. [2021c] and the proof is in Section E.1. We now apply Theorem 3
to{ewt}.
The verification of Assumptions A1andA2is identical to that in Section 5.2and is thus omitted.
For Assumption A3, we define fW∗.=
Jπ
Πww∈W∗
. It is apparently nonempty, closed, and
convex.
For Assumption A4, we define eA.=Ey∼dYh
eA(y)i
andeb.=Ey∼dYh
eb(y)i
and therefore realize
thehin(SA) ash(ew) =eAew+eb. Noticing that eAΓ(ew) +eb=0(Lemma 19), we then have
h(ew) =eA(ew−Γ(ew)). The term ⟨∇L(ew), h(ew)⟩can thus be written as (ew−Γ(ew))⊤eA(ew−Γ(ew)).
Next, we prove that when cβis large enough, eAis n.d. on R×ker(X1)⊥.
Lemma 10. Letcβbe sufficiently large. Then there exists a constant C10>0such that ∀z∈
R×ker(X1)⊥,zTeAz≤ −C10∥z∥2.
The proof is in Section E.3. By definition, we have ewt∈R×ker(X1)⊥andΓ(ew)∈R×ker(X1)⊥.
Soew−Γ(ew)∈R×ker(X1)⊥, yielding
⟨ewt−Γ(ewt),eA(ewt−Γ(ewt))⟩ ≤ − C10∥ewt−Γ(ewt)∥2,
which verifies Assumption A4.
For Assumption A5, we define eX=
10⊤
0X
. Assumption A5(1) is verified below.
Lemma 11. There exists a positive constant C11, such that for any ew∈fW∗,eXew=C11.
The proof is in Section E.4. With H(ew, y) =eA(y)ew+eb(y), the verification of Assumption A5(2)
and (3) is similar to Lemmas 8and18and is thus omitted. Invoking Theorem 3 then yields the
convergence rate of E[L(ewt)], i.e., the convergence rate of d(ewt,fW∗)2by the definition of L. The
next key observation is that d(ewt,fW∗)2= (ˆJt−Jπ)2+d(wt,W∗)2(Lemma 20), which completes
the proof.
86 Experiments
We now empirically examine linear TD with linearly dependent features. Following the practice of
Sutton and Barto [2018], we use constant learning rates αandβinstead of αtandβtto facilitate
experiments. We use a variant of Boyan’s chain [Boyan, 1999] with 15 states ( |S|= 15 ) and 5 actions
(|A|= 5) under a uniform policy π(a|s) = 1 /|A|, where the feature matrix X∈R15×5is designed
to be of rank 3 (more details in Section F). The weight convergence to a set is indeed observed. It is
within expectation that different λrequires different α, β.
0.00.51.01.5
Steps×106246=0.9
d(wt,W*)=0.1
=0.005
=0.01
0.00.51.01.5
Steps×106510=0.5
=0.005
=0.01
0.00.51.01.5
Steps×1061020=0.9
=0.005
=0.01
Figure 1: Convergence of (Discounted TD) withγ= 0.9, α∈ {0.005,0.01}. Curves are averaged
over 10 runs with shaded regions (too small to be visible) indicating standard errors.
0.00.51.01.5
Steps×1061.01.21.4d(wt,W*)=0.1, =0.01
=0.01
=0.02
=0.1
0.00.51.01.5
Steps×1060.250.500.75=0.5, =0.01
=0.01
=0.02
=0.1
0.00.51.01.5
Steps×106123=0.9, =0.01
=0.01
=0.02
=0.1
Figure 2: Convergence of (Average Reward TD) withβ= 0.01, α∈ {0.01,0.02,0.1}. Curves are
averaged over 10 runs with shaded regions (too small to be visible) indicating standard errors.
7 Conclusion
This paper provides the first finite sample analysis of linear TD with arbitrary features in both
discounted and average reward settings, fulfilling the long standing desiderata of Peter [1992],
Tsitsiklis and Roy [1996, 1999], enabled by a novel stochastic approximation result concerning the
convergence rate to a set. The key methodology contributions include a novel Lyapunov function
based on the distance to a set and a novel decomposition of the feature matrix for the average-reward
setting. We envision the techniques developed in this work can easily transfer to the analyses of
other linear RL algorithms. That being said, one limitation of the work is its focus on linear function
approximation. Extension to neural networks with neural tangent kernels (cf. Cai et al. [2019]) is a
possible future work. Another limitation is that this work considers only L2convergence rates but
the convergence mode of random variables are versatile. Establishing almost sure convergence rates,
Lpconvergence rates, and high probability concentration bounds (cf. Qian et al. [2024]) is also a
possible future work.
Acknowledgments and Disclosure of Funding
This work is supported in part by the US National Science Foundation under grants III-2128019 and
SLES-2331904.
9References
David Abel, André Barreto, Benjamin Van Roy, Doina Precup, Hado van Hasselt, and Satinder Singh. A
definition of continual reinforcement learning. ArXiv Preprint, 2023.
Amir Beck. First-order methods inoptimization. SIAM, 2017.
Richard Bellman. A markovian decision process. Journal ofmathematics andmechanics, 1957.
Richard Bellman. Dynamic programming. Science, 1966.
Albert Benveniste, Michel Métivier, and Pierre Priouret. Adaptive Algorithms andStochastic Approximations .
Springer, 1990.
Dimitri P Bertsekas and John N Tsitsiklis. Neuro-Dynamic Programming . Athena Scientific Belmont, MA,
1996.
Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Proceedings oftheConference onLearning Theory, 2018.
Ethan Blaser and Shangtong Zhang. Asymptotic and finite sample analysis of nonexpansive stochastic approxi-
mations with markovian noise. ArXiv Preprint, 2025.
Liu Bo, Liu Ji, Ghavamzadeh Mohammad, Mahadevan Sridhar, and Petrik Marek. Finite-sample analysis of
proximal gradient td algorithms. ArXiv Preprint, 2020.
Vivek Borkar, Shuhang Chen, Adithya Devraj, Ioannis Kontoyiannis, and Sean Meyn. The ode method for
asymptotic statistics in stochastic approximation and reinforcement learning. ArXiv Preprint, 2021.
Vivek S Borkar. Stochastic approximation: adynamical systems viewpoint. Springer, 2009.
Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation and reinforce-
ment learning. SIAM Journal onControl andOptimization, 2000.
Justin A. Boyan. Least-squares temporal difference learning. In Proceedings oftheInternational Conference on
Machine Learning, 1999.
Mario Bravo and Roberto Cominetti. Stochastic fixed-point iterations for nonexpansive maps: Convergence and
error bounds. ArXiv Preprint, 2022.
Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference and q-learning provably
converge to global optima. ArXiv Preprint, 2019.
Xuyang Chen, Jingliang Duan, Yingbin Liang, and Lin Zhao. Global convergence of two-timescale actor-critic
for solving linear quadratic regulator. Proceedings oftheAAAI Conference onArtificial Intelligence , 2023a.
Zaiwei Chen. Non-asymptotic guarantees for average-reward q-learning with adaptive stepsizes. ArXiv Preprint ,
2025.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample analysis of
contractive stochastic approximation using smooth convex envelopes. ArXiv Preprint, 2020.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample analysis of
off-policy td-learning via generalized bellman operators. ArXiv Preprint, 2021a.
Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A lyapunov theory for
finite-sample guarantees of asynchronous q-learning and td-learning variants. ArXiv Preprint, 2021b.
Zaiwei Chen, Siva Theja Maguluri, and Martin Zubeldia. Concentration of contractive stochastic approximation:
Additive and multiplicative noise. ArXiv Preprint, 2023b.
Zaiwei Chen, Sheng Zhang, Zhe Zhang, Shaan Ul Haque, and Siva Theja Maguluri. A non-asymptotic theory of
seminorm lyapunov stability: From deterministic to stochastic iterative algorithms. ArXiv Preprint, 2025.
Leah M Hackman. Faster gradient-td algorithms, 2013.
Khimya Khetarpal, Matthew Riemer, Irina Rish, and Doina Precup. Towards continual reinforcement learning:
A review and perspectives. Journal ofArtificial Intelligence Research, 2022.
Harold Kushner and G George Yin. Stochastic approximation andrecursive algorithms andapplications .
Springer Science & Business Media, 2003.
10Chandrashekar Lakshminarayanan and Csaba Szepesvári. Linear stochastic approximation: How far does
constant step-size and iterate averaging go? In Proceedings oftheInternational Conference onArtificial
Intelligence andStatistics, 2018.
Shuze Liu, Shuhang Chen, and Shangtong Zhang. The ODE method for stochastic approximation and reinforce-
ment learning with markovian noise. Journal ofMachine Learning Research, 2025a.
Xinyu Liu, Zixuan Xie, and Shangtong Zhang. Linear q-learning does not diverge in l2: Convergence rates to a
bounded set. In Proceedings oftheInternational Conference onMachine Learning, 2025b.
Yang Long, Zheng Gang, Zhang Yu, Zheng Qian, Li Pengfei, and Pan Gang. On convergence of gradient
expected sarsa( λ). In Proceedings oftheAAAI Conference onArtificial Intelligence, 2021.
Hamid Reza Maei. Gradient temporal-difference learning algorithms. PhD thesis, University of Alberta, 2011.
Aritra Mitra. A simple finite-time analysis of td learning with linear function approximation. IEEE Transactions
onAutomatic Control, 2025.
Dal Fabbro Nicolò, Adibi Arman, Mitra Aritra, and J. Pappas George. Finite-time analysis of asynchronous
multi-agent td learning. ArXiv Preprint, 2024.
Dayan Peter. The convergence of td( λ) for general λ.Machine Learning, 1992.
Martin L Puterman. Markov decision processes: discrete stochastic dynamic programming . John Wiley & Sons,
2014.
Xiaochi Qian and Shangtong Zhang. Revisiting a design choice in gradient temporal difference learning. In
Proceedings oftheInternational Conference onLearning Representations, 2025.
Xiaochi Qian, Zixuan Xie, Xinyu Liu, and Shangtong Zhang. Almost sure convergence rates and concentration
of stochastic approximation and reinforcement learning with markovian noise. ArXiv Preprint, 2024.
Shuang Qiu, Zhuoran Yang, Jieping Ye, and Zhaoran Wang. On finite-time convergence of actor-critic algorithm.
IEEE Journal onSelected Areas inInformation Theory, 2021.
Mark Bishop Ring. Continual learning inreinforcement environments . PhD thesis, The University of Texas at
Austin, 1994.
Eugene Seneta. Non-negative matrices andMarkov chains. Springer Science & Business Media, 2006.
Ul Haque Shaan and Theja Maguluri Siva. Stochastic approximation with unbounded markovian noise: A
general-purpose theorem. ArXiv Preprint, 2024.
Maity Sreejeet and Mitra Aritra. Adversarially-robust td learning with markovian data: Finite-time rates and
fundamental limits. ArXiv Preprint, 2025.
Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning.
InProceedings oftheConference onLearning Theory, 2019.
Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 1988.
Richard S Sutton and Andrew G Barto. Reinforcement Learning: AnIntroduction (2nd Edition) . MIT press,
2018.
Richard S. Sutton, Csaba Szepesvári, and Hamid Reza Maei. A convergent o(n) temporal-difference algorithm
for off-policy learning with linear function approximation. In Advances inNeural Information Processing
Systems, 2008.
Richard S. Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári,
and Eric Wiewiora. Fast gradient-descent methods for temporal-difference learning with linear function
approximation. In Proceedings oftheInternational Conference onMachine Learning, 2009.
Ganesh Swetha, Uddin Mondal Washim, and Aggarwal Vaneet. Order-optimal global convergence for average
reward reinforcement learning via actor-critic approach. ArXiv Preprint, 2024.
John N. Tsitsiklis and Benjamin Van Roy. Analysis of temporal-diffference learning with function approximation.
InIEEE Transactions onAutomatic Control, 1996.
John N. Tsitsiklis and Benjamin Van Roy. Average cost temporal-difference learning. Automatica, 1999.
11Jiuqi Wang and Shangtong Zhang. Almost sure convergence of linear temporal difference learning with arbitrary
features. ArXiv Preprint, 2024.
Yue Wu, Weitong Zhang, Pan Xu, and Quanquan Gu. A finite-time analysis of two time-scale actor-critic
methods. In Advances inNeural Information Processing Systems, 2020.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Improving sample complexity bounds for (natural) actor-critic
algorithms. In Advances inNeural Information Processing Systems, 2020a.
Tengyu Xu, Zhe Wang, and Yingbin Liang. Non-asymptotic convergence analysis of two time-scale (natural)
actor-critic algorithms. ArXiv Preprint, 2020b.
Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang. Doubly robust off-policy actor-critic: Conver-
gence and optimality. ArXiv Preprint, 2021.
Peng Yang, Jin Kaicheng, Zhang Liangyu, and Zhang Zhihua. Finite sample analysis of distributional td learning
with linear function approximation. ArXiv Preprint, 2025.
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably global convergence of actor-critic: A
case for linear quadratic regulator with ergodic cost. In Advances inNeural Information Processing Systems ,
2019.
Huizhen Yu. Least squares temporal difference methods: An analysis under general conditions. SIAM Journal
onControl andOptimization, 2012.
Huizhen Yu. On convergence of emphatic temporal-difference learning. In Proceedings oftheConference on
Learning Theory, 2015.
Huizhen Yu. Weak convergence properties of constrained emphatic temporal-difference learning with constant
and slowly diminishing stepsize. Journal ofMachine Learning Research, 2016.
Wang Yue, Zhou Yi, and Zou Shaofeng. Finite-time error bounds for greedy-gq. Machine Learning, 2024.
Shangtong Zhang and Shimon Whiteson. Truncated emphatic temporal difference methods for prediction and
control. Journal ofMachine Learning Research, 2022.
Shangtong Zhang, Bo Liu, and Shimon Whiteson. GradientDICE: Rethinking generalized offline estimation of
stationary values. In Proceedings oftheInternational Conference onMachine Learning, 2020a.
Shangtong Zhang, Bo Liu, Hengshuai Yao, and Shimon Whiteson. Provably convergent two-timescale off-
policy actor-critic with function approximation. In Proceedings oftheInternational Conference onMachine
Learning, 2020b.
Shangtong Zhang, Yi Wan, Richard S. Sutton, and Shimon Whiteson. Average-reward off-policy policy
evaluation with function approximation. In Proceedings oftheInternational Conference onMachine Learning ,
2021a.
Shangtong Zhang, Hengshuai Yao, and Shimon Whiteson. Breaking the deadly triad with a target network. In
Proceedings oftheInternational Conference onMachine Learning, 2021b.
Shangtong Zhang, Remi Tachet, and Romain Laroche. Global optimality and finite sample analysis of softmax
off-policy actor critic under state distribution mismatch. Journal ofMachine Learning Research, 2022.
Shangtong Zhang, Remi Tachet Des Combes, and Romain Laroche. On the convergence of sarsa with linear
function approximation. In Proceedings oftheInternational Conference onMachine Learning, 2023.
Sheng Zhang, Zhe Zhang, and Siva Theja Maguluri. Finite sample analysis of average-reward td learning and
q-learning. In Advances inNeural Information Processing Systems, 2021c.
Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approxima-
tion. In Advances inNeural Information Processing Systems, 2019.
12A Auxiliary Lemmas and Notations
Lemma 12 (Discrete Gronwall Inequality, Lemma 8 in Section 11.2 of Borkar [2009]) .For non-
negative real sequences {xn, n≥0}and{an, n≥0}and scalar L≥0, it holds
xn+1≤C+LPn
i=0aixi∀n=⇒xn+1≤(C+x0) exp( LPn
i=0ai)∀n.
Lemma 13 (Lemma 11 of Zhang et al. [2022]) .For sufficiently large t0, it holds that
ταt=O(log(t+t0)), α t−ταt,t−1=Olog(t+t0)
(t+t0)ξ
.
Lemma 13 ensures that there exists some t >0(depending on t0) such that for all t≥t, it holds
thatt≥ταt. Also, it ensures that for sufficiently large t0, we have αt−ταt,t−1<1. Throughout the
appendix, we always assume t0is sufficiently large and t≥t. We will refine (i.e., increase) talong
the proof when necessary.
B Proofs in Section 3
B.1 Proof of Lemma 1
Proof. Letxi∈Rddenote the i-th column of X. Without loss of generity, let the first mcolumns be
linearly independent.
Case 1: When 1∈col(X), there must exist mscalars {ci}such thatPm
i=1cixi=1. Apparently, at
least one of {ci}must be nonzero. Without loss of generity, let xm̸= 0. We then have
xm=1
cm(1−m−1X
i=1cixi).
In other words, xmcan be expressed as linear combination of {x1, . . . , x m−1}and1. Since X
has a column rank m, we are able to express {xm+1, . . . , x d}by linear combination {x1, . . . , x m}
and thus further by linear combination of {x1, . . . , x m−1}and1. Let Z1.= [x1, . . . , x m−1]be the
firstm−1columns of XandZ2.= [xm, . . . , x d]be the rest. We now know that there exists some
C∈R(m−1)×(d−m+1)(i.e., coefficients of the lienar combination) such that
Z2=Z1C+ [θm1, . . . , θ d1],
where θm, . . . θ dare scalars (i.e., “coordinates” along the 1-axis), e.g., θm=1
cm. This means that
we can express Xas
X= [Z1Z1C] + [θ11, . . . , θ d1] (11)
withθ1=···=θm−1= 0. Now define
X1.= [Z1Z1C], X2.= [θ11, . . . , θ d1].
We note that 1/∈col(Z1). Otherwise, there would exist scalars {c′
i}such thatPm−1
i=1c′
ixi=1.
Then we getPm−1
i=1(ci−c′
i)xi+cmxm= 0, which is impossible because {xi}i=1,···,mare linearly
independent. Since col(X1) = col( Z1), we then have 1/∈col(X1).
Case 2: When 1/∈col(X), we can trivially define X1=XandX2= 0. Additionally, we can still
further decompose X1as
X1= [Z1Z1C], (12)
where Z1is now the first mcolumns of X. Apparently, we still have 1/∈col(X1).
Lemma 14. Let Assumption 3.1 hold. Then A=X1Dπ(Pλ−I)X1,b=X⊤
1Dπ(rλ−Jπ
1−λ1).
Proof. Apply the decomposition shown in Lemma 1, we can get
A=(X1+X2)⊤Dπ(Pλ−I)(X1+X2)
13=X⊤
1Dπ(Pλ−I)X1+X⊤
2Dπ(Pλ−I)X1+X⊤
1Dπ(Pλ−I)X2+X⊤
2Dπ(Pλ−I)X2
=X⊤
1Dπ(Pλ−I)X1,
where the last equality holds because (Pλ−I)1= 0 and1⊤Dπ(Pλ−I) =d⊤
πPλ−d⊤
π= 0.
Similarly, for bwe can obtain
b=(X1+X2)⊤Dπ(rλ−Jπ
1−λ1)
=X⊤
1Dπ(rλ−Jπ
1−λ1) +X⊤
2Dπ(rλ−Jπ
1−λ1)
=X⊤
1Dπ(rλ−Jπ
1−λ1) +θ⊤(d⊤
π(I−λPπ)−1rπ−Jπ
1−λ)
=X⊤
1Dπ(rλ−Jπ
1−λ1) +θ⊤(1
1−λd⊤
πrπ−Jπ
1−λ)
=X⊤
1Dπ(rλ−Jπ
1−λ1).
Here, the fourth inequality holds because d⊤
π(I−λPπ) = (1 −λ)d⊤
π, which gives us d⊤
π=
(1−λ)d⊤
π(I−λPπ)−1. The last inequality holds since Jπ=d⊤
πrπ. This completes the proof.
Lemma 15. Let Assumption 3.1 hold. Then W∗is nonempty.
Proof. In view of (11) and(12), we have X1= [Z1Z1C]. Notably, Z1has a full column rank
and1/∈col(Z1). Decompose w.=
w1
w2
accordingly and recall (4)and Lemma 14, we can rewrite
Aw+b= 0as

Z⊤
1
(Z1C)⊤
Dπ(Pλ−I)[Z1Z1C]
w1
w2
=−Z⊤
1Dπ(rλ−Jπ
1−λ1)
−(Z1C)⊤Dπ(rλ−Jπ
1−λ1)
,
which thus gives us the following simultaneous equations
(
Z⊤
1Dπ(Pλ−I)Z1w1+Z⊤
1Dπ(Pλ−I)Z1Cw2=−Z⊤
1Dπ(rλ−Jπ
1−λ1)
(Z1C)⊤Dπ(Pλ−I)Z1w1+ (Z1C)⊤Dπ(Pλ−I)Z1Cw2=−(Z1C)⊤Dπ(rλ−Jπ
1−λ1).
We now prove the claim by constructing a solution. Choose any w2∈ker(Z1C)(e.g., w2= 0), the
equations then become
(
Z⊤
1Dπ(Pλ−I)Z1w1=−Z⊤
1Dπ(rλ−Jπ
1−λ1)
C⊤Z⊤
1Dπ(Pλ−I)Z1w1=−C⊤Z⊤
1Dπ(rλ−Jπ
1−λ1).
Since Z1is full rank and 1/∈Z1, Lemma 7 of Tsitsiklis and Roy [1999] shows Z⊤
1Dπ(Pλ−I)Z1is
n.d. and thus invertible. Choose w1=−(Z⊤
1Dπ(Pλ−I)Z1)−1Z⊤
1Dπ(rλ−Jπ
1−λ1)then satisfies
the equations. This completes the proof.
Lemma 16. Let Assumption 3.1 hold. Then
W∗={w∗}+ ker( X1)andker(X1) ={w|Xw=c1, c∈R}.
Proof. For any solution w∗, w∗∗∈W∗, according to the definition of W∗in(4), we have Aw∗+b=
0andAw∗∗+b=0. That is A(w∗−w∗∗) =0. By multiplying (w∗−w∗∗)⊤on both side we can
get
(w∗−w∗∗)⊤X⊤Dπ(Pλ−I)X(w∗−w∗∗) = 0 .
According to the Perron-Frobenius theorem with Assumption 3.1, v⊤Dπ(Pλ−I)v= 0if and only
ifv=c1for some c∈R. Therefore, we must have X(w∗−w∗∗) =c1for some c∈R. That is,
(X1+X2)(w∗−w∗∗) =c1. Recall the definition of X2in(11), we have X2(w∗−w∗∗) = (θ⊤(w∗−
14w∗∗))1. This means X1(w∗−w∗∗) =c′1withc′=c−θ⊤(w∗−w∗∗). Since 1/∈col(X1), we must
havec′= 0. That is, w∗−w∗∗∈ker(X1). Thus, we have established that W∗={w∗}+ ker( X1).
Furthermore, if w∈ker(X1), we have Xw= (X1+X2)w= (θ⊤w)1. IfXw=c1, we have
X1w=c1−X2w= (c−θ⊤w)1. But 1/∈col(X1). So we must have c−θ⊤w= 0, i.e.,
w∈ker(X1). This completes the proof of ker(X1) ={w|Xw=c1, c∈R}.
C Proofs in Section 5.1
Lemma 17. For sufficiently large t0, there exists a constant C17such that the following statement
holds. For any t≥tand any i∈[t−ταt, t], it holds thatwi−wt−ταt≤C17αt−ταt,i−1(∥wi−Γ(wi)∥+ 1).
Proof. In this proof, to simplify notations, we define shorthand t1.=t−ταtandCx.= max s∥x(s)∥.
Given Lemma 13, we can select a sufficiently large t0such that for any t≥t,
exp 
CA5Cxαt−ταtt−1
<3,
CA5Cxαt−ταtt−1<1
6.
We then bound ∥wi−wt1∥as
∥wi−wt1∥ ≤i−1X
k=t1∥αkH(wk, Yk+1)∥
≤i−1X
k=t1αkCA5(∥Xwk−Xwt1∥+∥Xwt1∥+ 1) (Assumption A5)
≤i−1X
k=t1αkCA5(∥Xwt1∥+ 1) +i−1X
k=t1αkCA5(∥Xwk−Xwt1∥)
≤i−1X
k=t1αkCA5(∥Xwt1∥+ 1) +i−1X
k=t1αkC17,1(∥wk−wt1∥)
≤CA5αt1,i−1(∥Xwt1∥+ 1) exp( C17,1αt1,t−1),(Lemma 12)
where C17,1.=CA5Cx. We then have
∥wi−wt1∥
≤CA5αt1,i−1(∥Xwi−Xwt1∥+∥Xwi∥+ 1) exp( C17,1αt1,t−1)
≤CA5Cxexp(C17,1αt1,t−1)αt1,i−1∥wi−wt1∥+ exp( C17,1αt1,t−1)(∥Xwi∥+ 1)CA5αt1,i−1
≤1
2∥wi−wt1∥+C17,2αt1,i−1(∥Xwi∥+ 1),
where C17,2.= 3CA5. Thus, we have
∥wi−wt1∥ ≤2C17,2αt1,i−1(∥Xwi∥+ 1)
≤2C17,2αt1,i−1(C2(∥wi−Γ(wi)∥+ 1) + 1)
≤C17αt1,i−1(∥wi−Γ(wi)∥+ 1),
where C17.= 2C17,2(C2+ 1) . This completes the proof.
C.1 Proof of Lemma 2
Proof.
∥Xw∥=∥Xw−XΓ(w) +XΓ(w)∥
≤ ∥X(w−Γ(w))∥+∥XΓ(w)∥
≤ ∥X∥∥w−Γ(w)∥+CA5 (Assumption A5)
15C.2 Proof of Lemma 3
Proof. According to the definition of H(wt, Yt)in (10),
∥H(wt, Yt)∥2
≤C2
A5(∥Xwt∥+ 1)2(By Assumption A5)
≤2C2
A5(∥Xwt∥2+ 1)
≤2C2
A5(C2
2(∥wt−Γ(wt)∥+ 1)2+ 1)
≤2C2
A5(2C2
2(∥wt−Γ(wt)∥2+ 1) + 1)
≤C3(∥wt−Γ(wt)∥2+ 1),
where C3.= 2C2
A5(2C2
2+ 1) . This completes the proof.
C.3 Proof of Lemma 4
Proof. We first decompose ⟨wt−Γ(wt), H(wt, Yt)−h(wt)⟩into three components similarly to
Srikant and Ying [2019] as
⟨wt−Γ(wt), H(wt, Yt)−h(wt)⟩
=⟨(wt−Γ(wt))−(wt−ταt−Γ(wt−ταt)), H(wt, Yt)−h(wt)⟩
| {z }
T1
+⟨wt−ταt−Γ(wt−ταt), H(wt, Yt)−H(wt−ταt, Yt) +h(wt−ταt)−h(wt)⟩
| {z }
T2
+⟨wt−ταt−Γ(wt−ταt), H(wt−ταt, Yt)−h(wt−ταt)⟩
| {z }
T3.
We leverage Lemma 2 and (9) to bound them one by one as follows.
Bounding T1:
T1≤(wt−Γ(wt))−(wt−ταt−Γ(wt−ταt))
| {z }
T11·∥H(wt, Yt)−h(wt)∥| {z }
T12.
For the first term, we have
T11=wt−Γ(wt)−wt−ταt−Γ(wt−ταt)
≤wt−wt−ταt+Γ(wt)−Γ(wt−ταt)
≤2wt−wt−ταt(Since W∗is convex)
≤2C17αt−ταt,t−1(∥wt−Γ(wt)∥+ 1) (Lemma 17)
For the second term, we have
T12≤CA5(∥Xwt∥+ 1) + CA5(∥Xwt∥+ 1)
≤2CA5(C2(∥wt−Γ(wt)∥+ 1) + 1)
≤C4,1(∥wt−Γ(wt)∥+ 1),
where C4,1.= 2CA5(C2+ 1) . Therefore, we can get
T1≤2C17C4,1αt−ταt,t−1(∥wt−Γ(wt)∥+ 1)2.
Choosing C4,a.= 4C17C4,1then yields the bound
T1≤C4,aαt−ταt,t−1(∥wt−Γ(wt)∥2+ 1).
Bounding T2:
T2=⟨wt−ταt−Γ(wt−ταt), H(wt, Yt)−H(wt−ταt, Yt) +h(wt−ταt)−h(wt)⟩
≤wt−ταt−Γ(wt−ταt)
| {z }
T21·H(wt, Yt)−H(wt−ταt, Yt) +h(wt−ταt)−h(wt)⟩
| {z }
T22.
16For the first term, we have:
T21=(wt−ταt−Γ(wt−ταt))−(Γ(wt)−Γ(wt))
≤wt−ταt−Γ(wt)+Γ(wt)−Γ(wt−ταt)
≤wt−ταt−Γ(wt)+wt−wt−ταt
≤wt−Γ(wt) +wt−ταt−wt+wt−wt−ταt
≤ ∥wt−Γ(wt)∥+ 2wt−wt−ταt
≤ ∥wt−Γ(wt)∥+ 2C17αt−ταt,t−1(∥wt−Γ(wt)∥+ 1) (Lemma 17)
≤C4,2(∥wt−Γ(wt)∥+ 1).(Lemma 13) (13)
For the second term, we have:
T22≤H(wt, Yt)−H(wt−ταt, Yt)+h(wt)−h(wt−ταt)
≤2CA1wt−ταt−wt
≤C4,3C17αt−ταt,t−1(∥wt−Γ(wt)∥+ 1).(Lemma 17) (14)
Combine the result in (13) and (14), we have:
T2≤C4,2C4,3C17αt−ταt,t−1(∥wt−Γ(wt)∥+ 1)2.
Choosing C4,b.= 2C4,2C4,3C17then yield the bound
T2≤C4,bαt−ταt,t−1(∥wt−Γ(wt)∥2+ 1).
Bounding T3:
T3=
wt−ταt−Γ(wt−ταt), H(wt−ταt, Yt)−h(wt−ταt)
.
Take expectation on both sides, we can get
E[T3] =E
⟨wt−ταt−Γ(wt−ταt), H(wt−ταt, Yt)−h(wt−ταt)⟩
=Eh
E
⟨wt−ταt−Γ(wt−ταt), H(wt−ταt, Yt)−h(wt−ταt)⟩wt−ταt
Yt−ταti
=Eh
⟨wt−ταt−Γ(wt−ταt),Eh
H(wt−ταt, Yt)−h(wt−ταt)wt−ταt
Yt−ταti
⟩i
≤E
wt−ταt−Γ(wt−ταt)
| {z }
T31·Eh
H(wt−ταt, Yt)−h(wt−ταt)|wt−ταt
Yt−ταti
| {z }
T32
.
We have
T32≤αt(Xwt−ταt+ 1) (By (7) and (9))
≤αt(Xwt−ταt−Xwt+∥Xwt∥+ 1)
≤αt(Xwt−ταt−Xwt+C2(∥wt−Γ(wt)∥+ 1) + 1)
≤αt(C17Cxαt−ταt,t−1(∥wt−Γ(wt)∥+ 1) + C2∥wt−Γ(wt)∥+C2+ 1)
≤C4,4αt(∥wt−Γ(wt)∥+ 1).
Thus, together with (13), we obtain
E[T3]≤C4,cαt(∥wt−Γ(wt)∥2+ 1),
where C4,c.=C4,2C4,4. Finally, denote C4.=C4,a+C4,b+C4,cthen completes the proof.
17C.4 Proof of Lemma 5
Proof. We recall that
∥wt−Γ(wt)∥2= 2L(wt).
Aligning Assumption A4, Lemmas 3 and 4 with (8), we get
E[L(wt+1)]
≤(1−CA4αt)E[L(wt)] +C4αtαt−ταt,t−1(2E[L(wt)] + 1) +C3
2α2
t+C3α2
tE[L(wt)]
≤ 
1−2CA4αt+ 2C4αtαt−ταt,t−1+C3α2
t
E[L(wt)] +C4αtαt−ταt,t−1+C3
2α2
t.
Furthermore, we aim to derive an upper bound for E[L(wt)]that depends on the initial expected loss
E[L(w0)]and decreases over time. First, let’s denote the coefficients as CtandDt:
Ct.= 1−2CA4αt+ 2C4αtαt−ταt,t−1+C3α2
t,
Dt.=C4αtαt−ταt,t−1+C3
2α2
t.
For sufficiently large t0andt≥t, we obtain 4C4αt−ταt,t−1+C3αt< C A4. Thus, the recursive
inequality further becomes:
E[L(wt+1)]≤(1−CA4αt)E[L(wt)] +Dt,
where Dt=O(αtαt−ταt,t−1).
C.5 Proof of Theorem 3
Proof. To express E[L(wt)]in terms of E[L(w0)], we recursively apply the inequality:
E[L(wt)]≤tY
i=t(1−CA4αi)E[L(wt)] +tX
j=t
tY
i=j+1(1−CA4αi)
Dj.
Denote E1.=Qt
i=t(1−CA4αi)E[L(wt)],E2.=Pt
j=tQt
i=j+1(1−CA4αi)
ln(j+t0)
(j+t0)2ξ, and κ=
CA4α. Recall we have αt=α
(t+t0)ξ. For E1, sett0> κ=CA4α, we have
tY
i=t(1−CA4αi)E[L(wt)] =tY
i=t
1−CA4α
(i+t0)ξ
E[L(wt)]
≤tY
i=t
1−κ
i+t0
E[L(wt)]
=E[L(wt)]tY
i=ti+t0−κ
i+t0
≤E[L(wt)]t+t0
t+t0−κ⌊κ⌋
.
ForE2, we have
E2=tX
j=t
tY
i=j+1i+t0−κ
i+t0
ln(j+t0)
(j+t0)2ξ
=t−⌊κ⌋X
j=t
tY
i=j+1i+t0−κ
i+t0
ln(j+t0)
(j+t0)2ξ+tX
j=t−⌊κ⌋+1
tY
i=j+1i+t0−κ
i+t0
ln(j+t0)
(j+t0)2ξ
18≤t−⌊κ⌋X
j=tj+ 1 + t0
t+t0−κ⌊κ⌋ln(j+t0)
(j+t0)2ξ+⌊κ⌋ln(t+t0)
(t− ⌊κ⌋+ 1 + t0)2ξ
≤ln(t+t0)
(t+t0−κ)⌊κ⌋CThm3,1t−⌊κ⌋X
j=t(j+t0)⌊κ⌋−2ξ+⌊κ⌋ln(t+t0)
(t−κ+ 1 + t0)2ξ
Case 1: ⌊κ⌋ −2ξ >0
E2≤ln(t+t0)
(t+t0−κ)⌊κ⌋CThm3,2 (t− ⌊κ⌋+t0)⌊κ⌋−2ξ+1+⌊κ⌋ln(t+t0)
(t−κ+ 1 + t0)2ξ
≤ln(t+t0)
(t+t0−κ)2ξ−1CThm3,3 +⌊κ⌋ln(t+t0)
(t−κ+ 1 + t0)2ξ
≤CThm3,4ln(t+t0)
(t+t0)2ξ−1
.
Case 2: ⌊κ⌋ −2ξ≤0
E2≤ln(t+t0)
(t+t0−κ)⌊κ⌋CThm3,1 (t− ⌊κ⌋+ 1) + ⌊κ⌋ln(t+t0)
(t−κ+ 1 + t0)2ξ
≤ln(t+t0)
(t+t0−κ)⌊κ⌋−1CThm3,5 +⌊κ⌋ln(t+t0)
(t−κ+ 1 + t0)2ξ
≤CThm3,6ln(t+t0)
(t+t0)⌊κ⌋−1
.
Starting from the update of wt+1, we have
∥wt+1∥ ≤ ∥ wt∥+αt∥H(wt, Yt+1)∥ ≤ ∥ wt∥+αtCA1(∥wt∥+ 1).
That is, ∥wt+1∥ ≤α0CA1+Pt
i=0(α0CA1+ 1)∥wi∥. Applying discrete Gronwall inequality, we
obtain ∥wt∥ ≤(CA1+∥w0∥) expPt−1
t=0(1 +α0CA1)
= (CA1+∥w0∥) exp 
t+tα0CA1
.
Denoting CThm3,1.= exp 
2t+ 2tα0CA1
andCThm3,2.= 2 max( CThm3,4 , CThm3,6 )then completes the
proof.
D Proofs in Section 5.2
D.1 Proof of Lemma 6
Proof. Lety= (s, a, s′, e)∈ Y andCx.= max s∥x(s)∥. We have
∥H(w, y)−H(w′, y)∥=e(γx(s′)⊤−x(s)⊤)(w−w′)≤2CxCe∥w−w′∥.
Furthermore,
sup
y∈Y∥H(0, y)∥= sup
y∈Y∥r(s, a)e∥ ≤max
s,a|r(s, a)|Ce,
which completes the proof.
D.2 Proof of Lemma 18
Lemma 18. There exist a constant C18andτ∈[0,1)such that ∀w
∥E[H(w, Y t+n)|Yt]−h(w)∥ ≤C18τn(∥Xw∥+ 1).
Proof. Given the Markov property, we only need to prove the case of t= 1. Recall that we use
y= (s, a, s′, e). Define shorthand
δ((s, a, s′), w).=r(s, a) +γx(s′)⊤w−x(s)⊤w,
19δn+1(w).=δ((Sn, An, Sn+1), w).
By (10), we can get
H(w, Y n+1) =δn+1(w)en.
By expanding en, we get
E[H(w, Y n+1)|Y1]
=E[δn+1(w)en|Y1]
=E"
δn+1(w)nX
k=0(γλ)n−kx(Sk)|S0#
.
Now define a two-sided Markov chain¯St,¯At	
t=...,−2,−1,0,1,2,...such that Pr ¯St=s
=
dπ(s),Pr ¯At=a|¯St=s
=π(a|s), i.e., the new chain always stay in the stationary distribution of
the original chain. Similarly, define
¯δn+1(w).=δ((¯Sn,¯An,¯Sn+1), w).
We then have
E"
δn+1(w)nX
k=0(γλ)n−kx(Sk)|S0#
=E"
¯δn+1(w)nX
k=−∞(γλ)n−kx(¯Sk)#
| {z }
f0(n)
+E"
δn+1(w)nX
k=0(γλ)n−kx(Sk)|S0#
−E"
¯δn+1(w)nX
k=0(γλ)n−kx(¯Sk)#
| {z }
f1(n)
−E"
¯δn+1(w)−1X
k=−∞(γλ)n−kx(¯Sk)#
| {z }
f2(n).
In the proof of Lemma 6.7 of Bertsekas and Tsitsiklis [1996], it is proved that
f0(n) =Aw+b,
which coincides with h(w). Thus the rest of the proof is dedicated to proving that f1(n)andf2(n)
decay geometrically. For f2(n), we have¯δn+1(w)x(¯Sk)≤C18,1(∥Xw∥+ 1) for some C18,1
(cf. (16)). We then have
∥f2(n)∥ ≤C18,1(∥Xw∥+ 1)−1X
k=−∞(γλ)n−k
=C18,1(∥Xw∥+ 1)( γλ)n∞X
k=1(γλ)k.
Forf1(n), since {St}adopts geometric mixing, there exists some τ1∈[0,1)andC18,2>0such that
X
sPr(Sk=s)−Pr ¯Sk=s≤C18,2τk
1.
Then we have
E[δn+1(w)x(Sk)|S0]−E¯δn+1(w)x(¯Sk)
=X
sPr(Sk=s|S0)x(Sk)E[δn+1(w)|Sk=s]−X
sdπ(s)x(¯Sk)E¯δn+1(w)|¯Sk=s
.
20Noticing that E[δn+1(w)|Sk=s] =E¯δn+1(w)|¯Sk=s
due to the Markov property, we obtain
E[δn+1(w)x(Sk)|S0]−E¯δn+1(w)x(¯Sk)≤C18,2τk
1C18,1(∥Xw∥+ 1).
This means
∥f2(n)∥ ≤C18,2C18,1(∥Xw∥+ 1)nX
k=0(γλ)n−kτk
1.
Noticing that
nX
k=0(γλ)n−kτk
1≤nmax{γλ, τ 1}n
then completes the proof.
D.3 Proof of Lemma 7
Proof. We start with proving ∀w∈ker(A)⊥, w⊤Aw≤ −C7∥w∥2. This is apparently true if
w=0. Now fix any w∈ker(A)⊥andw̸=0, which implies that Aw̸=0. Now we prove by
contradiction that w⊤Aw̸= 0. Otherwise, if w⊤Aw= 0, we have w⊤X⊤Dπ(γPλ−I)Xw= 0.
Since Dπ(γPλ−I)is n.d., we then get Xw=0, further implying Aw=0, which is a contradiction.
We have now proved that w⊤Aw̸= 0. We next prove that w⊤Aw < 0. This is from the fact that A
is n.d., i.e., for ∀z∈Rd, z⊤Az≤0. But w⊤Aw̸= 0. So we must have w⊤Aw < 0. Finally, we
use an extreme theorem argument to complete the proof. Define Z.=
w|w∈ker(A)⊥,∥w∥= 1	
.
Because z∈Zimplies z∈ker(A)⊥andz̸= 0, we have ∀z∈Z, z⊤Az < 0. Since Zis clearly
compact, the extreme value theorem confirms that the function z7→z⊤Azobtains its minimum value
inZ, denoted as −C7<0, i.e., we have
∀z∈Z, z⊤Az≤ −C7. (15)
For any w∈ker(A)⊥andw̸=0, we havew
∥w∥∈Z, sow⊤Aw≤ −C7∥w∥2, which completes the
proof of the first part.
We now prove that ∀w∈Rd, w−Γ(w)∈ker(A)⊥. We recall that Γis the orthogonal projection to
W∗={w|Aw+b= 0}. Since Γis the orthogonal projection to W∗, we know w−Γ(w)∈W⊥
∗.
Fix any w∗∈W∗and let z∈ker(A), we then have A(w∗+z) +b=0sow∗+z∈W∗. We then
have
⟨w−Γ(w), z⟩=⟨w−Γ(w), w∗+z⟩ − ⟨w−Γ(w), w∗⟩= 0−0 = 0 ,
confirming that w−Γ(w)∈ker(A)⊥, which completes the proof.
D.4 Proof of Lemma 8
Proof. Lety= (s, a, s′, e)∈ Y, sincex(s)⊤w≤max s∈Sx(s)⊤w≤ ∥Xw∥, according to (10),
we have
∥H(w, y)∥=e 
r(s, a) +γx(s′)⊤w−x(s)⊤w (16)
≤Ce(|r(s, a)|+γx(s′)⊤w+x(s)⊤w)
≤Ce(CR+ (γ+ 1)∥Xw∥)
≤C8(∥Xw∥+ 1),
where C8.=Ce(CR+γ+ 1) . For∥h(w)∥, we have
∥h(w)∥=∥Ey∼dY[H(w, y)]∥ ≤Ey∼dY[∥H(w, y)∥]≤C8(∥Xw∥+ 1),
which completes the proof.
21E Proofs in Section 5.3
E.1 Proof of Lemma 9
Proof. The update ton
ˆJto
in (Average Reward TD) is
ˆJt+1=ˆJt+αt
cβRt+1−cβˆJt
.
This matches the first row of
eA(Yt)ewt+eb(Yt) =−cβ 0
−ΠetΠet(x(St+1)⊤−x(St)⊤)ˆJt
Πwt
+
cβRt+1
Rt+1Πet
.
Now consider the update for wt
wt+1=wt+αt
Rt+1−ˆJt+x(St+1)⊤wt−x(St)⊤wt
et.
Applying the projection matrix Πon both sides yields
Πwt+1−Πwt=αtΠ
Rt+1−ˆJt+x(St+1)⊤wt−x(St)⊤wt
et
=
Rt+1−ˆJt+x(St+1)⊤wt−x(St)⊤wt
Πet
=
Rt+1−ˆJt+x(St+1)⊤Πwt−x(St)⊤Πwt
Πet.
To see the last equality, we recall Lemma 1 and recall Π =X†
1X1. We then have
XΠw=X1Πw+1θ⊤Πw
=X1w+1θ⊤Πw.
This means that
x(s′)⊤Πw−x(s)⊤Πw=x1(s′)⊤w−x1(s)⊤w,
where we use x1(s)to denote the s-th row of X1. We also have
x(s′)⊤w−x(s)⊤w=(x1(s′) +θ)⊤w−(x(s) +θ)⊤w
=x1(s′)⊤w−x1(s)⊤w,
which confirms the last equality and then completes the proof.
E.2 Proof of Lemma 19
Lemma 19. eAΓ(ew) +eb=0
Proof. According to the definition of Γ(ew),Γ(ew)∈fW∗.=
Jπ
Πww∈W∗
. We have
eA=Ey∼dYh
eA(y)i
=E(s,a,s′,e)∼dY−cβ 0
−ΠeΠ 
e(x(s′)⊤−x(s)⊤)
=−cβ 0
−ΠEdY[e] ΠA
,
eb=Ey∼dYh
eb(y)i
=E(s,a,s′,e)∼dY
cβr(s, a)
r(s, a)Πe
=cβJπ
ΠEdY[e]Jπ+ Πb
. (17)
Therefore, for the first row of eAΓ(ew) +eb, we get cβ(Jπ−Jπ) = 0 . For the second row, we can get
−ΠEdY[e]Jπ+ ΠAΠw+ ΠEdY[e]Jπ+ Πb
=Π(AΠw+b)
=Π(X⊤
1Dπ(Pλ−I)X1Πw+b)
=Π(X⊤
1Dπ(Pλ−I)X1w+b)
=Π(Aw+b)
=0,
where the second equality comes with the definition of Π. This completes the proof.
22E.3 Proof of Lemma 10
Proof. Ifz= 0, the lemma trivially holds. So now let Let z=
z1
z2
∈R×ker(X1)⊥,z̸= 0. With
(17), we have
eA=−cβ 0
−ΠE(s,a,s′,e)∼dY[e] ΠA
=−cβ 0
−ΠEdY[e] ΠX⊤
1Dπ(Pλ−I)X1
(Lemma 14) .
For simplicity, define q.=EdY[e], B.=X⊤
1Dπ(Pλ−I)X1. We then have
z⊤eAz=
z1z⊤
2
−cβz1
Π(−qz1+Bz2)
=−cβz2
1+z⊤
2Π(−qz1+Bz2).
Recall that Π =X†
1X1and it is symmetric, we can get
z⊤
2Π(−qz1+Bz2) = (Π z2)⊤(−qz1+Bz2) =z⊤
2(−qz1+Bz2),
where the last equality holds because z2∈ker(X⊥
1). Thus,
z⊤eAz=−cβz2
1−z⊤
2qz1+z⊤
2Bz2.
We now characterize z⊤
2Bz2. Apparently, z⊤
2Bz2≤0always holds because Dπ(Pλ−I)is n.s.d.
In view of (5), the equality holds only if X1z2=c1. But 1/∈col(X1)andz2∈ker(X1)⊥. So
the equality holds only when z2= 0. Now we have proved that ∀z2∈ker(X1)⊥, z2̸= 0, it holds
thatz⊤
2Bz2<0. Using the normalization trick and the extreme value theorem again (cf. (15)), we
confirm that there exists some constant C10,1>0such that ∀z2∈ker(X1)⊥,
z⊤
2Bz2≤ −C10,1∥z2∥2.
Since z̸= 0, we now discuss two cases.
Case 1: z1= 0, z2̸= 0.In this case, we have z⊤eAz=z⊤
2Bz2<0.
Case 2: z1̸= 0.In this case, we have
z⊤eAz=−cβz2
1+z1z⊤
2q+z⊤
2Bz2≤ −cβz2
1+|z1|∥z2∥∥q∥ −C10,1∥z2∥2.
By completing squares, it is easy to see that when cβis sufficiently large (depending on ∥q∥and
C10,1), it holds z⊤eAz < 0because z1̸= 0.
Combining both cases, we have proved that ∀z∈R×ker(X1)⊥, z̸=0, it holds that
z⊤eAz < 0.
Using the normalization trick and the extreme value theorem again (cf. (15)) then completes the
proof.
E.4 Proof of Lemma 11
Proof. By definition, fW∗=
Jπ
Πww∈W∗
. In view of Lemma 16, let w∗be any fixed vector
inW∗. Then any ew∗∈fW∗can be written as
ew∗=
Jπ
Π(w∗+w0)
with some w0∈ker(X1). We then have
eXew∗=
Jπ
XΠ(w∗+w0)
=
Jπ
XΠw∗
,
where the last equality holds because Πis the orthogonal projection to ker(X1)⊥. This means that
eXew∗is a constant regardless of ew∗, which completes the proof.
23E.5 Proof of Lemma 20
Lemma 20. (ˆJt−Jπ)2+d(wt,W∗)2=d(ewt,fW∗)2.
Proof. We recall that Πis the orthogonal projection to ker(X1)⊥. LetΠ′be the orthogonal projection
toker(X1). We recall from Lemma 16 that W∗={w∗}+ ker( X1)withw∗being any fixed point in
W∗. Thus for any w∗∈W∗, we can write it as w∗+w0with some w0∈ker(X1). Then for any
w∈Rd, we have
d(w,W∗)2= inf
w∗∈W∗∥w−w∗∥2
= inf
w0∈ker(X1)∥w−w∗−w0∥2
= inf
w0∈ker(X1)∥Πw+ Π′w−Πw∗−Π′w∗−w0∥2
= inf
w0∈ker(X1)∥Πw−Πw∗∥2+∥Π′w−Π′w∗−w0∥2
=∥Πw−Πw∗∥2,
where the last equality holds because we can select w0= Π′w−Π′w∗. Define ΠW∗.=
Πw|w∈W∗	
. Then we have
d(Πw,ΠW∗) = inf
w∗∈W∗∥Πw−Πw∗∥
= inf
w0∈ker(X1)∥Πw−Π(w∗+w0)∥
=∥Πw−Πw∗∥,
where the last equality holds because w0∈ker(X1)andΠis the projection to ker(X1)⊥soΠw0= 0.
We now have ∀w, d(w,W∗) =d(Πw,ΠW∗). Then we have
d(ewt,fW∗)2
=(ˆJt−Jπ)2+d(ewt,ΠW∗)2
=(ˆJt−Jπ)2+d(Πwt,ΠW∗)2
=(ˆJt−Jπ)2+d(wt,W∗)2,
which completes the proof.
F Details of Experiments
We use a variant of Boyan’s chain [Boyan, 1999] with 15 states ( s0, s1, . . . , s 14) and 5 actions
(a0, . . . , a 4). The chain has deterministic transitions. For s2, . . . , s 14, the action a0goes to si−1and
the actions a1toa4go to si−2;s1always transitions to s0;s0transitions uniformly randomly to any
state. The reward function is
r(s, a) =1ifs=s0
0otherwise.
24We use a uniform random policy π(a|s) = 0 .5. The feature matrix X∈R15×5is designed to be of
rank 3.
X=
0.07 0 .11 0 .18 0 .14 0 .61
0.13 0 .19 0 .32 0 .26 0 .45
0.11 0 .17 0 .28 0 .22 0 .39
0.24 0 .36 0 .60 0 .48 0 .84
0.18 0 .28 0 .46 0 .36 1 .00
0.20 0 .30 0 .50 0 .40 1 .06
0.31 0 .47 0 .78 0 .62 1 .45
0.29 0 .45 0 .74 0 .58 1 .39
0.42 0 .64 1 .06 0 .84 1 .84
0.40 0 .62 1 .02 0 .80 1 .78
0.47 0 .73 1 .20 0 .94 2 .39
0.53 0 .81 1 .34 1 .06 2 .23
0.58 0 .9 1 .48 1 .16 2 .78
0.60 0 .92 1 .52 1 .20 2 .84
0.67 1 .03 1 .70 1 .34 3 .45

Each experiment runs for 1.5×106steps, averaged over 10runs. These experiments were con-
ducted on a server equipped with an AMD EPYC 9534 64-Core Processor, with each run taking
approximately 1minute to complete. Memory requirements are negligible.
25