arXiv:2505.21362v1  [cs.CL]  27 May 2025Evaluating LLM Adaptation to Sociodemographic Factors:
User Profile vs. Dialogue History
Qishuai Zhong1Zongmin Li1Siqi Fan2Aixin Sun1
1Nanyang Technological University, Singapore
2University of Electronic Science and Technology of China, Chengdu, China
Abstract
Effective engagement by large language mod-
els (LLMs) requires adapting responses to
users’ sociodemographic characteristics, such
as age, occupation, and education level. While
many real-world applications leverage dialogue
history for contextualization, existing evalua-
tions of LLMs’ behavioral adaptation often fo-
cus on single-turn prompts. In this paper, we
propose a framework to evaluate LLM adapta-
tion when attributes are introduced either (1)
explicitly via user profiles in the prompt or (2)
implicitly through multi-turn dialogue history.
We assess the consistency of model behavior
across these modalities. Using a multi-agent
pipeline, we construct a synthetic dataset pair-
ing dialogue histories with distinct user profiles
and employ questions from the Value Survey
Module (VSM 2013) (Hofstede and Hofstede,
2016) to probe value expression. Our findings
indicate that most models adjust their expressed
values in response to demographic changes, par-
ticularly in age and education level, but consis-
tency varies. Models with stronger reasoning
capabilities demonstrate greater alignment, in-
dicating the importance of reasoning in robust
sociodemographic adaptation.
1 Introduction
Large Language Models (LLMs) and their chat-
bot applications have garnered significant attention
since the release of ChatGPT (OpenAI et al., 2024;
Dam et al., 2024). Numerous open-source frame-
works (Wolf et al., 2020; Kwon et al., 2023) now
enable organizations to deploy LLM on-premises
across various domains. For AI service providers,
hosted models are expected to align responses ap-
propriately with users’ sociodemographic charac-
teristics, as well as their cultural and ethical prefer-
ences, to enhance engagement and interaction qual-
ity (Sicilia et al., 2024; Altenburger et al., 2024).
Unlike humans, who tend to maintain consistent
values, ethics, and social norms across contexts,
I'm a 23 -year-old software engineer.
How important is it to have enough time for 
personal or home life?
Having time for your personal or home life is very 
important, even at 23 and early in your career
I'm a 23 -year-old software engineer. Can you give 
me some short -term career advice?
I have a Bachelor's degree in Computer Science. 
Should I go for a Master's?
How important is it to have enough time for 
personal or home life?
As a young software engineer...
Whether you should apply for a Master's depends 
on...
It's very important — even at 23 and early in your 
software engineering career.Explicit profile Profile  derived from dialogue
Is the LLM being consistent in replies?Figure 1: We evaluate whether the model can adjust
response values according to identical user attributes
presented in different formats, and assess the consis-
tency across these formats.
studies have shown that LLMs exhibit variabil-
ity in their expressed values, which are learned
from human-generated training data and shaped
by contextual cues (Kharchenko et al., 2024; Ko-
vaˇc et al., 2023). This variability presents a risk:
LLMs may inadvertently perpetuate harmful stereo-
types, such as labeling Generation Z as “Digital
Addicts” (Twenge, 2017). To mitigate such issues
and foster user trust, LLMs should dynamically tai-
lor their responses to reflect user expectations—a
capability we refer to as behavioral adaptation .
Sociodemographic attributes of user profiles
(e.g., age, education, occupation, nationality) are
strongly correlated with cultural norms and val-
ues related to family, authority, and social behav-
ior (Fung et al., 2016; Lomazzi and Seddig, 2020;
Gelfand and Raver, 2011). Recent work has ex-
plored value alignment between LLMs and user
profiles (Yao et al., 2024; Zhang et al., 2024; Suki-
ennik et al., 2025). However, these studies largely
focus on single-turn inputs where user profiles are
1explicitly provided in the prompt (see Figure 1, top).
This leaves a gap in understanding whether LLMs
maintain behavioral consistency when profiles are
instead inferred implicitly through dialogue (see
Figure 1, bottom).
We identify two key challenges in such dialogue-
based evaluation: (i) Can LLMs accurately infer
demographic attributes from chat history? (ii) If so,
can they adapt their responses accordingly?
Dialogue history provides essential context
for identifying user traits in real-world applica-
tions (Dam et al., 2024). Prior work has devel-
oped datasets to evaluate this capability. In our
study, we leverage the FaithfulPersonaChat bench-
mark (Jandaghi et al., 2023) to assess persona
recognition in Llama3.1-8B-Instruct as a rep-
resentative LLM (see Appendix A). Our findings
confirm that the model can reliably infer at least
one persona attribute from multi-turn dialogue, par-
tially addressing challenge (i).
Building on this, we propose a novel evalua-
tion framework to quantify how LLMs adapt their
value expression when presented with demographic
information either explicitly (via user profile) or
implicitly (via dialogue), to address challenge (ii).
However, existing benchmarks lack dialogues an-
notated with demographic attributes, which are cru-
cial for controlled comparisons. To address this,
we introduce an agent-based generation pipeline
that constructs an evaluation dataset with aligned
sociodemographic attributes across both input for-
mats. In summary, our contributions are threefold:
•We introduce an evaluation framework that
assesses LLM value adaptation across two
input formats, (i) single-turn prompts with
explicit user profiles, and (ii) multi-turn dia-
logues where profiles are embedded implicitly,
and measures consistency across both.
•We present a novel, agent-based dataset
construction method for generating profile-
aligned dialogue data.
•We evaluate multiple open-source LLMs
using the Value Survey Module (VSM
2013) (Hofstede and Hofstede, 2016) to mea-
sure value expression.
Our experiments show that most models adjust
their expressed values in response to demographic
changes, especially in age and education level.
Moreover, the degree of value adjustment is pos-
itively correlated with the magnitude of attributechange. However, consistency across input formats
varies by model. Smaller models exhibit greater
variability, while larger models with stronger rea-
soning capabilities show better alignment across
formats. Notably, reasoning-augmented models
likeQwQ-32B (Qwen et al., 2025) achieve the high-
est consistency, underscoring the critical role of
reasoning in robust sociodemographic adaptation.
2 Literature Review
This study examines LLM behavior adaptation in
multi-turn human-model interactions and assesses
value consistency across profile presented condi-
tions. Given its intersection with persona attribute
extraction and cultural value alignment in LLMs,
we survey related work in both fields.
2.1 Persona Attributes Understanding
Evaluations of language models’ understanding of
persona attributes typically center on two tasks:
next-utterance prediction and persona expansion.
Standard benchmarks such as PersonaChat (Zhang
et al., 2018), RealPersonaChat (Yamashita et al.,
2023), and FaithfulPersonaChat (Jandaghi et al.,
2023) provide dialogues annotated with descriptive
persona statements ( e.g., “I’m a pet lover”) for
these tasks. Other efforts, like Pchatbot (Qian et al.,
2021), compile large-scale Chinese dialogues from
Weibo and judicial forums but lack explicit demo-
graphic mappings. LiveChat (Gao et al., 2023)
augments live-stream conversations with streamer
personas that include demographic attributes, yet
this information serves only as auxiliary context
for next-utterance prediction.
Despite these resources, most datasets consist
of human–human dialogues embedding persona
descriptions rather than demographic profiles, hin-
dering controlled analysis of LLM adaptation to
quantifiable attributes. For example, grouping by
age is straightforward, but contrasting more ab-
stract traits—such as “running enthusiast” versus
“someone who lost a dog”—yields unreliable com-
parisons. To overcome this limitation, we construct
a synthetic dataset specifically tailored for rigorous
evaluation of sociodemographic adaptation.
2.2 Evaluating Values of Models
Several studies have evaluated LLMs on how they
express social and cultural values in response to dif-
ferent prompts. A common approach involves us-
ing research instruments like Hofstede’s Value Sur-
vey Module (VSM) (Hofstede and Hofstede, 2016),
2which has been applied in prior work (Kharchenko
et al., 2024; Arora et al., 2023; Masoud et al., 2024)
to assess whether models align their responses with
cultural contexts. Despite differences in methodol-
ogy, findings consistently show that LLMs adjust
their value expressions based on contextual cues.
Other studies have constructed evaluation
datasets based on the World Values Sur-
vey (Haerpfer et al., 2020), including GlobalOp-
inionQA (DURMUS et al., 2024) and WorldVal-
ueBench (Zhao et al., 2024). The former also in-
corporates value questions from Pew1finding that
most LLMs tend to favor Western perspectives.
The latter focuses on evaluating models’ aware-
ness of demographic contexts, revealing that even
advanced LLMs struggle to capture the nuances of
multicultural value systems.
Recent studies have investigated model values
within specific contexts. BiasLens (Li et al., 2024)
systematically examines social biases in LLMs
through role-playing scenarios, while Moore et al.
(2024) evaluates value consistency across prompt
variations, revealing generally stable model outputs.
Liu et al. (2025) introduces a benchmark dataset
to assess LLMs’ ability to infer implicit cultural
values from natural conversational contexts, empha-
sizing the challenges of nuanced attitude detection
and open-ended cultural reasoning.
Unlike prior studies, we analyze LLMs’ value
expression patterns using multi-turn human–model
interactions, which better reflect real-world chatbot
inputs. We also evaluate behavioral consistency
when sociodemographic attributes are provided ex-
plicitly versus implicitly within the dialogue.
3 Research Targets
We define behavior adaptation capability as a
model’s ability to adjust response values and tone
in accordance with users’ sociodemographic at-
tributes. A key focus of our study is to examine
whether this adaptation remains consistent when
the same demographic information is supplied ex-
plicitly in a single-turn prompt versus implicitly
via earlier dialogue (see Figure 1).
We use the Value Survey Module (VSM
2013) (Hofstede and Hofstede, 2016), grounded in
Hofstede’s Cultural Dimensions Theory (Gerlach
and Eriksson, 2021), to quantify cultural values.
This questionnaire features multiple-choice items
on workplace dynamics and decision-making, each
1https://www.pewresearch.org/
Out-of-Context Detector
User s …
1 2 3
User  Simulato r QA LLMDialogue  Generation  
Controlle rFigure 2: Dataset generation framework architecture.
Each iteration: (i) user_simulator LLM is queried to
generate a question simulating the user’s perspective
based on their profile, (ii) out-of-context detector vali-
dates the question to ensure consistency with the user’s
profile, and (iii) qa_llm responds to the question.
with five options (IDs 1–5). From the original 24
items, we select an 18-question subset Q, omit-
ting emotional and health-related items. We apply
this survey to evaluate model behavior adaptation
across below experimental scenarios.
Behavior Adaptation to User Profile ( BA_user ):
This scenario evaluates whether models can adjust
their responses based on explicit user profile con-
sisting of sociodemographic attributes presented in
the context, e.g., “Answer questions based on the
given user profile: age: 23, job title: data scientist,
gender: male, education: Bachelor’s degree.”
Behavior Adaptation to Dialogue History
(BA_dialogue ): Instead of relying on explicit at-
tributes, models are tested on their ability to in-
fer and adapt from dialogue history (Gupta et al.,
2024). This scenario mimics real-world interac-
tions, where the model must interpret user intent
and context from prior exchanges.
Consistency Across Profile and Dialogue His-
tory ( Consistency ): Beyond behavioral adapta-
tion, our framework also evaluates whether mod-
els maintain behavioral consistency when process-
ing equivalent user attributes presented in different
representational formats. Specifically, we expect
models to respond similarly to the value survey
when the same demographic attributes are provided
through explicit user profiles or implicitly inferred
from dialogue history.
4 Dialogue Dataset Generation
While BA_user assessment is straightforward, eval-
uating BA_dialogue andConsistency is hindered
by the absence of datasets meeting two key criteria:
(1) human–model dialogues with organically em-
bedded demographic attributes, and (2) explicit
3mappings between each dialogue and its corre-
sponding user profile.
Drawing inspiration from prior works (Ab-
dullin et al., 2024; Chen et al., 2024), we de-
sign a multi -agent workflow to generate synthetic,
career -advice dialogues from user profiles sourced
from a curated simulated dataset.2Each dialogue
maps to a unique profile, embedding demographic
attributes—age, education, occupation, and nation-
ality—within contextually grounded interactions.
This career -focused domain aligns with our value
assessment framework, enhancing interpretability
in downstream evaluations.
The dialogue is generated iteratively by the work-
flow under the supervision of a generation con-
troller, as illustrated in Figure 2. The controller
orchestrates three key LLM components:
User Simulator ( user_simulator )We employ
Gpt-4o-2024-08-06 (OpenAI et al., 2024) to
emulate a user seeking career advice via ques-
tion–answer (QA) interactions with an LLM. Each
query generated by the simulator is guided by: (1)
user demographic attributes for personalization, (2)
instructions and preceding dialogue, guiding the
generation of contextually relevant queries, and (3)
predefined conversation objectives and termination
criteria. The simulator ends the dialogue once the
specified termination condition is satisfied.
Out-of-Context Detector ( ooc_detector ): We
employ Gpt-4o-mini-2024-07-18 (OpenAI et al.,
2024) to validate the questions generated by the
user simulator. It ensures that each question aligns
with the user’s profile and maintains consistent first-
person framing. If inconsistencies are detected, the
ooc_detector directly revises the question.
Question Answering LLM ( qa_llm ): The LLM
responds to the simulated user’s queries with its
default configuration to ensure natural interactions.
To replicate real-world chatbot behavior, past dia-
logue history is always prepended to the latest user
question, following standard practices for context
injection in human-LLM communication.
Each generation loop terminates when the user
simulator decides to conclude the dialogue or the
maximum iteration limit ( max_runs ) is reached.
More details on the prompt design for each com-
ponent are provided in Figure 6 (Appendix B), and
the complete conversation generation procedure
is outlined in Algorithm 1 (Appendix C). In total,
2https://www.kaggle.com/datasets/
ravindrasinghrana/employeedataset/dataDimensions LLM Judge Human
Attribute Coverage 4.14 3.64
Attribute Correctness 4.76 4.97
Question Diversity 4.52 4.65
Relevance 4.63 4.26
Table 1: Overall ratings for generated dialogues by
the LLM judge and 50 human-rated samples. Despite
stricter human judgments, high scores across all four
dimensions confirm dialogue quality.
1000 dialogue sets, denoted by D, are generated,3
each mapped to a unique user from the seed dataset.
The full user set is denoted by U.
4.1 Dataset Evaluation
To assess the quality of the generated dataset, we
conduct both human and LLM-based evaluations.
LLM evaluation follows the widely adopted “LLM-
as-a-judge” methodology (Zheng et al., 2023; Gu
et al., 2025). Only questions generated by the
user_simulator are assessed, as the qa_llm role-
plays as itself to respond to these simulated queries.
Assessments are conducted across four dimensions:
Attribute Coverage. The number of demographic
attributes explicitly mentioned (up to 5).
Attribute Correctness. The number of correctly
referenced demographic values. For example, if
the user’s age, gender, and job title are mentioned,
but an incorrect age is used, the score for Attribute
Coverage is 3 and for Attribute Correctness is 2.
Question Diversity. The variety of topics covered,
reflects the simulator’s ability to generate distinct,
contextually rich questions. For example, if four
questions are generated but all focus solely on short-
term career advice, then the score is 1.
Relevance. The extent to which the questions re-
main contextually appropriate for career advice,
and align with the qa_llm ’s prior responses to
maintain coherent conversational flow.
For human evaluation, three postgraduate an-
notators score the generated questions on a 0 to
5 scale, with higher values indicating better qual-
ity. All annotators independently assess the same
subset of 50 randomly selected samples. For au-
tomated evaluation, we use gpt-4o-2024-08-06
as the judge to score all samples. Identical scoring
guidelines are supplied to both human and LLM
raters (Appendix D), in accordance with best prac-
tices from Leng (2023). To reduce variability, the
3https://github.com/FerdinandZhong/model_
behavior_adaption
4LLM judge evaluates each sample with 10 different
random seeds, and we report the average score.
We assess alignment between average human
and LLM judge ratings on the shared subset using
the Pearson correlation coefficient (Freedman et al.,
2007) and the two-way mixed-effects intraclass cor-
relation coefficient (ICC(3, k)) (Shrout and Fleiss,
1979). We omit Fleiss’ Kappa due to its sensitivity
to category prevalence in our skewed data (Hoehler,
2000). Results (Appendix E) demonstrate strong
concordance between human and automated evalu-
ations. Summary statistics for all four evaluation
dimensions are presented in Table 1, confirming
the high quality of the generated dataset.
5 Behavior Adaptation Evaluation
Using our synthetic dataset, we assess LLM behav-
ior adaptation via two scenarios.
In the BA_user scenario, each time a model
is queried with a VSM question qj∈Qand
an explicit user profile u∈U, where qjde-
notes the jth question. The model must return
aselected_option_id (1–5) corresponding to
the question’s option_ids —the IDs of available
choices—along with a justification and the log
probability distribution over option_ids . We de-
note this composite response by rj
u.
In the BA_dialogue scenario, each time a model
is presented with a synthetic dialogue d∈Dfol-
lowed by a VSM question qj, it generates a re-
sponse rj
dunder the same requirements. The com-
plete sets of responses {rj
u}and{rj
d}for each
model are denoted by RUandRD, respectively.
The querying workflow is illustrated in Figure 3.
5.1 Distance Definition
The core component of each response ris a nor-
malized probability distribution Pover all possi-
bleoption_ids . This distribution is computed
from the model’s log probability outputs (see Ap-
pendix G for details). For instance, if a response
rhas a selected option_id of 2, the distribu-
tion might be [0.1,0.7,0.05,0.0,0.15], where each
value represents the relative likelihood of the cor-
responding option_id . After normalization, the
total probability sums up to 1.
We quantify the unit divergence between two
responses r’s by calculating the Jensen-Shannon di-
vergence (JSD) between their corresponding prob-
ability distribution P’s. That is, JSD(Pj
u∥Pj
u′)
donates the divergence between rj
uandrj
u′whereuandu′refer to two distinct user profiles.
To assess BA_user andBA_dialogue , both U
andDare partitioned into groups gbased on at-
tributes including age, education, occupation, and
nationality. For each group and question index j,
we compute the Jensen–Shannon centroid cj—the
distribution minimizing the total JSD to all group
responses Pj
i(Nielsen, 2020). Starting from the
mean distribution ¯Pj, the centroid is obtained by
cj= arg min
cnX
i=1JSD 
c∥Pj
i
,
where Pj
iis the ith response distribution for ques-
tionjin a group of size n.
The overall divergence between two groups g
andg′is then defined as:
Distance (g, g′) =1
|J||J|X
j=1JSD(cj
g∥cj
g′),
This centroid-based approach helps mitigate the
effects of outliers and uneven group sizes.
We establish baseline values for both BA_user
andBA_dialogue using a consistent methodology.
To illustrate, consider the baseline for BA_user
evaluation. After grouping RUby sociodemo-
graphic attribute and computing a centroid for
each group, we define the global centroid as the
Jensen–Shannon centroid of all responses in RU.
The baseline is then calculated as the average dis-
tance between each group centroid and the global
centroid. Comparing this baseline to the inter-
group divergences allows us to quantify the model’s
sensitivity to demographic variation. For exam-
ple, when grouping RUby age, the divergence
between the “<30” and “>60” cohorts should no-
ticeably exceed the baseline, while the divergence
between “<30” and “30–40” should remain below
it—illustrating that greater age gaps drive greater
variation in generated responses.
To evaluate the Consistency scenario, we com-
pare responses RuandRdcorresponding to the
same user, by using their selected option_id
sequences rather than the full probability dis-
tributions. Let Su= [s1, s2, . . . , s m], and
Sd= [t1, t2, . . . , t m], where sjis the selected
option_id in 1 to 5 for the jth question (the same
applies to tj), and mis the total number of ques-
tions, and m= 18 in our setting.
Since the option_ids are ordinal and questions
are independent, we employ the Earth Mover’s Dis-
5𝑢
𝑑 𝑅𝑑𝑅𝑢𝑃𝑢1=[0.7,0.2,0.04,0.01,0.05]Figure 3: Model querying workflow with key components. Here, udenotes a user profile, and dis a synthetic
dialogue. Each line r∈Rrepresents the response to a VSM question, which includes a normalized probability
distribution Pover the 5 option_ids .
tance (EMD) (Rubner et al., 1998) to quantify align-
ment between SuandSd. Let hu(k)andhd(k)
denote the frequencies of value kinSuandSd,
respectively, and let K= 5be the total number of
discrete bins. Then
EMD( Su, Sd) =K−1X
x=1xX
k=1hu(k)
m−xX
k=1hd(k)
m.
We define a model’s Consistency as the mean
EMD across all matched user profile uand his/her
dialogue dpairs or (u, d)’s, with a baseline com-
puted over an equivalent number of random (u, d)
pairings i.e.,randomly matching a user profile with
another user’s dialogue. This metric enables direct
comparison of consistency when the same profile
is presented in different formats.
Table 2 provides the details of the measured di-
vergences and baseline values for all scenarios.
5.2 Experiment Setting
Following the workflow outlined in Figure 3, we
evaluate multiple open-source LLMs, including the
Qwen2.5 family, Llama3.1 family, DeepSeek-V3 ,
and the reasoning model QwQ-32B (Qwen et al.,
2025; Dubey and et al., 2024; DeepSeek-AI, 2024),
to generate RUandRD. Model outputs are struc-
tured in JSON format using XGrammar (Dong
et al., 2024) for streamlined processing. Each
model is queried once per prompt, except for the
reasoning model QwQ-32B , which is queried twice.
The first query is unconstrained to allow for rea-
soning content generation, which is then appended
to the second query to elicit a structured response.
Full prompt designs are listed in Appendix F.
6 Evaluation Outcomes
Recall that the selected ID is the one assigned the
highest probability by the model. The probability
BA_user BA_dialogue
0.0 0.2 0.4 0.6 0.8 1.0Llama3.1-8B-Instruct
Llama3.1-70B-Instruct
DeepSeek-V3
Qwen2.5-7B-Instruct
Qwen2.5-72B-Instruct
QwQ-32B0.71
0.58
0.75
0.72
0.96
0.98
0.83
0.82
0.91
0.91
0.94
0.91
Averaged Probability of "selected_option_id"Figure 4: Mean probability of the
“selected_option_id ” inBA_user andBA_dialogue ,
reflecting model confidence. Most models show similar
decisiveness across both scenarios.
value associated with this ID reflects the model’s
confidence in its selection. We treat this probability
as the confidence score for the selected ID and use
the average of these scores across all responses to
estimate the model’s overall confidence. As illus-
trated in Figure 4, all models—except Llama3.1-
8B-Instruct , which shows slightly reduced confi-
dence in the dialogue setting—exhibit consistently
high confidence across both contexts, supporting
the interpretation that their selections reflect gen-
uine preferences and reinforcing the validity of our
subsequent analyses.
6.1 Behavior Adaptation
Next, we assess behavior adaptation by varying
demographic attributes. Responses in RUandRD
are grouped by age, education, occupation, and na-
tionality, and we compute inter-group divergences
and corresponding baselines as specified in Table 2.
Adaptation is quantified as the ratio of each diver-
gence to its baseline (baseline=1). Detailed anal-
yses for age and education appear below; further
results are available in Appendix H.
Age: We group responses RUandRDinto 10-
6Scenarios Measured Distance Baseline
BA_user1
|J|P|J|
j=1JSD
cj
g∥cj
g′
, g, g′⊆U1
|J|1
|G|P|J|
j=1P|G|
g=1JSD 
cj
g∥cj
U
BA_dialogue1
|J|P|J|
j=1JSD
cj
g∥cj
g′
, g, g′⊆D1
|J|1
|G|P|J|
j=1P|G|
g=1JSD 
cj
g∥cj
D
Consistency1
|U|P|U|
i=1EMD (Su, Sd)1
|U|P|U|
i=1I(ui̸→di)EMD (Su, Sd)
Table 2: List of quantified divergences and baselines across evaluation scenarios. For BA_user andBA_dialogue ,
responses are partitioned into attribute-based groups g(age,education level ,occupation , and country ) to assess
their influence on model behavior. Scenario Consistency measures the alignment between user profiles uand their
corresponding dialogue histories d.Jrepresents the set of questions. ui̸→dimeans the user profile uiand
dialogue diare not from a same user.
Models Measured Distance ↓Baseline Distance /Baseline ↓
Llama3.1-8B-Instruct 0.305 0.312 0.978
Llama3.1-70B-Instruct 0.214 0.225 0.951
Qwen2.5-7B-Instruct 0.276 0.276 1.000
Qwen2.5-72B-Instruct 0.176 0.190 0.926
DeepSeek-V3 0.118 0.128 0.922
QwQ-32B 0.112 0.125 0.896
Table 3: Results of evaluating LLMs’ capability to maintain response consistency across different context formats.
Both absolute distances and relative divergence ratios over the baseline are displayed in the table. QwQ-32B has
both the lowest absolute distance values and the lowest relative value of distance over baseline.
year brackets ( e.g., “30–40”), with “ <30”
and “ >60” as boundary categories. Figure 5
(first row) presents results for both BA_user and
BA_dialogue . In both scenarios, models exhibit
a positive correlation between age disparity and
behavioral divergence ( e.g., maximum divergence
between “ <30” and “ >60”). Notably, models ex-
hibit greater consistency in the BA_dialogue sce-
nario than in BA_user , indicating that age infor-
mation conveyed through dialogue enables more
stable adaptation across age groups.
Education Level: Educational attainment provides
a precise grouping criterion. Figure 5 (second
row) reports the divergences and baselines for both
BA_user andBA_dialogue . As with age, models
exhibit greater divergence in responses’ values as
educational disparity increases (e.g., highest diver-
gence between high school and doctoral degrees),
whether the attribute is provided explicitly or via
dialogue. These results further demonstrate mod-
els’ capacity to incorporate educational background
into value and nuance adaptation.
Additional evaluations using other attribute-
based groupings are presented in Appendix H.
These findings show that most LLMs accurately
infer sociodemographic attributes and adjust their
responses accordingly, with larger attribute differ-
ences producing greater shifts in expressed values.
Crucially, despite dialogue history being less ex-plicit than user profiles, models still adapt their
behavior to align with user characteristics.
6.2 Consistency across Context Formats
We next evaluate each model’s behavioral
consistency across context formats (scenario
Consistency ). Following Section 5.1, we com-
pute, for each model, both the distance and its base-
line. While absolute distances quantify response
variability across formats, they may conflate be-
havioral alignment with format-induced noise (He
et al., 2024). To disentangle these effects, we also
report the ratio of measured distance to baseline:
lower ratios signify stronger consistency in adapt-
ing to demographic cues across differing formats.
Table 3 summarizes these metrics. Smaller
tested models (with parameters smaller than 10B)
exhibit both higher absolute EMDs and ratios near
or equal to 1, indicating that prompt-format vari-
ability outweighs demographic alignment. In con-
trast, larger models achieve lower ratios, reflect-
ing greater robustness to format changes and im-
proved consistency in behavior adaptation. Further-
more, we observe a positive correlation between
the measured consistency and benchmark perfor-
mance on language understanding and reasoning
tasks (e.g., MMLU Pro (Wang et al., 2024), Big-
Bench Hard (Suzgun et al., 2022)): models with su-
perior cognitive capabilities tend to maintain higher
7Llama3.1-8B-Instruct Llama3.1-70B-Instruct DeepSeek-V3 Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct QwQ-32B
30-40 vs 40-5030-40 vs 50-6030-40 vs >60 40-50 vs 50-60
40-50 vs >60
50-60 vs >60
<30 vs 30-40
<30 vs 40-50 <30 vs 50-60<30 vs >600.01.12.23.34.55.66.7(a)BA_user grouped by “age”
30-40 vs 40-5030-40 vs 50-6030-40 vs >60 40-50 vs 50-60
40-50 vs >60
50-60 vs >60
<30 vs 30-40
<30 vs 40-50 <30 vs 50-60<30 vs >600.01.12.33.44.55.76.8 (b)BA_dialogue grouped by “age”
Bachelor's_Degree vs High_SchoolBachelor's_Degree vs Master's_Degree Bachelor's_Degree vs PhD
High_School vs Master's_Degree
High_School vs PhD Master's_Degree vs PhD0.01.02.03.04.15.16.1
(c)BA_user grouped by “education evel”
Bachelor's_Degree vs High_SchoolBachelor's_Degree vs Master's_Degree Bachelor's_Degree vs PhD
High_School vs Master's_Degree
High_School vs PhD Master's_Degree vs PhD0.01.12.33.44.55.76.8 (d)BA_dialogue grouped by “education evel”
Figure 5: The measurement results for BA_user andBA_dialogue are shown below. The first row compares groups
by “Age,” while the second row presents results for “Education Level.” Most models exhibit a positive correlation
between computed distances and demographic differences.
consistency across formats.
The reasoning-augmented model QwQ-32B ,
specifically trained through reinforcement learn-
ing for reasoning, achieves the highest consistency,
despite its relatively smaller size. We attribute
this superior consistency to its enhanced reason-
ing capabilities. To substantiate this, we recorded
and analyzed its reasoning traces, revealing that
QwQ-32B systematically revisits and integrates the
provided sociodemographic attributes, whether ex-
plicitly stated or inferred from dialogue history.
This iterative process enables the model to select
responses more precisely aligned with the given
attributes, promoting consistency across context
formats. The ability to methodically revisit and
incorporate attribute details likely contributes to its
superior alignment performance. Reasoning pro-
cess samples are provided in Appendix I.7 Conclusion
This study presents a novel framework for evalu-
ating how LLMs adapt their behavioral outputs
when sociodemographic attributes are provided
through two distinct interaction formats: (i) ex-
plicit single-turn prompts and (ii) multi-turn dia-
logue integration. We then systematically measure
cross-format consistency to assess robust sociode-
mographic adaptation. Our findings indicate that
most models adjust effectively to single-format at-
tribute changes, particularly in attributes like age
and education level, with the degree of value ad-
justment positively correlated with the magnitude
of attribute change. However, significant discrepan-
cies arise in cross-format scenarios. Smaller mod-
els often struggle to maintain consistent alignment
across formats, while larger, reasoning-augmented
models demonstrate more stable performance.
8Limitations
This study has a few limitations that require further
investigation in future research.
•Prompt Design: Prompts for querying mod-
els to determine the most suitable response
are designed based on our experience and ex-
periments with sample cases, given the lack of
established guidelines for optimizing prompts
in value survey question answering. Future re-
search could investigate how prompt content
variations (besides the embedded information)
impact models’ behavior adaptation.
•Limited Scope of Value Survey: The value
survey used to assess model behavior consists
of a relatively small number of questions from
the VSM 2013 survey, which has been crit-
icized for its simplicity and limited scope.
However, it aligns well with our generated
dataset, as it focuses on values reflected in
career-related questions. Future studies could
enhance the evaluation by incorporating larger
and more diverse question sets.
•Single Source of Dialogue: The evaluation of
BA_dialogue is based on dialogues generated
by GPT-4O as the QA bot. When providing
the tested models with the dialogue history,
GPT’s specific response styles may influence
the behavior of the tested models, which could
be further explored in future work.
Ethical Considerations
Our study examines how LLMs adapt outputs to
users’ sociodemographic contexts through explicit
(profile -based) and implicit (dialogue -embedded)
input formats. Misalignment or inconsistencies in
model outputs can reinforce stereotypes or erode
user trust in cross -cultural interactions. To enable
controlled, privacy -preserving evaluations, we cu-
rate and open -source a synthetic dataset, with all
user profiles derived from a publicly available syn-
thetic dataset (see Section 4), thereby eliminating
any risk of personal identifying information (PII)
exposure or consent violations. However, given
the dataset’s controlled topical focus, excessive
reliance on these examples without careful con-
sideration of their suitability for use cases may
introduce bias and overlook the broader diversity
of real-world interactions.References
Yelaman Abdullin, Diego Molla-Aliod, Bahadorreza
Ofoghi, John Yearwood, and Qingyang Li. 2024.
Synthetic dialogue dataset generation using llm
agents. Preprint , arXiv:2401.17461.
Kristen M. Altenburger, Hongda Jiang, Robert E. Kraut,
Yi-Chia Wang, and Jane Dwivedi-Yu. 2024. Exam-
ining the role of relationship alignment in large lan-
guage models. Preprint , arXiv:2410.01708.
Arnav Arora, Lucie-Aimée Kaffee, and Isabelle Augen-
stein. 2023. Probing pre-trained language models
for cross-cultural differences in values. Preprint ,
arXiv:2203.13722.
Kedi Chen, Qin Chen, Jie Zhou, He Yishen, and Liang
He. 2024. DiaHalu: A dialogue-level hallucination
evaluation benchmark for large language models. In
Findings of the Association for Computational Lin-
guistics: EMNLP 2024 , pages 9057–9079, Miami,
Florida, USA. Association for Computational Lin-
guistics.
Sumit Kumar Dam, Choong Seon Hong, Yu Qiao, and
Chaoning Zhang. 2024. A complete survey on llm-
based ai chatbots. Preprint , arXiv:2406.16937.
DeepSeek-AI. 2024. Deepseek-v3 technical report.
Preprint , arXiv:2412.19437.
Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang
Lai, Ziyi Xu, Yilong Zhao, and Tianqi Chen. 2024.
Xgrammar: Flexible and efficient structured gener-
ation engine for large language models. Preprint ,
arXiv:2411.15100.
Abhimanyu Dubey and Abhinav Jauhri et al. 2024. The
llama 3 herd of models. Preprint , arXiv:2407.21783.
Esin DURMUS, Karina Nguyen, Thomas Liao,
Nicholas Schiefer, Amanda Askell, Anton Bakhtin,
Carol Chen, Zac Hatfield-Dodds, Danny Hernan-
dez, Nicholas Joseph, Liane Lovitt, Sam McCan-
dlish, Orowa Sikder, Alex Tamkin, Janel Thamkul,
Jared Kaplan, Jack Clark, and Deep Ganguli. 2024.
Towards measuring the representation of subjective
global opinions in language models. In First Confer-
ence on Language Modeling .
David Freedman, Robert Pisani, and Roger Purves.
2007. Statistics (international student edition).
Pisani, R. Purves, 4th edn. WW Norton & Company,
New York .
Helene H Fung, Yuan Wan Ho, Rui Zhang, Xin Zhang,
Kimberly A Noels, and Kim-Pong Tam. 2016. Age
differences in personal values: Universal or cultural
specific? Psychol. Aging , 31(3):274–286.
Jingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, and
Baoyuan Wang. 2023. LiveChat: A large-scale per-
sonalized dialogue dataset automatically constructed
from live streaming. In Proceedings of the 61st An-
nual Meeting of the Association for Computational
9Linguistics (Volume 1: Long Papers) , pages 15387–
15405, Toronto, Canada. Association for Computa-
tional Linguistics.
Michele J Gelfand and Jana L et al. Raver. 2011. Differ-
ences between tight and loose cultures: a 33-nation
study. Science , 332(6033):1100–1104.
Philipp Gerlach and Kimmo Eriksson. 2021. Measuring
cultural dimensions: External validity and internal
consistency of hofstede’s VSM 2013 scales. Front.
Psychol. , 12:662604.
Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,
Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,
Shengjie Ma, Honghao Liu, Saizhuo Wang, Kun
Zhang, Yuanzhuo Wang, Wen Gao, Lionel Ni,
and Jian Guo. 2025. A survey on llm-as-a-judge.
Preprint , arXiv:2411.15594.
Akash Gupta, Ivaxi Sheth, Vyas Raina, Mark Gales, and
Mario Fritz. 2024. LLM task interference: An initial
study on the impact of task-switch in conversational
history. In Proceedings of the 2024 Conference on
Empirical Methods in Natural Language Processing ,
pages 14633–14652, Miami, Florida, USA. Associa-
tion for Computational Linguistics.
C Haerpfer, R Inglehart, A Moreno, C Welzel,
K Kizilova, J Diez-Medrano, M Lagos, and P Norris.
2020. World Values Survey: Round Seven - Country-
Pooled Datafile . JD Systems Institute & WVSA Sec-
retariat, Madrid, Spain & Vienna, Austria.
Jia He, Mukund Rungta, David Koleczek, Arshdeep
Sekhon, Franklin X Wang, and Sadid Hasan. 2024.
Does prompt formatting have any impact on llm per-
formance? Preprint , arXiv:2411.10541.
Fred K. Hoehler. 2000. Bias and prevalence effects on
kappa viewed in terms of sensitivity and specificity.
Journal of clinical epidemiology , 53 5:499–503.
G Hofstede and G. J. Hofstede. 2016. VSM
2013. https://geerthofstede.com/
research-and-vsm/vsm-2013/ . Accessed:
2024-1-11.
Pegah Jandaghi, XiangHai Sheng, Xinyi Bai, Jay Pujara,
and Hakim Sidahmed. 2023. Faithful persona-based
conversational dataset generation with large language
models. Preprint , arXiv:2312.10007.
Julia Kharchenko, Tanya Roosta, Aman Chadha, and
Chirag Shah. 2024. How well do llms represent
values across cultures? empirical analysis of llm
responses based on hofstede cultural dimensions.
Preprint , arXiv:2406.14805.
Grgur Kova ˇc, Masataka Sawayama, Rémy Portelas,
Cédric Colas, Peter Ford Dominey, and Pierre-
Yves Oudeyer. 2023. Large language models as
superpositions of cultural perspectives. Preprint ,
arXiv:2307.07870.Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
cient memory management for large language model
serving with pagedattention. In Proceedings of the
ACM SIGOPS 29th Symposium on Operating Systems
Principles .
Quinn Leng. 2023. Best practices for LLM
evaluation of RAG applications: A case
study on the databricks documentation
bot. https://www.databricks.com/blog/
LLM-auto-eval-best-practices-RAG . Accessed:
2025-04-13.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad, Abdelrahman Mohamed, Omer Levy,
Veselin Stoyanov, and Luke Zettlemoyer. 2019.
BART: denoising sequence-to-sequence pre-training
for natural language generation, translation, and com-
prehension. CoRR , abs/1910.13461.
Xinyue Li, Zhenpeng Chen, Jie M. Zhang, Yiling Lou,
Tianlin Li, Weisong Sun, Yang Liu, and Xuanzhe Liu.
2024. Benchmarking bias in large language models
during role-playing. Preprint , arXiv:2411.00585.
Ziyi Liu, Priyanka Dey, Zhenyu Zhao, Jen tse Huang,
Rahul Gupta, Yang Liu, and Jieyu Zhao. 2025. Can
llms grasp implicit cultural values? benchmarking
llms’ metacognitive cultural intelligence with cq-
bench. Preprint , arXiv:2504.01127.
Vera Lomazzi and Daniel Seddig. 2020. Gender
role attitudes in the international social survey pro-
gramme: Cross-national comparability and relation-
ships to cultural values. Cross-Cultural Research ,
54:106939712091545.
Reem I. Masoud, Ziquan Liu, Martin Ferianc, Philip
Treleaven, and Miguel Rodrigues. 2024. Cultural
alignment in large language models: An explana-
tory analysis based on hofstede’s cultural dimensions.
Preprint , arXiv:2309.12342.
Jared Moore, Tanvi Deshpande, and Diyi Yang. 2024.
Are large language models consistent over value-
laden questions? In Findings of the Association
for Computational Linguistics: EMNLP 2024 , pages
15185–15221, Miami, Florida, USA. Association for
Computational Linguistics.
Frank Nielsen. 2020. On a generalization of the
jensen–shannon divergence and the jensen–shannon
centroid. Entropy , 22(2).
OpenAI, Josh Achiam, and Steven Adler et al. 2024.
Gpt-4 technical report. Preprint , arXiv:2303.08774.
Hongjin Qian, Xiaohe Li, Hanxun Zhong, Yu Guo,
Yueyuan Ma, Yutao Zhu, Zhanliang Liu, Zhicheng
Dou, and Ji-Rong Wen. 2021. Pchatbot: A large-
scale dataset for personalized chatbot. In Proceed-
ings of the 44th International ACM SIGIR Confer-
ence on Research and Development in Information
Retrieval , SIGIR ’21, page 2470–2477, New York,
NY , USA. Association for Computing Machinery.
10Qwen, :, and An Yang et al. 2025. Qwen2.5 technical
report. Preprint , arXiv:2412.15115.
Y . Rubner, C. Tomasi, and L.J. Guibas. 1998. A metric
for distributions with applications to image databases.
InSixth International Conference on Computer Vi-
sion (IEEE Cat. No.98CH36271) , pages 59–66.
P E Shrout and J L Fleiss. 1979. Intraclass correla-
tions: uses in assessing rater reliability. Psychol.
Bull. , 86(2):420–428.
Anthony Sicilia, Jennifer C. Gates, and Malihe Alikhani.
2024. Humbel: A human-in-the-loop approach for
evaluating demographic factors of language mod-
els in human-machine conversations. Preprint ,
arXiv:2305.14195.
Nicholas Sukiennik, Chen Gao, Fengli Xu, and Yong Li.
2025. An evaluation of cultural value alignment in
llm. Preprint , arXiv:2504.08863.
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se-
bastian Gehrmann, Yi Tay, Hyung Won Chung,
Aakanksha Chowdhery, Quoc V . Le, Ed H. Chi,
Denny Zhou, and Jason Wei. 2022. Challenging
big-bench tasks and whether chain-of-thought can
solve them. Preprint , arXiv:2210.09261.
Jean M. Twenge. 2017. Have smartphones destroyed a
generation? The Atlantic .
Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni,
Abhranil Chandra, Shiguang Guo, Weiming Ren,
Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max
Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue,
and Wenhu Chen. 2024. Mmlu-pro: A more robust
and challenging multi-task language understanding
benchmark. Preprint , arXiv:2406.01574.
Adina Williams, Nikita Nangia, and Samuel Bowman.
2018. A broad-coverage challenge corpus for sen-
tence understanding through inference. In Proceed-
ings of the 2018 Conference of the North American
Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, Volume 1
(Long Papers) , pages 1112–1122. Association for
Computational Linguistics.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond, Clement Delangue, Anthony Moi, Pier-
ric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-
icz, Joe Davison, Sam Shleifer, Patrick von Platen,
Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
Teven Le Scao, Sylvain Gugger, Mariama Drame,
Quentin Lhoest, and Alexander M. Rush. 2020. Hug-
gingface’s transformers: State-of-the-art natural lan-
guage processing. Preprint , arXiv:1910.03771.
Sanae Yamashita, Koji Inoue, Ao Guo, Shota
Mochizuki, Tatsuya Kawahara, and Ryuichiro Hi-
gashinaka. 2023. RealPersonaChat: A realistic per-
sona chat corpus with interlocutors’ own personal-
ities. In Proceedings of the 37th Pacific Asia Con-
ference on Language, Information and Computation ,
pages 852–861, Hong Kong, China. Association for
Computational Linguistics.Jing Yao, Xiaoyuan Yi, and Xing Xie. 2024. Clave:
An adaptive framework for evaluating values of llm
generated responses. Preprint , arXiv:2407.10725.
Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur
Szlam, Douwe Kiela, and Jason Weston. 2018. Per-
sonalizing dialogue agents: I have a dog, do you
have pets too? In Proceedings of the 56th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 2204–2213,
Melbourne, Australia. Association for Computational
Linguistics.
Zhaowei Zhang, Ceyao Zhang, Nian Liu, Siyuan Qi,
Ziqi Rong, Song-Chun Zhu, Shuguang Cui, and
Yaodong Yang. 2024. Heterogeneous value align-
ment evaluation for large language models. Preprint ,
arXiv:2305.17147.
Wenlong Zhao, Debanjan Mondal, Niket Tandon, Dan-
ica Dillion, Kurt Gray, and Yuling Gu. 2024. World-
valuesbench: A large-scale benchmark dataset for
multi-cultural value awareness of language models.
Preprint , arXiv:2404.16308.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judging
llm-as-a-judge with mt-bench and chatbot arena. In
Proceedings of the 37th International Conference on
Neural Information Processing Systems , NIPS ’23,
Red Hook, NY , USA. Curran Associates Inc.
11A Capability of LLMs for Recognizing
Persona Attributes
As a preliminary stage of this study, we select
Llama3.1-8B-Instruct as a representative model
to verify its ability to extract user attributes from di-
alogue. This model is one of the smallest evaluated
in our study. For evaluation, we utilize the Faith-
fulPersonaChat (Jandaghi et al., 2023) synthetic
dataset, which comprises 5,648 unique synthetic
personas and 11,001 conversations. Each conver-
sation takes place between two users (User1 and
User2), and the persona profiles of both partici-
pants are provided alongside the dialogue. Every
synthetic user is assigned five persona attributes,
with at least one of these attributes explicitly men-
tioned in their dialogues.
Given the dataset structure, we prompt the tested
model with a conversation and ask it to identify one
mentioned persona attribute for each user (evalu-
ated one at a time). The model selects from a
list of ten candidate attributes, which includes the
user’s five correct attributes and five randomly sam-
pled attributes from other users. The selected at-
tribute is then evaluated against two reference set-
tings: (1) the ground truth, i.e., the user’s complete
set of five correct attributes, and (2) the output
from gpt-4o-mini (OpenAI et al., 2024), which
is prompted to extract all identifiable persona at-
tributes from the conversation. If the selected at-
tribute appears in either reference, the model re-
ceives a score of 1; otherwise, the score is 0. The
final score is calculated by aggregating the hit rate
across all 11,001 conversations for each user. We
test the model under different prompting strategies,
ranging from zero-shot to ten-shot settings.
The results, shown in Table 4, confirm that the
model can effectively identify at least one persona
attribute from dialogues. This validation supports
our main research objective: to investigate whether
models can adapt their responses based on the rec-
ognized attributes.
BPrompts Design for Dataset Generation
Agent
User Simulator ( user_simulator ): We design
two types of prompts—initial and subsequent—as
illustrated in Figure 6 to guide the LLM in sim-
ulating the user for question generation. The ini-
tial prompt starts the conversation, while subse-
quent prompts incorporate chat history to main-
tain context. This iterative approach enables thegeneration of relevant follow-up questions and
ensures dialogue progression. To prevent the
user_simulator from misinterpreting its role, the
generated dialogue is embedded within a single
message block in subsequent prompts.
Question Answering LLM ( qa_llm ): No addi-
tional prompt design is applied when querying the
LLM in the role of a QA agent. It is treated as a
standard chatbot, receiving the question generated
by the user_simulator directly.
Out-of-Context Detector ( ooc_detector ): We
specify two criteria for evaluating the generated
question in the system prompt for ooc_detector :
(1) whether the question accurately reflects the user
profile information and (2) whether it is framed
in the first person. The user profile and generated
question are provided in the user prompt. This
design is illustrated in Figure 7.
C Pseudocode for Dataset Generation
The complete procedure for generating the dialogue
dataset is outlined in the pseudocode presented in
Algorithm 1. During dialogue generation, we set
the threshold dialogue _max _runs to 5.
D Prompts Design for LLM Judge
To enhance the reliability of the LLM judge
when evaluating questions generated by
user_simulator , we provide detailed scor-
ing instructions in the prompt, including explicit
criteria for each possible score. Additionally,
we require the model to articulate its reasoning
behind each score, encouraging a more deliberate
evaluation process. Prompts of all 4 dimensions
are listed in Figures 8, 9, 10, 11
E Alignment between LLM Judge and
Human Rates
We measure the alignment between the scores as-
signed by the LLM judge and the average hu-
man ratings across 50 sampled dialogues. The
results, presented in Table 5, show strong align-
ment for Dimensions 1, 3, and 4. The relatively
lower alignment for Attribute Correctness is likely
due to high overall scores and low variance in
this dimension, as the correctness of attributes in
the generated dialogues is largely ensured by the
robust user_simulator and additional validation
performed by the out-of-context detector in
the dialogue generation pipeline.
12Prompt Type ModelsHit Rate
vsGround Truth vsgpt-4o-mini-2024-07-18
User1 User2 User1 User2
Zero-shotLlama3-8B-Instruct 0.826 0.864 0.732 0.784
Llama3.1-8B-Instruct 0.821 0.864 0.731 0.796
One-shotLlama3-8B-Instruct 0.849 0.895 0.775 0.839
Llama3.1-8B-Instruct 0.837 0.888 0.756 0.827
Five-shotLlama3-8B-Instruct 0.851 0.906 0.784 0.854
Llama3.1-8B-Instruct 0.848 0.905 0.774 0.853
Ten-shotLlama3-8B-Instruct 0.856 0.910 0.783 0.855
Llama3.1-8B-Instruct 0.852 0.905 0.775 0.853
Random – 0.5 0.5 0.18 0.19
Table 4: Recognition accuracy of persona attributes from dialogue by LLMs. We evaluate both “Llama3.1-8B-
Instruct” and “Llama3-8B-Instruct”, which belong to the same Llama model family. Model scores are compared
against the expected accuracy of randomly selecting one correct attribute from the list of 10 candidates.
Procedure 1 Generation of Dialogues
Require: seed _dataset ,user _simulator ,qa_llm,ooc_detector ,dialogue _max _runs
1:initialize dialogues
2:foreachuser _profile inseed _dataset do
3: initialize conversation _history
4: user _question ←user _simulator (initial _prompt ∪user _profile )
5: ifooc_detector (user _question, user _profile )then
6: continue ▷Skip this profile if out-of-context (OOC)
7: end if
8: conversation _history.append (user _question )
9: llm_output ←qa_llm(user _question )
10: conversation _history.append (llm_output )
11: while True do
12: user _question, end _conversation ← user _simulator (conversation _history ∪
following _prompt ∪user _profile )
13: ifend_conversation orlen(conversation _history )≥dialogue _max _runs then
14: dialogues.append (conversation _history )
15: break ▷End conversation or exceed max runs
16: end if
17: ifooc_detector (user _question, user _profile )then
18: break ▷Skip this profile if OOC detected
19: end if
20: conversation _history.append (user _question )
21: llm_output ←qa_llm(user _question )
22: conversation _history.append (llm_output )
23: end while
24:end for
13Dimensions ICC(3, k)Pearson Correlation
Attribute Coverage 0.85 0.75
Attribute Correctness 0.16 0.19
Question Diversity 0.63 0.55
Relevance 0.80 0.67
Table 5: Alignment scores, measured using ICC(3, k) and the Pearson Correlation Coefficient, were computed
between the LLM judge and human annotators across four evaluation dimensions, based on a sample of 50 dialogues.
You are role-playing as a user seeking career advice from  a chatbot. Always respond using the JSO N form at System  
Here are the details of the user you are sim ulating: 
{ Age, Education Level, Job Title, Country} 
You aim  to engage with a chatbot to explore career guidance in the following areas: 
1. Career direction in the next 5 years 
2. Career direction in the next 10 years 
3. Essential skills for career growth 
4. Relevant certiﬁcations to obtain 
Start the conversation by asking for short-term  career suggestions, either explicitly m entioning your age and job title or subtly hinting
at them . As the discussion evolves, progressively share m ore personal details to obtain tailored advice and deeper insights. 
Always respond using the following JSO N form at:   
{ "proposed_question": …} User 
You are role-playing as a user seeking career advice from a chatbot. Review the conversation history between you (the user) and the
chatbot, then continue the role-play. Always respond in JSON format. System 
Here are the details of the user you are simulating: 
{ Age, Education Level, Job Title, Country} 
Through the conversation, your goal is to seek career guidance in the following areas: 
1. Career direction for the next 5 years 
2. Career direction for the next 10 years 
3. Essential skills for career growth 
4. Relevant certiﬁcations to pursue 
If all four areas are addressed, conclude the conversation. Otherwise, generate a follow-up question to gather more insights. 
Feel free to share additional details about yourself in your follow-up questions, either based on the chatbot’s responses or as needed
for more tailored advice. 
Always respond using the following JSON format:   
{ "proposed_question": ...,"end_conversation": true/false} User Conversation history:   
User: ... 
Chatbot: ... 
User 
Subsequent Prompt Initial Prompt 
Figure 6: The prompts guiding the user_simulator are structured as follows: the green text is the system prompt
defining the LLM’s role; the blue text specifies the user profile, conversation objectives, termination conditions, and
response format; the yellow text depicts the chat history between the simulated user and the QA LLM.
You are an out-of-context detector. Compare the details in a user-provided question with the user proﬁle. Ensure the question meets
the following criteria: 
1. Reﬂects only information present in the proﬁle. 
2. Is written in the ﬁrst-person perspective and seeks career suggestions. 
If the question fails to meet these criteria, rewrite it accordingly.   
Respond in JSON format: {"has_out_of_context": true/false, "updated_question": "..."}. Leave "updated_question" empty if no
discrepancies are found. System 
User proﬁle: 
{ Age, Education Level, Job Title, Country} 
Question: 
As someone working in a leadership role in technology, what are some short-term career suggestions for continuing growth and
advancement? User 
User 
Figure 7: The prompt guiding the ooc_detector is structured as follows: the green text represents the system
prompt, which defines the criteria for the LLM to check, while the blue text contains the user profile and the
generated question.
14You will be given a a set of questions and a persona attributes. 
Your task is to provide a 'total rating' scoring how many attributes mentioned in the questions. The score should be given based on
the exact number of mentioned attributes. 
Give your answer as a ﬂoat on a scale of 0 to 5, you don't need to care about the correctness of the attribute values. 
Scoring details: 
* no attributes mentioned --> 0 
* mentioning 1 attribute --> 1 
* mentioning 2 attributes --> 2 
* mentioning 3 attributes --> 3 
* mentioning 4 attributes --> 4 
* mentioning 5 attributes --> 5 
  
Always respond using the following JSON format: {{ "rating": int, "reason": str}}' System 
User proﬁle: 
{ Age, Education Level, Job Title, Country} 
User Here are the questions: 
Question 1: ... 
Question 2:... 
... User Figure 8: The prompt designed for guiding the LLM judge to score the generated dialogues from the perspective of
“Attribute Coverage”
You will be given a a set of questions and a persona attributes. 
Your task is to provide a 'total rating' scoring if attributes values are correctly mentioned in the questions. 
Initial score is 5, for every wrong attribute value detected in the questions, deducting one point. 
You don't need to deduct point for missing attributes. 
Give your answer as a ﬂoat on a scale of 0 to 5. 
Scoring details: 
* all mentioned attributes are having the correct values --> 5 
* one attribute value is wrong --> 4 
* two attribute values are wrong --> 3 
* three attribute values are wrong --> 2 
* four attribute values are wrong --> 1 
* ﬁve attribute values are wrong --> 0 
  
Always respond using the following JSON format: {{ "rating": int, "reason": str}}' System 
User proﬁle: 
{ Age, Education Level, Job Title, Country} 
User Here are the questions: 
Question 1: ... 
Question 2:... 
... User 
Figure 9: The prompt designed for guiding the LLM judge to score the generated dialogues from the perspective of
“Attribute Correctness”
You will be given a a set of questions and a persona attributes. 
Your task is to provide a 'total rating' scoring if questions are repeating the similar content, more repetitions, lower the score. 
Please give a score based on the number of unique questions. 
Give your answer as a ﬂoat on a scale of 1 to 5. 
Scoring details: 
* all ﬁve questions are having the unique contents --> 5 
* 4 unique contents, and 1 repeated question --> 4 
* 3 unique contents and 2 repeated questions --> 3 
* only 2 unique contents among 5 questions, 3 questions are having the repeated contents --> 2
* all ﬁve questions are having the same content --> 1 
         
Always respond using the following JSON format: {{ "rating": int, "reason": str}}' 
Write your reason concisely. System 
User proﬁle: 
{ Age, Education Level, Job Title, Country} 
User Here are the questions: 
Question 1: ... 
Question 2:... 
... User 
Figure 10: The prompt designed for guiding the LLM judge to score the generated dialogues from the perspective
of “Question Diversity”
15You will be given a a set of questions and a persona attributes. 
Your task is to provide a 'total rating' for the readability of the questions. 
All questions are about seeking the career advice from the chatbot. 
Please give a score based on the readability of each question. 
Give your answer as a ﬂoat on a scale of 1 to 5. 
Scoring details: 
* Start with the score as 3. 
* If there's any reactions to the chatbot's response in the questions --> plus 1 to the score. 
* If the langauge is natural and the questions are organized in a progressive manner --> plus 1 to the score. 
* If the language is not natural and the following questions keep repeating the user's self introduction --> minus 1 from the score. 
* If the language is too polite and sounds like seeking the advice from seniors --> minus 1 from the socre. 
Always respond using the following JSON format: {{ "rating": int, "reason": str}}' 
Write your reason concisely. System 
User proﬁle: 
{ Age, Education Level, Job Title, Country} 
User Here are the questions: 
Question 1: ... 
Question 2:... 
... User Figure 11: The prompt designed for guiding the LLM judge to score the generated dialogues from the perspective
of “Relevance”
F Prompts Design for Querying Models
To ensure prompt naturalness, we devised two
scenario-specific templates that vary in system
prompt framing, information presentation, and
question phrasing. The BA_user template directs
the model: “ You are a chatbot designed to provide
precise, personalized answers based on the given
user profile ,” whereas the BA_dialogue template
frames the model as the user: “ Assume you are me;
help me find the most suitable answer to the fol-
lowing question .” Full examples of both templates
are provided in Figure 12. As described, reasoning
models are queried twice per question to ensure
a structured reasoning process. All prompts are
crafted to naturally guide models in understanding
embedded user attributes and making selections
accordingly. To maintain coherence, questions con-
taining explicit user attributes are presented in the
third-person perspective, while those based on di-
alogue history adopt the first-person perspective,
aligning with the conversational format.
G Probability Normalization for
option_ids
The probability distribution Pover option_ids
for each response ris derived from the model’s
log probabilities by applying the exponentiation
function. Given that only the top 5 log probabil-
ities from the full vocabulary are output by the
model, any option_ids not included in this set are
assigned a probability of 0.0. The resulting values
are then normalized by dividing each probabilityby the sum of all five option_ids probabilities,
ensuring that the final distribution satisfies the con-
dition:
5X
i=1p(i) = 1 .
H Extra Evaluation Outcomes for
BA_user andBA_dialogue
Nationality : Unlike attributes such as “Age” and
“Education Level,” “Nationality” encompasses a
large number of unique values. To address this,
we categorize RUandRDbased on each country’s
“Development Level,” which we argue effectively
captures cultural and value differences when incor-
porating country information into the model. The
grouping is derived from a mapping list generated
by the “gpt-4o-2024-08-06”. All tested models
exhibit a strong awareness of country-based differ-
ences when classified into “Developed,” “Devel-
oping,” and “Third World” categories. Figure 13
visualizes this evaluation alongside analyses of al-
ternative country-grouping methods.
Job Category : The original dataset contains
hundreds of distinct job titles. To facilitate
analysis, we apply zero-shot classification using
bart-large-mnli (Lewis et al., 2019), trained on
MultiNLI (Williams et al., 2018), to map each ti-
tle to a predefined set of job categories. We then
group responses by these categories, as illustrated
in Figure 14. While this approach introduces an
16You are a chatbot designed to provide precise
and personalized answers to questions based
on the given user proﬁle. Analyze the question
carefully and tailor your response to match the
user's context. System 
Here are the details of the user proﬁle: 
{ Age, Education Level, Job Title, Country} 
Below is the question: 
{ Question } 
{ Option List } 
Answer the question in json format:   
{”option_id”: int, “reason”: str} User 
User 
User (a) Prompt Design for BA_user
As someone working in a leadership role ... 
1. **Stay Updated with Trends**: ... User 
Assistant 
You are a chatbot designed to provide precise
and personalized answers to questions.
Analyze the question carefully and tailor your
response to match the user's context. System 
Below is the question: 
{ Question } 
{ Option List } User 
Answer the question in json format:   
{”option_id”: int, “reason”: str} User Assume you are me, help me ﬁnd the most
suitbale answer to the following question. User Generated
Dialogue 
...... (b) Prompt Design for BA_dialogue
You are a chatbot designed to provide precise
and personalized answers to questions based
on the given user proﬁle. Analyze the question
carefully and tailor your response to match the
user's context. System 
Here are the details of the user proﬁle: 
{ Age, Education Level, Job Title, Country} 
Answer the question in json format:   
{”option_id”: int, “reason”: str} User 
User 
User Assistant Below is the question: 
{ Question } 
{ Option List } 
<think>Model’s reasoning</think> 
Query Once for
Reasoning 
(c) Prompt Design for BA_user
As someone working in a leadership role ... 
1. **Stay Updated with Trends**: ... User 
Assistant 
You are a chatbot designed to provide precise
and personalized answers to questions.
Analyze the question carefully and tailor your
response to match the user's context. System 
Below is the question: 
{ Question } 
{ Option List } User 
Answer the question in json format:   
{”option_id”: int, “reason”: str} User Assume you are me, help me ﬁnd the most
suitbale answer to the following question. User Generated
Dialogue 
...... 
Assistant <think>Model’s reasoning</think> 
Query Once for
Reasoning (d) Samples for BA_dialogue grouped by Education
Figure 12: Prompt designs for querying models to answer value-based questions given contextual information.
Reasoning models are queried twice: the first query allows free-form reasoning responses, while the second query,
with the reasoning content appended, enforces a structured output format.
additional layer of abstraction—potentially reduc-
ing the sensitivity to inter-group differences com-
pared to attributes like age or education level—a
consistent pattern emerges: responses to individ-
uals in Science and Technology roles exhibit the
greatest response similarity across both BA_user
andBA_dialogue scenarios for most models. Con-
versely, when models identify users from Business-
related sectors, the human values reflected in their
responses tend to diverge more significantly from
those associated with other sectors.
Position Level : Following a similar approach
to grouping responses by job categories, we use
bart-large-mnli to map job titles to predefined
position levels, organizing the results accordingly.
The outcomes are presented in Figure 15. Com-
pared to the “Job Category” grouping, the posi-
tion level analysis shows a more consistent patternacross BA_user andBA_dialogue for all models.
Notably, responses for “Entry Level” and “Senior
Management” users exhibit the most pronounced
differences, while the distances between “Entry
Level” and “C-Suite” users are smaller than ex-
pected. This is likely because the classifier tends
to assign only titles like “Chief of Staff” to the “Se-
nior Management” category, while the “C-Suite”
group contains a broader range of titles, resulting
in a more heterogeneous sample.
I Reasoning Samples
In our evaluation, we observe that the reasoning-
capable model exhibits greater consistency in their
responses across different input formats. Our
analysis suggests that the model would review
all retrieved demographic attributes when they
are “reasoning”–regardless of whether they are ex-
17Llama3.1-8B-Instruct Llama3.1-70B-Instruct DeepSeek-V3 Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct QwQ-32B
Developed vs DevelopingDeveloped vs Third_World
Developing vs Third_World0.01.02.03.04.05.06.07.0(a)BA_user grouped by “devlopment level”
Developed vs DevelopingDeveloped vs Third_World
Developing vs Third_World0.01.02.13.14.15.26.27.2 (b)BA_dialogue grouped by “devlopment level”
Figure 13: The visualized measurement results for BA_user andBA_dialogue grouped by the “development level”
of the user’s nationality.
plicitly stated or implicitly embedded in dialogue
history–during the reasoning process. This reflec-
tive step helps align their responses more closely
with the given context. Figure 16 illustrates two
examples: one where the user profile is explicitly
provided, and another where it is inferred from
prior dialogue.
18Llama3.1-8B-Instruct Llama3.1-70B-Instruct DeepSeek-V3 Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct QwQ-32B
Business vs CreativeBusiness vs EducationBusiness vs HealthBusiness vs HospitalityBusiness vs ScienceBusiness vs TechCreative vs Education
Creative vs Health
Creative vs Hospitality
Creative vs Science
Creative vs Tech
Education vs Health
Education vs Hospitality
Education vs Science
Education vs Tech
Health vs HospitalityHealth vs ScienceHealth vs TechHospitality vs ScienceHospitality vs TechScience vs Tech0.01.12.23.24.35.46.57.5(a)BA_user grouped by “job category”
Business vs CreativeBusiness vs EducationBusiness vs HealthBusiness vs HospitalityBusiness vs ScienceBusiness vs TechCreative vs Education
Creative vs Health
Creative vs Hospitality
Creative vs Science
Creative vs Tech
Education vs Health
Education vs Hospitality
Education vs Science
Education vs Tech
Health vs HospitalityHealth vs ScienceHealth vs TechHospitality vs ScienceHospitality vs TechScience vs Tech0.01.02.13.14.25.26.27.3
(b)BA_dialogue grouped by “job category”
Figure 14: Visualized measurement results for BA_user andBA_dialogue grouped by “job category,” derived from
job titles classified using the pre-trained bart-large-mnli model (Lewis et al., 2019).
19Llama3.1-8B-Instruct Llama3.1-70B-Instruct DeepSeek-V3 Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct QwQ-32B
C-Suite vs Entry_LevelC-Suite vs ManagementC-Suite vs Professional C-Suite vs Senior_Management
Entry_Level vs Management
Entry_Level vs Professional
Entry_Level vs Senior_Management
Management vs Professional Management vs Senior_ManagementProfessional vs Senior_Management0.01.22.43.6(a)BA_user grouped by “position level”
C-Suite vs Entry_LevelC-Suite vs ManagementC-Suite vs Professional C-Suite vs Senior_Management
Entry_Level vs Management
Entry_Level vs Professional
Entry_Level vs Senior_Management
Management vs Professional Management vs Senior_ManagementProfessional vs Senior_Management0.01.22.53.75.0
(b)BA_dialogue grouped by “position level”
Figure 15: Visualized measurement results for BA_user andBA_dialogue grouped by “position level,” derived
from job titles classified using the pre-trained bart-large-mnli model (Lewis et al., 2019).
20<think>Okay, so I need to ﬁgure out the best answer for this question about how important personal or home life time is for an ideal
job, assuming I'm the CTO from Micronesia with a high school education. Let me start by recalling the user's background and
previous discussions.\n\nThe user is a CTO, which is a leadership role in tech. They've been advised to focus on various
certiﬁcations and emerging tech areas like AI, cybersecurity, cloud computing, etc. They're aiming for long-term growth in tech
leadership, possibly moving into roles like Chief Digital Oﬃcer, entrepreneurship, or consulting. \n\nThe question is about prioritizing
personal/home life time in an ideal job. The options range from 1 (utmost) to 5 (no importance). Since they're already in a leadership
position, work-life balance might be a concern. Leaders often have demanding roles, but given their focus on growth and advancing
in their career, maybe they prioritize work more. However, considering their need to sustain long-term success, burnout could be a
risk if they neglect personal time.\n\nLooking back at the previous advice, there was an emphasis on self-reﬂection and continuous
learning. The user might value maintaining energy through personal time to keep eﬀective. Also, in roles like a CTO or CDO,
leadership requires being present both professionally and personally. \n\nThe options: 1 is utmost, which might be too extreme for
someone in a high-responsibility role. 2 (very important) seems balanced. 3 is moderate, which might indicate they can sacriﬁce
some personal time. Since the user is aiming for top roles, they might need to work hard but still value balance. \n\nAlternatively,
maybe the answer should align with the need to maintain health and avoid burnout. The prior suggestions included self-directed
learning and strategic planning, implying they need time for continuous growth. \n\nTherefore, the best answer is probably 2: very
important. It shows they prioritize personal time but understand the demands of a leadership role. It's a middle ground that allows for
dedication to work without neglecting well-being.\n</think> <think>Okay, so I need to ﬁgure out how important having suﬃcient time for personal or home life would be for this user when
considering an ideal job, other than their current one. Let me start by looking at their proﬁle.\n\nThe user is a 33-year-old male from
Micronesia. He has a high school education and 8 years of experience, currently working as a Chief Technology Oﬃcer. Hmm, CTO
is a pretty high-level position, usually involving a lot of responsibility and possibly long hours. But the question is about an ideal job,
not his current one. So maybe he's looking for something diﬀerent?\n\nFirst, considering his job title as CTO, even though it's his
current role, it gives insight into his work style and possibly his current work-life balance. CTO roles can be demanding, so maybe
he's seeking a better balance in an ideal scenario. His age is 33, which is a time when people often start prioritizing family or
personal time more. Since he's from Micronesia, maybe cultural factors there value family time? I'm not too sure about Micronesian
culture speciﬁcs, but generally, in many cultures, family is important.\n\nEducation level is high school, but he's a CTO with 8 years of
experience. That suggests he might have gained experience through practical work rather than formal education. Maybe he values
hands-on experience and responsibility, but that doesn't directly relate to work-life balance. \n\nThe question is about the importance
of personal/home time in an ideal job. Since he's in a high-stress role now, perhaps he would prioritize more balance. The options
are from 1 (utmost) to 5 (no importance). \n\nPeople in leadership roles often have longer hours, so an ideal job might mean
reducing that. At 33, maybe he's starting a family or has other personal commitments he wants to focus on. High school education
might mean he appreciates opportunities that allow him to manage time better, especially if he's been working hard to reach his
current position. \n\nI think the most likely answers are either 1 or 2. Maybe 1 if he's really stressed now, but perhaps 2 as a safer
bet. Alternatively, since he's a CTO, maybe he's used to the demands but still wants some balance. Let's see... The options are \"of
utmost importance\" (1) versus \"very important\" (2). \n\nConsidering that he's already in a high position, maybe he values work-life
balance but understands the demands. However, since it's an ideal job, he might prioritize it highly. I think the answer is either 1 or 2.
Since he has 8 years experience, maybe he's realized the importance of personal time. Let me go with 1, but maybe the user might
think 2. Hmm. Alternatively, maybe moderate? No, that seems less likely for someone in a high-stress job. \n\nWait, the options are 1
to 5. Let me think again. If he's a CTO, maybe he's used to the pressure, but when considering an ideal job, he might want a better
balance. So \"of utmost importance\" (1) would be the top priority. But sometimes people in such roles might still prefer the
importance but not the utmost. Maybe \"very important\" (2). \n\nAlternatively, maybe the user's location in Micronesia aﬀects this.
Maybe in Micronesia, family is central, so he would choose 1. Since I don't have speciﬁc cultural data, but generally, in many Paciﬁc
Island nations, community and family are important. So perhaps he would prioritize it highly. \n\nI think the best answer here is 1. But
I'm not 100% sure. Alternatively, maybe 2. Let me check again. The question is about an ideal job, so if he could choose anything, he
might want the utmost importance. So I'll go with 1.\n</think> With User Proﬁle 
With Dialogue
History Figure 16: Samples of the model’s reasoning given the same context information in different formats.
21