How does Alignment Enhance LLMs’ Multilingual
Capabilities? A Language Neurons Perspective
Shimao Zhang1†*Zhejian Lai1*Xiang Liu1*Shuaijie She1Xiao Liu2
Yeyun Gong2Shujian Huang1‡Jiajun Chen1
1National Key Laboratory for Novel Software Technology, Nanjing University
2Microsoft Research Asia
{smzhang,laizj,liuxiang,shesj}@smail.nju.edu.cn
{xiao.liu.msrasia,yegong}@microsoft.com
{huangsj,chenjj}@nju.edu.cn
Abstract
Multilingual Alignment is an effective and representative paradigm to enhance
LLMs’ multilingual capabilities, which transfers the capabilities from the high-
resource languages to the low-resource languages. Meanwhile, some researches on
language-specific neurons reveal that there are language-specific neurons that are
selectively activated in LLMs when processing different languages. This provides
a new perspective to analyze and understand LLMs’ mechanisms more specifically
in multilingual scenarios. In this work, we propose a new finer-grained neuron iden-
tification algorithm, which detects language neurons (including language-specific
neurons and language-related neurons) and language-agnostic neurons. Further-
more, based on the distributional characteristics of different types of neurons, we
divide the LLMs’ internal process for multilingual inference into four parts: (1)
multilingual understanding, (2) shared semantic space reasoning, (3) multilingual
output space transformation, and (4) vocabulary space outputting. Additionally,
we systematically analyze the models before and after alignment with a focus on
different types of neurons. We also analyze the phenomenon of “Spontaneous Mul-
tilingual Alignment”. Overall, our work conducts a comprehensive investigation
based on different types of neurons, providing empirical results and valuable in-
sights for better understanding multilingual alignment and multilingual capabilities
of LLMs.1
1 Introduction
By training on the extensive corpus, large language models (LLMs) demonstrate outstanding language
capabilities [Grattafiori et al., 2024, Yang et al., 2024, Liu et al., 2024, Zhang et al., 2025]. However,
due to the unbalanced pretraining corpus across different languages, LLMs have very uneven per-
formance on high-resource languages and low-resource languages [Huang et al., 2023, Zhu et al.,
2023, Zhang et al., 2024]. Therefore, researchers have conducted comprehensive explorations to
further enhance the multilingual performance of LLMs. A straightforward approach is increasing the
proportion of non-English texts during pretraining [Ni et al., 2021, Yang et al., 2024] or performing
continual pretraining with multilingual texts [Liu et al., 2021, Ji et al., 2024]. But these approaches
often entail high computational costs and substantial amounts of multilingual data.
*Equal contribution.
†Work done during his internship at MSRA.
‡Corresponding author.
1The project will be available at https://github.com/NJUNLP/Language-Neurons-Alignment .
Preprint. Under review.arXiv:2505.21505v1  [cs.CL]  27 May 2025Considering LLMs’ great performance on the high-resource languages, multilingual alignment
has emerged as a representative paradigm for enhancing multilingual capabilities by transferring
knowledge from high-resource to low-resource languages [Zhao et al., 2024a, She et al., 2024].
A representative example is MAPO She et al. [2024], which improves multilingual alignment by
utilizing a well-trained multilingual translation model to compute alignment scores based on the
conditional generation probability of translating non-English responses into English.
Language-specific ?Language-agnostic ?Language-related?
❌
❌
Figure 1: A neuron’s activation probability
across different languages. This neuron that ex-
hibits high activation probabilities across multi-
plebutnot all languages can’t be correctly cate-
gorized under the existing methodology.Many studies conduct systematic mechanism anal-
yses of the multilingual alignment and LLMs’
multilingual capabilities. Zhao et al. [2024b]
split the multilingual processing workflow into
three parts: multilingual understanding, resolv-
ing tasks, and generating outputs in the target
language. This three-stage inference workflow
clearly demonstrates how LLMs leverage English
as a pivot language to handle multilingualism us-
ing a unified pattern. Inspired by the neurobio-
logical underpinnings of human language facul-
ties, Tang et al. [2024] conducts a fine-grained
identification by detecting the language-specific
neurons. Their results indicate that LLMs predom-
inantly utilize a small subset of neurons to pro-
cess the particular target language. Furthermore,
these language-specific neurons are primarily sit-
uated in the model’s top and bottom layers [Tang
et al., 2024], which is consistent with the three-
stage multilingual workflow of Zhao et al. [2024b].
However, we notice a key limitation in the existing
language-specific neuron identification methodol-
ogy: Some neurons are shared across multiple
languages but are not entirely language-agnostic. Such neurons are incorrectly categorized as either
language-specific or language-agnostic neurons under the existing framework. We present a case
study in Figure 1. Furthermore, we aim to systematically explore an important question: Can we bet-
ter analyze and understand how multilingual alignment enhances the LLMs’ multilingual capabilities
from the perspective of language neurons?
In this work, we comprehensively investigate the multilingual alignment of LLMs from the perspective
of language neurons, where we deploy MAPO as a representative multilingual alignment algorithm.
Considering the above limitation of the existing language-specific neuron identification methodology,
we define language neurons as the union of language-specific neurons andlanguage-related
neurons , as opposed to language-agnostic neurons . We separately distinguish language-related
neurons from both language-specific and language-agnostic neurons, which allows for more precise
analyses. Furthermore, we propose a new language neuron identification algorithm in our work,
which is able to identify language-related neurons that are in fact shared across multiple languages.
Then we analyze the models before and after alignment, focusing on the changes in different types
of neurons. Based on the distributional characteristics of these neurons, we divide LLMs’ internal
process for multilingual inference into four parts: (1) multilingual understanding, (2) shared semantic
space reasoning, (3) multilingual output space transformation, and (4) vocabulary space outputting.
Our findings reveal that different parts exhibit distinct dependencies on different types of neurons,
and that multilingual alignment significantly enhances the activation of the corresponding types of
neurons across the relevant layers. Additionally, we analyze the important “Spontaneous Multilingual
Alignment” [Zhang et al., 2024] phenomenon in LLMs, providing insights into the roles of language-
agnostic neurons and language-related neurons shared across languages. For further analysis, we also
provide observations about the uniqueness of English and the neuron distributions. Overall, based
on different types of neurons, we present empirical results and valuable insights that contribute to a
deeper understanding of multilingual alignment and the multilingual capabilities of LLMs.
22 Related Work
2.1 Multilingual Alignment
Conducting pretraining or continual pretraining on the multilingual corpus is a straightforward and
effective method to enhance LLMs’ multilingual capabilities [Ni et al., 2021, Ji et al., 2024]. However,
these methods typically require substantial investments in time, data, and computational resources.
Thus, many researchers perform multilingual alignment to improve LLMs’ multilingual performance
by transferring the capabilities from high-resource languages to low-resource languages [Eronen et al.,
2023, Zhao et al., 2024c,a, She et al., 2024], which efficiently and effectively improves the model
performance in low-resource language scenarios. Furthermore, Zhang et al. [2024] first finds the
“Spontaneous Multilingual Alignment” phenomenon in LLMs, which demonstrates that conducting
multilingual alignment based on a small number of languages effectively improves the alignment
even between English and many languages unseen during alignment.
2.2 Mechanistic Interpretability
In addition to enhancing LLMs’ multilingual performance, research on the underlying mechanisms
of multilingual capabilities in LLMs is still ongoing. It is crucial for us to understand and explain
the LLMs and related methods explicitly. Typically, the existing approaches primarily perform
mechanistic interpretability analyses by observing the internal states of the model [Nostalgebraist,
2020, Zhang et al., 2024, Zhao et al., 2024b, Mousi et al., 2024]. Overall, neuron states and latent
intermediate logits are both important objects of observation. For latent logits, Wendler et al. [2024]
utilizes logit lens [Nostalgebraist, 2020] to directly project the logits in the intermediate layers to the
vocabulary space, which reveals the latent participation of English in the intermediate layers. For
neuron states, Hu et al. [2024] analyzes the neuron activation overlap to measure the extent of shared
neuron activation across different languages.
2.3 Language-Specific Neurons
Many studies have revealed the language-related and language-agnostic components in LLMs. At
the layer level, the multilingual processing of LLMs is considered to involve three stages [Zhao
et al., 2024b, Wendler et al., 2024]: converting multilingual inputs into a shared semantic space,
intermediate-layer reasoning, and outputting in the target language. The top and bottom layers of
the model handle multilingual processing, while the intermediate layers perform inference in similar
patterns across different languages. This demonstrates a distinct division of labor within the model at
the layer level regarding language specificity.
Furthermore, many studies investigate the finer-grained methods for language-specific neuron identi-
fication [Kojima et al., 2024, Tang et al., 2024]. Tang et al. [2024] categorizes activated neurons into
language-specific neurons and language-agnostic neurons. They detect language-specific neurons by
calculating language activation probability entropy on massive text. However, we find that some neu-
rons are activated by multiple languages (i.e., not language-specific), yet are not universally activated
across all languages (i.e., not language-agnostic). Simply categorizing activated neurons into two
classes blurs this distinction. Thus, we propose a new method to identify neurons, which categorizes
activated neurons into three types: language-specific, language-related, and language-agnostic.
3 Methodology
In this section, we introduce our overall analysis pipeline in our work. First, we review the existing
multilingual alignment algorithm and neuron analysis techniques as a preliminary study in §3.1.
Then we introduce the multilingual alignment algorithm we utilize in our work in §3.2. Finally, for
mechanistic interpretability analysis, we introduce our new method for detecting language-specific,
language-related, and language-agnostic neurons (§3.3).
3.1 Preliminary Study
Most LLMs are pretrained mainly on the high-resource language corpus, which leads to LLMs’
unstable and unbalanced performance in multilingual scenarios. As a representative multilingual
3alignment algorithm, Multilingual-Alignment-as-Preference Optimization (MAPO) [She et al., 2024]
effectively and efficiently improves the LLMs’ multilingual performance. Additionally, it is also
important for us to understand and analyze the mechanism of LLMs’ multilingual capabilities and
multilingual alignment. Moreover, some studies on the identification of the language-specific and
language-agnostic neurons in LLMs [Tang et al., 2024, Kojima et al., 2024]. It is found that LLMs’
capabilities of processing a particular language mainly come from a small subset of neurons [Tang
et al., 2024].
However, there are still many important questions waiting for further investigation. On the one hand,
many methods overlook neurons activated by multiple languages yet not language-agnostic, namely
language-related neurons that lie between language-specific and language-agnostic categories. On
the other hand, research from the perspective of language neurons on the underlying mechanisms of
LLMs’ multilingual alignment and multilingual capabilities remains quite limited, which is essential
for better understanding and improving the multilingual performance of LLMs.
3.2 Multilingual Alignment
MAPO is a typical multilingual alignment algorithm to align the reasoning capabilities of non-English
language responses with those of English, which serves as the pivot language. Specifically, for a
given query Xin a target (non-English) language and its corresponding English variant XEng, we
collect their respective responses YandYEng. An off-the-shelf translation model, parameterized by
θ, is deployed to estimate the conditional generation probability P(Y|YEng;θ)by force-decoding Y
conditioned on YEng. A higher conditional probability is interpreted as stronger alignment between
the target language response and its English counterpart. This probability is then used as an alignment
score, denoted rθ(X, Y).
This alignment score can be integrated into preference optimization algorithms. For instance,
in PPO [Schulman et al., 2017], rθ(X, Y)can be directly employed as the reward score. In
DPO [Rafailov et al., 2023], for each target language, ndistinct outputs are generated. Based
on the alignment score these noutputs are used to form n
2
preference pairs (Yw, Yl), where Ywis
deemed superior to Yldue to a higher alignment score. The model is then optimized by Eq1.
LDPO(πθ;πref) =−E(X,Yw,Yl)∼D
logσ
βlogπθ(Yw|X)
πref(Yw|X)−βlogπθ(Yl|X)
πref(Yl|X)
(1)
3.3 Language Neurons Identification
Following Tang et al. [2024], the neurons in our work are defined as a linear transformation of a
single column in a weighted matrix Wfollowed by a non-linear activation, SiLU [Shazeer, 2020].
For the j-th neuron in the i-th layer, its activation probability when processing responses in language
kis computed as:
pk
i,j=E 
I 
SiLU( xiWi)j>0
|language k
(2)
We define language neurons as those exhibiting higher activation probabilities for some languages,
while discriminating from others. To select neurons that, despite exhibiting relatively higher entropy,
still demonstrate high activation probabilities for some languages, we additionally incorporate the
maximum activation probability, as formulated in Eq 3:
score i,j=−lX
k=1p′k
i,jlogp′k
i,j−λmax
1≤k≤lpk
i,j, (3)
where p′
i,jrepresents the probability distribution pi,jafter normalization and λis a balancing
coefficient. Neurons with scores falling in the lowest percentile, specifically, the bottom 5% are
selected.
Furthermore, to identify how many languages each selected neuron is related to, we introduce a
threshold τand compute:
Ni,j=lX
k=1I 
pk
i,j> τ
. (4)
4A neuron is considered as a language-specific neuron ifNi,j= 1, and as a language-related neuron
if1< N i,j< l. Both of them belong to language neuron . In contrast, a neuron is considered
language-agnostic neuron if it exhibits high activation probabilities across all llanguages.
Finally, given our focus on multilingual reasoning tasks, we select neurons exclusively based on
responses from multilingual reasoning datasets, rather than relying on multilingual plain text [Tang
et al., 2024].
4 Experiments
4.1 Experimental Setup
Following She et al. [2024], we conduct our experiments and analyses on the mathematical reasoning
tasks and different languages. In this section, we introduce our experimental settings in detail.
Models We include two different models in our experiments and analyses. Following She
et al. [2024], we conduct our experiments on MistralMathOctopus-7B2and MetaMathOctopus-
7B3. MistralMathOctopus is obtained by fine-tuning MetaMath-Mistral [Yu et al., 2023] with
MGSM8KInstruct [Chen et al., 2023]. MetaMathOctopus is obtained by fine-tuning MetaMath [Yu
et al., 2023] with MGSM8KInstruct. Considering limited computational resources and reproducibility,
we directly utilize the publicly released base models. Our analyses are mainly based on MistralMath-
Octopus in the main text and we report more results in the Appendix.
Datasets We conduct experiments on two representative mathematical reasoning benchmarks,
MGSM [Shi et al., 2022] and MSV AMP [Chen et al., 2023]. MGSM is a widely used benchmark for
multilingual mathematical reasoning evaluation. MSV AMP is an out-of-domain test set in contrast to
MGSM, which evaluates robustness and generalization [Zhu et al., 2024, She et al., 2024].
Languages Following She et al. [2024], we choose the following 10 different languages for analysis
in our work. As a pivot language, English (en) is used as the alignment target. We also choose
Chinese (zh), Russian (ru), German (de), French (fr), Spanish (es), Japanese (ja), Swahili (sw),
Thai (th) and Bengali (bn) as 9 representative non-English languages.
Implementations Due to limited computational resources, our exploration focuses on the most
effective DPO variant of MAPO [She et al., 2024]. We select 1, 4, and 8 tasks from the
NumGLUE [Mishra et al., 2022], an arithmetic reasoning benchmark, and translate questions into 9
languages, consistent with the MGSM, thereby creating a multilingual seed dataset. To construct
preference pairs, we sample responses using the corresponding base models and employ NLLB-200-
distilled-600M4as the translation model to obtain alignment scores. Finally, for each model and
each target language (excluding English), we gain 10,000 preference pairs. Training is conducted
using LoRA [Hu et al., 2022]. During the neuron selection stage, we perform force-decoding on
the responses of the MGSM or MSV AMP dataset to obtain the activation probabilities of neurons
for each language. Based on empirical results on development sets, we set the balancing coefficient
λ= 0.04and the threshold τ= 0.5. Additional implementation details are provided in Appendix B.
4.2 Language Neurons Identification
Based on the neuron identification algorithm introduced in §3.3, we identify the language-specific
neurons, language-related neurons, and language-agnostic neurons in the model. To further validate
the effectiveness of our algorithm, we follow Tang et al. [2024] by examining changes in the
perplexity of LLMs by deactivating the identified language neurons across different languages.
Experiments are conducted on both the base model and the aligned model, with results presented in
Figure 2. We report the results of both language-specific neurons and language neurons.
It can be found that whether deactivating language-specific neurons or all language neurons, the
results consistently exhibit the same pattern: the diagonal elements in each row show the highest
2https://huggingface.co/kevinpro/MistralMathOctopus-7B
3https://huggingface.co/kevinpro/MetaMathOctopus-7B
4https://huggingface.co/facebook/nllb-200-distilled-600M
5en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.01 0.03 -0.02 0.00 0.00 0.01 -0.01 0.01 0.03 0.05
-0.00 0.06 0.01 -0.01 0.00 -0.00 -0.01 -0.00 -0.00 0.01
0.01 0.00 0.02 0.00 0.01 0.01 -0.00 0.00 0.00 -0.01
-0.02 -0.00 0.00 0.53 -0.01 -0.02 -0.01 -0.00 -0.00 -0.00
-0.01 0.02 0.01 0.02 0.51 0.00 0.00 0.01 0.01 0.02
-0.01 -0.01 -0.01 -0.00 -0.02 0.05 -0.00 0.00 0.00 0.00
-0.03 -0.01 -0.02 -0.02 -0.02 -0.01 0.09 0.00 -0.01 -0.00
0.14 0.11 0.10 0.10 0.10 0.05 0.03 0.97 0.03 0.09
-0.03 -0.02 -0.00 -0.01 -0.01 0.02 0.00 -0.00 2.26 0.00
-0.00 0.00 -0.00 -0.02 -0.04 -0.01 -0.01 0.01 0.03 0.87
0.00.51.01.52.0(a) Base - Language-Specific Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language-0.01 0.05 -0.05 0.02 -0.01 0.01 -0.01 0.01 0.02 0.04
0.04 0.13 -0.02 -0.00 0.00 -0.01 -0.02 -0.00 -0.01 0.04
0.01 0.01 0.05 0.02 0.02 0.01 0.02 0.00 0.00 0.04
0.02 0.00 -0.02 0.75 0.01 -0.01 -0.00 -0.00 -0.00 0.02
-0.01 0.03 0.02 0.04 0.52 0.01 0.02 0.01 0.01 0.08
0.04 0.01 -0.02 0.01 -0.01 0.07 0.00 0.00 0.00 0.03
-0.05 -0.01 -0.04 -0.01 -0.02 -0.01 0.08 -0.00 -0.01 0.02
-0.02 -0.00 -0.03 0.00 -0.03 -0.09 -0.08 0.72 -0.03 -0.01
-0.03 -0.02 -0.02 -0.01 -0.02 -0.03 -0.01 -0.00 1.79 0.03
-0.00 -0.01 -0.01 0.00 -0.01 0.00 -0.01 0.01 0.02 0.91
 0.000.250.500.751.001.251.501.75 (b) Aligned - Language-Specific Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.00 0.10 0.01 -0.01 0.00 0.38 0.29 0.08 0.58 0.06
0.00 5.42 0.32 0.24 0.24 0.22 0.15 0.12 0.18 0.16
-0.01 0.03 2.07 0.36 0.25 0.02 0.20 0.01 0.04 0.20
-0.07 -0.04 0.30 3.43 0.28 0.05 0.12 0.01 0.10 0.20
0.04 0.08 0.29 0.32 4.12 0.07 0.14 0.05 0.08 0.30
0.07 0.31 0.16 0.12 0.16 5.15 2.41 0.34 0.44 0.24
-0.05 0.57 0.17 0.18 0.23 2.63 15.17 1.14 1.07 0.25
0.16 1.32 0.23 0.26 0.38 1.45 5.33 19.49 2.50 0.64
-0.00 0.79 0.07 0.22 0.19 2.06 3.66 2.71 28.19 0.51
-0.08 0.07 0.20 0.26 0.32 0.05 0.16 0.12 0.14 3.32
0510152025
(c) Base - Language Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.02 0.15 -0.01 0.01 -0.02 0.54 0.31 0.10 0.85 0.07
0.05 4.60 0.42 0.37 0.34 0.22 0.22 0.06 0.12 0.27
-0.03 0.05 2.61 0.53 0.38 0.03 0.22 -0.01 0.04 0.34
-0.03 0.00 0.40 5.21 0.44 -0.00 0.11 -0.00 0.13 0.32
0.04 0.12 0.37 0.48 4.69 0.05 0.19 0.04 0.09 0.41
0.08 0.29 0.23 0.19 0.26 5.34 2.36 0.29 0.44 0.31
-0.00 0.39 0.27 0.25 0.35 2.75 13.97 0.79 1.07 0.48
0.03 0.69 0.08 0.26 0.35 0.78 4.44 13.63 2.50 0.70
0.03 0.71 0.07 0.33 0.27 2.12 3.17 2.12 23.49 0.75
-0.01 0.08 0.27 0.39 0.37 0.07 0.21 0.11 0.18 3.51
05101520 (d) Aligned - Language Neurons
Figure 2: PPL changes of MistralMathOctopus on MGSM after deactivating language-specific
neurons or language neurons. “Base” indicates the results of the base model. “Aligned” indicates
the results of the aligned model. For comparison, the results of Tang et al. [2024] are provided in
Appendix D.
values. Notably, deactivating language neurons leads to a more pronounced effect compared to
deactivating only language-specific neurons. These observations support the following findings: (1)
Our algorithm effectively identifies language-specific and language-related neurons; (2) For a given
language, in addition to its language-specific neurons, there are also a substantial number of shared
language-related neurons contributing to its performance; (3) Deactivating all the language-related
neurons of one language doesn’t cause significant impacts on the model’s performance in other
languages. The above findings confirm the validity of the language neurons identified by our method
and further provide insights into the characteristics of language neurons.
4.3 Layer-wise Functionality Analysis
Based on the identified neurons, we perform layer-wise functional analyses of all layers in the LLMs.
We begin by analyzing the distributions of different types of neurons in the base model. And we
report the results in Figure 3. Through the analysis of the distribution of different types of neurons,
we can further divide the LLMs’ internal process for multilingual inference into four parts rather than
the three-stage division discussed in some works [Wendler et al., 2024, Zhao et al., 2024b]:
1.Multilingual Understanding: In the initial layers, the number of language neu-
rons (language-specific and language-related neurons) peaks, while the number of language-
agnostic neurons is relatively low. The model maps multilingual inputs into a unified
semantic space at this stage.
2.Shared Semantic Space Reasoning: In the intermediate layers, the model engages in
reasoning within a shared semantic space across different languages. During this stage,
language neurons are largely absent, whereas language-agnostic neurons become dominant.
3.Multilingual Output Space Transformation: The model transfers features into the multi-
lingual output space in this stage in preparation for generating the final output. In this part,
6051015202530Layer050010001500200025003000
NumberLanguage-Specific
Language-Related
Language-AgnosticFigure 3: Layer-wise distribution of the different types of neurons of MistralMathOctopus on MGSM.
the number of language neurons reaches a peak again, while the number of language-agnostic
neurons drops to the lowest point.
4.Vocabulary Space Outputting: In the last layer, the model maps vectors of different
languages into a shared vocabulary space to generate outputs. The number of both language-
related and language-agnostic neurons rises sharply, while the language-specific neurons are
fewer than those in several previous layers.
Meanwhile, the distribution of different types of neurons aligns with the conclusions from existing
studies mentioned above. Overall, we can find that the number of neurons varies correspondingly
with the different inference stages of LLMs.
4.4 Layer-wise Neuron Changes Analysis
In §4.3, we investigate the fundamental distribution of neurons and the basic partitioning of
the model. We further analyze the changes in different types of neurons before and after
multilingual alignment. Based on the four functional stages in LLMs, we quantify the layer-
wise changes ( ∆) in the number of different types of neurons. Figure 4 presents the re-
sults for language-specific neurons, language-related neurons, and language-agnostic neurons.
0 5 10 15 20 25 30
Layer25
0255075100125150
Language-Specific
Language-Related
Language-Agnostic
Figure 4: Layer-wise changes in the number of dif-
ferent types of neurons of MistralMathOctopus on
MGSM.During the multilingual understanding stage,
the number of language neurons increases,
while language-agnostic neurons decrease. In
the subsequent shared semantic space rea-
soning stage, language-agnostic neurons in-
crease substantially, whereas language neu-
rons remain stable and nearly absent. Then,
in the third stage, as language-agnostic neu-
rons decrease, language neurons increase over-
all. Additionally, we notice that the number
of language-related neurons show an upward
trend. Finally, in the last stage, the number
of language-agnostic increases significantly
in the aligned model, accompanied by a re-
duction in language neurons. We also report
the results of different checkpoints during the
alignment process in Appendix F.
Overall, we find that language neurons and
language-agnostic neurons exhibit generally
opposite trends across different layers, which
71 2 3 4 5 6 7 8 9 10
N600
400
200
02004006008001000
-486-23662154209161201 200171971Figure 5: Changes in the number of neurons shared by Nlanguages after alignment.
Table 1: Accuracy of the MistralMathOctopus base model and aligned model on MGSM. "X/Y ⇒T"
indicates that languages X and Y are used for multilingual alignment.
MGSM bn th sw ja zh ru de es fr en Avg.
base 43.6 53.2 50.4 55.6 59.6 59.2 61.2 62.8 56.8 75.6 57.8
zh/de⇒en46.4 55.6 59.2 56.8 64.0 71.2 66.8 71.2 69.2 75.2 63.6
sw/th⇒en48.8 58.8 59.2 56.4 68.4 68.4 69.2 69.6 70.4 77.6 64.7
corresponds to the characteristics of LLMs at different stages of inference. Especially, at the last
stage, language-agnostic neurons play an more important role than language neurons. Multilingual
alignment facilitates more effective activation of the appropriate neurons at each stage, thereby
improving the model’s capability to handle multilingual tasks.
4.5 Macroscopic Analysis of Different Types of Neurons
We further conduct macroscopic analysis for different types of neurons. In our neuron identification
algorithm introduced in §3.3, the number of languages that share a specific neuron is an attribution
characterizing all types of activated neurons. Since our study involves 10 languages, the valid range of
Nis from 1to10. Among these, values of Nfrom2to9correspond to language-related neurons. As
special cases in our work, N= 1represents language-specific neurons, while N= 10 corresponds
to language-agnostic neurons.
We report the changes in the number of neurons after multilingual alignment for each value of N
ranging from 1 to 10 in Figure 5. The results show a decrease in the number of language-specific
neurons, while an increase in the number of language-related neurons, which are shared across
multiple languages. This indicates that multilingual alignment encourages LLMs to develop and
utilize more shared language-related neurons, rather than on language-specific neurons, which are
applicable to only a single language. Meanwhile, during the alignment process, the model improves
its understanding of task-relevant common knowledge. Therefore, the overall number of language-
agnostic neurons also increases significantly.
4.6 Spontaneous Multilingual Alignment Analysis
The important “Spontaneous Multilingual Alignment” phenomenon is first revealed and discussed
by Zhang et al. [2024], which means that conducting alignment in a small number of languages
significantly improves multilingual alignment even between English and many languages unseen
during the alignment process. We further analyze this phenomenon in our experiments. As shown
in Table 1, spontaneous multilingual alignment also emerges under the multilingual alignment
strategy employed in our study. Except for the languages used for alignment, LLMs exhibit notable
performance gain in other unaligned languages.
8Table 3: Average number of different types of
neurons for English and non-English languages
of MistralMathOctopus on MGSM. We round
the results to the nearest integer.
Language Language-Specific Language-Related
English 46 603
non-English 613 2006Table 4: Overlap ratio of different types of neu-
rons across different domains andbefore and
after alignment . Following She et al. [2024],
MSV AMP is regarded as an out-of-domain
dataset. The results of MistralMathOctopus on
MGSM are used as the fiducial value.
Variable (%) Language-Specific Language-Related
Domain 80.7 92.3
Alignment 95.6 92.1
To understand how multilingual alignment generalizes to other languages, we analyze the changes
in different types of neurons before and after multilingual alignment based on our method.
Table 2: Average results of neuron count changes
across multiple languages. “Trained” indicates the
trained languages in the spontaneous multilingual
alignment experiment. “Others” indicates other lan-
guages except the trained languages. We round the
results to the nearest integer.
Language Language-Specific Language-Related
Trained -37 +232
Others -36 +205Taking the case of “zh/de ⇒en” as a represen-
tative example, we report the average results
in Table 2. For the trained languages, the num-
ber of language-specific neurons decreases,
while the number of language-related neu-
rons increases. This indicates that the aligned
languages tend to utilize more language-
related neurons shared with other languages
rather than exclusive language-specific neu-
rons. Moreover, we extend this analysis to lan-
guages other than the trained languages and
observe a similar phenomenon. These findings
indicate that multilingual alignment facilitates
the use of language-related neurons while reducing the reliance on language-specific neurons in both
trained and other unseen languages. We hypothesize that the new language-related neurons shared
with trained languages contribute to the performance improvement on other unseen languages.
4.7 Further Analysis
Uniqueness of English Since current LLMs are primarily pretrained on English data, English is
often regarded as playing a special role within LLMs [Wendler et al., 2024]. In our experiments, we
also observe that English exhibits markedly different characteristics compared to other non-English
languages. Based on the identified neurons in our work, in Figure 2, we can find that deactivating
the language neurons of English has a negligible impact on the model’s performance in English,
which is entirely different from the behavior observed in other languages. This is also consistent
with the results of Tang et al. [2024]. Furthermore, based on this finding, we quantify the number
of language neurons for English and non-English languages based on the MistralMathOctopus
base model (Table 3). Our analysis reveals that English has significantly fewer neurons than other
languages, both in terms of language-specific and language-related neurons. We hypothesize that
this is due to the fact that English actually possesses numerous language-related neurons. And since
English serves as a pivot language, these language-related neurons are likely shared with almost all
other languages. It causes them to be confounded with language-agnostic neurons.
Stability of Neuron Distributions We discuss the stability of neuron distributions across different
data domains , as well as before and after alignment . To quantify the stability of neuron distributions,
we compute the neuron overlap ratio in both settings, with the results summarized in Table 4. We can
find that although the exact positions of a few language neurons may vary across different settings, the
positional distribution of most language neurons remains stable. This also indicates good reliability
and generalization of the language neurons identified under fixed hyperparameters.
5 Conclusion
In this work, we systematically investigate the multilingual alignment from the perspective of
language neurons. We propose a new language neuron identification algorithm based on entropy
and probability value, which detects the language-specific neurons, language-related neurons, and
9language-agnostic neurons in LLMs. The validity of the identified neurons is confirmed through
deactivation ablation experiments. Furthermore, we examine the multilingual alignment mechanism
by analyzing the roles of different types of neurons. Based on their distributional characteristics, we
categorize LLMs’ internal process into four functional parts. Our analysis reveals that multilingual
alignment enhances the model’s utilization of the corresponding types of neurons across different
functional parts. Meanwhile, we find that alignment promotes a greater reliance on shared language-
related neurons across languages, rather than on language-specific neurons. We also explore the
phenomenon of spontaneous multilingual alignment. Additionally, we provide further analysis and
more empirical results based on the preceding findings.
References
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of
models. arXiv preprint arXiv:2407.21783 , 2024.
An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,
Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint
arXiv:2412.15115 , 2024.
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,
Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint
arXiv:2412.19437 , 2024.
Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, and Yeyun Gong.
Process-based self-rewarding language models. arXiv preprint arXiv:2503.03746 , 2025.
Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao, Ting Song, Yan Xia, and Furu
Wei. Not all languages are created equal in llms: Improving multilingual capability by cross-
lingual-thought prompting. arXiv preprint arXiv:2305.07004 , 2023.
Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen,
and Lei Li. Multilingual machine translation with large language models: Empirical results and
analysis. arXiv preprint arXiv:2304.04675 , 2023.
Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen, Xin Huang, Xue Han, Junlan Feng, Chao
Deng, and Shujian Huang. Getting more from less: Large language models are good spontaneous
multilingual learners. arXiv preprint arXiv:2405.13816 , 2024.
Minheng Ni, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lijuan Wang, Dongdong Zhang,
and Nan Duan. M3p: Learning universal representations via multitask multilingual multimodal pre-
training. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition ,
pages 3977–3986, 2021.
Zihan Liu, Genta Indra Winata, and Pascale Fung. Continual mixed-language pre-training for
extremely low-resource neural machine translation. arXiv preprint arXiv:2105.03953 , 2021.
Shaoxiong Ji, Zihao Li, Indraneil Paul, Jaakko Paavola, Peiqin Lin, Pinzhen Chen, Dayyán O’Brien,
Hengyu Luo, Hinrich Schütze, Jörg Tiedemann, et al. Emma-500: Enhancing massively multilin-
gual adaptation of large language models. arXiv preprint arXiv:2409.17892 , 2024.
Jun Zhao, Zhihao Zhang, Luhui Gao, Qi Zhang, Tao Gui, and Xuanjing Huang. Llama beyond
english: An empirical study on language capability transfer. arXiv preprint arXiv:2401.01055 ,
2024a.
Shuaijie She, Wei Zou, Shujian Huang, Wenhao Zhu, Xiang Liu, Xiang Geng, and Jiajun Chen. Mapo:
Advancing multilingual reasoning through multilingual alignment-as-preference optimization.
arXiv preprint arXiv:2401.06838 , 2024.
Yiran Zhao, Wenxuan Zhang, Guizhen Chen, Kenji Kawaguchi, and Lidong Bing. How do large
language models handle multilingualism? arXiv preprint arXiv:2402.18815 , 2024b.
10Tianyi Tang, Wenyang Luo, Haoyang Huang, Dongdong Zhang, Xiaolei Wang, Xin Zhao, Furu
Wei, and Ji-Rong Wen. Language-specific neurons: The key to multilingual capabilities in large
language models. arXiv preprint arXiv:2402.16438 , 2024.
Juuso Eronen, Michal Ptaszynski, and Fumito Masui. Zero-shot cross-lingual transfer language
selection using linguistic similarity. Information Processing & Management , 60(3):103250, 2023.
Yiran Zhao, Wenxuan Zhang, Huiming Wang, Kenji Kawaguchi, and Lidong Bing. Adamergex:
Cross-lingual transfer with large language models via adaptive adapter merging. arXiv preprint
arXiv:2402.18913 , 2024c.
Nostalgebraist. interpreting gpt: the logit lens. https://www.lesswrong.com/posts/
AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens , 2020.
Basel Mousi, Nadir Durrani, Fahim Dalvi, Majd Hawasly, and Ahmed Abdelali. Exploring align-
ment in shared cross-lingual spaces. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar,
editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers) , pages 6326–6348, Bangkok, Thailand, August 2024. As-
sociation for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.344. URL https:
//aclanthology.org/2024.acl-long.344/ .
Chris Wendler, Veniamin Veselovsky, Giovanni Monea, and Robert West. Do llamas work in english?
on the latent language of multilingual transformers. In Proceedings of the 62nd Annual Meeting
of the Association for Computational Linguistics (Volume 1: Long Papers) , pages 15366–15394,
2024.
Peng Hu, Sizhe Liu, Changjiang Gao, Xin Huang, Xue Han, Junlan Feng, Chao Deng, and Shujian
Huang. Large language models are cross-lingual knowledge-free reasoners. arXiv preprint
arXiv:2406.16655 , 2024.
Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka, and Yutaka Matsuo. On the multi-
lingual ability of decoder-based pre-trained language models: Finding and controlling language-
specific neurons. arXiv preprint arXiv:2404.02431 , 2024.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea
Finn. Direct preference optimization: Your language model is secretly a reward model. Advances
in Neural Information Processing Systems , 36:53728–53741, 2023.
Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 , 2020.
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo
Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for
large language models. arXiv preprint arXiv:2309.12284 , 2023.
Nuo Chen, Zinan Zheng, Ning Wu, Ming Gong, Dongmei Zhang, and Jia Li. Breaking lan-
guage barriers in multilingual mathematical reasoning: Insights and observations. arXiv preprint
arXiv:2310.20246 , 2023.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush V osoughi,
Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are mul-
tilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057 , 2022.
Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, and Alexandra Birch. Question
translation training for better multilingual reasoning. arXiv preprint arXiv:2401.07817 , 2024.
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral,
and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning
tasks. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the
60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,
pages 3505–3523, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi:
10.18653/v1/2022.acl-long.246. URL https://aclanthology.org/2022.acl-long.246/ .
11Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022.
Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan
Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. Trl: Transformer reinforcement
learning. https://github.com/huggingface/trl , 2020.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimiza-
tions enable training deep learning models with over 100 billion parameters. In Proceedings of
the 26th ACM SIGKDD international conference on knowledge discovery & data mining , pages
3505–3506, 2020.
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model
serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating
Systems Principles , 2023.
12A Limitations
We provide insights and analysis results for multilingual alignment and multilingual capabilities of
LLMs, which allows for a better understanding of multilingualism. Despite we have conducted a
systematic investigation in our work, there are still some limitations waiting for research. Due to the
limited resources, we only conduct experiments on two different models and two different datasets.
We are willing to perform a more comprehensive analysis on different scenarios if more resources are
available in the future. Additionally, we don’t perform a finer-grained analysis of neurons within the
same layer in this work. We would like to explore this in our future work.
B Implementation Details
Our experiments are conducted on 4 NVIDIA RTX A6000 GPUs or 4 NVIDIA GeForce RTX 3090
GPUs. We use the TRL [von Werra et al., 2020] and DeepSpeed [Rasley et al., 2020] frameworks for
preference alignment, and the vLLM engine Kwon et al. [2023] for inference.
B.1 MAPO
We employ the officially released scripts5to generate the preference data. For the alignment process,
we set the learning rate to 1e-6 and the batch size to 16. LoRA is utilized to fine-tune the model with
a LoRA rank of 64, a LoRA alpha of 128, and a LoRA dropout rate of 0.05. The total number of
training steps is set to 1000. It takes 7.5 hours for one alignment.
B.2 Language Neurons Identification
We spend 25 minutes calculating the activation probability of each neuron with MGSM dataset on a
single NVIDIA GeForce RTX 3090 GPU. We spend 2 hours calculating the activation probability of
each neuron with MSV AMP dataset on a single NVIDIA GeForce RTX 3090 GPU. After obtaining
the activation probability, it takes 3 minutes to identify how many languages each selected neuron is
related to.
C Licenses for Used Assets
We list the names of the licenses for each asset we utilized in our work:
• MGSM: CC-BY-SA-4.0
• MSV AMP: Apache-2.0
• GSM8KInstruct: Apache-2.0
• NumGLUE: Apache-2.0
• MistralMathOctopus: Apache-2.0
• MetaMathOctopus: Apache-2.0
• NLLB-200-distilled-600M: CC-BY-NC-4.0
• TRL: Apache-2.0
• DeepSpeed: Apache-2.0
• vLLM: Apache-2.0
D Deactivation Ablation Experiments
For the MistralMathOctopus model, Figure 6 presents the results of the deactivation ablation ex-
periments conducted on MGSM using the neuron identification algorithm proposed by Tang et al.
[2024]. Additionally, Figure 7 presents the results of the deactivation ablation experiments on the
out-of-domain test set, MSV AMP. To verify the generalization of our identification algorithm, we
also deploy the MetaMathOctopus to conduct the deactivation ablation experiment, with the result in
Figure 8.
5https://github.com/NJUNLP/MAPO
13E Layer-wise distribution of the different types of neurons
In Figure 9a and 9b, we separately display the layer-wise distribution of the different types of neurons
of MistarlMathOctopus on MSV AMP and MetaMathOctopus on MGSM to validate the generalization
of our finding discussed in §4.3.
F Layer-wise Changes in the Number of Different Types of Neurons During
Alignment
Figure 10 shows the same pattern discussed in §4.4, thereby supporting its generalization. Fur-
thermore, for each dataset, we examine the layer-wise changes in the number of different types of
neurons in the process of alignment in Figure 11, 12 and 13. The notation “ ckpt-x” denotes the model
checkpoint obtained after xtraining steps.
G Spontaneous Multilingual Alignment
We report the accuracy of the base model on the MGSM and MSV AMP benchmarks in Table 5.
The expected accuracy of random guessing is 50.0%. "X/Y ⇒T" indicates that languages X and
Y are used for alignment. The best results for each language are highlighted. It is evident that
models trained on multilingual translation data substantially outperform the original model across
a wide range of languages, indicating that multilingual training significantly enhances the model’s
multilingual capabilities.
14en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.01 0.02 -0.01 -0.00 -0.00 -0.05 -0.02 0.00 -0.00 0.01
0.02 0.87 0.07 0.05 0.04 0.06 0.03 0.06 0.10 0.05
0.01 0.00 0.21 0.05 0.04 -0.00 0.06 -0.00 0.01 0.06
-0.01 0.01 -0.02 0.40 0.03 -0.02 0.00 -0.00 0.02 0.06
0.02 0.06 0.03 0.05 0.93 0.03 0.01 0.01 0.02 0.10
0.00 -0.01 -0.01 -0.04 -0.01 0.87 0.59 0.09 0.22 0.04
-0.05 0.06 -0.05 -0.04 0.01 0.19 7.32 0.40 0.62 0.03
0.01 0.28 -0.03 -0.01 -0.00 -0.01 1.64 8.65 0.86 0.07
-0.01 0.13 -0.02 -0.01 -0.01 0.07 0.83 0.96 14.82 0.06
-0.02 0.00 -0.04 -0.02 -0.03 -0.00 0.02 0.02 0.02 1.33
02468101214(a) Base - Language-Specific Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.03 0.01 -0.04 0.01 -0.02 -0.07 -0.03 -0.00 -0.01 0.03
-0.00 0.79 0.06 0.08 0.04 0.03 -0.01 0.01 0.04 0.05
0.05 -0.01 0.25 0.10 0.06 -0.04 0.06 -0.01 -0.00 0.06
-0.00 -0.11 -0.01 0.67 0.02 -0.05 -0.02 -0.01 0.00 0.06
-0.02 0.04 0.04 0.07 0.96 -0.00 0.00 -0.00 -0.00 0.10
0.01 0.00 -0.01 -0.03 -0.01 0.93 0.62 0.08 0.19 0.05
-0.03 0.03 -0.05 -0.04 0.03 0.23 6.48 0.34 0.55 0.07
0.01 0.07 -0.03 0.02 0.02 -0.13 1.39 6.67 0.75 0.08
-0.02 0.11 -0.03 0.03 0.00 0.02 0.64 0.88 11.88 0.08
-0.03 -0.03 -0.02 -0.01 -0.01 -0.02 0.01 0.01 0.01 1.32
0246810 (b) Aligned - Language-Specific Neurons
Figure 6: PPL changes of MistralMathOctopus on MGSM when deploying the algorithm proposed
by Tang et al. [2024].
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.04 0.02 -0.06 0.00 -0.02 -0.02 -0.02 -0.00 0.00 0.03
-0.01 0.03 0.01 -0.02 0.01 -0.00 -0.01 -0.00 -0.01 -0.01
0.04 0.00 -0.04 -0.01 0.02 0.01 0.00 0.01 -0.00 0.04
-0.01 -0.00 -0.01 0.17 0.01 -0.01 -0.01 -0.00 0.00 0.01
-0.01 0.02 0.01 0.02 0.17 0.02 0.01 0.01 0.00 0.03
-0.04 -0.01 -0.03 -0.02 -0.02 0.04 -0.00 -0.00 -0.00 -0.01
-0.08 -0.03 -0.06 -0.04 -0.04 -0.02 0.09 -0.01 -0.01 -0.03
0.22 0.17 0.15 0.17 0.15 0.11 0.05 1.02 0.06 0.14
-0.01 -0.03 -0.01 -0.02 -0.02 0.01 -0.01 -0.00 2.71 -0.01
-0.05 -0.01 -0.01 -0.01 -0.02 -0.01 -0.01 0.01 0.04 0.68
0.00.51.01.52.02.5
(a) Base - Language-Specific Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.01 0.03 -0.12 0.01 -0.05 -0.04 -0.01 -0.01 -0.00 0.05
0.05 0.02 -0.03 -0.02 0.02 -0.02 -0.00 -0.01 -0.01 0.05
-0.02 0.02 -0.06 0.01 0.02 -0.00 0.01 0.00 -0.00 0.04
-0.05 0.01 -0.01 0.73 0.02 -0.01 0.00 0.00 -0.00 0.07
0.02 0.03 -0.00 0.04 0.21 0.01 0.01 0.01 0.00 0.10
-0.05 0.03 -0.02 0.00 0.00 0.05 0.01 -0.00 0.00 0.08
-0.13 -0.04 -0.05 -0.03 -0.04 -0.04 0.10 -0.01 -0.01 0.05
-0.04 0.01 -0.03 0.02 0.01 -0.04 -0.02 0.70 0.00 0.02
-0.05 -0.01 0.00 0.01 0.00 0.00 -0.00 -0.00 2.22 0.08
-0.04 -0.02 -0.00 0.02 0.01 0.01 -0.02 0.01 0.01 1.00
 0.00.51.01.52.0 (b) Aligned - Language-Specific Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.07 0.01 -0.02 -0.01 -0.06 0.65 0.23 0.09 0.68 0.02
-0.01 3.86 0.15 0.13 0.09 0.30 0.12 0.11 0.13 0.11
0.01 -0.01 1.06 0.12 0.13 0.04 0.07 0.00 0.02 0.10
-0.02 -0.16 0.05 2.11 0.14 0.06 0.04 0.00 0.04 0.12
0.00 0.00 0.11 0.13 2.30 0.08 0.06 0.04 0.04 0.19
0.00 0.23 0.08 0.05 0.03 4.36 1.90 0.54 0.68 0.13
-0.19 0.25 -0.13 0.28 0.11 1.76 10.79 0.98 0.88 0.13
0.14 0.83 0.14 0.25 0.33 1.06 3.74 15.65 1.86 0.34
-0.00 0.40 -0.09 0.16 0.06 1.17 2.63 2.24 22.69 0.17
-0.19 -0.02 0.00 0.12 0.12 0.09 0.08 0.06 0.07 2.27
05101520
(c) Base - Language Neurons
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.05 0.06 -0.04 0.03 -0.08 0.55 0.22 0.09 0.63 0.07
-0.06 3.87 0.14 0.13 0.11 0.29 0.14 0.04 0.06 0.18
-0.06 0.01 1.57 0.23 0.22 0.08 0.11 -0.02 -0.01 0.23
0.00 -0.01 0.18 3.43 0.26 0.08 0.04 -0.01 0.01 0.25
0.10 0.00 0.24 0.28 3.07 0.16 0.09 0.03 0.01 0.31
-0.07 0.51 0.09 0.17 0.09 6.01 2.47 0.57 0.78 0.20
0.08 0.61 0.14 0.30 0.33 2.68 11.97 0.81 1.04 0.39
0.12 0.87 0.10 0.31 0.31 1.01 3.85 12.26 1.98 0.44
0.14 0.78 0.11 0.38 0.31 1.87 3.16 2.14 21.28 0.47
-0.14 -0.01 0.11 0.27 0.25 0.10 0.05 0.05 0.01 2.87
0.02.55.07.510.012.515.017.520.0 (d) Aligned - Language Neurons
Figure 7: PPL changes of MistralMathOctopus on MSV AMP after deactivating language-specific
neurons or language neurons. “Base” indicates the results of the base model. “Aligned” indicates the
results of the aligned model.
15en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.07 -0.02 -0.05 0.00 -0.00 0.45 0.24 0.03 0.03 0.05
-0.11 2.55 0.05 0.04 0.01 0.21 0.07 0.11 0.01 0.01
-0.02 -0.05 0.24 0.07 0.01 0.40 0.20 0.01 0.02 0.07
-0.04 0.02 0.07 0.46 0.08 0.01 -0.04 0.01 0.02 0.07
-0.04 0.16 0.07 0.14 0.49 0.32 0.15 0.02 0.03 0.02
0.01 0.46 -0.03 0.01 0.02 0.61 0.37 0.50 0.20 0.02
-0.06 0.47 -0.04 0.02 0.14 0.61 1.04 0.75 0.52 0.03
0.03 1.15 0.01 0.01 0.15 0.62 0.64 2.18 0.86 0.35
0.05 0.49 0.04 0.06 0.01 0.64 0.77 1.26 2.52 0.17
-0.01 0.14 0.11 0.09 0.12 0.38 0.13 0.09 0.01 1.78
0.00.51.01.52.02.5(a) Base - Language Neurons on MGSM
en ru es frde zh ja bn th sw
T arget Languageen ru esfr de zhja bn th swDeactivated Language0.10 0.03 -0.02 -0.01 0.08 0.46 0.27 0.03 0.06 0.09
-0.09 2.35 0.08 0.02 0.04 0.20 0.11 0.08 0.02 0.01
-0.00 -0.01 0.29 0.06 0.04 0.36 0.20 0.01 0.03 0.09
-0.01 -0.01 0.13 0.48 0.14 0.06 -0.00 0.02 0.05 0.11
-0.05 0.19 0.09 0.09 0.55 0.32 0.14 0.02 0.05 0.02
0.03 0.46 0.02 -0.03 0.07 0.63 0.41 0.46 0.20 0.02
0.03 0.53 0.00 0.01 0.25 0.61 1.19 0.65 0.49 0.04
0.10 1.09 0.09 0.02 0.26 0.64 0.73 2.05 0.83 0.39
0.06 0.59 0.08 0.03 0.07 0.67 0.94 1.19 2.45 0.16
0.09 0.14 0.17 0.11 0.21 0.38 0.17 0.08 0.04 1.84
0.00.51.01.52.0 (b) Aligned - Language Neurons on MGSM
Figure 8: PPL changes of MetaMathOctopus on MGSM after deactivating language neurons. “Base”
indicates the results of the base model. “Aligned” indicates the results of the aligned model.
051015202530Layer050010001500200025003000
NumberLanguage-Specific
Language-Related
Language-Agnostic
(a) MistralMathOctopus on MSV AMP.
051015202530Layer050010001500200025003000
NumberLanguage-Specific
Language-Related
Language-Agnostic (b) MetaMathOctopus on MGSM.
Figure 9: Layer-wise distribution of the different types of neurons.
0 5 10 15 20 25 30
Layer15
10
5
051015
Language-Specific
Language-Related
Language-Agnostic
Figure 10: Layer-wise changes in the number of MetaMathOctopus
160 5 10 15 20 25 30
Layer80
60
40
20
020
ckpt-200
ckpt-400
ckpt-600(a) MGSM
0 5 10 15 20 25 30
Layer120
100
80
60
40
20
02040
ckpt-200
ckpt-400
ckpt-600 (b) MSV AMP
Figure 11: Layer-wise changes in the number of language-specific neurons of MistralMathOctopus
during the alignment.
0 5 10 15 20 25 30
Layer100
80
60
40
20
0204060
ckpt-200
ckpt-400
ckpt-600
(a) MGSM
0 5 10 15 20 25 30
Layer120
100
80
60
40
20
02040
ckpt-200
ckpt-400
ckpt-600 (b) MSV AMP
Figure 12: Layer-wise changes in the number of language-related neurons of MistralMathOctopus
during the alignment.
0 5 10 15 20 25 30
Layer100
0100200300400500
ckpt-200
ckpt-400
ckpt-600
(a) MGSM
0 5 10 15 20 25 30
Layer100
0100200300400500
ckpt-200
ckpt-400
ckpt-600 (b) MSV AMP
Figure 13: Layer-wise changes in the number of language-agnostic neurons of MistralMathOctopus
during the alignment.
17Tested on MGSM bn th sw ja zh ru de es fr en Avg.
base 43.6 53.2 50.4 55.6 59.6 59.2 61.2 62.8 56.8 75.6 57.8
zh⇒en 49.6 58.4 54.8 56.4 65.2 70.0 66.4 72.8 68.8 78.4 64.1
zh/de⇒en 46.4 55.6 59.2 56.8 64.0 71.2 66.8 71.2 69.2 75.2 63.6
sw/th⇒en 48.8 58.8 59.2 56.4 68.4 68.4 69.2 69.6 70.4 77.6 64.7
zh/es/ru⇒en 46.0 56.4 58.8 54.8 63.2 70.8 68.8 71.6 69.6 76.8 63.7
zh/es/fr/ja/de/sw/ru/th/bn ⇒en49.6 60.0 56.4 56.4 64.4 64.8 65.2 65.2 61.2 76.4 62.0
Tested on MSV AMP bn th sw ja zh ru de es fr en Avg.
base 49.3 62.5 60.6 60.9 67.4 64.9 66.5 67.6 67.2 77.0 64.4
zh⇒en 52.8 62.5 60.9 63.7 66.6 67.5 69.4 69.8 69.7 76.5 65.9
zh/de⇒en 53.2 62.0 62.9 65.3 66.1 68.7 69.9 69.0 70.1 77.0 66.4
sw/th⇒en 53.8 67.4 65.1 67.7 71.9 71.4 71.9 72.0 72.8 78.9 69.3
zh/es/ru⇒en 52.9 63.2 61.5 63.0 67.8 68.1 69.8 68.7 70.6 78.3 66.4
zh/es/fr/ja/de/sw/ru/th/bn ⇒en54.9 65.9 65.6 68.6 69.7 70.2 72.3 71.6 71.6 77.3 68.8
Table 5: Accuracy of the base model and aligned variants on benchmarks.
18