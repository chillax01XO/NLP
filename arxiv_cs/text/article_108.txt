arXiv:2505.20890v1  [cs.CV]  27 May 2025Frequency Composition for Compressed and Domain-Adaptive Neural Networks
Yoojin Kwon1*Hongjun Suh1*Wooseok Lee1*Taesik Gong2Songyi Han3Hyung-Sin Kim1
1Seoul National University2UNIST3Google
{ideastraw, hjsuh319, andylws, hyungkim }@snu.ac.kr
taesik.gong@unist.ac.kr
syhan@google.com
Abstract
Modern on-device neural network applications must op-
erate under resource constraints while adapting to un-
predictable domain shifts. However, this combined
challenge—model compression and domain adaptation—
remains largely unaddressed, as prior work has tackled
each issue in isolation: compressed networks prioritize effi-
ciency within a fixed domain, whereas large, capable mod-
els focus on handling domain shifts. In this work, we
propose CoDA , a frequency composition-based framework
that unifies Compression and Domain Adaptation. Dur-
ing training, CoDA employs quantization-aware training
(QAT) with low-frequency components, enabling a com-
pressed model to selectively learn robust, generalizable fea-
tures. At test time, it refines the compact model in a source-
free manner (i.e., test-time adaptation, TTA), leveraging the
full-frequency information from incoming data to adapt to
target domains while treating high-frequency components
as domain-specific cues. CoDA can be integrated synergis-
tically into existing QAT and TTA methods. CoDA is eval-
uated on widely used domain-shift benchmarks, including
CIFAR10-C and ImageNet-C, across various model archi-
tectures. With significant compression, it achieves accu-
racy improvements of 7.96%p on CIFAR10-C and 5.37%p
on ImageNet-C over the full-precision TTA baseline.
1. Introduction
Resource constraints and domain shifts are critical hur-
dles for the practical deployment of deep neural networks
(DNNs). Research has traditionally tackled these issues
in isolation: compressed DNNs focus on efficiency within
a fixed target domain (often the same as the source do-
main) [37, 48], whereas large, capable models handle do-
main shifts [12, 20, 48]. However, various modern on-
device applications, such as extended reality, video surveil-
lance, agricultural monitoring, and autonomous robotics,
increasingly demand solutions that handle both [1, 3, 22,
*Equal contribution44]. In these scenarios, DNNs must run on resource-
constrained devices while simultaneously adapting to dy-
namic, evolving environments.
In this paper, we investigate the combined challenges
of model compression and domain shifts , paving the
way for robust and efficient DNNs in real-world settings.
To achieve this dual goal, we propose CoDA , an end-to-
end pipeline that spans both training and testing phases to
produce compact, robust, and adaptable models. Specif-
ically, we leverage frequency decomposition via a 2D
Fourier transformation [5], which partitions an image’s spa-
tial characteristics into distinct frequency components [6,
7, 21, 41, 42]. As each frequency component contains
non-overlapping information, higher magnitudes in low-
frequency components (LFC) suggest that slowly changing
patterns, such as smooth textures or surfaces, dominate the
image, whereas higher magnitudes in the high-frequency
components (HFC) indicate rapidly changing details such
as detailed object edges.
During training, CoDA aims to create a quantized model
that selectively acquires more generalizable knowledge
from the source domain. Since a compact model must
be highly selective due to limited capacity, we prioritize
broader generalization over capturing every fine-grained de-
tail in the source domain. Existing quantization-aware train-
ing (QAT) methods, however, do not support this scenario
since they focus solely on maximizing source-domain accu-
racy to match full-precision performance, rather than fos-
tering generalizability [37, 48]. To address this gap, we
conduct an empirical study revealing that QAT’s robustness
benefits significantly from strategic frequency decomposi-
tion (Section 3.2). In particular, training on reconstructed
images dominated by LFC enables the quantized model
to concentrate on learning generalized features, effectively
mitigating its capacity constraints.
After deployment, a compact model that has been ro-
bustly trained on LFC may still lack domain-specific de-
tails for diverse target domains. On resource-constrained
devices in dynamic environments, the model must adapt
1Model
ResNet18 ResNet50Method
w/o CoDA w/ CoDAQAT
FP LSQQAT
FP LSQ
0 2 4 6 8
Model size (# of bits) 1e82426283032343638Accuracy (%)(a) NORM-based TTA
0 2 4 6 8
Model size (# of bits) 1e82628303234363840 (b) TENT-based TTA
Figure 1. Effectiveness of CoDA when applied to various models
(ResNet18 and ResNet50), TTA methods (NORM [28, 31] and
TENT [34]) and QAT method (LSQ [10]) using three bitwidths (2,
4, and 8 bits). We train on ImageNet and evaluate on ImageNet-C.
to incoming, unlabeled test data in a source-free manner,
a process known as test-time adaptation (TTA) [4, 13, 27–
29, 31, 34, 47]. To meet this requirement, our test-phase
procedure refines the compact model using only target-
domain inputs, while preserving the general knowledge ac-
quired during training. Specifically, our TTA utilizes the
full-frequency components (FFC) of the test data to capture
richer, domain-specific details, treating the LFC (general)
and HFC (domain-specific) differently (Section 3.3). This
balanced strategy integrates both generalized and domain-
specific insights, resulting in a model that is both efficient
and robust.
To the best of our knowledge, CoDA is the first method to
address generalizability and test-time adaptability of quan-
tized models. Importantly, rather than competing with exist-
ing TTA and QAT methods, CoDA can be integrated into
these methods . Figure 1 demonstrates CoDA ’s effective-
ness on ResNet-18/50 [15] under domain shifts from Im-
ageNet [9] to ImageNet-C [17]. Notably, when combined
with CoDA , widely used TTA methods NORM [28, 31] and
TENT [34] show significant performance improvements on
quantized models. Compared to TTA-applied full-precision
models, CoDA delievers up to 5.06%p higher accuracy
on ResNet-18 and achieves comparable performance on
ResNet-50, all while reducing model size by 4-16 ×. We
further evaluate CoDA across various models and domain-
shift scenarios, comparing it against existing QAT and TTA
methods without using frequency composition. Our results
underscore CoDA ’s ability to simultaneously achieve supe-
rior accuracy and significant model compression, highlight-
ing the importance of integrating compression and adapta-
tion for emerging on-device applications.
2. Related Work
2.1. Frequency Decomposition for Computer Vision
Image data can be transformed between the image domain
and the frequency domain using 2D discrete cosine trans-formation or 2D Fourier transformation. The human visual
system primarily perceives the LFC of visual data, whereas
covolutional neural networks (CNNs) are capable of pro-
cessing both LFC and HFC [36]. Since labels are generated
by humans, who primarily rely on LFC, CNNs can addition-
ally exploit HFC during training. While, HFC are typically
learned later than LFC in the training process.
EfficientTrain [39] incorporates this understanding into
curriculum learning by initially training with images recon-
structed from LFC and gradually transitioning to FFCs in
the later stages. Another use of frequency decomposition is
domain generalization/adaptation to handle domain shifts.
Since each frequency component of images can reserve dis-
tinct information, FDA [42] utilizes LFC to create target-
style source images. Domain generalization is also explored
via middle-frequency components [21] and phase compo-
nents [7, 41]. Chattopadhyay et al. [6] mitigate the lack of
HFC in synthetic images for the Syn-to-Real task by adding
scaled noise to the amplitude.
2.2. Quantization-Aware Training
Quantization-Aware Training (QAT) aims to optimize both
the quantizer and model weights during training. As a re-
sult, QAT methods [10, 24, 45, 49] can achieve performance
close to that of their full-precision counterparts, unlike post-
training quantization (PTQ). LSQ [10] and LQ [45] train
uniform and non-uniform quantizers at train time, respec-
tively. However, existing QAT methods have been eval-
uated only in the source domain, leaving their robustness
in dynamic environments largely unexplored. The regular-
ization effect of QAT has been recognized since the first
QAT scheme emerged [8], yet little research has focused
on this property. Some recent studies have explored the
theoretical and domain generalization aspects of QAT mod-
els [2, 23, 46]. Our work combines frequency composition
with QAT, extending its potential for domain generalization.
2.3. Test-Time Adaptation
Test-Time Adaptation (TTA) aims to adapt a pretrained
model to distribution shifts between training and test data in
a source-free manner. A common approach involves adjust-
ing batch normalization (BN) layer statistics [27, 28, 31],
which include the running means and variances calculated
over the entire train data. Nado et al . [28] discard the
training-based BN statistics and recalculate them based
solely on the current test batch. In contrast, Schneider
et al. [31] and DUA [27] blend training-set statistics with
those derived from the test batch. Another line of work ad-
justs affine parameters in BN layers [13, 14, 29, 30, 34, 38].
Although these methods have shown promise for full-
precision models, our work is the first to explore TTA for
compact, quantized models.
2Train (Sour ce Domain)
ReLUQ
BN Convμs, σsLFC QA T
FFT  xlfc
EMA
+
flfc
fhfcμlfc, σlfc
μhfc, σhfcμt,
σtμs, σsxsource
fFABNReLUQ
FABN Convμt, σtTTA with F ABNTest (T arget Domain)
Frequency composition
xlfc xhfcxtarget
LFCs HFCsx
Figure 2. An illustration of the proposed CoDA .Left: Using Fast Fourier Transformation, an image can be decomposed into HFC
consisting of fast-changing patterns (i.e. edges or stripes) and LFC consisting of slow-changing patterns (i.e. smooth shape). During
training, CoDA focus on learning generalizable features from LFC rather than irregular patterns in HFC ( LFC QAT ).Right : At test time,
under domain shift, CoDA utilizes full-frequency of target data and adapts BN layers with our frequency-aware BN ( FABN ); In the lower
frequency of intermediate activations, we utilize running statistics of them to initialize and gradually update the BN statistics. Meanwhile,
in the higher frequency, we maintain the original distribution from the activation without composition into source distribution. Finally, both
statistics from low-/high-frequency are adequately combined and used to normalize the test batch.
3. Method
We present CoDA , an end-to-end pipeline designed to
achieve both model compression and test-time adaptation
for on-device applications. During training, to handle do-
main shifts with a low-precision model, we encourage QAT
to learning generalizable knowledge from the train data. At
test time, TTA should should distinguish domain-invariant
and domain-specific information in the target data to adapt
effectively without discarding general knowledge. To this
end, CoDA leverages frequency composition during both
training and test phases.
Our key intuition is that each frequency component in
an image, obtained via a Fourier transform, contains dis-
tinct information that affects training and testing differ-
ently under domain shifts. As shown in Figure 2, divid-
ing an image into LFC and HFC separates smooth and gen-
eral shapes (LFC) from finer and rapidly changing details
(HFC). Therefore, performing QAT on LFC-only data is
more robust under domain shifts compared to training on
FFCs. At test time, TTA employs FFCs to incorporate the
target domain’s specific details, treating information of dif-
ferent frequency range separately. Given that the model
learns solely from LFC during training, TTA adapts them to
align with the LFC of the target data while simultaneously
learning the HFC of the target data from scratch.
Building on these insights, CoDA integrates (1) low-
frequency components QAT (LFC QAT) and (2) TTA with
Frequency-aware Batch Normalization (FABN), improving
both generalizability and adaptability of quantized models.
3.1. Frequency Composition for Input Images
CoDA applies a 2D Fourier transformation F(·)on an in-
put image x, converting its spatial pixel patterns into multi-ple frequency components, z=F(x), in the 2D frequency
domain. The inverse, F−1(·), restores spatial information.
Given a radius threshold r, low-pass filtering LPF (z;r)
isolates only LFC, while high-pass filtering HPF (z;r)
captures only HFC [36, 39]. The LFC variant xlfcand HFC
variant xhfcof the original image xare reconstructed by
applying inverse Fourier transformation as follows:
xlfc=F−1(LPF (z;r)),xhfc=F−1(HPF (z;r)).(1)
Here, xlfcandxhfcrepresent distinct, non-overlapping pat-
terns, so that x=xlfc+xhfc. As the radius rincreases,
LFC data ( xlfc) holds more information, whereas HFC data
(xhfc) contains less. Importantly, as shown in Figure 2,
LFC data preserves slowly varying, smooth features, while
HFC data retains sharper transitions and intricate details.
3.2. Low-Frequency Components QAT (LFC QAT)
Given that a quantized model has less capacity than its full-
precision (FP) counterpart, a QAT scheme requires strate-
gic and selective learning to preserve the essential infor-
mation from the train data. Assuming that LFC information
is more generalizable [36, 39, 40, 43] and easier to capture,
CoDA applies QAT solely to LFC of the train data.
3.2.1. Efficacy of QAT in Learning from LFC
To investigate the effectiveness of LFC QAT, Table 1
compares full-precision (FP) and QAT-applied models
(LSQ [10] and LQ [45]) when trained on LFC and HFC
that are filtered with various radius. Both LSQ and LQ uti-
lize 2-bit integer quantization for weights and activations
(i.e., w2a2), while FP uses 32-bit real values (i.e., w32a32).
We use ResNet26 [16] on CIFAR10 [25] and ResNet50 [15]
on ImageNet [9], testing all models on FFC data.
3Table 1. Classification accuracy(%) of ResNet26 and ResNet50 on
CIFAR10 and ImageNet, respectively, with different QAT meth-
ods, filter types and radius values for training.
Filter typeCIFAR10 — ResNet26 ImageNet — ResNet50
r FP LSQ LQ r FP LSQ LQ
No filter - 90.35 86.11 89.06 - 66.96 60.14 63.35
Low pass filter4 65.66 68.99 71.94 28 59.26 50.61 55.51
8 81.98 80.82 84.73 56 64.91 58.55 61.83
12 85.07 83.95 87.48 84 66.24 60.05 62.73
16 86.83 83.46 88.94 112 66.93 60.51 63.63
High pass filter4 74.07 54.35 66.45 28 0.17 0.20 0.52
8 21.21 12.76 19.72 56 0.11 0.13 0.23
12 11.02 12.00 11.96 84 0.15 0.11 0.22
16 10.11 10.94 9.63 112 0.15 0.13 0.15
Table 2. Classification accuracy(%) of ResNet26 and ResNet50
on CIFAR10-C and ImageNet-C, respectively, with different QAT
methods and various radius values in low pass filters for training.
Filter typeCIFAR10-C — ResNet26 ImageNet-C — ResNet50
r FP LSQ LQ r FP LSQ LQ
No filter - 53.86 50.15 54.35 - 27.74 19.76 22.44
Low pass filter4 55.24 59.51 60.60 28 30.40 21.90 25.23
8 64.61 65.45 67.53 56 29.76 20.82 25.06
12 63.42 62.24 63.87 84 28.06 19.77 23.20
16 56.77 55.20 58.10 112 28.08 19.72 23.38
HFC-based Learning. Table 1 shows that all three meth-
ods exhibit very low accuracy when trained solely on HFC.
In the more challenging ImageNet task, HFC-based training
results in ∼0% accuracy regardless of the radius rand the
method used. This confirms that LFC contain the neces-
sary informastion for image classification. For the easier
task of CIFAR10, HFC-based training achieves better ac-
curacy, suggesting the presence of some meaningful infor-
mation within HFC. For example, with high-pass filtering
using a radius of 4, the FP model achieves 74.07% accu-
racy. However, both LSQ [10] and LQ [45] significantly
under-perform compared to the FP model, with an accuracy
drop of 8 ∼20%p. This demonstrates the limitations of QAT
in effectively learning from HFC compared to FP models.
LFC-based Learning. When trained exclusively on LFC,
all three methods maintain decent accuracy on both datasets
even as the radius rdecreases, confirming the essential role
of LFC. Interestingly, on CIFAR10, LQ consistently outper-
forms the FP models, while LSQ surpasses the FP model
when the radius is 4. The superior performance of QAT
with a very low precision underscores that QAT effectively
extracts valuable knowledge from LFC .
3.2.2. Robustness of LFC QAT
Building on the efficacy of QAT with LFC, we further ex-
plore the robustness of QAT under domain shifts when ex-
clusively trained on LFC. We evaluate FP (w32a32), LSQ
(w2a2), and LQ (w2a2) models, trained on CIFAR10 [25]
and ImageNet [9] using LFC with various radius r, by test-
ing on corrupted CIFAR10-C [17] and ImageNet-C [17].
Gau.
Shot
Imp.
Def.
Mot.
Zoom
Snow
Fro.
Fog
Brit.
Cont.
Elas.
Pix.
JPEGGau.
Shot
Imp.
Def.
Mot.
Zoom
Snow
Fro.
Fog
Brit.
Cont.
Elas.
Pix.
JPEG
0.000.050.100.150.20
Inter-domain distance(a) Distance of LFC
Gau.
Shot
Imp.
Def.
Mot.
Zoom
Snow
Fro.
Fog
Brit.
Cont.
Elas.
Pix.
JPEGGau.
Shot
Imp.
Def.
Mot.
Zoom
Snow
Fro.
Fog
Brit.
Cont.
Elas.
Pix.
JPEG
0.000.050.100.150.20
Inter-domain distance (b) Distance of HFC
Figure 3. Inter-domain distance matrices of LFC and HFC fre-
quency domain image. The values in Figure 3a appear signifi-
cantly smaller than those in Figure 3b, indicating that LFC is more
domain-invariant compared to HFC. Details of distance matrix cal-
culation are provided in the supplementary materials.
(a) FFC Clean
 (b) FFC Corrupt.
 (c) LFC Clean
 (d) LFC Corrupt.
Figure 4. The loss landscapes of quantized ResNet26 on CIFAR10
(Clean) and CIFAR10-C (Corrupt.) trained on different frequency
ranges. The quantization method is LSQ [10] and the quantization
level is 2-bit. FFC refers to the full frequency of test data; LFC
refers to the lower frequency range in a low pass filter of radius 8.
Sharpness and concave regions generally indicate less robustness.
We use visualization methods following previous works [11, 26].
Table 2 indicates that the robustness of all three meth-
ods on ImageNet-C is marginally impacted by the filtering
range applied to train data. For example, although LFC
with r=28 (i.e., the narrowest range) contain much less
information than FFC, models trained on these restricted
LFC achieve similar or even better accuracy on ImageNet-
C compared to those trained on FFC.
Results on CIFAR10-C are more dramatic, with all meth-
ods showing significantly enhanced robustness as the filter-
ing range narrows. Specifically, training with LFC at r=8
increases accuracy by 11 ∼15%p over FFCs. This demon-
strates that the LFC knowledge remains robust under do-
main shifts , whereas HFC information contributes little or
can even be detrimental to model robustness. In addition,
on CIFAR10-C, LQ (w2a2) provides better accuracy than
FP when trained on LFC from CIFAR10. The superior per-
formance of LFC-trained LQ on both CIFAR10 (Table 1)
and CIFAR10-C (Table 2) confirms that QAT effectively
learns domain-invariant features from LFC , resulting in
a compressed but generalizable model.
For a deeper understanding, Figure 3 compares the
frequency-domain features of LFC and HFC between dif-
ferent corruptions in CIFAR10-C, using 10 random sam-
ples per class. The results show that LFC exhibit signif-
icantly smaller distances across different corruptions com-
pared to HFC, confirming their domain-invariant properties.
4In addition, Figure 4 illustrates the robustness of LFC QAT
using the loss landscape visualization [26]. To this end,
we train ResNet26 models using the LSQ scheme on ei-
ther FFC or LFC data in CIFAR10, testing the models on
both CIFAR10 (clean) and CIFAR10-C (corrupted). When
tested on clean images (Figures 4a and 4c), the FFC-trained
model (Figure 4a) achieves a deeper minimum compared to
the LFC-trained model (Figure 4c), leading to higher accu-
racy as shown in Table 2. However, the FFC-trained model
has a sharper, partially concave landscape, indicating it may
quickly lose its optimal performance under domain shifts.
This vulnerability is confirmed by testing on corrupted
images (Figures 4b and 4d), where the FFC-trained model
exhibits a deteriorated minimum compared to the LFC-
trained model, leading to lower accuracy as shown in Ta-
ble 2. This shallow landscape suggests that the FFC-trained
model must significantly change its parameters (i.e., reshap-
ing the landscape) to handle domain shifts. In contrast, the
LFC-trained model presents a smoother, more convex land-
scape (Figure 4c), indicating a more malleable state that is
well-prepared for future adaptation under domain shifts.
3.3. TTA with Frequency-Aware BN (FABN)
While LFC QAT shows greater robustness to domain shifts
compared to the baseline QATs, further adaption is needed
to optimize post-deployment performance on the target data
distribution. To achieve this, CoDA employs TTA, an
emerging paradigm that addresses domain shifts using only
unlabeled test samples. Although the model is trained only
on LFC, the adaptation phase utilizes FFC data to fully
capture domain-specific details comprehensively. However,
when adapting batch normalization (BN) statistics for a tar-
get domain, existing TTA methods [27, 31] assume that the
model has learned from FFC data in the source domain. In
contrast, in the context of LFC QAT, the model’s BN statis-
tics contain information of LFC but lack details of HFC
from the source domain. This necessitates separately han-
dling LFC and HFC in the target data.
To address this, we propose Frequency-Aware BN
(FABN), as in Figure 2. At each time step t, FABN applies
two bandpass filters, using the same radius ras the training
phase, to separate each BN layer’s input feature finto low-
frequency feature flfcand high-frequency feature fhfc. Then,
flfcandfhfcare processed differently as follows:
Adaptation for LFC. For the low-frequency feature flfc, we
initialize the BN statistics from the source domain as the
running mean ˆµlfc,tand variance ˆσlfc,tat test time:
ˆµlfc,0= ˆµs,ˆσlfc,0= ˆσs, (2)
and apply an exponential moving average (EMA) to esti-
mate batch statistics for LFC at time step tas follows:
ˆµlfc,t= (1−α)·ˆµlfc,t−1+α·µlfc,t,
ˆσ2
lfc,t= (1−α)·ˆσ2
lfc,t−1+α·σ2
lfc,t.(3)Table 3. Classification accuracy(%) of CoDA on CIFAR10-C. Dif-
ferent frequency range means that test image is filtered through
different low-/high-frequency filters before inference and adpated
with FABN.
FFCLFC HFC
r=4 r=8 r=12 r=4 r=8 r=12
Ours (LSQ w2a2) 71.22 41.94 65.11 66.59 41.78 17.41 10.66
Ours (LQ w2a2) 76.15 42.92 70.11 72.10 45.38 16.67 10.73
gaussian_noiseshot_noise
impulse_noisedefocus_blurglass_blurmotion_blurzoom_blursnow frostfog
brightnesscontrast
elastic_transformpixelate
jpeg_compression01234SSE (Sum of errors)
Figure 5. SSE comparison between LSQ [10] with and without
CoDA . SSE is measured between ˆµsandˆµlfc,tfor the LFC-trained
model ( CoDA ), and between ˆµsandˆµtfor the FFC-trained model.
SSE is gathered over all layers. Quantization level is 2-bit.
Here, µlfc,tandσ2
lfc,tare the mean and variance calculated
from the feature flfcof the current batch at time t.αis a
hyperparameter for EMA.
Adaptation for HFC. Since the model has not learned any
high-frequency feature during training, for fhfc, we do not
use the model’s statistics. Instead, we use the mean and
variance calculated from the feature fhfcof the current batch
directly as the estimated mean ˆµhfc,tand variance ˆσhfc,tat
each time step t:
ˆµhfc,t=µhfc,t,ˆσ2
hfc,t=σ2
hfc,t. (4)
Integrating LFC and HFC. After computing the individ-
ual mean and standard deviation for flfcandfhfc, we com-
bine them to produce the final BN parameters ˆµtandˆσt:
ˆµt= ˆµlfc,t+ ˆµhfc,t,ˆσ2
t= ˆσ2
lfc,t+ ˆσ2
hfc,t. (5)
Importantly, while our FABN adjusts only BN statistics, it
can be synergistically combined with existing TTA meth-
ods that update affine parameters via back propagation, such
as TENT [34] and SAR [30].
3.3.1. Impact of Frequency Composition
We conduct an experiment to understand how frequency
composition affects the performance of our FABN TTA.
For this experiment, CIFAR10 is used for training and
CIFAR10-C (severity 5) is used for adaptation and testing.
While the model is trained on LFC with r=8, various radius
of low-/high-frequency filters are applied to test data during
adaptation. Table 3 shows the results.
Across all configurations, using FFC rather than LFC
or HFC enhances the performance with FABN TTA. This
5Table 4. Classification accuracy(%) of ResNet18 and ResNet50 on ImageNet-C with and without TTA. Baseline models are trained with
FFCs while ours is trained with LFC. The highest accuracy is in bold . In quantized models, both weight and activation values are quantized
to 2, 4, and 8 bits.
ResNet-18 ResNet-50
32-bit 2-bit 4-bit 8-bit 32-bit 2-bit 4-bit 8-bit
FP LSQ LQ LSQ LQ LSQ LQ FP LSQ LQ LSQ LQ LSQ LQ
w/o TTA 22.36 16.25 18.66 19.90 20.89 20.15 21.11 27.74 19.76 22.23 21.57 22.96 22.58 15.70
NORM [28] 29.13 24.44 28.35 28.50 28.68 29.23 30.42 36.20 30.44 32.65 31.30 31.93 31.87 26.52
CoDA - 31.45 34.28 33.49 34.47 34.19 34.50 - 35.25 38.03 36.23 36.47 36.65 35.21
TENT [34] 30.62 27.74 30.42 30.73 30.07 31.25 31.71 38.55 33.02 35.03 33.70 33.96 34.11 30.96
TENT + CoDA - 31.73 34.29 33.70 33.83 34.40 34.47 - 35.33 37.99 36.39 33.83 36.76 35.21
SAR [30] 33.88 29.47 30.42 31.86 30.07 32.13 31.70 41.49 34.02 35.04 34.40 33.95 34.86 30.95
SAR + CoDA - 31.22 34.17 33.56 33.71 34.41 34.35 - 35.78 37.90 36.49 36.40 36.73 35.09
demonstrates the importance of using full-frequency during
the adaptation stage, even though the model is trained only
on LFC. Specifically, Table 3 shows that unlike the efficacy
achieved from LFC-based training, a smaller range of LFC
degrades adaptation performance compared to FFC adapta-
tion. When the radius rfor test data is 4, smaller than that
for the training data (i.e., r=8), the model performance is
even degraded after adaptation. Similarly, relying only on
HFC often fails at adaptation. Overall, both LFC and HFC
significantly contribute to FABN TTA by providing rich
information in the target domain.
3.3.2. Interaction with LFC QAT
The rationale for integrating LFC QAT and FABN is that a
model trained with LFC QAT, being inherently more robust
than one trained with FFC QAT, may require only modest
adaptation of its BN parameters for LFC data at test time.
To investigate this, Figure 5 depicts the sum of squared er-
rors (SSE) between ˆµsandˆµlfc,tin the LFC-trained model,
and between ˆµsandˆµtin the FFC-trained model under
various domain shifts. In all configurations, the LFC BN
parameters of the LFC-trained model demonstrate a lower
need for adaptation than the full BN parameters of the FFC-
trained model. This suggests that LFC BN parameters are
more stable under domain shifts, supporting the approach
of treating them separately from HFC BN parameters.
4. Evaluation
Datasets. We use four widely recognized benchmarks
for evaluating model robustness under domain shifts.
CIFAR10-C [17] is used as target data for models trained on
CIFAR10 [25], while ImageNet-C [17], ImageNet-R [18],
and ImageNet-Sketch [35] serve as target data for models
trained on ImageNet.
Network architectures. We evaluate various models in-
cluding the ResNet [16], MobileNet [19] and Efficient-
Net [32] architectures to compare the size and performance
of different models.QAT & TTA Methods. Two general QAT methods are
used for this study: LSQ [10] and LQ [45]. Four other TTA
methods are used in comparison to CoDA : NORM [28, 31],
TENT [34], SAR [30], and CoTTA [38]. Further implemen-
tation details are described in supplementary materials.
4.1. Overall Results
Tables 4, 5, and 6 show TTA performance on ImageNet-
C, CIFAR10-C, and ImageNet-R/-Sketch dataset, respec-
tively. While ImageNet-C and CIFAR10-C are standard
TTA benchmarks, ImageNet-R/-Sketch are additionally uti-
lized for evaluating generalizability of our approach.
ImageNet-C. Table 4 shows that all TTA baselines [28, 30,
34] suffer performance degradation when adapting FFC-
trained quantized models. Compared to the TTA accu-
racy of their full-precision (FP) counterparts, these base-
lines generally struggle with reduced precision. In con-
trast, CoDA integrates seamlessly with various TTA and
QAT baselines, boosting performance across different ar-
chitectures and bitwidth settings. In all settings, CoDA -
enhanced methods achieve state-of-the-art results. Notably,
on ResNet-18, CoDA even outperforms full-precision mod-
els with TTA by up to 5.37%p , while reducing model
size by 4 to 16 times . These results demonstrate that
CoDA ’s end-to-end pipeline, combining LFC-based QAT
and frequency-decomposed TTA, effectively tackles both
model compression and domain shifts.
CIFAR10-C. Table 5 shows that CoDA on both LQ and
LSQ outperforms the TTA baselines in all bitwidths. CoDA
shows better performance compared to FP models that are
larger in capacity, showing consistency with ImageNet-C
experiment results. Specifically, CoDA demonstrates up
to 7.96%p higher accuracy compared to TTA-applied FP
models while reducing model size by 4 to 16 times , proving
the effectiveness of our approach.
ImageNet-R/-Sketch. Table 6 shows that CoDA remains
effective on domain shift datasets other than corruption,
mostly outperforming other TTA baselines.
6Table 5. Classification accuracy(%) of ResNet26 on CIFAR10-
C with and without TTA. Baseline models are trained with FFC
while ours is trained with LFC. The highest accuracy is in bold .
Method32-bit 2bit 4bit 8bit
FP LSQ LQ LSQ LQ LSQ LQ
w/o TTA 58.43 58.86 59.72 52.65 55.40 51.60 53.16
NORM 68.42 65.52 69.29 64.23 68.05 64.23 67.47
TENT 72.64 70.18 73.72 70.43 73.15 71.09 72.90
SAR 71.66 68.99 73.60 69.46 73.16 69.52 73.06
CoDA - 71.22 76.15 72.50 76.38 72.86 75.52
Table 6. Classification accuracy(%) of ResNet18/50 on ImageNet-
R/-Sketch with and without TTA. Baseline models are trained with
FFCs while ours is trained with LFCs. Both weight and activation
values are quantized to 2 bits. The highest accuracy is in bold .
ImageNet-R ImageNet-Sketch
ResNet-18 ResNet-50 ResNet-18 ResNet-50
LSQ LQ LSQ LQ LSQ LQ LSQ LQ
w/o TTA 19.64 21.96 21.77 24.93 9.62 11.51 10.62 13.58
NORM [28] 21.66 24.22 23.65 27.16 11.38 12.87 12.58 16.02
TENT [34] 20.54 23.59 23.29 26.79 10.15 11.65 12.71 15.33
SAR [30] 20.87 23.56 23.26 26.70 10.82 11.69 12.99 15.28
CoDA 21.85 24.58 24.02 27.81 9.90 12.98 13.30 16.21
Table 7. Classification accuracy (%) of different lightweight mod-
els on CIFAR10-C when LSQ is used for the QAT method. The
highest accuracy is in bold .
32-bit 2-bit 4-bit 8-bit
MobileNet v3-small
w/o TTA 54.74 53.38 55.46 53.89
NORM [28] 68.85 69.71 70.01 69.11
TENT [34] 73.83 74.38 75.23 74.90
SAR [30] 73.82 74.33 75.09 74.85
CoDA – 74.49 75.92 75.71
MobileNet v3-large
w/o TTA 58.11 56.55 55.41 56.71
NORM [28] 72.34 72.86 72.96 73.27
TENT [34] 77.26 76.76 77.57 77.32
SAR [30] 77.05 76.72 77.51 77.32
CoDA – 77.51 79.40 80.06
EfficientNet-b0
w/o TTA 57.41 54.85 60.25 59.52
NORM [28] 70.60 71.75 74.01 73.93
TENT [34] 79.04 78.42 79.03 79.18
SAR [30] 78.89 78.20 78.69 79.00
CoDA – 78.59 80.41 80.62
In summary, our evaluation on benchmarks against ex-
isting methods highlights the practical advantages of intelli-
gently leveraging frequency components, especially in sce-
narios where domain shifts and limited resources co-exist.
CoDA maintains robust performance across diverse oper-
ational settings, making it a compelling solution for real-
world applications with varying data distributions.Table 8. Classification accuracy(%) of ResNet18 on ImageNet-C
under continually changing target domain with TTA. The model
parameters never be reset except for the case of stochastic restora-
tion by CoTTA. Baseline models are trained with FFC while ours
is trained with LFC. The highest accuracy is in bold .
FP LSQ
32-bit 2-bit 4-bit 8-bit
w/o TTA 22.36 16.25 19.90 20.15
TENT [34] 32.76 28.70 31.63 32.06
CoTTA [38] 34.14 17.72 31.87 32.44
CoDA - 31.35 33.43 34.07
Table 9. Classification accuracy (%) by varying ablative settings
inCoDA on CIFAR10-C and ImageNet-C. “Base” refers to FFC-
based learning on QAT. The highest accuracy is in bold .
LFC FABN CIFAR10-C ImageNet-C
Base 59.72 22.23
LFC-only ✓ 67.58 23.44
FABN-only ✓ 73.96 35.98
LFC + FABN ( CoDA )✓ ✓ 76.15 38.03
4.2. Lightweight Models
We conduct extensive experiments on more lightweight
model architectures such as MobileNet [19] and Efficient-
Net [32] which are designed to efficiently learn visual in-
formation with a minimum amount of parameters. Table 7
shows that CoDA is still effective beyond the baseline TTAs
under quantization. CoDA achieves the highest top-1 accu-
racy across all models, even surpassing FP models, demon-
strating that selective learning on domain-agnostic LFC in-
formation and frequency-based TTA enhance robustness,
even in the lightweight models.
4.3. Continual Domain Shifts
We also investigate robustness under continually changing
target domains, comparing CoDA with CoTTA [38], which
is designed to alleviate catastrophic forgetting. As shown
in Table 8, while CoDA is not specifically designed to ad-
dress the forgetting issue, it achieves competitive or even
higher accuracy than CoTTA [38]. This is because CoDA
retains only domain-agnostic LFC information and entirely
changes domain-specific HFC information in BN statistics
across changing domains.
4.4. Ablation Study
Impact of individual components. We conduct an abla-
tive study to further investigate the effectiveness of CoDA ’s
technical components: Low-Frequency QAT ( LFC ) and
Frequency-Aware BN ( FABN ). Table 9 shows the re-
sult for CIFAR10-C with ResNet26 and ImageNet-C with
ResNet50 with 2-bit quantized LQ [45]. ‘Base’ refers to
FFC-based learning on QAT. Each component shows im-
provement over Base. Notably, on ImageNet-C, the accu-
732 64 128 256
T est Batch Size657075Accuracy (%)
NORM
TENTSAR
Ours(a) LSQ
32 64 128 256
T est Batch Size657075
NORM
TENTSAR
Ours (b) LQ
Figure 6. Classification accuracy (%) under the effect of different
batch sizes and TTA methods on CIFAR10-C with ResNet26.
racy gap (15.8%p) between CoDA and Base has a larger
improvement over the sum of the individual gains of LFC-
only (1.21%p) and FABN-only (12.54%p). This highlights
the synergy between LFC and FABN within CoDA .
Impact of test batch size. We also investigate the effect of
varying test batch size, which impacts the adaptation pro-
cess of BN-based TTA methods. Figure 6 shows the result
on CIFAR10-C. We found that the performance of NORM,
TENT, and SAR drops significantly when the batch size
is smaller, indicating that they are susceptible to smaller
batch sizes. In contrast, CoDA shows robustness over vari-
ous batch sizes. This is mainly because it retains the well-
trained LFC running mean, while rapidly adapting to the
HFC of each incoming batch.
4.5. Feature Embedding Analysis
To further understand the compatibility between QAT and
LFC (Section 3.2), we visualize feature embeddings of three
example classes (airplane, automobile, and bird) in CI-
FAR10 using t-SNE [33] (Figure 7). We train ResNet26
with full precision and using LSQ and LQ schemes on ei-
ther FFC or LFC data in CIFAR10, testing the models on
both CIFAR10 (clean) and CIFAR10-C (corrupted).
For all cases including FP, LSQ, and LQ, the LFC-
trained models achieve significantly better feature align-
ment between clean (solid circles) and corrupted (empty cir-
cles) images within each class. Interestingly, when trained
on FFC information, both LSQ and LQ exhibit more dis-
persed feature embeddings across multiple clusters com-
pared to FP, demonstrating QAT’s weakness in learning
generalizable information from the source domain. How-
ever, this limitation is alleviated when using only LFCs dur-
ing training. In the LFC-trained models, both LSQ and LQ
enable feature embeddings to converge around a primary
cluster. For instance, the bird class (green) is dispersed in
the FFC-trained models but forms a more cohesive cluster
in the LFC-trained models. Overall, the results show empir-
ical evidence of the compatibility between QAT and LFC,
suggesting that LFC-based QAT is promising for addressing
resource constraints and domain shifts simultaneously.
Figure 7. t-SNE visualization of embeddings from FFC/LFC-
trained FP, LSQ [10], and LQ [45] models. Tested on three exam-
ple classes (airplane, automobile, and bird) of CIFAR10/-C with
pixelate corruption (severity 5). Overlapping embeddings of clean
and corrupted images in LFC indicate natural feature alignment.
5. Conclusion
This study highlights the necessity of tackling compression
and adaptation jointly when deploying neural networks on
resource-constrained devices in dynamic environments—an
area previously overlooked by standard QAT and TTA base-
lines. To address this gap, we introduced CoDA , a novel
frequency-aware framework that unifies QAT and TTA by
leveraging LFCs at training time and FABN at test time.
This combination enhances both the robustness and adapt-
ability of quantized models under domain shifts. Our main
results demonstrate substantial accuracy gains in low-bit
settings, underscoring the efficacy of CoDA for practical
deployment. Furthermore, our extensive evaluations with
various models and datasets show that CoDA effectively re-
tains LFC even in lightweight models and applies to diverse
distributions and datasets. Our findings reveal the positive
impact of selectively and adaptively exploiting frequency
components for both training and adaptation. We believe
this opens a promising path toward more effective, domain-
resilient neural network deployment in real-world applica-
tions.
8References
[1] Michele Antonazzi, Matteo Luperto, N Alberto
Borghese, and Nicola Basilico. R2snet: Scalable
domain adaptation for object detection in cloud–
based robotic ecosystems via proposal refinement. In
2024 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS) , pages 2676–2682.
IEEE, 2024. 1
[2] MohammadHossein AskariHemmat, Ahmadreza
Jeddi, Reyhane Askari Hemmat, Ivan Lazarevich,
Alexander Hoffman, Sudhakar Sah, Ehsan Saboori,
Yvon Savaria, and Jean-Pierre David. Qgen: On the
ability to generalize in quantization aware training.
arXiv preprint arXiv:2404.11769 , 2024. 2
[3] Michele Boldo, Mirco De Marchi, Enrico Martini,
Stefano Aldegheri, and Nicola Bombieri. Domain-
adaptive online active learning for real-time intelligent
video analytics on edge devices. IEEE Transactions
on Computer-Aided Design of Integrated Circuits and
Systems , 43(11):4105–4116, 2024. 1
[4] Malik Boudiaf, Romain Mueller, Ismail Ben Ayed,
and Luca Bertinetto. Parameter-free online test-time
adaptation. In Proceedings of the IEEE/CVF Con-
ference on Computer Vision and Pattern Recognition ,
pages 8344–8353, 2022. 2
[5] E Oran Brigham. The fast Fourier transform and its
applications . Prentice-Hall, Inc., 1988. 1
[6] Prithvijit Chattopadhyay, Kartik Sarangmath, Vivek
Vijaykumar, and Judy Hoffman. Pasta: Proportional
amplitude spectrum training augmentation for syn-to-
real domain generalization. In Proceedings of the
IEEE/CVF International Conference on Computer Vi-
sion, pages 19288–19300, 2023. 1, 2
[7] Guangyao Chen, Peixi Peng, Li Ma, Jia Li, Lin Du,
and Yonghong Tian. Amplitude-phase recombina-
tion: Rethinking robustness of convolutional neural
networks in frequency domain. In Proceedings of the
IEEE/CVF International Conference on Computer Vi-
sion, pages 458–467, 2021. 1, 2
[8] Matthieu Courbariaux, Yoshua Bengio, and Jean-
Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations.
Advances in neural information processing systems ,
28, 2015. 2
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. Imagenet: A large-scale hierarchi-
cal image database. In 2009 IEEE conference on com-
puter vision and pattern recognition , pages 248–255.
Ieee, 2009. 2, 3, 4, 12
[10] Steven K Esser, Jeffrey L McKinstry, Deepika
Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. arXiv
preprint arXiv:1902.08153 , 2019. 2, 3, 4, 5, 6, 8, 12[11] Pierre Foret, Ariel Kleiner, Hossein Mobahi, and
Behnam Neyshabur. Sharpness-aware minimiza-
tion for efficiently improving generalization. arXiv
preprint arXiv:2010.01412 , 2020. 4, 13
[12] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei
Yao, Michael W Mahoney, and Kurt Keutzer. A survey
of quantization methods for efficient neural network
inference. In Low-Power Computer Vision , pages
291–326. Chapman and Hall/CRC, 2022. 1
[13] Taesik Gong, Jongheon Jeong, Taewon Kim, Yewon
Kim, Jinwoo Shin, and Sung-Ju Lee. Note: Robust
continual test-time adaptation against temporal corre-
lation. Advances in Neural Information Processing
Systems , 35:27253–27266, 2022. 2
[14] Taesik Gong, Yewon Kim, Taeckyung Lee, Sorn Chot-
tananurak, and Sung-Ju Lee. Sotta: Robust test-time
adaptation on noisy data streams. Advances in Neural
Information Processing Systems , 36, 2024. 2
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Deep residual learning for image recognition.
InProceedings of the IEEE conference on computer
vision and pattern recognition , pages 770–778, 2016.
2, 3, 12
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
Sun. Identity mappings in deep residual networks. In
Computer Vision–ECCV 2016: 14th European Con-
ference, Amsterdam, The Netherlands, October 11–
14, 2016, Proceedings, Part IV 14 , pages 630–645.
Springer, 2016. 3, 6, 12
[17] Dan Hendrycks and Thomas Dietterich. Benchmark-
ing neural network robustness to common corruptions
and perturbations. arXiv preprint arXiv:1903.12261 ,
2019. 2, 4, 6, 12
[18] Dan Hendrycks, Steven Basart, Norman Mu, Saurav
Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,
Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The
many faces of robustness: A critical analysis of out-
of-distribution generalization. In Proceedings of the
IEEE/CVF international conference on computer vi-
sion, pages 8340–8349, 2021. 6, 12
[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-
Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang,
Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al.
Searching for mobilenetv3. In Proceedings of the
IEEE/CVF international conference on computer vi-
sion, pages 1314–1324, 2019. 6, 7
[20] Lukas Hoyer, Dengxin Dai, and Luc Van Gool.
Daformer: Improving network architectures and train-
ing strategies for domain-adaptive semantic segmen-
tation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition , pages
9924–9935, 2022. 1
9[21] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shi-
jian Lu. Fsdr: Frequency space domain randomiza-
tion for domain generalization. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition , pages 6891–6902, 2021. 1, 2
[22] Raza Imam, Muhammad Huzaifa, Nabil Man-
sour, Shaher Bano Mirza, and Fouad Lamghari.
Domain adaptable fine-tune distillation framework
for advancing farm surveillance. arXiv preprint
arXiv:2402.07059 , 2024. 1
[23] Saqib Javed, Hieu Le, and Mathieu Salzmann. Qt-
dog: Quantization-aware training for domain general-
ization. arXiv preprint arXiv:2410.06020 , 2024. 2
[24] Qing Jin, Linjie Yang, Zhenyu Liao, and Xiaon-
ing Qian. Neural network quantization with scale-
adjusted training. In BMVC , 2020. 2
[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning
multiple layers of features from tiny images. 2009.
3, 4, 6, 12
[26] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer,
and Tom Goldstein. Visualizing the loss landscape of
neural nets. Advances in neural information process-
ing systems , 31, 2018. 4, 5, 13
[27] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger,
and Horst Bischof. The norm must go on: Dynamic
unsupervised domain adaptation by normalization. In
Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 14765–
14775, 2022. 2, 5
[28] Zachary Nado, Shreyas Padhy, D Sculley, Alexan-
der D’Amour, Balaji Lakshminarayanan, and Jasper
Snoek. Evaluating prediction-time batch normal-
ization for robustness under covariate shift. arXiv
preprint arXiv:2006.10963 , 2020. 2, 6, 7, 13
[29] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo
Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan.
Efficient test-time model adaptation without forget-
ting. In International conference on machine learning ,
pages 16888–16905. PMLR, 2022. 2
[30] Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan
Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan.
Towards stable test-time adaptation in dynamic wild
world. arXiv preprint arXiv:2302.12400 , 2023. 2, 5,
6, 7, 13
[31] Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver
Bringmann, Wieland Brendel, and Matthias Bethge.
Improving robustness against common corruptions by
covariate shift adaptation. Advances in neural infor-
mation processing systems , 33:11539–11551, 2020. 2,
5, 6, 13
[32] Mingxing Tan and Quoc Le. Efficientnet: Rethinking
model scaling for convolutional neural networks. InInternational conference on machine learning , pages
6105–6114. PMLR, 2019. 6, 7
[33] Laurens Van der Maaten and Geoffrey Hinton. Visu-
alizing data using t-sne. Journal of machine learning
research , 9(11), 2008. 8
[34] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno
Olshausen, and Trevor Darrell. Tent: Fully test-time
adaptation by entropy minimization. arXiv preprint
arXiv:2006.10726 , 2020. 2, 5, 6, 7, 13
[35] Haohan Wang, Songwei Ge, Zachary Lipton, and
Eric P Xing. Learning robust global representations by
penalizing local predictive power. Advances in neural
information processing systems , 32, 2019. 6, 12
[36] Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P
Xing. High-frequency component helps explain the
generalization of convolutional neural networks. In
Proceedings of the IEEE/CVF conference on com-
puter vision and pattern recognition , pages 8684–
8694, 2020. 2, 3
[37] Jindong Wang, Cuiling Lan, Chang Liu, Yidong
Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun
Zeng, and S Yu Philip. Generalizing to unseen do-
mains: A survey on domain generalization. IEEE
transactions on knowledge and data engineering , 35
(8):8052–8072, 2022. 1
[38] Qin Wang, Olga Fink, Luc Van Gool, and Dengxin
Dai. Continual test-time domain adaptation. In Pro-
ceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition , pages 7201–7211,
2022. 2, 6, 7, 13
[39] Yulin Wang, Yang Yue, Rui Lu, Tianjiao Liu, Zhao
Zhong, Shiji Song, and Gao Huang. Efficienttrain:
Exploring generalized curriculum learning for training
visual backbones. In Proceedings of the IEEE/CVF
International Conference on Computer Vision , pages
5852–5864, 2023. 2, 3
[40] Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-
Kuang Chen, and Fengbo Ren. Learning in the fre-
quency domain. In Proceedings of the IEEE/CVF con-
ference on computer vision and pattern recognition ,
pages 1740–1749, 2020. 3
[41] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng
Wang, and Qi Tian. A fourier-based framework for do-
main generalization. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recogni-
tion, pages 14383–14392, 2021. 1, 2
[42] Yanchao Yang and Stefano Soatto. Fda: Fourier do-
main adaptation for semantic segmentation. In Pro-
ceedings of the IEEE/CVF conference on computer vi-
sion and pattern recognition , pages 4085–4095, 2020.
1, 2
[43] Dong Yin, Raphael Gontijo Lopes, Jon Shlens,
Ekin Dogus Cubuk, and Justin Gilmer. A fourier per-
10spective on model robustness in computer vision. Ad-
vances in Neural Information Processing Systems , 32,
2019. 3
[44] Xiangyun Zeng, Siok Yee Tan, and Moham-
mad Faidzul Nasrudin. Adapt-net: A unified ob-
ject detection framework for mobile augmented real-
ity.IEEE Access , 2024. 1
[45] Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye,
and Gang Hua. Lq-nets: Learned quantization for
highly accurate and compact deep neural networks. In
Proceedings of the European conference on computer
vision (ECCV) , pages 365–382, 2018. 2, 3, 4, 6, 7, 8
[46] Kaiqi Zhang, Ming Yin, and Yu-Xiang Wang.
Why quantization improves generalization: Ntk of
binary weight neural networks. arXiv preprint
arXiv:2206.05916 , 2022. 2
[47] Marvin Zhang, Sergey Levine, and Chelsea Finn.
Memo: Test time robustness via adaptation and aug-
mentation. Advances in neural information processing
systems , 35:38629–38642, 2022. 2
[48] Sicheng Zhao, Xiangyu Yue, Shanghang Zhang, Bo
Li, Han Zhao, Bichen Wu, Ravi Krishna, Joseph E
Gonzalez, Alberto L Sangiovanni-Vincentelli, San-
jit A Seshia, et al. A review of single-source deep
unsupervised visual domain adaptation. IEEE Trans-
actions on Neural Networks and Learning Systems , 33
(2):473–493, 2020. 1
[49] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou,
He Wen, and Yuheng Zou. Dorefa-net: Training
low bitwidth convolutional neural networks with low
bitwidth gradients. arXiv preprint arXiv:1606.06160 ,
2016. 2
11Frequency Composition for Compressed and Domain-Adaptive Neural Networks
(Supplementary Material)
A. Experimental Details
We use pre-activation [16] based ResNet [15] models. All
models are trained from scratch. ResNet26 [16] is used
for training on CIFAR10 [25], while ResNet18/50 [15] are
used for training on ImageNet [9]. In frequency analysis
(Section 3), we use three different random seeds to train
and test on CIFAR10 [25] / CIFAR10-C [17], reporting
the average accuracies and standard deviations. For Ima-
geNet [9], only one random seed is used due to the sig-
nificant computational burden caused by its sheer volume.
For CIFAR10 [25], we run our training on a single GPU
(NVIDIA GeForce RTX 3090) and 4 multi GPUs (NVIDIA
GeForce A100) for ImageNet [9].
A.1. Datasets Details
CIFAR10 / CIFAR10-C. CIFAR10 [25] consists of
60,000 images of size 32×32 pixels across 10 classes, with
6,000 images per class. It is divided into 50,000 training
images and 10,000 test images. CIFAR10-C [17] applies
15 types of corruption which includes noise, blur, weather
effects, and digital distortions, to CIFAR10 test data. Each
corruption type includes a total of 50,000 for 5 severity lev-
els - 10,000 per each level.
ImageNet / ImageNet-C. ImageNet [9] consists of
1,331,167 images of varying image sizes across 1,000
classes. It is divided into 1,281,167 training images and
50,000 validation images. Throughout the whole paper,
the test set of ImageNet refers to the validation set of Ima-
geNet. ImageNet-C [17] applies 15 types of corruption to
ImageNet test data, equivalent to CIFAR10-C. Each corrup-
tion type includes a total of 250,000 for 5 severity levels -
50,000 per each level.
ImageNet-R / ImageNet-Sketch. ImageNet-R [18] con-
sists of 30,000 images, containing various naturally occur-
ring renditions (e.g., painting, sculpture, embroidery, etc.)
of 200 ImageNet object classes. ImageNet-Sketch [35] con-
sists of 50,000 images, containing 50 sketch images for each
of the 1000 ImageNet classes.
A.2. Baseline Details
For baselines, we refer to the official implementations of the
original authors. We use the hyperparameters reported on
their paper and code. We use torch.optim.SGD for training.Also, we utilize 2-bit quantized models for LSQ and LQ,
unless specified otherwise. For experiments in model com-
parison (Figure 1), LSQ 4-/8-bits quantized ResNet18/50
are also used. Additional details of the implementations are
provided below.
A.2.1. FP
FP. Full-precision models are not quantized.
For CIFAR10, we use learning rate (lr) as 0.1, momen-
tum as 0.9, and weight decay as 1.0e-4. The learning rate
scheduler is set to torch.optim.lrscheduler.StepLR with step
size 30. Batch size is 32 and each model is trained for 90
epochs.
For ImageNet, we use learning rate (lr) as 0.1, momen-
tum as 0.9, and weight decay as 1.0e-4. Learning rate
scheduler is set to torch.optim.lrscheduler.StepLR with step
size 30. Batch size is 256 and each model is trained for 90
epochs.
A.2.2. QAT
LSQ. Following the original paper, [10], we initialize step
sizesass=2wpwhere wrefers to the mean of weights in
the corresponding layer and pis2bitwidth−1−1. We use
the reported hyperparameter setting which showed the best
accuracy by each hyperparameter.
For CIFAR10, we use learning rate (lr) as 0.1, mo-
mentum as 0.9, and weight decay as 5.0e-4. Learn-
ing rate scheduler (lrsch) is set to MultiStepLR of
torch.optim.lrscheduler with milestones of [10, 30, 50, 70].
Batch size is 256 and each model is trained for 90 epochs.
For ImageNet, we use learning rate (lr) as 0.01, mo-
mentum as 0.9, and weight decay as 2.5e-4. Learn-
ing rate scheduler (lrsch) is set to CosineAnnealingLR of
torch.optim.lrscheduler with Tmax is 90. Batch size is 256
and each model is trained for 90 epochs.
LQ. For CIFAR10, we use learning rate (lr) as 0.1, mo-
mentum as 0.9, and weight decay as 1.0e-4. Learn-
ing rate scheduler (lrsch) is set to MultiStepLR of
torch.optim.lrscheduler with milestones of [82, 123]. Batch
size is 128 and each model is trained for 200 epochs.
For ImageNet, we use learning rate (lr) as 0.01, mo-
mentum as 0.9, and weight decay as 1.0e-4. Learn-
ing rate scheduler (lrsch) is set to MultiStepLR of
torch.optim.lrscheduler with milestones of [10, 30, 60, 80,
1295, 105]. Batch size is 256 and each model is trained for
120 epochs.
A.2.3. TTA
NORM [28, 31] For all experiments, we let each model
see 64 samples for each batch at test-time.
Tent [34] For CIFAR10 experiments, we utilize Adam
optimizer with a learning rate of 0.001. For ImageNet ex-
periments, we utilize an SGD optimizer with a learning rate
of 0.00025. We follow the hyperparameter from the original
paper [34]. We use 64 as the batch size for both datasets.
SAR [30] For both CIFAR10 and ImageNet experi-
ments, we utilize SGD optimizer as the base optimizer of
SAM [11] with a learning rate of 0.00025 and the threshold
for filtering samples E0of0.4×ln 1000 using batch size
64, following the original paper [30].
CoTTA [38] For both CIFAR-10 and ImageNet experi-
ments, we utilize Adam optimizer with a learning rate of
0.001, 32 augmentations, and restoration probability pof
0.01, following the original paper [38].
B. Loss Landscape
We visualize the loss landscapes of the full precision (FP)
models comparing them to those of QAT models shown in
Figure 4 of Section 3.2.2. FP models show a smoother sur-
face due to no quantization. The relative positions of the
optima match the accuracy results from Tables 1 and 2.
Unlike QAT models, full-precision models show no con-
cave regions. For both cases tested on clean and corrupted
images, the landscapes of the LFC-trained model (Figures
8a and 8b) tend to be sharper compared to those of the FFC-
trained model (Figures 8c and 8d), which is the opposite
for QAT models. This indicates that FP models may gain
less benefits from LFC-training compared to QAT models.
(a) FFC Clean
 (b) FFC Corrupt.
 (c) LFC Clean
 (d) LFC Corrupt.
Figure 8. The loss landscapes of full precision ResNet26 on CI-
FAR10 (Clean) and CIFAR10-C (Corrupt.) trained on different
frequency ranges. FFC refers to the full frequency of test data;
LFC refers to lower frequency range in low pass filter of radius 8.
We use visualization method following previous works [11, 26].C. Choice of Frequency Filtering Radius for
Low-frequency QAT
To determine the appropriate frequency range for the ra-
diusrinLPF (low-pass filtering), we conduct a frequency
analysis on each dataset (i.e., CIFAR10 and ImageNet) in
Sec 3.2.1. As radius rincreases up to half of the input im-
age size, the evaluation accuracy of the LFC-trained model
on the uncorrupted test set converges to that of the FFC-
trained model. The choices of radii rdepend on the input
size, which vary across datasets (i.e., 224 for ImageNet and
32 for CIFAR10). Therefore, we set the radius rto represent
the same proportion of the input size for each dataset. The
corresponding rows in the tables of Sec 3.2.1 indicate these
proportional values across datasets. Finally, to ensure that
LPF preserves a more robust and lower frequency range,
we set r=8 for CIFAR10 and r=56 for ImageNet.
D. Evaluation Details
We provide per corruption results for Tables 4 and 5 of
Sec 4. Results for default TTA baselines and CoDA are
shown in Tables 11, 12 and 15. Results for CoDA enhanced
methods are shown in Tables 13 and 14.
E. Model Size Calculation
We define model size as the number of bits that the model
takes. As shown in Table 16, CoDA (Ours) on QAT mod-
els outperform the majority of TTA methods applied to the
much larger FP models. For example, a ResNet18 model
contains approximately 11 million parameters, meaning the
32-bit full-precision model size is about 374 million bits,
while its 2-bit quantized counterpart is about 23 million bits,
which is approximately 16 times smaller. Table 16 shows
the total number of bits of our implemented models. The
values in Fig 1 are from Table 16.
F. Inter-domain Distance Calculation
To compare how domain-invariant the LFC and HFC
are, we calculate the inter-domain distances for each and
constructed a matrix based on these values. From the
CIFAR10-C dataset, we select one class and randomly
choose a single uncorrupted original image from this class,
along with its corresponding 15 corrupted versions. Each
corrupted image is transformed into frequency domain fea-
tures using FFT. We then apply LPF and HPF to these fre-
quency domain features, extracting LFC and HFC in fre-
quency domain, respectively. To quantify the variation of
LFC and HFC features across different domains, we cal-
culate the cosine distances separately between the LFC fea-
tures and between the HFC features of images with different
corruptions. This results in a 15×15matrix for each fea-
ture type. We repeat this process for 10 randomly selected
13Table 10. FABN improves TTA baselines and complements DeepAug
under quantization on ImageNet-C. TTAs are conducted after DeepAug.
ResNet18 ResNet50
FP LSQ (2bit) FP LSQ (2bit)
NoDeepAug 22.36 16.25 27.74 19.76
DeepAug 41.29 27.74 51.91 32.97
NORM 49.82 36.72 58.59 42.53
+CoDA - 42.13 - 46.74
TENT 50.71 39.42 59.24 44.45
+CoDA - 42.44 - 46.97
SAR 54.24 42.27 57.14 46.70
+CoDA - 39.94 - 47.62
samples within the same class, averaging their resulting ma-
trices. Finally, we perform this procedure for all classes, ob-
taining the overall inter-domain cosine distance matrix. A
distance matrix with larger values indicates that the corre-
sponding frequency component exhibits greater variability
across domains. According to our resulting distance ma-
trix presented in Figure 3, the values in the LFC distance
matrix are relatively small, indicating that the LFC is more
domain-invariant compared to the HFC.
G. Applicability to Domian Generalization
methods
Table 10 show that CoDA remains orthogonal and com-
plementary to not only widely-used TTA but also domain
generalization (DG) methods. In all tested scenarios on
ImageNet-C, CoDA consistently improves TTA baselines
under low-bit quantization. Remarkably, in the ResNet-50
LSQ (2bit) setting, CoDA achieves 47.62%, outperform-
ing full-precision models with TTA by up to 5.09%p. On
ResNet-18, CoDA enhances methods like TENT and SAR,
with improvements of +3.02%p and +5.35%p, respectively,
compared to their quantized counterparts. These results
highlight that CoDA further boosts accuracy when com-
bined with a strong DG baseline, underscoring its distinct
contribution.
H. License of Assets
Datasets CIFAR10 (MIT License), CIFAR10-C (Apache
2.0), ImageNet-C (Apache 2.0), ImageNet-R (MIT Li-
cense), ImageNet-Sketch (MIT License)
Codes Code for Fourier transformation and low-/high-
pass filtering of EfficientTrain (MIT License), torchvision
for ResNet18 and ResNet50 (Apache 2.0), official repos-
itory of LQ (MIT License), official repository of TENT
(MIT License), official repository of SAR (BSD 3-Clause
License), and the official repository of CoTTA (MIT Li-
cense).
14Table 11. Average classification accuracy (%) and their standard deviations on ImageNet-C by ResNet18, shown per corruption, QAT, and
TTA method. Averaged over three runs.
QAT TTA Gaussian Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Brightness Contrast Elastic Pixelate JPEG Avg
FPw/o TTA10.59 10.58 10.08 16.18 10.26 18.35 20.29 17.01 14.46 25.94 47.60 21.89 37.48 36.37 38.2522.36±0.00 ±0.00 ±0.00 ±0.00 ±0.01 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00
NORM21.07 20.81 20.08 16.69 13.04 23.97 25.28 24.66 21.27 37.29 50.64 38.74 42.87 41.01 39.5929.13±0.08 ±0.11 ±0.01 ±0.11 ±0.03 ±0.12 ±0.05 ±0.07 ±0.05 ±0.08 ±0.08 ±0.06 ±0.05 ±0.12 ±0.01
Tent22.92 22.40 20.61 18.44 15.28 29.05 28.73 24.86 21.18 39.50 50.37 40.49 44.83 42.12 38.5130.62±0.18 ±0.12 ±0.05 ±0.15 ±0.14 ±0.06 ±0.05 ±0.05 ±0.14 ±0.20 ±0.10 ±0.07 ±0.11 ±0.02 ±0.16
SAR27.85 27.26 25.86 22.36 19.57 32.28 31.55 28.37 24.05 42.21 51.14 42.76 46.66 44.36 41.9333.88±0.04 ±0.06 ±0.09 ±0.15 ±0.24 ±0.13 ±0.08 ±0.05 ±0.04 ±0.24 ±0.02 ±0.19 ±0.13 ±0.06 ±0.06
LSQ
(2-bit)w/o TTA4.22 4.15 3.79 10.27 7.86 13.75 16.73 10.72 9.24 14.03 41.11 10.57 39.23 25.26 32.8016.25±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00
NORM16.27 15.92 15.44 9.44 11.89 18.26 21.90 19.04 18.40 27.15 45.82 27.04 44.09 38.49 37.4424.44±0.01 ±0.08 ±0.14 ±0.10 ±0.05 ±0.02 ±0.12 ±0.05 ±0.10 ±0.08 ±0.09 ±0.11 ±0.07 ±0.10 ±0.15
Tent23.34 20.96 20.40 15.35 16.36 23.39 27.01 19.44 18.73 30.73 46.07 30.37 45.17 40.29 38.5127.74±0.07 ±0.02 ±0.11 ±0.06 ±0.16 ±0.09 ±0.03 ±0.16 ±0.03 ±0.16 ±0.05 ±0.00 ±0.18 ±0.28 ±0.05
SAR25.52 23.49 22.76 17.56 18.73 25.79 28.65 21.00 19.87 33.12 46.22 32.96 45.52 41.32 39.5829.47±0.07 ±0.13 ±0.18 ±0.19 ±0.27 ±0.16 ±0.15 ±0.08 ±0.04 ±0.16 ±0.03 ±0.10 ±0.12 ±0.11 ±0.14
CoDA
(Ours)27.68 24.62 25.05 20.87 21.30 27.60 29.63 20.30 21.10 33.51 47.33 34.94 47.29 47.51 42.7731.43±0.04 ±0.08 ±0.03 ±0.05 ±0.06 ±0.02 ±0.09 ±0.13 ± 0.03 ±0.06 ±0.11 ±0.09 ±0.08 ±0.02 ±0.05
LQ
(2-bit)w/o TTA7.67 7.75 6.67 12.26 10.80 16.53 17.86 13.33 10.80 13.96 43.70 9.45 41.69 29.29 38.2018.66±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00
NORM19.45 18.86 18.02 14.55 15.15 23.46 25.65 22.87 21.11 33.92 48.63 35.67 45.85 39.79 42.2828.35±0.06 ±0.04 ±0.03 ±0.16 ±0.12 ±0.12 ±0.03 ±0.10 ±0.03 ±0.11 ±0.18 ±0.07 ±0.08 ±0.07 ±0.09
Tent23.73 21.76 21.33 17.68 18.30 27.56 29.00 23.56 21.65 36.57 48.38 36.93 46.44 41.44 42.0130.42±0.17 ±0.12 ±0.10 ±0.13 ±0.11 ±0.03 ±0.08 ±0.10 ±0.08 ±0.17 ±0.05 ±0.04 ±0.09 ±0.02 ±0.11
SAR23.67 21.75 21.30 17.67 18.18 27.49 28.93 23.61 21.64 36.62 48.32 37.00 46.52 41.51 42.0230.42±0.15 ±0.04 ±0.15 ±0.11 ±0.09 ±0.14 ±0.08 ±0.09 ±0.12 ±0.16 ±0.10 ±0.17 ±0.13 ±0.16 ±0.10
CoDA
(Ours)30.58 29.15 28.43 22.11 22.52 31.64 31.93 25.82 24.98 37.92 49.03 38.95 47.75 47.80 45.4834.27±0.11 ±0.14 ±0.03 ±0.11 ±0.07 ±0.16 ±0.04 ±0.10 ±0.10 ±0.04 ±0.04 ±0.02 ±0.03 ±0.16 ±0.09
LSQ
(4-bit)w/o TTA6.76 6.97 5.96 12.06 9.53 16.05 19.21 14.76 13.07 19.53 45.94 15.46 39.99 35.75 37.5419.91±0.01 ±0.00 ±0.01 ±0.00 ±0.00 ±0.02 ±0.00 ±0.02 ±0.01 ±0.01 ±0.01 ±0.01 ±0.00 ±0.02 ±0.02
NORM19.74 19.28 18.93 13.21 14.19 22.80 25.55 23.63 21.69 33.40 49.38 37.00 45.94 42.06 40.6528.50±0.57 ±0.74 ±0.77 ±1.40 ±1.44 ±1.79 ±0.87 ±0.62 ±0.14 ±0.85 ±0.15 ±0.56 ±0.81 ±0.06 ±0.75
Tent24.65 22.31 21.50 18.09 16.98 27.94 29.55 23.63 21.54 35.47 49.73 37.82 46.85 43.75 41.1030.73±0.05 ±0.09 ±0.06 ±0.02 ±0.04 ±0.10 ±0.12 ±0.03 ±0.02 ±0.07 ±0.21 ±0.22 ±0.11 ±0.12 ±0.01
SAR26.06 24.23 23.21 19.52 18.43 29.31 30.53 24.91 22.36 36.97 49.99 38.97 47.14 44.42 41.9431.87±0.03 ±0.13 ±0.07 ±0.07 ±0.04 ±0.04 ±0.12 ±0.11 ±0.07 ±0.05 ±0.12 ±0.13 ±0.16 ±0.13 ±0.11
CoDA
(Ours)28.18 26.41 25.19 22.11 21.12 30.69 31.48 23.67 23.27 36.70 50.26 40.09 48.89 49.87 44.5533.50±0.04 ±0.01 ±0.05 ±0.04 ±0.05 ±0.05 ±0.06 ±0.03 ± 0.10 ±0.04 ± 0.05 ±0.10 ±0.13 ±0.07 ±0.01
LQ
(4-bit)w/o TTA6.96 6.81 6.23 15.48 10.99 19.37 20.91 15.71 12.70 20.65 45.28 16.12 42.19 37.44 36.5620.89±0.00 ±0.00 ±0.00 ±0.03 ±0.02 ±0.01 ±0.00 ±0.01 ±0.00 ±0.01 ±0.01 ±0.02 ±0.00 ±0.03 ±0.01
NORM19.33 18.94 18.47 14.09 14.91 23.76 26.10 24.07 21.56 33.92 49.28 37.30 46.22 42.12 40.1128.68±0.70 ±0.71 ±0.74 ±1.49 ±1.40 ±1.72 ±1.02 ±0.74 ±0.07 ±0.93 ±0.06 ±0.57 ±0.79 ±0.07 ±0.73
Tent21.55 20.21 18.52 17.65 17.80 29.01 29.49 24.17 20.69 36.47 48.93 37.78 47.12 42.58 39.0330.07±0.17 ±0.10 ±0.09 ±0.04 ±0.02 ±0.08 ±0.07 ±0.09 ±0.10 ±0.05 ±0.10 ±0.21 ±0.12 ±0.05 ±0.16
SAR21.53 20.20 18.55 17.67 17.79 29.02 29.50 24.14 20.70 36.40 48.93 37.80 47.15 42.55 39.0630.07±0.16 ±0.11 ±0.06 ±0.06 ±0.06 ±0.10 ±0.05 ±0.11 ±0.03 ±0.02 ±0.04 ±0.21 ±0.12 ±0.13 ±0.07
CoDA
(Ours)29.02 27.89 25.92 24.77 23.50 32.86 32.54 25.72 24.52 38.28 49.89 40.92 48.66 48.60 43.8534.46±0.05 ±0.06 ±0.05 ±0.07 ±0.05 ±0.10 ±0.07 ±0.04 ±0.08 ±0.09 ±0.06 ±0.03 ±0.12 ±0.05 ±0.03
LSQ
(8-bit)w/o TTA5.77 6.06 4.95 12.26 10.14 16.67 20.05 15.66 13.44 20.07 45.94 15.47 39.12 38.64 37.9320.14±0.00 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.00 ±0.00 ±0.01 ±0.01 ±0.02 ±0.02 ±0.00
NORM20.66 20.20 20.28 14.53 14.60 23.77 26.15 24.14 22.26 35.12 49.67 37.42 45.25 43.11 41.3529.23±0.06 ±0.03 ±0.02 ±0.06 ±0.10 ±0.10 ±0.12 ±0.11 ±0.07 ±0.11 ±0.07 ±0.13 ±0.09 ±0.15 ±0.17
Tent24.76 22.45 21.89 17.55 17.93 28.98 29.94 24.41 22.37 36.75 50.09 38.38 46.75 44.45 42.0031.25±0.02 ±0.06 ±0.07 ±0.14 ±0.05 ±0.10 ±0.05 ±0.16 ±0.03 ±0.08 ±0.07 ±0.08 ±0.12 ±0.11 ±0.05
SAR26.24 23.94 23.38 18.77 19.01 29.96 30.62 25.32 23.06 37.87 50.24 39.03 47.03 44.91 42.6232.13±0.02 ±0.06 ±0.05 ±0.09 ±0.06 ±0.05 ±0.09 ±0.19 ±0.02 ±0.10 ±0.06 ±0.06 ±0.08 ±0.15 ±0.06
CoDA
(Ours)29.03 27.36 26.59 21.38 22.19 32.09 32.19 24.71 24.17 37.88 50.61 40.70 48.33 50.01 45.3034.17±0.07 ±0.09 ±0.11 ±0.06 ±0.03 ±0.06 ±0.04 ±0.10 ± 0.07 ±0.03 ±0.09 ±0.04 ±0.08 ±0.05 ±0.05
LQ
(8-bit)w/o TTA6.09 6.57 5.85 15.56 10.70 19.41 21.17 16.58 12.99 22.22 46.81 17.29 42.13 36.61 36.6221.11±0.01 ±0.00 ±0.01 ±0.00 ±0.00 ±0.00 ±0.01 ±0.00 ±0.01 ±0.02 ±0.01 ±0.01 ±0.01 ±0.00 ±0.00
NORM20.44 20.17 19.85 16.26 15.71 24.97 27.66 26.77 23.52 38.29 51.45 39.86 47.65 42.69 40.9430.42±0.05 ±0.02 ±0.07 ±0.03 ±0.08 ±0.10 ±0.04 ±0.11 ±0.02 ±0.02 ±0.06 ±0.05 ±0.02 ±0.03 ±0.03
Tent22.70 21.10 19.77 18.41 17.19 29.97 30.66 27.02 22.92 40.11 51.65 40.34 49.16 43.97 40.6831.71±0.06 ±0.05 ±0.05 ±0.07 ±0.11 ±0.15 ±0.09 ±0.06 ±0.06 ± 0.18 ±0.03 ±0.11 ±0.12 ±0.11 ±0.08
SAR22.71 21.12 19.77 18.36 17.16 29.91 30.62 27.04 22.92 40.11 51.66 40.35 49.15 43.97 40.6731.70±0.07 ±0.06 ±0.07 ±0.11 ±0.10 ±0.11 ±0.00 ±0.06 ±0.06 ±0.16 ±0.04 ±0.10 ±0.11 ±0.11 ±0.09
CoDA
(Ours)28.82 27.93 26.05 25.05 23.69 32.84 32.56 25.77 24.67 38.50 49.69 40.95 48.50 49.12 42.9934.48±0.16 ±0.06 ±0.15 ±0.32 ±0.13 ±0.03 ±0.06 ±0.04 ± 0.20 ±0.12 ±0.23 ± 0.01 ±0.24 ± 0.06 ±0.05
15Table 12. Average classification accuracy (%) and their standard deviations on ImageNet-C by ResNet50, shown per corruption, QAT, and
TTA method. Averaged over three runs.
QAT TTA Gaussian Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Brightness Contrast Elastic Pixelate JPEG Avg
FPw/o TTA15.45 14.99 14.50 21.22 12.66 23.18 24.82 22.47 20.50 32.61 55.83 28.78 42.44 40.03 46.6227.74±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00
NORM29.10 27.62 27.77 23.05 18.37 30.56 31.97 31.51 27.80 45.39 58.67 46.66 49.78 48.17 46.5336.20±0.08 ±0.08 ±0.08 ±0.11 ±0.05 ±0.09 ±0.02 ±0.05 ±0.03 ±0.10 ±0.06 ±0.06 ±0.10 ±0.10 ±0.11
Tent31.57 31.30 29.71 25.52 21.16 36.49 36.73 33.02 28.02 48.36 58.47 49.07 53.02 49.55 46.2438.55±0.19 ±0.19 ±0.03 ±0.06 ±0.07 ±0.07 ±0.02 ±0.13 ±0.16 ±0.03 ±0.06 ±0.11 ±0.07 ±0.06 ±0.09
SAR35.76 35.82 34.79 29.21 25.87 39.85 39.08 36.52 30.56 50.19 58.84 50.55 54.38 51.91 48.9241.48±0.11 ±0.15 ±0.25 ±0.10 ±0.10 ±0.10 ±0.19 ±0.08 ±0.12 ±0.06 ±0.03 ±0.07 ±0.04 ±0.09 ±0.12
LSQ
(2-bit)w/o TTA5.22 5.21 4.54 13.74 10.31 17.77 20.79 13.83 12.05 16.79 46.77 13.09 44.00 33.59 38.6919.76±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00 ±0.01 ±0.01 ±0.00
NORM21.69 21.06 20.55 14.58 16.98 24.17 28.22 24.66 23.53 34.13 51.83 35.37 50.30 46.10 43.4830.44±0.16 ±0.02 ±0.07 ±0.11 ±0.07 ±0.05 ±0.02 ±0.02 ±0.04 ±0.24 ±0.06 ±0.13 ±0.04 ±0.09 ±0.11
Tent23.16 22.79 21.13 21.81 21.84 30.37 33.47 25.10 24.07 37.65 52.28 39.13 51.12 47.13 44.3333.03±0.13 ±0.10 ±0.03 ±0.06 ±0.12 ±0.12 ±0.05 ±0.10 ±0.03 ±0.05 ±0.11 ±0.12 ±0.03 ±0.14 ±0.09
SAR25.03 24.62 22.95 23.08 22.91 31.39 34.07 26.26 24.60 38.66 52.45 40.04 51.36 47.69 45.1234.02±0.14 ±0.04 ±0.12 ±0.22 ±0.10 ±0.15 ±0.11 ±0.17 ±0.16 ±0.04 ±0.05 ±0.12 ±0.04 ±0.14 ±0.11
CoDA
(Ours)23.84 24.04 22.45 26.97 25.46 33.59 35.33 25.07 25.07 38.52 52.77 41.59 52.56 53.04 48.0535.22±0.08 ±0.08 ±0.11 ±0.09 ±0.02 ±0.03 ±0.15 ±0.11 ±0.09 ±0.07 ±0.09 ±0.06 ±0.02 ±0.08 ±0.08
LQ
(2-bit)w/o TTA8.89 8.95 8.19 17.25 11.49 21.72 25.12 17.67 13.33 21.14 49.39 15.66 43.98 30.58 40.1122.23±0.01 ±0.01 ±0.00 ±0.00 ±0.00 ±0.00 ±0.01 ±0.00 ±0.01 ±0.01 ±0.00 ±0.00 ±0.00 ±0.00 ±0.00
NORM22.57 22.06 21.72 18.28 18.21 27.56 30.87 28.76 25.36 38.23 54.74 40.91 50.61 46.47 43.3932.65±0.08 ±0.04 ±0.06 ±0.13 ±0.11 ±0.08 ±0.14 ±0.08 ±0.11 ±0.12 ±0.19 ±0.08 ±0.17 ±0.15 ±0.07
Tent25.09 24.95 23.25 23.35 21.45 32.95 35.15 29.32 26.15 41.38 54.74 43.22 52.02 47.99 44.4135.03±0.13 ±0.22 ±0.15 ±0.02 ±0.07 ±0.07 ±0.17 ±0.02 ±0.08 ±0.17 ±0.05 ±0.10 ±0.01 ±0.04 ±0.15
SAR25.10 24.93 23.13 23.37 21.49 32.96 35.16 29.31 26.14 41.39 54.79 43.25 52.07 48.04 44.4435.04±0.14 ±0.16 ±0.05 ±0.08 ±0.01 ±0.11 ±0.10 ±0.05 ±0.08 ±0.15 ±0.10 ±0.14 ±0.03 ±0.03 ±0.03
CoDA
(Ours)27.84 28.44 26.56 28.97 25.80 36.21 37.57 28.71 27.63 42.81 55.25 47.05 53.87 54.47 48.9738.01±0.12 ±0.14 ±0.04 ±0.02 ±0.18 ±0.07 ±0.04 ±0.02 ±0.07 ±0.18 ±0.13 ±0.11 ±0.09 ±0.09 ±0.05
LSQ
(4-bit)w/o TTA5.88 6.08 4.92 14.97 10.95 19.02 22.21 16.03 14.49 21.62 49.34 17.09 43.32 36.62 40.9921.57±0.01 ±0.01 ±0.02 ±0.01 ±0.01 ±0.00 ±0.01 ±0.03 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.01 ±0.04
NORM22.83 21.91 22.02 16.30 16.24 25.44 28.35 25.81 23.83 35.26 53.10 39.36 49.28 45.41 44.4131.30±0.07 ±0.08 ±0.06 ±0.06 ±0.06 ±0.05 ±0.07 ±0.07 ±0.04 ±0.09 ±0.17 ±0.03 ±0.09 ±0.13 ±0.01
Tent26.47 25.48 23.83 21.06 19.79 30.67 32.23 26.25 23.92 38.77 53.53 41.34 50.33 47.00 44.7833.70±0.06 ±0.11 ±0.08 ±0.07 ±0.04 ±0.11 ±0.03 ±0.06 ±0.08 ±0.09 ±0.20 ±0.04 ±0.05 ±0.09 ±0.08
SAR27.62 26.70 25.11 21.69 20.63 31.44 32.86 27.07 24.42 39.56 53.59 41.86 50.63 47.49 45.3734.40±0.16 ±0.07 ±0.11 ±0.09 ±0.08 ±0.12 ±0.03 ±0.03 ±0.08 ±0.07 ±0.12 ±0.07 ±0.06 ±0.21 ±0.05
CoDA
(Ours)29.96 29.41 27.61 25.11 23.03 33.81 33.99 25.59 25.56 39.01 53.91 43.28 51.82 53.33 48.2936.25±0.05 ±0.19 ±0.06 ±0.01 ±0.02 ±0.08 ±0.12 ±0.06 ±0.06 ±0.13 ±0.08 ±0.14 ±0.02 ±0.04 ±0.10
LQ
(4-bit)w/o TTA10.33 10.02 9.85 16.63 12.10 19.98 24.01 17.56 14.04 23.34 49.37 18.56 43.47 34.93 40.2322.96±0.01 ±0.01 ±0.01 ±0.02 ±0.03 ±0.03 ±0.00 ±0.06 ±0.03 ±0.01 ±0.01 ±0.01 ±0.05 ±0.02 ±0.01
NORM22.35 21.82 21.40 18.09 16.76 26.32 29.91 27.24 24.47 36.92 54.01 41.60 49.59 45.11 43.3631.93±0.08 ±0.09 ±0.08 ±0.11 ±0.13 ±0.07 ±0.07 ±0.08 ±0.18 ±0.13 ±0.11 ±0.07 ±0.14 ±0.08 ±0.06
Tent25.34 25.31 23.27 21.33 19.10 31.60 33.19 27.69 24.81 40.11 53.84 42.94 50.44 47.05 43.3633.96±0.18 ±0.08 ±0.13 ±0.06 ±0.05 ±0.17 ±0.10 ±0.06 ±0.12 ±0.09 ±0.10 ±0.04 ±0.06 ±0.06 ±0.07
SAR25.29 25.27 23.17 21.36 19.16 31.63 33.13 27.61 24.80 40.09 53.81 42.94 50.47 47.07 43.3933.95±0.06 ±0.05 ±0.17 ±0.09 ±0.08 ±0.06 ±0.12 ±0.04 ±0.12 ±0.07 ±0.09 ±0.06 ±0.07 ±0.05 ±0.02
CoDA
(Ours)28.82 28.99 26.30 26.19 22.97 34.54 35.00 26.69 25.83 40.57 54.01 45.24 51.47 53.23 47.5336.49±0.11 ±0.08 ±0.24 ±0.08 ±0.15 ±0.14 ±0.08 ±0.18 ± 0.12 ±0.15 ±0.06 ±0.17 ±0.24 ±0.17 ±0.14
LSQ
(8-bit)w/o TTA7.03 6.79 6.20 15.63 11.69 19.68 22.38 17.40 15.50 22.26 49.14 18.45 44.20 40.63 41.7722.58±0.01 ±0.03 ±0.00 ±0.01 ±0.01 ±0.00 ±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.01 ±0.00 ±0.01
NORM23.72 23.31 22.40 16.92 16.66 26.24 28.47 26.41 24.18 36.36 52.86 39.79 49.16 46.96 44.6231.87±0.04 ±0.05 ±0.07 ±0.01 ±0.12 ±0.06 ±0.04 ±0.11 ±0.06 ±0.09 ±0.12 ±0.08 ±0.12 ±0.06 ±0.03
Tent28.32 27.43 25.94 20.27 19.86 30.76 31.87 26.90 24.39 39.11 53.34 40.18 50.28 47.88 45.1834.11±0.06 ±0.08 ±0.05 ±0.16 ±0.12 ±0.14 ±0.06 ±0.07 ±0.04 ±0.09 ±0.06 ±0.07 ±0.05 ±0.15 ±0.09
SAR29.28 28.43 27.07 21.26 20.88 31.59 32.66 27.76 25.06 40.12 53.48 40.86 50.59 48.25 45.7134.87±0.14 ±0.09 ±0.08 ±0.11 ±0.04 ±0.13 ±0.14 ±0.05 ±0.05 ±0.03 ±0.11 ±0.05 ±0.04 ±0.10 ±0.12
CoDA
(Ours)32.11 31.25 30.07 23.54 23.45 33.78 34.17 26.09 25.67 39.03 53.89 42.80 52.06 53.81 48.4336.68±0.05 ±0.08 ±0.03 ±0.07 ±0.03 ±0.19 ±0.02 ±0.09 ± 0.08 ±0.08 ± 0.04 ±0.09 ±0.06 ±0.03 ±0.09
LQ
(8-bit)w/o TTA2.81 3.31 2.86 6.46 7.48 10.34 10.55 9.94 5.52 15.24 34.63 11.91 27.05 25.73 27.8613.45±0.01 ±0.00 ±0.00 ±0.02 ±0.01 ±0.00 ±0.02 ±0.01 ±0.01 ±0.01 ±0.01 ±0.00 ±0.01 ±0.01 ±0.01
NORM17.15 17.11 16.70 12.85 13.87 18.56 19.20 23.38 21.09 29.85 43.86 27.56 35.68 32.42 31.9324.08±0.06 ±0.03 ±0.04 ±0.09 ±0.05 ±0.05 ±0.03 ±0.06 ±0.05 ±0.07 ±0.06 ±0.04 ±0.07 ±0.02 ±0.12
Tent24.80 24.58 22.17 17.26 16.68 25.15 25.12 27.65 23.48 37.05 47.30 33.35 43.00 39.25 37.6829.63±0.00 ±0.05 ±0.06 ±0.09 ±0.12 ±0.08 ±0.05 ±0.08 ±0.07 ±0.07 ±0.07 ±0.05 ±0.10 ±0.03 ±0.05
SAR24.75 24.59 22.16 17.25 16.67 25.13 25.13 27.65 23.48 37.04 47.28 33.33 43.00 39.26 37.6829.63±0.06 ±0.03 ±0.06 ±0.11 ±0.11 ±0.06 ±0.04 ±0.08 ±0.07 ±0.06 ±0.07 ±0.05 ±0.11 ±0.01 ±0.05
CoDA
(Ours)31.78 31.97 29.79 26.51 25.25 33.70 32.31 27.41 25.70 39.01 47.88 37.41 46.46 46.83 43.2735.02±0.06 ±0.06 ±0.11 ±0.01 ±0.07 ±0.07 ±0.03 ±0.05 ± 0.14 ±0.05 ±0.13 ±0.14 ±0.06 ±0.08 ±0.01
16Table 13. Average classification accuracy (%) on ImageNet-C by ResNet18, shown per corruption, QAT, and TTA method.
QAT TTA gaus shot impul defcs gls mtn zm snow frost fog brt cnt els px jpg A VG
FPw/o TTA 10.59 10.58 10.08 18.26 10.26 18.35 20.29 17.01 14.46 25.94 21.89 37.26 47.60 36.37 38.25 22.36
NORM 20.98 20.97 20.07 16.72 13.02 23.84 25.24 19.73 24.58 21.23 37.26 50.63 48.00 42.83 40.87 29.09
TENT 22.72 22.53 20.58 18.26 15.13 29.06 31.54 28.48 21.21 21.59 44.43 49.11 44.82 42.13 38.61 30.61
SAR 27.82 27.19 25.76 22.21 19.83 31.51 35.33 20.38 25.49 28.50 41.66 51.16 46.67 44.24 41.15 33.91
LSQ (2-bit)w/o TTA 4.22 4.15 3.79 10.57 7.86 13.75 15.73 10.72 9.24 14.03 19.43 39.23 39.28 25.26 32.80 16.25
CODA 27.69 24.57 25.06 20.88 21.28 27.62 29.73 20.38 21.13 33.45 47.37 47.53 46.77 46.95 42.71 31.45
SAR + CODA 27.79 24.85 25.23 21.19 21.62 28.16 29.89 20.63 21.42 34.10 47.42 47.59 42.89 47.77 42.82 31.73
TENT + CODA 25.63 23.34 25.07 21.44 21.01 29.02 29.99 19.97 20.38 21.13 34.14 46.79 46.77 46.95 42.66 31.22
LQ (2-bit)w/o TTA 7.67 7.75 6.67 12.26 10.80 16.53 17.86 13.33 10.80 13.96 43.70 9.45 41.69 29.29 38.20 18.66
CODA 30.45 28.99 28.46 22.23 22.58 31.83 31.98 25.88 25.09 37.90 48.99 38.98 47.77 47.63 45.47 34.28
SAR + CODA 30.45 28.97 28.49 22.23 22.59 31.81 32.04 25.91 25.09 37.93 48.99 38.93 47.79 47.69 45.48 34.29
TENT + CODA 30.46 29.05 28.39 22.23 22.37 31.73 31.91 25.85 24.91 37.70 48.80 38.52 47.76 47.53 45.36 34.17
LSQ (4-bit)w/o TTA 6.75 6.97 5.98 12.06 9.53 16.03 19.21 14.78 13.06 19.52 45.93 15.47 39.98 35.77 37.56 19.91
CODA 28.15 26.40 25.14 22.14 21.16 30.66 31.41 23.69 23.24 36.75 50.32 40.08 48.79 49.90 44.56 33.49
SAR + CODA 28.35 26.63 25.42 22.40 21.26 30.98 31.74 23.98 23.35 37.17 50.30 40.50 48.95 49.80 44.72 33.70
TENT + CODA 28.10 26.49 25.23 23.00 21.14 31.21 31.50 23.42 22.65 37.14 49.93 41.02 48.54 49.47 44.63 33.56
LQ (4-bit)w/o TTA 6.96 6.81 6.23 15.45 10.97 19.39 20.91 15.70 12.70 20.66 45.29 16.09 42.18 37.47 36.55 20.89
CODA 29.00 27.96 25.97 24.79 23.52 32.76 32.48 25.76 24.53 38.27 49.92 40.93 48.67 48.55 43.89 34.47
SAR + CODA 28.39 27.01 25.44 24.67 22.65 32.24 31.79 24.74 23.63 37.74 48.94 40.74 48.37 48.09 43.04 33.83
TENT + CODA 28.39 26.70 25.30 24.69 22.47 32.38 31.77 24.69 23.40 37.68 48.89 40.34 48.17 47.84 42.97 33.71
LSQ (8-bit)w/o TTA 5.77 6.07 4.94 12.25 10.15 16.66 20.06 15.65 13.44 20.07 45.95 15.46 39.15 38.62 37.94 20.15
CODA 29.11 27.31 26.56 21.45 22.16 32.15 32.23 24.73 24.11 37.85 50.67 40.69 48.41 50.07 45.34 34.19
SAR + CODA 29.26 27.77 26.88 21.76 22.38 32.25 32.42 25.01 24.48 38.27 50.76 40.85 48.51 50.08 45.39 34.40
TENT + CODA 29.55 27.93 27.05 22.39 22.28 32.64 32.22 24.69 24.27 38.25 50.40 41.45 48.06 49.76 45.23 34.41
LQ (8-bit)w/o TTA 6.10 6.57 5.86 15.56 10.70 19.41 21.16 16.58 13.00 22.20 46.82 17.28 42.12 36.61 36.62 21.11
CODA 28.66 27.94 26.18 25.27 23.80 32.86 32.56 25.81 24.81 38.56 49.63 40.96 48.32 49.08 43.04 34.50
SAR + CODA 28.70 27.93 26.20 25.18 23.76 32.85 32.58 25.68 24.77 38.66 49.46 40.90 48.41 48.98 42.97 34.47
TENT + CODA 28.67 27.96 26.14 25.12 23.72 32.62 32.47 25.57 24.69 38.51 49.26 40.69 48.26 48.86 42.77 34.35
Table 14. Average classification accuracy (%) on ImageNet-C by ResNet50, shown per corruption, QAT, and TTA method.
QAT TTA gaus shot impul defcs gls mtn zm snow frost fog brt cnt els px jpg A VG
FPw/o TTA 15.45 14.99 14.5 21.22 12.66 23.18 24.82 22.47 20.50 32.61 55.83 28.78 42.44 40.03 46.62 27.74
NORM 29.18 27.57 27.85 23.13 18.32 30.61 31.96 31.55 27.83 45.39 58.69 46.63 49.76 48.21 46.60 36.22
TENT 31.69 31.10 29.68 25.57 21.10 36.41 36.71 32.98 28.2 48.39 58.48 48.99 52.94 49.56 46.22 38.53
SAR 35.81 35.66 34.56 29.32 25.78 39.79 39.25 36.43 30.70 50.17 58.84 50.62 54.40 52.00 48.96 41.49
LSQ (2-bit)w/o TTA 5.22 5.21 4.54 13.74 10.31 17.77 20.79 13.83 12.05 16.79 46.77 13.09 44.00 33.59 38.69 19.76
CODA 23.77 24.13 22.55 27.07 25.44 33.57 35.45 25.13 24.98 38.57 52.84 41.54 52.55 52.99 48.13 35.25
TENT + CODA 24.09 24.29 22.84 27.18 25.46 33.61 35.31 25.15 25.07 38.71 52.83 41.57 52.51 52.97 48.33 35.33
SAR + CODA 24.92 25.19 23.07 27.88 25.88 34.32 35.54 25.48 25.16 39.68 52.80 42.53 52.68 52.98 48.52 35.78
LQ (2-bit)w/o TTA 8.89 8.96 8.19 17.25 11.49 21.72 25.12 17.67 13.33 21.14 49.39 15.66 43.98 30.58 40.11 22.23
CODA 27.87 28.51 26.53 28.96 25.88 36.14 37.57 28.70 27.71 42.77 55.40 47.01 53.79 54.57 49.02 38.03
TENT + CODA 27.90 28.40 26.52 29.13 25.56 36.28 37.40 28.65 27.51 42.76 55.15 46.96 54.13 54.26 49.17 37.99
SAR + CODA 27.97 28.47 26.46 28.98 25.76 35.95 37.34 28.50 27.50 42.60 54.97 46.61 53.99 54.37 49.04 37.90
LSQ (4-bit)w/o TTA 5.89 6.09 4.89 14.98 10.96 19.02 22.22 16.07 14.48 21.61 49.35 17.10 43.34 36.63 40.95 21.57
CODA 30.00 29.29 27.65 25.12 23.03 33.88 33.93 25.53 25.54 38.96 53.90 43.18 51.82 53.30 48.30 36.23
TENT + CODA 30.12 29.60 27.96 25.10 23.11 34.08 34.09 25.84 25.70 39.13 53.96 43.37 51.94 53.36 48.45 36.39
SAR + CODA 30.33 29.77 27.94 25.69 23.39 34.14 34.37 25.63 25.35 39.72 53.69 43.67 51.82 53.43 48.39 36.49
LQ (4-bit)w/o TTA 10.34 10.02 9.84 16.65 12.13 19.95 24.01 17.62 14.01 23.33 49.38 18.57 43.41 34.95 40.24 22.96
CODA 28.73 28.93 26.10 26.12 23.13 34.50 35.08 26.55 25.75 40.58 54.06 45.14 51.32 53.42 47.60 36.47
TENT + CODA 28.39 27.01 25.44 24.67 22.65 32.24 31.79 24.74 23.63 37.74 48.94 40.74 48.37 48.09 43.04 33.83
SAR + CODA 28.67 28.80 26.08 26.05 23.19 34.54 34.81 26.57 25.90 40.61 53.96 44.94 51.44 53.04 47.37 36.40
LSQ (8-bit)w/o TTA 7.02 6.82 6.20 15.62 11.70 19.68 22.37 17.39 15.51 22.27 49.15 18.48 44.21 40.63 41.78 22.59
CODA 32.06 31.28 30.04 23.47 23.44 33.72 34.19 26.04 25.63 39.02 53.86 42.74 52.12 53.79 48.36 36.65
TENT + CODA 32.17 31.35 30.30 23.69 23.50 33.79 34.19 26.19 25.73 39.19 54.05 42.89 52.14 53.78 48.47 36.76
SAR + CODA 32.16 31.45 30.26 24.09 23.60 33.80 34.13 26.16 25.39 39.20 53.76 43.05 51.99 53.53 48.34 36.73
LQ (8-bit)w/o TTA 3.35 4.00 3.26 9.48 9.87 13.59 14.13 11.54 9.57 18.96 37.39 12.65 31.59 25.64 30.51 15.70
CODA 33.36 33.18 31.55 25.58 25.51 32.70 33.38 26.71 25.80 38.85 47.63 37.19 46.77 46.81 43.08 35.21
TENT + CODA 33.48 33.41 31.83 25.57 25.44 32.66 33.36 26.64 25.65 38.79 47.58 37.08 46.69 46.75 43.27 35.21
SAR + CODA 33.45 33.30 31.77 25.59 25.34 32.55 33.20 26.46 25.49 38.63 47.35 36.88 46.50 46.62 43.16 35.09
17Table 15. Average classification accuracy (%) and their standard deviations on CIFAR10-C by ResNet26, shown per corruption, QAT, and
TTA method. Averaged over three runs.
QAT TTA Gaussian Shot Impulse Defocus Glass Motion Zoom Snow Frost Fog Brightness Contrast Elastic Pixelate JPEG Avg
FPw/o TTA31.80 37.39 29.29 57.88 45.04 63.94 55.67 68.09 55.59 65.06 85.69 33.26 67.94 42.12 69.2153.86±0.01 ±0.01 ±0.01 ±0.01 ±0.01 ±0.02 ±0.01 ±0.01 ±0.02 ±0.02 ±0.01 ±0.00 ±0.01 ±0.01 ±0.00
NORM58.59 60.87 51.62 73.24 55.21 75.02 71.96 72.19 68.49 76.85 86.12 60.38 71.75 66.17 70.7367.95±0.14 ±0.12 ±0.07 ±0.16 ±0.21 ±0.02 ±0.10 ±0.02 ±0.18 ±0.15 ±0.09 ±0.22 ±0.18 ±0.06 ±0.19
Tent67.08 69.46 59.09 81.85 59.22 80.21 79.90 76.91 74.86 78.53 86.72 82.47 71.45 74.56 71.1874.23±0.26 ±0.15 ±0.22 ±0.05 ±0.27 ±0.13 ±0.10 ±0.01 ±0.18 ±0.15 ±0.16 ±0.28 ±0.08 ±0.08 ±0.28
SAR65.67 68.19 57.81 81.02 58.08 79.20 78.88 76.42 74.03 77.51 86.75 81.38 70.78 73.37 69.9473.27±0.15 ±0.17 ±0.28 ±0.12 ±0.07 ±0.03 ±0.09 ±0.27 ±0.07 ±0.08 ±0.09 ±0.09 ±0.05 ±0.08 ±0.17
LSQ
(2-bit)w/o TTA34.88 39.95 36.96 47.92 41.50 53.61 51.67 60.97 50.95 54.98 80.14 23.06 65.69 38.23 72.0050.17±0.03 ±0.19 ±0.14 ±0.19 ±0.12 ±0.29 ±0.29 ±0.14 ±0.04 ±0.13 ±0.21 ±0.04 ±0.13 ±0.08 ±0.13
NORM58.46 61.17 55.46 67.46 56.32 67.82 69.41 65.25 64.93 67.63 80.76 42.01 71.61 62.34 73.5564.28±0.04 ±0.46 ±0.12 ±0.17 ±0.15 ±0.20 ±0.52 ±0.35 ±0.23 ±0.15 ±0.17 ±0.49 ±0.17 ±0.39 ±0.16
Tent65.10 67.51 61.41 73.63 59.06 71.04 74.91 68.14 67.36 67.09 79.83 56.17 70.49 70.12 72.1568.27±0.05 ±0.15 ±0.44 ±0.32 ±0.12 ±0.26 ±0.04 ±0.47 ±0.02 ±0.34 ±0.15 ±1.02 ±0.19 ±0.05 ±0.33
SAR62.72 65.24 59.88 71.89 56.89 69.18 72.52 66.02 65.98 63.49 79.10 56.26 68.78 67.34 70.8666.41±0.03 ±0.31 ±0.28 ±0.14 ±0.43 ±0.31 ±0.26 ±0.32 ±0.28 ±0.17 ±0.08 ±0.10 ±0.19 ±0.23 ±0.23
CoDA
(Ours)70.90 71.86 65.99 77.47 73.89 74.46 77.19 72.09 74.31 55.53 76.74 45.78 74.51 78.36 75.2270.95±0.40 ±0.21 ±0.23 ±0.21 ±0.23 ±0.15 ±0.11 ±0.27 ±0.14 ±0.28 ±0.09 ±0.16 ±0.24 ±0.12 ±0.10
LQ
(2-bit)w/o TTA45.20 48.97 47.82 52.42 46.04 56.24 55.70 65.97 56.72 59.32 83.53 19.19 68.15 38.16 71.5154.33±0.06 ±0.22 ±0.42 ±0.16 ±0.06 ±0.10 ±0.07 ±0.03 ±0.12 ±0.20 ±0.11 ±0.20 ±0.24 ±0.14 ±0.12
NORM63.78 66.26 60.30 69.34 55.66 70.55 71.33 70.78 69.24 71.69 84.35 32.99 71.96 63.61 73.6066.36±0.23 ±0.37 ±0.23 ±0.14 ±0.18 ±0.50 ±0.24 ±0.03 ±0.09 ±0.21 ±0.24 ±0.27 ±0.25 ±0.23 ±0.06
Tent69.35 71.58 61.86 80.02 59.27 78.04 79.17 73.53 73.51 75.32 85.08 74.92 72.02 73.30 73.4773.36±0.14 ±0.02 ±0.18 ±0.16 ±0.52 ±0.33 ±0.27 ±0.19 ±0.38 ±0.36 ±0.21 ±0.38 ±0.20 ±0.17 ±0.34
SAR69.32 71.60 61.88 80.05 59.30 78.01 79.11 73.55 73.42 75.27 85.10 74.90 72.06 73.30 73.4573.35±0.03 ±0.04 ±0.25 ±0.20 ±0.40 ±0.30 ±0.23 ±0.19 ±0.39 ±0.50 ±0.34 ±0.33 ±0.15 ±0.19 ±0.32
CoDA
(Ours)72.98 74.26 66.94 83.14 77.69 79.45 81.87 77.01 79.37 67.03 81.68 62.82 78.44 83.32 76.2776.15±0.29 ±0.12 ±0.29 ±0.34 ±0.15 ±0.29 ±0.38 ±0.36 ±0.02 ±0.38 ±0.19 ±0.05 ±0.10 ±0.26 ±0.20
Table 16. Model size and top-1 accuracy (%) comparison on ImageNet-C for each TTA method.
QATResNet18 ResNet50
Total num. of bits w/o TTA NORM TENT SARCoDA
(Ours)Total num. of bits w/o TTA NORM TENT SARCoDA
(Ours)
FP 374,064,384 22.36 29.13 30.62 33.88 - 27.74 36.20 38.55 41.48 -
LSQ (2-bit) 23,380,368 16.25 24.44 27.74 29.47 31.43 51,117,776 19.76 30.44 33.03 34.02 35.22
LQ (2-bit) 23,751,568 18.66 28.35 30.42 30.42 34.27 51,972,304 22.23 32.65 35.03 35.04 38.01
LSQ (4-bit) 46,759,392 19.91 28.50 30.73 31.87 33.50 102,232,096 21.57 31.30 33.70 34.40 36.25
LQ (4-bit) 47,503,136 20.89 28.68 30.07 30.07 34.46 103,944,608 22.96 31.93 33.96 33.95 36.49
LSQ (8-bit) 93,517,440 20.14 29.23 31.25 32.13 34.17 204,460,736 22.58 31.87 34.11 34.87 36.68
LQ (8-bit) 95,006,272 21.11 30.42 31.71 31.70 34.48 207,889,216 13.45 24.08 29.63 29.63 35.02
18