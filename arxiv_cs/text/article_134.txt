arXiv:2505.20734v1  [cs.LG]  27 May 2025Adversarial bandit optimization for
approximately linear functions
Zhuoyu Cheng1[0009 −0002−6631−4929], Kohei Hatano1,2[0000 −0002−1536−1269], and
Eiji Takimoto1[0000 −0001−9542−2553]
1Department of Informatics, Kyushu University, Japan
2RIKEN AIP, Japan
cheng.zhuoyu.874@s.kyushu-u.ac.jp
{hatano,eiji}@inf.kyushu-u.ac.jp
Abstract. We consider a bandit optimization problem for non-convex
andnon-smoothfunctions,whereineachtrialthelossfunctionisthesum
of a linear function and a small but arbitrary perturbation chosen after
observingtheplayer’schoice.Wegivebothexpectedandhighprobability
regret bounds for the problem. Our result also implies an improved high-
probabilityregretboundforthebanditlinearoptimization,aspecialcase
with no perturbation. We also give a lower bound on the expected regret.
Keywords: Bandit linear optimization ·Second keyword ·Another key-
word.
1 Introduction
Bandit optimization is a sequential game between a player and an adversary.
The game is played over Trounds, where Tis a positive natural number called
the horizon. The game is specified by a pair (K,F), where K ⊆Rdis a bounded
closed convex set and F ⊆ { f:K → R}is a function class. In each round
t∈[T], the player first chooses an action xt∈ Kand the adversary chooses
a loss function ft∈ F, and then the player receives the value ft(xt)as the
loss. Note that ftitself is unknown to the player. In this paper, we assume the
adversary is oblivious, i.e., the loss functions are specified before starting the
game3. The goal of the player is to minimize the regret
TX
t=1ft(xt)−min
x∈KTX
t=1ft(x) (1)
in expectation (expected regret) or with high probability (high-probability re-
gret).
For convex loss functions, the bandit optimization has been extensively stud-
ied (see, e.g.,[2,9,15]). O(d1/3T3/4)regret bounds are shown by Flaxman et
3We do not consider the case where the adversary is adaptive, i.e., it can choose the
t-th loss function ftdepending on the previous actions x1, . . . , x t−1.2 Z. Cheng et al.
al.[10]. Lattimore shows an information-theoretic regret bound eO(d2.5√
T)for
convex loss functions[14]. For linear loss functions, Abernethy et al. propose the
SCIRBLE algorithm and give an expected regret bound O(d√
TlnT)[2], achiev-
ing optimal dependence on T[8]. Lee et al. propose the SCRIBLE with lifting
and show a high-probability regret bound eO(d2√
T)[15].
Recently, non-convex functions are also getting popular in this literature. For
example, Agarwal et al. show a regret bound O(poly(d)T2/3)for smooth and
bounded non-convex functions[4]. Ghai et al. propose algorithms with regret
bounds O(poly(d)T2/3)under the assumption that non-convex functions are
reparametlized as some convex functions[12].
In this paper, we investigate the bandit optimization problem for a class of
non-convex non-smooth loss functions. Thefunction class consists of non-smooth
and non-convex functions that are "close" to linear functions, in the sense that
functions in the class can be viewed as linear functions with adversarial non-
convex perturbations whose amount is up to ϵ. Bandit optimization for linear
loss functions with stochastic noise (e.g., [1,5]) cannot be applied to our problem.
Also, standard Bandit linear optimization methods for estimating the gradient,
such as self-concordant barrier regularizer[13], cannot be effectively applied to
our problem. We propose a novel approach to analyzing high-probability regret,
introducing a new method for decomposing regret. Additionally, we propose a
novel method to bound ∥xt−u∥, where u∈ K, to account for the impact of
perturbations.
1. When ϵ̸= 0, we propose a modification of the SCRIBLE with lifting and
increasing learning rates [15] and utilize the properties of the ν-normal
barrier[17] to prove its high probability regret bound eO(d√
T+ϵdT), and we
also obtain its expected regret O(d√
TlnT+ϵdT).
2. When ϵ= 0, this problem becomes Bandit linear optimization, a special
case with no perturbation. Compared to Lee et al.’s results[15], holding with
probability 1−γ,O(ln2(dT)d2lnTq
Tlnln(dT)
γ), we use a different regret
decomposition approach to achieve a better high-probability regret bound
O(d√
TlnT+ lnTq
Tln(lnT
γ) + ln(lnT
γ)).
3. We prove a lower bound Ω(ϵT), implying that our bounds are tight w.r.t.
the parameter ϵ.
2 Related Work
The bandit linear optimization was first proposed by Awerbuch & Kleinberg
[6], who achieved a regret bound of O(d3/5T2/3)against an oblivious adver-
sary. Later, McMahan & Blum established a regret bound of O(dT3/4)when
facing an adaptive adversary[16]. A foundational approach in bandit optimiza-
tion problems involves gradient-based smoothing techniques. Abernethy et al.
presented pioneering work in this area and achieved an expected regret bound
ofO(d√
TlnT)when dealing with an oblivious adversary[3]. Bartlett et al.Adversarial bandit optimization for approximately linear functions 3
proposed a high-probability regret bound of O(d2/3√
TlndT)under a special
condition[7]. Subsequently, Lee et al. presented a high-probability regret bound
eO(d2√
T)for both oblivious and adaptive adversaries[15].
Unlike convex bandit problems, which have been extensively explored and
analyzed, non-convex bandits introduce unique challenges due to the complexity
of exploring and exploiting in a non-convex area. Gao et al. considered both
non-convex losses and non-stationary data and established a regret bound of
O(p
T+poly(T)[11]. Yang et al. achieved a regret bound of O(√TlogT)for
non-convex loss functions[19]. However, they both required the loss functions
to have smoothness properties, and our loss functions are neither convex nor
smooth.
2.1 Comparison to Lee et al.[15]
Our approach builds upon Lee et al.’s work. Below, we highlight the key differ-
ences between our method and Lee et al.’s in the context of the oblivious bandit
setting:
1. Simplified Regret Analysis: While Lee et al.’s regret analysis introduces un-
necessarycomplexityfortheobliviousbanditsetting,ourapproachsimplifies
the analysis, leading to more streamlined results.
2. Reduced Dependence on d: Lee et al.’s analysis results in a regret bound
with greater dependence on d, whereas our method derives a bound with
significantly reduced dependence on d(This distinction is demonstrated in
the introduction and further illustrated in the case where ϵ= 0).
3. Revised Generality of Problem Setting: Like the SCIRBLE algorithm, our
approach is more general, treating bandit linear optimization as a special
case within a broader problem framework.
3 Preliminaries
This section introduces some necessary notations and defines ϵ-approximately
linear function. Then we give our problem setting.
3.1 Notation
We abbreviate the 2-norm ∥·∥2as∥·∥. For a twice differentiable convex function
R:Rd→Rand any x, h∈Rd, let∥h∥x=∥h∥∇2R(x)=p
h⊤∇2R(x)h, and
∥h∥∗
x=∥h∥(∇2R(x))−1=p
h⊤(∇2R(x))−1h, respectively.
For any v∈Rd, letv⊥be the space orthogonal to v. LetSd
1={x| ∥x∥= 1}.
Thevector ei∈Rdisa standardbasis vectorwith avalueof 1inthe i-thposition
and0in all other positions. Iis an identity matrix with dimensionality implied
by context.4 Z. Cheng et al.
3.2 Problem Setting
LetK ⊆Rdbe a bounded and closed convex set such that for any x, y∈ K,
∥x−y∥ ≤D. Furthermore, we assume that Kcontains the unit ball centered at
the zero vector. Otherwise, we can apply an affine transformation to translate
the center point of the convex set to the origin. Let K′={(x,1) :x∈ K}. For
anyδ∈(0,1), letKδ={x|1
1−δx∈ K}andK′
δ={(x,1) :x∈ Kδ}, respectively.
Definition 1. A function f:K → Risϵ-approximately linear if there exists
θf∈Rdsuch that ∀x∈ K,|f(x)−θ⊤
fx| ≤ϵ.
For convenience, in the definition above, let σf(x) =f(x)−θ⊤
fx, and we omit
the subscript fofθfandσfif the context is clear. Note that |σ(x)| ≤ϵfor any
x∈ K.
In this paper, we consider the bandit optimization (K,F), where Fis the set
ofϵ-approximately linear functions f(x) =θ⊤x+σ(x)with∥θ∥ ≤G. The bandit
optimization for ϵ-approximately linear functions can be defined as the following
statement. For every round t= 1, .., T, the player chooses an action xt∈ K,
and the adversary simultaneously chooses a linear function ft(xt)(=θ⊤
txt)at
the same time. After observing the player’s choice xt, the adversary chooses a
perturbation σt(xt)(|σt(xt)| ≤ϵ). The value of ϵ-approximately linear function
ft(xt) =θ⊤
txt+σt(xt)is then revealed to the player. The goal of the player is
to minimize the regretPT
t=1ft(xt)−minx∈KPT
t=1ft(x).
4 Main Results
In this section, we first review SCRIBLE with lifting and increasing learning
rates, followed by presenting the main contributions of this paper with detailed
explanations.
4.1 SCRIBLE with lifting and increasing learning rates[15]
SCRIBLE with lifting and increasing learning rates introduces a dummy coordi-
nate with a value of 1 appended to all actions, resulting in the lifted decision set
K′={(x,1) :x∈ K}. This transformation lifts the bandit linear optimization
problem to Rd+1. The conic hull of this set is con(K) ={0}∪{(x, b) :x
b∈ K, x∈
Rd, b > 0}.
The algorithm performs SCRIBLE over the lifted decision set, using a ν-
normalbarrier Rdefinedoverthe con(K)(whichalwaysexists)astheregularizer
togeneratethesequence x′
1, ..., x′
t.Itset y′
t=x′
t+Atµtwhere At= [∇2R(x′
t)]−1
2
andµtis uniformly sampled at random from Sd+1
1∩(Ated+1)⊥. Since µtis
orthogonal to Ated+1, the last coordinate of Atµtis zero, ensuring that y′
t=
(yt,1)remains within K′. The actual point played is still yt. After playing ytand
observing ft(=θ⊤
tyt),itconstructsthelossestimatorthesamewayasSCRIBLE:
gt=d ft(yt)A−1
tµt. The analysis by Lee et al. shows that the first dcoordinates
ofgtare indeed an unbiased estimator of θt[15].Adversarial bandit optimization for approximately linear functions 5
SCRIBLE with lifting and increasing learning rates[15] utilizes the proper-
ties of the ν-normal barrier R, which are not available in the κ-self concor-
dant barrier T, as well as increasing learning rates, to bound ∥h∥∇2R(x′
t)≤
−∥h∥∇2R(x′
t+1)+νln(νT+ 1), where h∈ K′. Besides, Lee et al. divide the regret
mainly into two parts:PT
t=1[y⊤
tθt−x⊤
tgt+u⊤(gt−θt)]andPT
t=1(yt−u)⊤gt,
where u∈ K. They provide a bound forPT
t=1(yt−u)⊤gtand a high-probability
bound forPT
t=1[y⊤
tθt−x⊤
tgt+u⊤(gt−θt)], which helps derive high-probability
regretboundsforbothobliviousandadaptiveadversaries.However,forourprob-
lem, the bound they obtain for ∥h∥∇2R(x′
t)is still too large and does not help in
bounding ∥x′
t−h∥∇2R(x′
t). Again, since the largest eigenvalue of ∇2R(xt′)[17]
could potentially approach infinity, bounding ∥x′
t−h∥∇2R(x′
t)remains a chal-
lenge, and no previous work has addressed this.
4.2 SCRIBLE with lifting
For the decision set Kwith a ν-normal barrier on con(K), where con(K) =
{0} ∪ { (x, b) :x
b∈ K, x∈Rd, b > 0}, we apply Algorithm 1 to approximately
linear functions. Recall K′={(x,1) :x∈ K}.
We simplify the SCRIBLE with lifting and increasing learning rates[15]. We
do not use the increasing learning rates part but retain the lifting. This pre-
serves its advantages; for instance, the ν-normal barrier Ralways exists on
con(K). Additionally, the actual point played by the algorithm is ytandy′
t=
(yt,1) = x′
t+Atµtalways remains within K′. Furthermore, it adopts the same
update method as FTRL algorithm[13]: x′
t+1= arg min
x′∈K′ηPt
τ=1g⊤
τx′+R(x′).
Although it constructs the same loss estimator, gt=d ft(yt)A−1
tµtas the original
algorithm, gtis no longer an unbiased estimator of θt
Algorithm 1 SCRIBLE with lifting
Input: T, parameters η∈R, δ∈(0,1),ν-normal barrier Roncon(K)
1: Initialize: x′
1= arg min x′∈K′R(x′)
2:fort= 1, .., T do
3: let At= [∇2R(x′
t)]−1
2
4: Draw µtfromSd+1
1∩(Ated+1)⊥uniformly, set y′
t= (yt,1) = x′
t+Atµt.
5: Play yt, observe and incur loss ft(yt). Let gt=d ft(yt)A−1
tµt.
6: Update x′
t+1= arg min
x′∈K′ηPt
τ=1g⊤
τx′+R(x′)
7:end for
We present our main results: expected and high-probability regret bounds
for the problem.6 Z. Cheng et al.
Theorem 1. The algorithm with parameters η=√2νlogT
2d√
T, δ=1
T2guarantees
the following expected regret bound
E[TX
t=1ft(yt)−min
x∈KTX
t=1ft(x)]≤4dp
2νTlogT+GD
T+ 2Tϵ+dTϵ(2ν+√ν).(2)
Theorem 2. The algorithm with parameters η=√
2νlnT
2d√
T, δ=1
T2ensures that
with probability at least 1−γ
TX
t=1ft(yt)−min
x∈KTX
t=1ft(x)≤4d√
2νTlnT+GD
T+Tdϵ(2ν+√ν)+C(1+ϵ)s
8TlnC
γ+2GDlnC
γ+2Tϵ
(3)
where C=⌈lnGD⌉⌈ln((GD)2T)⌉.
We primarily divide the regret into partsPT
t=1(xt−u)⊤θtandPT
t=1(yt−xt)⊤θt
rather than following the approach of Lee et al.[15], where u∈ K. This means
that, for an oblivious adversary, calculating the regret does not require consid-
ering the variance of the estimator gtandθt, but only the variance between
ytandxt. This difference is a key factor that enables us to achieve a better
high-probability regret bound when ϵ= 0.
Firstly, we boundPT
t=1(xt−u)⊤θt. As mentioned earlier, for ϵ-approximately
linearfunctions,boundingPT
t=1(xt−u)⊤θtrequiresconsideringtheterm (dσt(yt)A−1
tµt)⊤(xt−
u). The main difficulty in bounding (dσt(yt)A−1
tµt)⊤(xt−u)is that the norm
∥xt−u∥is hard to bound. Thus, we make the entire analysis hold in Rd+1
and transform the problem from bounding the norm ∥xt−u∥to how to bound
the norm ∥x′
t−h∥, where h∈ K′. We present a straightforward yet necessary
Lemma 4, which helps to bound ∥h∥∇2R(x′
t)≤2ν. In addition, the properties
of the νnormal barrier tell us ∥x′
t∥∇2R(x′
t)=√ν. With these two conditions,
we can immediately deduce the bound ∥xt−h∥∇2R(x′
t)as2ν+√ν. This also
implies that increasing learning rates are not required in our case, as they are
solely aimed at controlling ∥h∥∇2R(x′
t)≤ −∥ h∥∇2R(x′
t+1)+νln(νT+ 1)in Lee et
al.’s paper[15].
Secondly, by obtaining the expected bound and high-probability bound forPT
t=1(yt−xt)⊤θt, we can derive the expected regret bound and high-probability
regret bound, respectively. For high-probability bound ofPT
t=1(yt−xt)⊤θt, un-
like SCRIBLE with lifting and increasing learning rates, which constrains the
decision set from K′toK′
δto ensure that x′
tis never too close to the boundary
(thus ensuring that the eigenvalues of Atare bounded, especially for bounding
∥h∥∇2R(x′
t)). Our approach does not require x′
tto stay away from the boundary.
Furthermore, we do not need to bound the eigenvalues of At, which gives us
greater flexibility in choosing the value of δ(such as1
T2), leading to a better
upper bound for the regret.
Finally, we prove the lower bound of regret in section 6.Adversarial bandit optimization for approximately linear functions 7
5 Proof
This section introduces the preliminary of ν-normal barrier, presents several
essential lemmas, and gives the proof of main theorems.
5.1 ν-normal barrier
We introduce the ν-normal barrier, providing its definitions and highlighting
several key properties that will be frequently used in the subsequent analysis.
Definition 2. LetΨ⊆Rdbe a closed and proper convex cone and let ν≥1.
A function R:int(Ψ)→R: is called a ν-logarithmically homogeneous self-
concordant barrier (or simply ν-normal barrier) on Ψif
1.Ris three times continuously differentiable and convex and approaches in-
finity along any sequence of points approaching the boundary of Ψ.
2. For every h∈Rdandx∈int(Ψ)the following holds:
dX
i=1dX
j=1dX
k=1∂3R(x)
∂xi∂xj∂xkhihjhk≤2∥h∥3
x, (4)
∥∇R (x)⊤h∥ ≤√ν∥h∥x, (5)
R(tx) =R(x)−νlnt,∀x∈int(Ψ), t > 0. (6)
Lemma 1 ([17,18]). IfRis aν-normal barrier on Ψ, Then for any x∈int(Ψ)
and any h∈Ψ, we have
∥x∥2
x=ν, (7)
∇2R(x)x=−∇R (x), (8)
∥h∥x≤ −∇R (x)⊤h, (9)
∇R(x)⊤(h−x)≤ν. (10)
Lemma 2 ([17]). IfRis aν-normal barrier on Ψ, then the Dikin ellipsoid
centered at x∈int(Ψ), defined as {y:∥y−x∥x≤1}, is always within Ψ.
Moreover,
∥h∥y≥ ∥h∥x(1− ∥y−x∥x) (11)
holds for any h∈Rdand any ywith∥y−x∥x≤1.
Lemma 3 ([13]). LetRis a ν-normal barrier over Ψ, then for all x, z∈
int(Ψ) :R(z)−R(x)≤νlog1
1−πx(z), where πx(z) = inf {t≥0 :x+t−1(z−x)∈
Ψ}.
ν-normal barrier plays a crucial role in addressing one of the key challenges in
this problem: bounding ∥x′
t−h∥x′
t, where h∈ K′. Equation 7 give ∥x′
t∥x′
t=√ν. Building upon Lemma 1, we introduce an effective Lemma 4 that aids in
bounding ∥h∥x′
t.8 Z. Cheng et al.
Lemma 4. IfRbe aν-normal barrier for Ψ⊆Rd, then for any x∈int(Ψ)and
anyh∈Ψ, we have
∥h∥x≤2ν. (12)
Proof.From Lemma 1, we have
∥h∥x≤ −∇R (x)⊤h≤ ∥∇R (x)⊤h|. (13)
Then,
∥∇R (x)⊤h|=∥∇R (x)⊤(h−x+x)| ≤ ∥∇R (x)⊤(h−x)|+∥∇R (x)⊤x|.(14)
By Lemma 1, ∥∇R (x)⊤(h−x)|+∥∇R (x)⊤x| ≤ν+∥x⊤∇2R(x)x|= 2ν.
With the help of Equation 7 and Lemma 4, it is easy to apply the triangle
inequality to derive ∥x′
t−h∥x′
t≤ ∥x′
t∥x′
t+∥h∥x′
t≤√ν+ 2ν, where h∈ K′..
5.2 Useful lemmas
In addition to the properties of the normal barrier and its related lemmas, we
also need to introduce some additional necessary lemmas.
Like Lemma 6 in the SCRIBLE algorithm[2], the next minimizer x′
t+1is
“close” to x′
t.However,therearetwodifferenceshere:thefirstisthat ∇ϕt−1(x′
t)̸=
0is possible, where ϕt(x′) =ηPt
τ=1g⊤
τx′+R(x′). And the second is that for
z=x′
t+αu, where uis a vector such that ∥u∥x′
t= 1andα∈(−1
2,1
2), we need
to satisfy z∈ K′instead of z∈ K.
Lemma 5. x′
t+1∈W4dη(x′
t), where Wr(x′) ={y∈ K′:∥y−x′∥x′< r}.
Proof.Recall that x′
t+1= arg min x′∈K′ϕt(x′), where ϕt(x′) =ηPt
τ=1g⊤
τx′+
R(x′). Let ht(x) =ϕt((x,1)) = ϕt(x′), then minht(x) = min ϕt(x′). Noticing
thathtisaconvexfunctionon Rdandstillholdsthebarrierproperty(approaches
infinityalonganysequenceofpointsapproachingtheboundaryof K).Byproper-
ties of convex functions, we can get ∇ht−1(xt) = 0and for the first dcoordinates
∇ϕt−1(x′
t) = 0.
Consider any point in z∈W1
2(x′
t). It can be written as z=x′
t+αufor some
vector usuch that ∥u∥x′
t= 1andα∈(−1
2,1
2). Noticing the d+ 1coordinate of
uis 0. Expanding,
ϕt(z) =ϕt(x′
t+αu)
=ϕt(x′
t) +α∇ϕt(x′
t)⊤u+α21
2u⊤∇2ϕt(ξ)u
=ϕt(x′
t) +α(∇ϕt−1(x′
t) +ηgt)⊤u+α21
2u⊤∇2ϕt(ξ)u
=ϕt(x′
t) +αηg⊤
tu+α21
2u⊤∇2ϕt(ξ)u,Adversarial bandit optimization for approximately linear functions 9
for some ξon the path between x′
tandx′
t+αuand the last equality holds
because ∇ϕt−1(x′
t)⊤u= 0. Setting the derivative with respect to αto zero, we
obtain
∥α∗∥=η∥g⊤
tu∥
u⊤∇2ϕt(ξ)u=η∥g⊤
tu∥
u⊤∇2R(ξ)u(15)
The fact that ξis on the line x′
ttox′
t+αuimplies that ∥ξ−x′
t∥x′
t≤ ∥αu∥x′
t≤1
2.
Hence, by Lemma 2
∇2R(ξ)⪰(1− ∥ξ−x′
t∥x′
t)2∇2R(x′
t)≻1
4∇2R(x′
t). (16)
Thus u⊤∇2R(ξ)u >1
4∥u∥x′
t=1
4,and α∗<4η∥g⊤
tu∥.Usingassumption max x∈K∥ft(x)∥ ≤
1,
g⊤
tu≤ ∥gt∥∗
x′
t∥u∥x′
t≤ ∥d ft(yt)A−1
tµt∥∗
x′
t≤q
d2µ⊤
tA−⊤
t(∇2R(x′
t))−1A−1
tµt≤d,
(17)
we conclude that ∥g⊤
tu∥ ≤d, and ∥α∗∥<4dη <1
2by our choice of ηandT.
We conclude that the local optimum arg min z∈W1
2(x′
t)ϕt(z)is strictly inside
W4dη(x′
t), and since ϕtis convex, the global optimum is
xt+1= arg min
z∈K′ϕt(z)∈W4dη(x′
t). (18)
Lemma 5 implies ∥x′
t+1−x′
t∥x′
t≤4dη. This result will help us bound g⊤
t(x′
t−
h), where h∈ K′(see Lemma 7).
This next lemma is based on Lemma B.9.[15], but due to the differences in
the loss functions, what we obtain is an unbiased estimate regarding gt,irather
than θt,i, for i∈[d]. Lee et al. state that Et[lt,i] =θt,i, for i∈[d][15]. Since
lt=d(θt,0)(x′
t+Atµt)A−1
tµtis identical to ours, we directly apply it to our
analyze.
Lemma 6. Letlt=d(θt,0)(x′
t+Atµt)A−1
tµt. ForAlgorithm1, wehave Et[lt,i] =
θt,i, for i∈[d].
TheregretboundofFTRLalgorithm[13]statesthatforevery u∈ K,PT
t=1∇⊤
txt−PT
t=1∇⊤
tu≤PT
t=1[∇⊤
txt−∇⊤
txt+1]+1
η[R(u)−R(x1)], where ∇trepresents the
gradient of the loss function ft. In our adaptation, we replaced ∇twith gtandK
withK′. This modification does not fundamentally alter the original result. Since
the update way x′
t+1= arg min
x′∈K′ηPt
τ=1g⊤
τx′+R(x′)satisfied the condition of
FTRL algorithm[13], we can apply Lemma 5.3. in [13] to Algorithm 1 as follow.
Lemma 7. For Algorithm 1 and for every h∈ K′,PT
t=1g⊤
tx′
t−PT
t=1g⊤
th≤PT
t=1[g⊤
tx′
t−g⊤
tx′
t+1] +1
η[R(h)− R(x′
1)].
The following lemma represents a key proof of this paper. Specifically, it
provides a bound forPT
t=1θ⊤
txt−PT
t=1θ⊤
tx∗. Due to Lemma 8, we only need
to considerPT
t=1θ⊤
tyt−PT
t=1θ⊤
txtwhen calculating the regret bound. This
result plays a crucial role in deriving both expected and high-probability regret
bounds.10 Z. Cheng et al.
Lemma 8. For Algorithm 1,let ft(xt) =θ⊤
txt+σt(xt)andx∗= arg min x∈KPT
t=1ft(x)
and we have
TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗≤2ηd2T+νlog(1
δ)
η+Tdϵ(2ν+√ν) +δDGT. (19)
Proof.Recall for any δ∈(0,1),Kδ={x|1
1−δx∈ K}andK′
δ={(x,1) :x∈ Kδ}.
Letx∗
δ=Q
Kδx∗, by properties of projections, then
∥x∗−x∗
δ∥= min
a∈Kδ∥x∗−a∥. (20)
Since (1−δ)x∗∈ Kδ, then
min
a∈K∥x∗−a∥ ≤ ∥ x∗−(1−δ)x∗∥ ≤δD. (21)
So,
∥x∗
δ−x∗∥ ≤δD. (22)
By Cauchy–Schwarz inequality and the fact that ∥θ∥ ≤Gand∥x∗
δ−x∗∥ ≤
δD,
TX
t=1θ⊤
tx∗
δ−TX
t=1θ⊤
tx∗≤δDGT. (23)
So,
TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗=TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗
δ+TX
t=1θ⊤
tx∗
δ−TX
t=1θ⊤
tx∗
≤TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗
δ+δDGT.
Letθ′
t= (θt, z), where zis the (d+ 1)th coordinate of dEt[(θt,0)⊤(x′
t+
Atµt)A−1
tµt]. From Lemma 6, we know dEt[(θt,0)⊤(x′
t+Atµt)A−1
tµt] =θ′
t.
Since gt=d f(yt)A−1
tµt=dθ′⊤
t(x′
t+Atµt)A−1
tµt+dσt(yt)A−1
tµt, and let Mt=
Et[dσt(yt)A−1
tµt], then θ′
t=Et[gt]−Mtand that we have
TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗
δ=TX
t=1(θ′⊤
tx′
t−z)−TX
t=1(θ′⊤
tx∗′
δ−z)
=TX
t=1(Et[gt]−Mt)⊤x′
t−TX
t=1(Et[gt]−Mt)⊤x∗′
δ
=TX
t=1Et[gt]⊤(x′
t−x∗′
δ) +TX
t=1M⊤
t(x∗′
δ−x′
t).
We boundPT
t=1M⊤
t(x∗′
δ−x′
t)firstly. By Cauchy–Schwarz inequality,
TX
t=1M⊤
t(x∗′
δ−x′
t)≤TX
t=1∥Mt∥∗
x′
t∥x∗′
δ−x′
t∥x′
t.Adversarial bandit optimization for approximately linear functions 11
By Jensen’s inequality,
∥Mt∥∗
x′
t=q
M⊤
t∇2(R(x′
t))−1Mt (24)
=q
Et[dσt(yt)A−1
tµt]⊤∇2(R(x′
t))−1Et[dσt(yt)A−1
tµt](25)
=q
d2Et[σt(yt)µt]⊤A−1
tA2
tA−1
tEt[σt(yt)µt] (26)
=q
d2Et[σt(yt)µt]⊤Et[σt(yt)µt] (27)
≤q
d2Et[σ2
t(yt)µ⊤
tµt] (28)
≤√
d2ϵ2 (29)
=dϵ. (30)
Then we bound ∥x∗′
δ−x′
t∥x′
t. From the triangle inequality,
∥x∗′
δ−x′
t∥x′
t≤ ∥x∗′
δ∥x′
t+∥x′
t∥x′
t.
By Lemma 1 and Lemma 4, we obtain ∥x∗′
δ∥x′
t≤2ν,∥x′
t∥x′
t=√ν.
So∥x∗′
δ−x′
t∥x′
t≤2ν+√νandPT
t=1M⊤
t(x∗′
δ−x′
t)≤Tdϵ(2ν+√ν). Then
boundPT
t=1Et[gt]T(x′
t−x∗′
δ)
By Lemma 7,
TX
t=1Et[gt]T(x′
t−x∗′
δ) =Et{TX
t=1g⊤
t(x′
t−x∗′
δ)}
≤Et{TX
t=1[g⊤
tx′
t−g⊤
tx′
t+1] +1
η[R(x∗′
δ)− R(x′
1)]}
≤Et{TX
t=1[∥gt∥∗
x′
t∥x′
t−x′
t+1∥x′
t]}+1
η(R(x∗′
δ)− R(x′
1)).
Lemma 5implies that ∥x′
t−x′
t+1∥x′
t≤4dηis trueby choice of η. Additionally,
from Eq. (17), we deduce that ∥gt∥∗
x′
t≤d. Therefore,
∥gt∥∗
x′
t∥x′
t−x′
t+1∥x′
t≤4ηd2, (31)
Et{TX
t=1[∥gt∥∗
x′
t∥x′
t−x′
t+1∥x′
t]} ≤4ηd2T. (32)
With Lemma 3,
1
η(R(x∗′
δ)− R(x′
1))≤νlog(1
δ)
η. (33)
Combine everything, we get
TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗
δ≤4ηd2T+νlog(1
δ)
η+Tdϵ(2ν+√ν).(34)
Now we are ready to prove Theorem 1.12 Z. Cheng et al.
5.3 Proof of Theorem 1
Proof.Recall ϵ-approximatelylinearfunctioncanbewriteas: f(x) =θ⊤x+σ(x).
Thus, the regret of SCRIBLE with lifting algorithm
E[TX
t=1ft(yt)−TX
t=1ft(x∗)] =E[TX
t=1[θ⊤
tyt+σt(yt)]−TX
t=1[θ⊤
tx∗+σt(x∗)]]
=E[TX
t=1θ⊤
tyt−TX
t=1θ⊤
tx∗] +E[TX
t=1σt(yt)−TX
t=1σt(x∗)].
Firstly, we bound the front of the above equation,
E[TX
t=1θ⊤
tyt−TX
t=1θ⊤
tx∗] =TX
t=1E[θ⊤
tyt]−TX
t=1E[θ⊤
txt] +TX
t=1E[θ⊤
txt]−TX
t=1E[θ⊤
tx∗].
From the Law of total expectation, we know
TX
t=1E[θ⊤
tyt]−TX
t=1E[θ⊤
txt] =TX
t=1E[θ⊤
t(yt−xt)]
=TX
t=1E[Et[θ⊤
t(yt−xt)]]
=TX
t=1E[Et[θ⊤
t(Atµt)]]
=TX
t=1E[θ⊤
tEt[(Atµt)]]
=TX
t=1E[θ⊤
t0]
=0.
Thus,
E[TX
t=1θ⊤
tyt−TX
t=1θ⊤
tx∗] =E[TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗]. (35)
From Lemma 8, we have
E[TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗]≤E[4ηd2T+νlog(1
δ)
η+Tdϵ(2ν+√ν) +δDGT ]
≤4ηd2T+νlog(1
δ)
η+Tdϵ(2ν+√ν) +δDGT.Adversarial bandit optimization for approximately linear functions 13
Since σtis chosen after knowing the player’s action, it can cause as large
a perturbation as possible. We using ∥σt(x)∥ ≤ ϵto boundPT
t=1E[σt(yt)−PT
t=1σt(x∗)]≤2Tϵand combination of everything, we get
Regret =E[TX
t=1ft(yt)−TX
t=1ft(x∗)]
≤4dp
2νTlogT+GD
T+Tdϵ(2ν+√ν) + 2Tϵ,
where η=√2νlogT
2d√
T, δ=1
T2.
5.4 Proof of Theorem 2
To establish the high-probability regret bound, we first introduce the necessary
Lemma 9.
Lemma 9 (Theorem 2.2. in [15]). LetX1, ..., X Tbe a martingale difference
sequence with respect to a filtration F1⊆...⊆FTsuch that E[Xt|Ft] = 0.
Suppose Bt∈[1, b]for a fixed constant bisFt-measurable and such that Xt≤Bt
holds almost surely. Then with probability at least 1−γwe have
TX
t=1Xt≤C(p
8V ln(C/γ) + 2B∗ln(C/γ)), (36)
where V= max {1;PT
t=1E[X2
t|Ft]}, B∗= max t∈[T]Bt, and C=⌈logb⌉⌈log(b2T)⌉.
The analysis in [15] employs Lemma 9 to derive a high-probability bound forPT
t=1[y⊤
tθt−x⊤
tgt+u⊤(gt−θt)]. In contrast, our approach defines Xt=θ⊤
tyt−
θ⊤
txtandderivesthehigh-probabilityboundforPT
t=1(θ⊤
tyt−θ⊤
txt).Thisdistinc-
tion in the application of Lemma 9 enables us to derive a tighter high-probability
upper bound for bandit linear optimization.
With the support of Lemma 8 and Lemma 9, we are ready to prove Theo-
rem 2.
Proof.LetXt=θ⊤
tyt−θ⊤
txt, then Et[Xt] =Et[θ⊤
tyt−θ⊤
txt] = 0,Xt=θ⊤
tyt−
θ⊤
txt≤ ∥θt∥∥yt−xt∥ ≤GDand
Et[X2
t] =Et[(θ⊤
tyt−θ⊤
txt)2]
=Et[(θ⊤
tyt)2+ (θ⊤
txt)2−2θ⊤
tytθ⊤
txt]
=Et[(θ⊤
tyt)2] +Et[(θ⊤
txt)2]−Et[2θ⊤
tytθ⊤
txt]
=Et[(θ⊤
tyt)2]−θ⊤
txtθ⊤
txt
≤(1 +ϵ)2.14 Z. Cheng et al.
Then,
TX
t=1ft(yt)−TX
t=1ft(x∗) =TX
t=1[θ⊤
tyt+σt(yt)]−TX
t=1[θ⊤
tx∗+σt(x∗)]
≤TX
t=1θ⊤
tyt−TX
t=1θ⊤
tx∗+TX
t=1σt(yt)−TX
t=1σt(x∗)
≤TX
t=1θ⊤
tyt−TX
t=1θ⊤
tx∗+ 2Tϵ
=TX
t=1θ⊤
tyt−TX
t=1θ⊤
txt+TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗+ 2Tϵ.
From Lemma 8, we know
TX
t=1θ⊤
txt−TX
t=1θ⊤
tx∗≤4d√
2νTlnT+GD
T+Tdϵ(2ν+√ν),(37)
where η=√
2νlnT
2d√
T, δ=1
T2. Then by Lemma 9,
P(TX
t=1(θ⊤
tyt−θ⊤
txt)≤C(p
8Vln(C/γ) + 2B∗ln(C/γ)))≥1−γ,(38)
where V= (1 + ϵ)2T, B∗=b=GD, and C=⌈lnGD⌉⌈ln((GD)2T)⌉. Combine
everything to conclude the proof.
5.5 Application to black-box optimization
From online to offline transformation, the result of this paper can also apply to
black-box optimization for ϵ-approximately linear function. This problem is im-
portant in that previous theoretical analyses for black box optimization can only
deal with linear/convex/smooth objectives in the adversarial environments (via
bandit convex optimization). So, it is quite meaningful to clarify the possibility
of the black box optimization problems without such restrictions. In fact, our
objective is not linear, nor smooth, even with a simple assumption.
Letˆxbe the output of algorithm 1, then from Theorem 1. we can easily prove
andensure f(ˆx)−minx∈Kf(x)≤4d√
2νlnT√
T+GD
T2+dϵ(2ν+√ν)+2ϵ.Additionally,
we provide a lower bound 2ϵfor this problem (see Lemma 10). We can see that
the difference between the lower bound and the upper bound is only dϵ(2ν+√ν)
asTapproachesinfinity.Thissuggeststhepotentialexistenceof"easier"settings
between the adversarial environment and the standard stochastic environment,
where better algorithms might be found. It also motivates us to explore these
settings further.Adversarial bandit optimization for approximately linear functions 15
6 Lower bound
In this section, we show a lower bound of the regret. To do so, we consider a
black-box optimization problem for the set Fofϵ-approximately linear functions
f:K → R. In the problem, we are given access to the oracle Offor some
f∈ F, which returns the value f(x)given an input x∈ K. The goal is to find a
point ˆx∈ Ksuch that f(ˆx)−minx∈Kf(x)is small enough. Then, the following
statement holds.
Lemma 10. For any algorithm Afor the black-box optimization problem for F,
there exists an ϵ-approximately linear function f∈ Fsuch that the output ˆxof
Asatisfies
f(ˆx)−min
x∈Kf(x)≥2ϵ. (39)
Proof.Firstly, suppose that the algorithm Ais deterministic. At iteration t=
1, ..., T, for any feedback y1, ..., y t−1∈R,Ashould choose the next query point
xtbased on the data observed so far. That is,
xt=A((x1, y1), ...,(xt−1, yt−1)). (40)
Assume that the final output ˆxis returned after Tqueries to the oracle Of. In
particular, we fix the Tfeedbacks y1=y2=···=yT=ϵ. Let z∈ Kbe such
thatz /∈ {x1, ..., x T,ˆx}. Then we define a function f:K →Ris as
f(x) =(
ϵ, x ̸=z,
−ϵ, x =z.(41)
The function fis indeed an ϵ-approximately linear function, as f(x) = 0⊤x+
σ(x), where σ(x) =ϵforx̸=zandσ(x) =−ϵforx=z. Further, we have
f(ˆx)−min
x∈Kf(x)≥2ϵ. (42)
Secondly, if algorithm Ais randomized. It means each xtis chosen randomly.
We assume the same feedbacks y1=y2=···=yT=ϵ. Let X={x1, ..., x T,ˆx}.
Then, there exists a point z∈ Ksuch that PX(z∈X) = 0, since Ez′[PX(z′∈
X|z′)] = Pz′,X(z′∈X) =EX[Pz′(z′∈X|X)] = 0, where the expectation on
z′is defined w.r.t. the uniform distribution over K. For the objective function
fdefined in (41), we have f(ˆx)−minx∈Kf(x)≥2ϵwhile fisϵ-approximately
linear.
Theorem 3. For any horizon T≥1and any player, there exists an adversary
such that the regret is at least 2ϵT.
Proof.We prove the statement by contradiction. Suppose that there exists a
player whose regret is less than 2ϵT. Then we can construct an algorithm for
the blackbox optimization problem from it by feeding the online algorithm16 Z. Cheng et al.
with Tfeedbacks of the blackbox optimization problem and by setting ˆx=
mint∈[T]f(xt). Then,
f(ˆx)−min
x∈Kf(x)≤PT
t=1f(xt)−PT
t=1minx∈Kf(x)
T<2ϵ,
which contradicts Lemma 10.
This lower bound indicates that Ω(ϵT)regret is inevitable for the bandit
optimization problem for ϵ-approximately linear functions. We conjecture that
the lower bound can be tightened to Ω(dϵT), but we leave it as an open problem.
References
1. Abbasi-Yadkori,Y.,Pál,D.,Szepesvári,C.:Improvedalgorithmsforlinearstochas-
tic bandits. Advances in neural information processing systems 24(2011)
2. Abernethy, J.D., Hazan, E., Rakhlin, A.: Competing in the dark: An efficient al-
gorithm for bandit linear optimization. In: COLT. pp. 263–274 (2008)
3. Abernethy, J.D., Hazan, E., Rakhlin, A.: Interior-point methods for full-
information and bandit online learning. IEEE Transactions on Information Theory
58(7), 4164–4175 (2012)
4. Agarwal, N., Gonen, A., Hazan, E.: Learning in non-convex games with an opti-
mization oracle. In: Conference on Learning Theory. pp. 18–29. PMLR (2019)
5. Amani, S., Alizadeh, M., Thrampoulidis, C.: Linear stochastic bandits under safety
constraints. Advances in Neural Information Processing Systems 32(2019)
6. Awerbuch, B., Kleinberg, R.D.: Adaptive routing with end-to-end feedback: Dis-
tributed learning and geometric approaches. In: Proceedings of the thirty-sixth
annual ACM symposium on Theory of computing. pp. 45–53 (2004)
7. Bartlett, P., Dani, V., Hayes, T., Kakade, S., Rakhlin, A., Tewari, A.: High-
probability regret bounds for bandit online linear optimization. In: Proceedings
of the 21st Annual Conference on Learning Theory-COLT 2008. pp. 335–342. Om-
nipress (2008)
8. Bubeck, S., Cesa-Bianchi, N., Kakade, S.M.: Towards minimax policies for online
linear optimization with bandit feedback. In: Conference on Learning Theory. pp.
41–1. JMLR Workshop and Conference Proceedings (2012)
9. Dani, V., Kakade, S.M., Hayes, T.: The price of bandit information for online
optimization. Advances in Neural Information Processing Systems 20(2007)
10. Flaxman, A.D., Kalai, A.T., Mcmahan, H.B.: Online convex optimization in the
banditsetting:gradientdescentwithoutagradient.In:Proceedingsofthesixteenth
annual ACM-SIAM symposium on Discrete algorithms. pp. 385 – 394 (2005).
https://doi.org/10.5555/1070432
11. Gao, X., Li, X., Zhang, S.: Online learning with non-convex losses and non-
stationary regret. In: International Conference on Artificial Intelligence and Statis-
tics. pp. 235–243. PMLR (2018)
12. Ghai,U.,Lu,Z.,Hazan,E.:Non-convexonlinelearningviaalgorithmicequivalence.
Advances in Neural Information Processing Systems 35, 22161–22172 (2022)
13. Hazan, E., et al.: Introduction to online convex optimization. Foundations and
Trends ®in Optimization 2(3-4), 157–325 (2016)Adversarial bandit optimization for approximately linear functions 17
14. Lattimore, T.: Improved regret for zeroth-order adversarial bandit convex op-
timisation. Mathematical Statistics and Learning 2, 311–334 (10 2020). https:
//doi.org/10.4171/msl/17
15. Lee, C.W., Luo, H., Wei, C.Y., Zhang, M.: Bias no more: high-probability data-
dependent regret bounds for adversarial bandits and mdps. Advances in neural
information processing systems 33, 15522–15533 (2020)
16. McMahan, H.B., Blum, A.: Online geometric optimization in the bandit setting
against an adaptive adversary. In: Learning Theory: 17th Annual Conference on
Learning Theory, COLT 2004, Banff, Canada, July 1-4, 2004. Proceedings 17. pp.
109–123. Springer (2004)
17. Nemirovski, A.: Interior point polynomial time methods in convex programming.
Lecture notes 42(16), 3215–3224 (2004)
18. Nesterov, Y., Nemirovskii, A.: Interior-point polynomial algorithms in convex pro-
gramming. SIAM (1994)
19. Yang, L., Deng, L., Hajiesmaili, M.H., Tan, C., Wong, W.S.: An optimal algorithm
for online non-convex learning. Proceedings of the ACM on Measurement and
Analysis of Computing Systems 2(2), 1–25 (2018)