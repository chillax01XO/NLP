arXiv:2505.20561v1  [cs.LG]  26 May 2025
2025-5-28
Beyond Markovian: Reflective Exploration via
Bayes-Adaptive RL for LLM Reasoning
Shenao Zhang1∗, Yaqing Wang2, Yinxiao Liu2, Tianqi Liu2, Peter Grabowski3, Eugene Ie3,
Zhaoran Wang1†, Yunxuan Li3†
1Northwestern University,2Google DeepMind,3Google
Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning
capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conven-
tional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and
depends on the history contexts only through the current state. Therefore, it remains unclear whether reflec-
tive reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy
this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the
expected return under a posterior distribution over Markov decision processes. This Bayesian formulation
inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via
belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based
on the observed outcomes, offering principled guidance on when and how the model should reflectively
explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL
outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with
improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL .
1. Introduction
Large Language Models (LLMs) have demonstrated impressive reasoning abilities, such as in solving
complex math problems. A key factor driving this progress is the use of Chain-of-Thought (CoT) reasoning
[50], where the model engages in intermediate deliberation before producing an answer. Building on
this, recent advances have employed Reinforcement Learning (RL) to further enhance LLM reasoning by
optimizing for verifiable outcome rewards [ 23,17,56,51]. Notably, RL-trained models have exhibited
emergentbehaviorssuchasgeneratinglongCoTsandengaginginself-reflection, aprocessofbacktracking
to previous states to correct earlier mistakes, also known as the “Aha moment” [ 17,61]. However, despite
these compelling phenomena, it remains unclear why and under what conditions reflective reasoning is
beneficial at test time, or whether such behaviors will emerge through conventional RL training.
Prevalent views attempt to explain the usefulness of test-time reflections as exploratory steps
that provide additional contexts for more optimal decision-making. Yet in Markovian RL, the explo-
ration–exploitation trade-off is resolved entirely during training: the agent interleaves exploration and
exploitation to learn a training-time optimal deterministic policy, but switches to pure exploitation at test
time. As a result, conventional RL allows a Markovian policy to be optimal by simply memorizing, i.e.,
deterministically outputting, training solutions once they are encountered by stochastic exploratory poli-
cies through repeated trial-and-error. Moreover, the Markov assumption restricts the policy to condition
decisions solely on the current state rather than on contextual information gathered through exploration,
offering no incentives to adaptively explore with reflections. In summary, under conventional Markovian
RL, there is no guarantee that reflective explorations will emerge during training, nor does it explain
∗Work done during an internship at Google.†Equal advising. Correspondence to: shenao@u.northwestern.edu, zhaoran.wang@northwestern.edu,
yunxuanli@google.com.Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
why such explorations might be advantageous at test time.
To address this gap, we propose grounding reflective reasoning with Bayes-Adaptive RL, which
explicitly optimizes for test-time generalization by maximizing the expected return under a posterior
distribution over Markov Decision Processes (MDPs). The objective incentivizes both reward-seeking
actions and epistemic explorations that gather information to reduce the MDP’s uncertainty, such as the
uncertainty regarding the progress made by different actions. This enables the model to adapt on-the-fly
at test time by updating its beliefs and switching strategies based on observed outcomes, naturally giving
rise to reflective exploration behaviors. We prove that the expected return of an adaptive policy can be
exponentially higher than the optimal Markovian policy at test time.
Building upon this formulation, we introduce a novel algorithm, Bayes-Adaptive RL for LLM Reasoning
(BARL). For each prompt, BARL performs online rollouts to generate a set of candidate answers, each
associatedwithanMDPhypothesis. Thestate-actionvalueisthencomputedbyweightingeachhypothesis
according to the model’s current belief, with penalties applied for mismatches between predicted and
observed rewards, thereby signaling when to switch strategies. BARL provides a principled mechanism
for integrating and revising plausible strategies, analogous to linearizing best-of-N reasoning, but with
explicit step-level guidance on whenandhowthe model should reflectively explore.
To illustrate the benefits of BARL, we begin with a synthetic task designed to mirror test-time
generalization in LLM reasoning. The agent receives a reward only when it repeats a prompt token
exactly three times, but the training and testing prompt tokens differ. Standard Markovian RL memorizes
the training solutions and fails to generalize. In contrast, BARL learns to switch strategies by eliminating
hypotheses, ultimately discovering the ground-truth MDP for optimal behavior.
We further evaluate BARL on math reasoning tasks using various LLMs, including Qwen2.5-Math-
1.5B, Qwen2.5-Math-7B, and R1-Distill-Llama-8B. Across these models, BARL consistently outperforms
Markovian RL algorithms, such as GRPO and a strong progress-reward baseline, on multiple benchmarks.
BARL achieves significantly greater token efficiency, requiring up to 39%fewer average tokens than the
progress baseline, 50%fewer than GRPO, and over 90%fewer than the Qwen2.5-Math-1.5B base model.
Moreover, we observe no strong correlation between overall model performance and the frequency of
reflections. Instead, BARL’s advantage stems from more efficient exploration and more effective thinking
tokens. We summarize the key takeaways of this paper as follows:
Key Takeaways: Why, How, and When Should LLMs Reason Reflectively
•Why:MarkovianRL neitherensurestheemergenceofreflectiveexplorationnorexplainsitsbenefits
at test time since (1) exploration is confined only to the training phase to learn, and purely exploit
at test time, an optimal deterministic policy that canmerely memorize training solutions, and
(2) the state-conditional policy lacks incentives to collect additional contexts and backtrack. In
contrast, Bayes-Adaptive RL , by optimizing test-time generalizability, encourages explorations to
gather contextual information that reduces the MDP uncertainty.
•How:BARL provides a principled way to stitch plausible strategies by maintaining a posterior
over MDP hypotheses, each associated with a sampled candidate answer. Reflective exploration
emerges naturally through hypothesis elimination, enabling on-the-fly adaptation.
•When: LLMs should self-reflect when discrepancies arise between their internal beliefs and
cumulative reward feedback—signaling strategy switching by downweighting hypotheses that
have high belief probabilities but are unlikely to be optimal given previous observations.
2Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
2. Related Work
LLM Reasoning. As an emerging capability of model scale, LLMs can generate intermediate CoTs to
solve complex reasoning tasks [ 50,25] and scale test-time performance by allocating more thinking
tokens [ 42,5]. Early efforts enhanced LLM reasoning via supervised fine-tuning on human-annotated
data [7,58,55] or linearized search traces [ 26,12]. However, due to the distribution shift between LLM
responses and curated data, LLM-generated data has proven effective through rejection sampling [ 8,57]
by filtering out low-quality rationales [ 60,59] or with EM iterations [ 41,63]. Recently, RL has gained
increasing interest for improving reasoning [ 1,19,45,40]. Process rewards [ 44,29] with Monte Carlo
unrolls [ 24,46,49,32] offer finer-grained feedback but are computationally expensive. Outcome-reward
RL [17] demonstrates emergent deliberative reasoning abilities such as self-reflection. Yet, limited work
has investigated the underlying mechanisms of such behaviors. In fact, recent findings suggest that
reflections do not consistently emerge from RL training and exhibit weak correlation with performance
[31]. Similar to our work, [ 52,37] also study the generalization of LLMs, from a meta-RL [ 9] perspective:
[52] justify deliberative reasoning as providing extra contexts, and [ 37] use progress reward [ 39] to
reduce regret in outcome-reward RL. Our method differs from [ 52] in that we ground reflective reasoning
in environment rewards, rather than relying solely on the internal CoT states generated by the model
itself. Compared to [ 37], which rewards golden strategies that make progress towards the correct answer,
BARL additionally encourages exploring plausible strategies under the Bayesian framework, allowing it
to account for uncertainty during both training and testing. We experimentally compare against a variant
of [37] that estimates progress using answer probability differences instead of more expensive Monte
Carlo unrolls. Besides, unlike [ 47,35] that manually design hypothesis proposal–selection pipelines, our
method achieves this through a more principled RL optimization procedure.
Reinforcement Learning. Conventional RL explores only during training, e.g. via 𝜖-greedy noise,
and exploits the optimal deterministic policy when deployed. Exceptions include works that explicitly
optimize maximum entropy objectives [ 18] to learn stochastic policies, primarily to accelerate training
convergence in settings where evaluation remains in-distribution, such as robotic control. Bayes-Adaptive
RL [4,11,16,13,27,62,30] has been studied to pursue the optimal exploration-exploitation trade-
off in uncertain environments to improve generalizability. When the true MDP identity is latent and
must be inferred from interaction (states, actions, rewards), Bayesian RL connects naturally to Partially
Observable MDPs [ 10,15]. Exact solutions of the Bayesian RL objective are often intractable, prompting
the development of approximate methods [ 16,3,6]. In our work, we adopt policy gradient that operates
over candidate answers, which differs from [ 14] that leverage value ensembles in offline RL and [ 36]
that applies SFT on an oracle Bayesian model’s outputs.
3. Problem Formulation
LLM Reasoning via RL. To enable the LLM policy 𝜋𝜃to reason, we first consider the finite-horizon MDP
defined by the state space S, action spaceA, horizon𝑇, and reward function 𝑟(𝑠,𝑎), where𝑠∈Sand
𝑎∈A. Here, the initial state 𝑠0is the prompt, and the action 𝑎𝑡is the𝑡-th step of the CoT, which can be
eitherseparatedbyspecialtokens[ 46]ordefinedasafixedlengthofreasoningtokens[ 32]. Weadoptthe
latter definition due to its simplicity. The state transition is deterministic by appending the new reasoning
step, i.e., 𝑠𝑡+1=𝑠𝑡+𝑎𝑡. Prior work [ 44,17] employs an outcome-level reward verifier(𝑠𝑇,𝑦∗
𝑠0), which
uses a verifier to perform a regular expression match (either 0or1) between 𝑠𝑇and the ground-truth
3Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
answer𝑦∗
𝑠0corresponding to the prompt 𝑠0. We extend this sparse-reward setting by incorporating a
progress reward [ 39,37], which quantifies the increase of the model’s probability of outputting 𝑦∗
𝑠0after
appending 𝑎at the CoT 𝑠, i.e., for 0≤𝑡≤𝑇−1:
𝑟(𝑠𝑡,𝑎𝑡)=𝜋𝜃(𝑦∗
𝑠0|𝑠𝑡+𝑎𝑡+</think>)−𝜋𝜃(𝑦∗
𝑠0|𝑠𝑡+</think>), (3.1)
where </think> is the end sign of thinking, such as the answer elicitation prompt “Based on the above
reasoning, the answer is \boxed” that we adopt. Compared to Monte-Carlo process rewards [ 32,37],
(3.1)is computationally efficient by avoiding multiple branched rollouts at each step, and the KV cache
of𝑠0:𝑇from CoT generations can also be reused.
For the Markovian RL objective JMarkov(𝜋𝜃):=𝔼𝑠0,𝜋𝜃[Í𝑇−1
𝑡=0𝑟(𝑠𝑡,𝑎𝑡)+verifier(𝑠𝑇,𝑦∗
𝑠0)], this reward
definition allows us to use telescoping in a way similar to reward shaping [34] to obtain
argmax
𝜋𝜃JMarkov(𝜋𝜃)=argmax
𝜋𝜃𝔼𝑠0,𝜋𝜃
𝜋𝜃(𝑦∗
𝑠0|𝑠0+𝑎0:𝑇−1+</think>)+verifier(𝑠𝑇+1,𝑦∗
𝑠0)
,
i.e., the optimal Markovian policy generates 𝑎0:𝑇−1to maximize the likelihood of the ground-truth 𝑦∗
𝑠0
and its verifier-evaluated correctness. The gradients for Markovian policies are
∇𝜃JMarkov(𝜋𝜃)=𝔼𝑠0,𝜋𝜃𝑇−1∑︁
𝑡=0∇𝜃log𝜋𝜃(𝑎𝑡|𝑠𝑡)·𝑄𝜋𝜃(𝑠𝑡,𝑎𝑡)
, (3.2)
where𝑄𝜋𝜃is the state-action value or advantage function [ 38]. The above setups consider the case
when the environment is predefined with certainty. The definitions naturally extend to any MDP
M:=(S,A,𝑟M,𝑇)where𝑟Mis defined w.r.t. the answer 𝑦M
𝑠0. The Q-value is then
𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)=𝔼𝜋𝜃𝑇−1∑︁
𝑡′=𝑡𝑟M(𝑠𝑡′,𝑎𝑡′)+verifier(𝑠𝑇,𝑦∗
𝑠0)
(3.3)
=𝔼𝜋𝜃
𝜋𝜃(𝑦M
𝑠0|𝑠𝑡+𝑎𝑡:𝑇−1+</think>)−𝜋𝜃(𝑦M
𝑠0|𝑠𝑡+</think>)+verifier(𝑠𝑇,𝑦∗
𝑠0)
.
𝑠1𝑠4
𝑠2𝑠3𝑠5𝑠6𝑠0
Figure 1|An ex-
ample of reflec-
tive reasoning.Reflective Exploration. We define reflective exploration as the pattern in which the
LLMbacktrackstoapriorstateafteranexploratorysteptotakedifferentactionsatthat
state. Specifically, anaturallanguagereflectivereasoningstepsuchas“Let’sreconsider
the geometric relationship” corresponds to a backtracking action that semantically
disregards the previous one or more steps. We illustrate this using a binary search
tree as in the right figure: for the trajectory 𝑠0𝑠1𝑠2𝑠1𝑠3,𝑠1𝑠2is an
exploration step, 𝑠2𝑠1is a reflective step that signals the strategy switch from 𝑠2
to𝑠3, and the “geometric relationship” in the above example originates from 𝑠1.
4. The Necessity of Bayes-Adaptive RL for Reflective Reasoning
Markovian RL. When the underlying MDP is known with certainty, the Markov property ensures
that the policy and value depend on the history ℎ𝑡=(𝑠0,𝑎0,𝑟0,...,𝑠𝑡)only through the state 𝑠𝑡, i.e.,
𝑄𝜋(ℎ𝑡,𝑎𝑡)=𝑄𝜋(𝑠𝑡,𝑎𝑡). In this setting, exploratory actions that aim to enrich the history ℎ𝑡with additional
4Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
contexts, such as incorrect attempts followed by backtracking, are unnecessary, as the current state 𝑠𝑡
already encodes all relevant information for optimal decision-making1.
Moreover, the optimal Q-function is 𝑄∗=max𝜋𝑄𝜋and the optimal policy 𝜋∗is greedy w.r.t. 𝑄∗. That
is,𝜋∗is a deterministic policy, where 𝜋∗(𝑎′|𝑠)=1for𝑎′=argmax𝑎𝑄∗(𝑠,𝑎).
Theorem 4.1. Optimality of Markovian RL is attained by deterministic non-reflective policies.
Reflective policies are more suboptimal than non-reflective policies in both discounted infinite-horizon
and finite-horizon MDPs, as the 𝑄-value of the wrong action is no larger than that of the correct action
since more tokens are needed to correct the error. Explorations should only occur during training, in
a trial-and-error manner with repeated episodes, to discover the golden answers. The Markovian RL
objective allows the optimal policy that memorizes these training answers to be fully exploited, with no
incentive to adaptively explore with reflections.
For the non-standard undiscounted infinite-horizon MDPs, where LLMs are encouraged to generate
infinitetokenswithout concerning token efficiency , reflective policies may be as optimal as non-reflective
ones. This is because the 𝑄-value of the wrong action canmatch that of the correct one if the error is
eventually corrected through reflection. This observation provides a partial explanation for the emergence
of “Aha moment” with long CoT. However, even in such settings, reflective reasoning may still fail to
emerge under Markovian RL, particularly if golden answers are discovered either directly or by pruning
incorrect exploratory steps. In other words, this only explains why reflective explorations canappear
during Markovian RL, instead of why these behaviors are preferable to simply memorizing training
solutions, nor whether they will emerge during training.
Next, we present Bayes-Adaptive RL, which explicitly optimizes for test-time generalization and
naturally induces reflective explorations.
Bayes-Adaptive RL. In a Bayes-Adaptive MDP (BAMDP) [4, 33, 28], the agent maintains uncertainty
over the underlying MDP, which is gradually reduced through interactions. Due to this implicit partial
observability [ 10,15], the policy and value depend on the full history ℎ𝑡, instead of only the state 𝑠𝑡, to
capture the agent’s evolving belief about the MDP parameters through cumulative observations. The
objective for BAMDPs is
JBayes(𝜋𝜃):=𝔼𝑠0,𝜋𝜃𝑇−1∑︁
𝑡=0𝔼M∼𝑝(M|ℎ𝑡)
𝑟M(𝑠𝑡,𝑎𝑡)
,
where𝑝(M|ℎ𝑡)is the posterior distribution of Mafter observing ℎ𝑡. This objective encourages the agent
to not only maximize immediate rewards based on the current belief but also explore to gather more
context about the uncertain MDP.
Optimal adaptive policies naturally induce exploratory reflection behaviors, which provide additional
contextual information even if the state remains identical. While reflective actions may be suboptimal
relative to the (unknown) ground-truth MDP, the gathered context, especially the rewards, reduces the
MDP’s uncertainty. This enables future policies to leverage the updated belief to act more optimally.
1Even if we view the state as the CoT history, i.e., explorations such as 𝑠1𝑠2are not semantically disregarded after
backtracking to 𝑠1, it still differs from ℎ𝑡where reward histories are additionally contained.
5Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
Comparison Between Markovian RL and Bayes-Adaptive RL
Markovian RL depends on the current state, not history contexts, for decision-making. Exploration
only happens during training with trial-and-error to discover return-maximizing action sequences,
which are fully exploited as the optimal deterministic policy. In contrast, Bayes-Adaptive RL
balances reward maximization with epistemic exploration throughout both training and testing,
enabling meaningful test-time scaling with improved efficiency.
Theorem 4.2. The test-time expected return of a Bayes-Adaptive policy can be exponentially higher in𝑇∗
than that of the optimal Markovian policy, where 𝑇∗is the minimal number of steps required to reach the
correct answer under an optimal deterministic policy.
Sketch proof. Consider the binary search tree in Figure 1. Let 𝑠2,𝑠3,𝑠5, and𝑠6be four equally possible
candidate answers at test time, i.e., the prior 𝑝(M1)=𝑝(M2)=𝑝(M3)=𝑝(M4)=1/4, where
𝑟M1(𝑠2)=1,𝑟M2(𝑠3)=1,𝑟M3(𝑠5)=1,𝑟M4(𝑠6)=1, and all other rewards are zero. Here, 𝑟(𝑠)represents
the reward of reaching 𝑠.
For any Markovian policy, the maximal return is 1/4(or1/2𝑑−1for a depth- 𝑑tree) since it is static and
cannot adapt according to the reward feedback when reaching the four candidate answers. In contrast,
the optimal Bayes-Adaptive policy has an expected return of 1. This is achieved by updating the posterior
𝑝(M|ℎ𝑡)based on observation ℎ𝑡to eliminate the hypotheses M1,M2,···, until the ground-truth MDP
has probability 1, in which the agent can act optimally to reach the true answer. □
5. Method
The policy gradient for Bayes-Adaptive RL is as follows, which differs from (3.2)by replacing the value
under a predefined Mwith a posterior-weighted value:
∇𝜃JBayes=𝔼𝑠0,𝜋𝜃𝑇−1∑︁
𝑡=0∇𝜃log𝜋𝜃(𝑎𝑡|𝑠𝑡)·𝔼M∼𝑝(M|ℎ𝑡)
𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)
, (5.1)
where we write the history ℎ𝑡as𝑠𝑡in𝜋and𝑄since in LLM reasoning, 𝑠𝑡already encodes the full sequence
of prior states and actions, and the reward is unavailable at test time so it’s absorbed into 𝜃as a function
of𝑠𝑡. Here, we use the state-action value instead of the advantage since the latter requires multiple Monte
Carlo rollouts at each step. By applying the Bayes rule, the posterior satisfies:
𝑝(M|ℎ𝑡)=𝑝(M|𝑠0:𝑡,𝑎0:𝑡−1,𝑟0:𝑡−1)∝𝑝(M|𝑠0:𝑡)·𝑝(𝑟0:𝑡−1|𝑠0:𝑡,𝑎0:𝑡−1,M), (5.2)
where𝑝(M|𝑠0:𝑡)conditions only on the CoT, excluding rewards, and can be interpreted as the model’s
probability of outputting solution 𝑦M
𝑠0. Interestingly, the second term 𝑝(𝑟0:𝑡−1|𝑠0:𝑡,𝑎0:𝑡−1,M)measures
the likelihood of observing the rewards 𝑟0:𝑡−1underMgiven the trajectory 𝑠0:𝑡. That is, we may write
𝑝(𝑟𝑡|𝑠𝑡,𝑎𝑡,M)∝ exp(−𝛽|𝑟𝑡−𝑟M(𝑠𝑡,𝑎𝑡)|)with a hyperparameter 𝛽to obtain
𝑝(𝑟0:𝑡−1|𝑠0:𝑡,𝑎0:𝑡−1,M)∝𝑡−1Ö
𝑡′=0𝑝(𝑟𝑡′|𝑠𝑡′,𝑎𝑡′,M)=𝑡−1Ö
𝑡′=0exp −𝛽𝑟𝑡′−𝑟M(𝑠𝑡′,𝑎𝑡′)
, (5.3)
where the proportionality holds since 𝑝(𝑟𝑡′|𝑟0:𝑡′−1,𝑠0:𝑡,𝑎0:𝑡−1,M)=𝑝(𝑟𝑡′|𝑠𝑡′,𝑎𝑡′,M).
6Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
Besides, the posterior-weighted value in (5.1) satisfies
𝔼M∼𝑝(M|ℎ𝑡)
𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)
=𝔼M∼𝑞(M|𝑠0)
𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)𝑝(M|ℎ𝑡)
𝑞(M|𝑠0)
=|M|∑︁
𝑖=0𝑄𝜋𝜃
M𝑖(𝑠𝑡,𝑎𝑡)𝑝(M𝑖|ℎ𝑡),(5.4)
where the proposal 𝑞(M|𝑠0)is the uniform distribution over the support of plausible MDPs, defined
w.r.t. the ground-truth answer and candidate answers extracted from the model’s CoTs. We draw |M|
CoT rollouts of 𝜋𝜃on prompt 𝑠0to form{M𝑖}|M|
𝑖=1. The importance ratios are then self-normalized over
{M𝑖}|M|
𝑖=1and the ground-truth MDP M0defined w.r.t. 𝑦∗
𝑠0. Substituting (5.2)and(5.3)into the above
equation and letting 𝑝(M𝑖|𝑠0:𝑡)be the model’s state-conditional belief, we obtain:
𝔼M∼𝑝(M|ℎ𝑡)𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)=|M|∑︁
𝑖=0𝑄𝜋𝜃
M𝑖(𝑠𝑡,𝑎𝑡)
|       {z       }
value inM𝑖·𝜋𝜃(𝑦M𝑖
𝑠0|𝑠𝑡+</think>)
|                          {z                          }
LLM’s belief inM𝑖’s plausibility·𝑡−1Ö
𝑡′=0exp −𝛽𝑟𝑡′−𝑟M𝑖(𝑠𝑡′,𝑎𝑡′)
|                             {z                             }
consistency b/w obs. & M𝑖’s pred.,(5.5)
where𝑟𝑡′is the actual observed reward as defined in (3.1), and𝑟M𝑖,𝑄𝜋𝜃
M𝑖are defined in (3.3)w.r.t. the
hypothesis MDPM𝑖. For clarity, we have omitted the normalization constant when computing 𝑝(M𝑖|ℎ𝑡)
from the last two terms. Algorithm 1 provides the unbatched version of the pseudocode.
Algorithm 1 Bayes-Adaptive RL for LLM Reasoning (BARL)
1:Input: LLM𝜋𝜃, prompt set{𝑠0}with ground-truth answers {𝑦∗
𝑠0}.
2:foreach prompt 𝑠0in the training data do
3:Sample|M|CoTs from 𝜋𝜃on𝑠0.
4:Extract the|M|candidate answers from each CoT to form a candidate set {𝑦M𝑖
𝑠0}|M|
𝑖=1.
5:foreach of the|M|CoTsdo
6: Calculate posterior-weighted value with (5.5) for each timestep 𝑡and update 𝜃with (5.1).
BARL offers a principled framework for stitching plausible strategies, analogous to linearized best-of-N
reasoning, but with explicit step-level guidance on whenandhowLLMs should reflectively explore.
Remark 5.1. BARL maximizes the weighted sum of values defined over each hypothesis MDP M𝑖. The
first weighting term 𝜋𝜃(𝑦M𝑖
𝑠0|·)captures LLM’s state-conditional belief in the plausibility of M𝑖. The
second product weighting term accumulates the discrepancy between predicted rewards 𝑟M𝑖(𝑠𝑡′,𝑎𝑡′)
and observed rewards 𝑟𝑡′, which serves as a reflective signal for strategy switching by downweighting
hypotheses that have high belief probabilities but are unlikely to be optimal.
6. How Bayes-Adaptive RL Helps Generalization: A Didactic Example
In this section, we present a didactic example to show how BARL facilitates test-time generalization.
Consider the action space Athat consists of three tokens, {0,1,2}, with one token generated at each
timestep. Thestateissimply 𝑠𝑡+1=𝑎𝑡. Theobjectiveistorepeattheprompttokenthreetimesconsecutively
within 29timesteps ( 33+2is the minimal length of a sequence to include all unique triplets). The prompt
token is 0or1at training time, and 2at test time. Episodes terminate when receiving a 1reward. This
setup is illustrated in Figure 2.
7Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
T r ain 
0 
1 0 … 
1 … 0 
1 0 
1 0 /1/2 
r=1 
T est 
2 2 … 2 2 
0 /1/2 
r=1 
Figure2|Setup: repeatingthe
prompt token (orange) three
times receives a 1reward.This synthetic task mirrors LLM reasoning, where the goal is not
only to learn specific strategies (here, generating particular triplets)
but also to acquire general problem-solving abilities, such as when and
how to switch to new strategies. These capabilities are essential for
handling distribution shifts between training and evaluation, a common
challenge when developing effective reasoning models.
Weusea 2-headtransformerencoderfollowedbyalinearlayerasthe
policyandtrainitusingthepolicygradientfromMarkovianRL (3.2)and
from BARL (5.1). For Markovian RL, the value 𝑄𝜋𝜃in the policy gradient
is1onlywhen 𝑠0:𝑇−1containstherewardingtriplet argmaxtri𝑟(tri)ofthe
ground-truth MDP, such as 000or111during training. For BARL, we set
𝛽=∞so thatÎ𝑡−1
𝑡′=0exp(−𝛽|𝑟𝑡′−𝑟M𝑖(𝑠𝑡′,𝑎𝑡′)|)= 1(argmaxtri𝑟M𝑖(tri)∉
𝑠0:𝑡), i.e., the product is 0when the rewarding triplet of M𝑖already appears in 𝑠0:𝑡and thus is invalidated,2
and1otherwise. We let the policy’s state-conditional belief be 𝑝(M𝑖|𝑠0:𝑡)=1forM𝑖whose rewarding
triplet aligns with the sampled policy action 𝑎𝑡, i.e., argmaxtri𝑟M𝑖(tri)=𝑠𝑡−1:𝑡+1, where𝑠𝑡+1=𝑎𝑡∼𝜋𝜃(·|𝑠𝑡).
Therefore, the posterior-weighted value from (5.4) becomes
𝔼M
𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)
=|M|∑︁
𝑖=1𝑄𝜋𝜃
M𝑖(𝑠𝑡,𝑎𝑡)𝑝(M𝑖|𝑠0:𝑡) 1 argmax
tri𝑟M𝑖(tri)∉𝑠0:𝑡= 1 
𝑠𝑡−1:𝑡+1∈{M𝑖}|M|
𝑖=1,𝑠𝑡−1:𝑡+1∉𝑠0:𝑡
,
where𝑎𝑡∼𝜋𝜃(·|𝑠𝑡). The above formulation incentivizes the policy to eliminate hypotheses and switch
to new strategies (i.e., new triplets) when the current strategy has been invalidated by earlier attempts
up to step 𝑡, which is illustrated in Figure 3. This form of adaptive exploration provides a minimalist
instantiation of BARL in synthetic settings. The difference between the above equation and (5.5)arises
because the agent in this example is aware of the zero reward associated with unterminated episodes.
Train
0
10
10
1
Markovian RL 2Test
memorize solutions ?…
fail to generalize random 0/1/2Train & Test 
010
Bayes-Adaptive RL 2
            r(010)=1  ✖
 r(010)=1  ✖
    
            r(102)=1  ✔
 r(102)=1 ✖
            r(001)=1  ✔
 r(001)=1  ✔
  … …                 … … 
              belief at 010   belief at 010233 MDPs’ 
hypotheses…until 
r=1
        r(010)=1   ✖
    
        r(102)=1   ✖
              … … 
        r(000)=1  ✔
belief at 0102…000 
switch strategies w. eliminated hypotheses 0
Figure 3|Illustration of the difference between Markovian RL and BARL in this didactic example.
We report the results in Figure 4, where accuracies are averaged over 50completions and the shadow
regions are the standard deviation across 3independent model training runs. The results show that
Markovian RL quickly finds and memorizes the training solutions but fails to generalize at test time. In
contrast, Bayes-Adaptive RL increases both training and testing accuracies. Furthermore, its accuracy and
convergence rate improve when given prior knowledge that rewarding triplets are repeated patterns, i.e.,
|M|=3with𝑟M1(000)=𝑟M2(111)=𝑟M3(222)=1and all other rewards are zero. This highlights the
advantage of more informative candidate sets, underscoring the importance of balancing the diversity and
plausibility of the candidates. Specifically, they should be diverse enough to capture test-time uncertainty,
yet constrained to only the most plausible candidates to shrink the hypothesis space.
2This is because 𝑟𝑡′=0for unterminated sequences and if argmaxtri𝑟M𝑖(tri)∈𝑠0:𝑡, i.e.,𝑟M𝑖(𝑠𝑡′,𝑎𝑡′)=1for some𝑡′, then
exp(−𝛽|𝑟𝑡′−𝑟M𝑖(𝑠𝑡′,𝑎𝑡′)|)=0and the resulting accumulated product is also 0.
8Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
0 50 100 150 200 250 300
Iteration0.00.20.40.60.81.0AccuracyMarkovian RL
Train
T est
0 50 100 150 200 250 300
Iteration0.60.70.80.91.0AccuracyBayes-Adaptive RL (||=33)
Train
T est
0 50 100 150 200 250 300
Iteration0.60.70.80.91.0AccuracyBayes-Adaptive RL (||=3)
Train
T est
Figure 4|(Left)Markovian RL (REINFORCE) memories training solutions and poorly generalizes.
(Middle) BARL generalizes well at test time and (Right) improves with more informative candidate sets
that strike a better balance between diversity andplausibility .
7. Experiments
7.1. Experiment Setups
In addition to the synthetic experiment in Section 6, we evaluate BARL on LLM math reasoning tasks.
Training is conducted on the Big-Math dataset [ 2] across various models, including Qwen2.5-Math-
1.5B, Qwen2.5-Math-7B [ 53], and DeepSeek-R1-Distill-Llama-8B [ 17]. Evaluation is performed on four
benchmarks: GSM8K [ 7], MATH [ 21], CollegeMath [ 43], and OlympiadBench [ 20]. During training,
the maximum prompt length is set to 512and the maximum response length is set to 1024. We provide
the AIME and AMC results in Appendix B.1 and exclude them from our main evaluation due to their
substantially longer context requirements, e.g., R1-Distill-Llama-8B has average response lengths of 2008
and1886tokens on AIME 2024 and AMC 2023, respectively. For BARL, we set 𝛽=1and|M|=5.
We compare BARL against two Markovian RL baselines that span outcome-reward and process-reward
RL. For the outcome-reward GRPO baseline, we set its group size to 5for a fair comparison with BARL
and, after performing a grid search over the KL-divergence coefficients [0,0.001,0.005,0.01], adopt
0.005as it yields the best overall performance across all benchmarks. For the process-reward baseline,
we adapt a variant of MRT [ 37] by integrating the progress reward defined in (3.1)into the outcome
reward, which we refer to as progress in the following sections. For all algorithms, we set the training
and rollout batch sizes to 128and1024, respectively. We train the Qwen and Llama models for 110and
60iterations, respectively, defined w.r.t. the rollout batches. The temperature during online sampling is
1.0and is 0.0during evaluation. For both BARL and the progress baseline, we set the number of tokens
for each reasoning step as 128. All algorithms are implemented using the OpenRLHF codebase [22].
7.2. Experiment Results
We report the pass@1 accuracies in Table 1. All models are trained using three random seeds, and we
calculate the mean and standard deviation of the resulting accuracies. For each algorithm, we report the
results of the checkpoint that has the highest average accuracy.
It can be observed that BARL achieves higher accuracies across most benchmarks and models. It
consistently outperforms Markovian RL baselines in terms of average accuracy, with larger gains observed
on challenging benchmarks that demand effective exploration, such as CollegeMath and OlympiadBench.
Notably, BARL delivers these gains with minimal computational overhead, which comes from calculating
the probabilities of candidate answers at the end of each step, reusing the cache from CoT generations.
9Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
Model GSM8K MATH CollegeMath OlympiadBench Average
Qwen2.5-Math-1.5B 40.0 34 .1 6 .6 21 .8 25 .6
GRPO 83.9(±0.5)71.5(±0.4) 45.1(±0.3) 33.4(±0.4) 58.4(±0.3)
Progress 84.8(±0.6)72.3(±0.4) 45.9(±0.3) 35.6(±0.2) 59.6(±0.2)
BARL 86.0(±0.6)72.7(±0.3)46.8(±0.2)35.8(±0.4)60.3(±0.1)
Qwen2.5-Math-7B 59.1 53.7 21.9 19.0 38.4
GRPO 90.3(±0.1)77.6(±0.4) 47.0(±0.3) 39.0(±0.2) 63.5(±0.2)
Progress 91.1(±0.3)78.8(±0.1) 47.2(±0.3) 41.1(±0.2) 64.5(±0.2)
BARL 91.7(±0.2)79.2(±0.3)47.5(±0.2)42.0(±0.4)65.1(±0.3)
R1-Distill-Llama-8B 82.0 65 .7 35 .8 26 .5 52 .5
GRPO 85.7(±0.5)74.3(±0.4) 39.6(±0.3) 36.0(±0.5) 58.9(±0.4)
Progress 85.9(±0.4)73.9(±0.4) 39.8(±0.5) 35.3(±0.2) 58.7(±0.4)
BARL 85.4(±0.6)73.9(±0.6)40.4(±0.3)37.2(±0.5)59.3(±0.4)
Table 1|Mean and standard deviation of the accuracies over three independent training runs.
7.3. Ablation Studies
Token Efficiency. We evaluate the token efficiency of BARL and baseline models by measuring the total
number of tokens required to solve a problem. Specifically, in Figure 5, we plot the pass@k accuracies
and the corresponding average numbers of tokens, serving as a proxy for performance per token. In all
the following ablations, we mainly analyze the models fine-tuned from Qwen2.5-Math-1.5B. We vary k
from 1to6and set the temperature to 1.0, but observe that the GRPO and base models are less robust
under sampling, resulting in decreased pass@1 performance (see Appendix B.3). To account for this, we
combine greedy decoding outputs with sampled outputs when computing token usage and accuracy. We
omit the base model from the plots due to its significantly higher token consumption and lower asymptotic
accuracy. We find that BARL achieves higher accuracies with substantially fewer tokens, requiring up to
39%fewer average tokens than the progress baseline, 50%fewer than GRPO, and over 90%fewer than
the base model.
500 1000 1500 2000 2500
T otal Numbers of T okens848688909294Accuracy
2.2x2.4xGSM8K
GRPO
Progress
BARL
1000 2000 3000 4000
T otal Numbers of T okens72.575.077.580.082.585.0Accuracy
1.3x1.6xMATH
GRPO
Progress
BARL
1000 2000 3000 4000 5000
T otal Numbers of T okens46485052Accuracy
1.5x
2.1xCollegeMath
GRPO
Progress
BARL
1000 2000 3000 4000 5000
T otal Numbers of T okens3035404550Accuracy
2.0x1.5xOlympiadBench
GRPO
Progress
BARL
Figure 5|BARL is more token efficient, achieving higher accuracies with fewer total numbers of tokens.
0 1 2 3 4 5 6
Difficulty Level0.000.050.100.150.200.250.30Reflection Freq.
Base
BARL
Figure 6|Results on GSM8K
(dashed) and MATH (solid).Reflective Reasoning Behaviors. To qualitatively assess the improved
tokenefficiencyofBARL,weanalyzethefrequencyofreflectivebehaviors
acrossproblemsofvaryingdifficultylevels,asshowninFigure6. Foreach
problem, we sample 6responses per model and define its difficulty level
by the number of incorrect responses. We use keyword-based detections
[31,54] to identify whether self-reflections appear in a response, and
a problem is considered to exhibit self-reflection if at least one of its
responses is identified. It can be observed that both models display fewer
reflections on easier problems, and the base model exhibits a higher
10Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
frequency of reflections despite achieving lower accuracies. This result reveals the weak correlation
between the performance of LLMs and the response length or the frequency of reflections. Rather, the
effectiveness of thinking tokens and the efficiency of explorations are the determining factors, which we
study in the following ablation.
Effectiveness of CoTs. We measure the effectiveness of the CoTs produced by different models by
calculating the average Bayesian state-action values at each timestep, which naturally captures both
the exploration and the exploitation aspects of the actions. Specifically, the Bayesian value is defined
as𝑄𝜋(𝑏𝑡,𝑠𝑡,𝑎𝑡)=𝔼𝜋,M∼𝑏𝑡[𝑟M(𝑠𝑡,𝑎𝑡)+𝑄𝜋(𝑏𝑡+1,𝑠𝑡+1,𝑎𝑡+1)], where the belief 𝑏𝑡=𝑝(M|ℎ𝑡). Unlike standard
Q-values, the Bayesian Q-value not only incorporates the expected returns (exploitation) but also captures
the value of information gained through belief updates (exploration).
200 400 600 800 1000
Number of T okens0.1
0.00.10.2Value
GSM8K
Base
GRPO
BARL
200 400 600 800 1000 1200
Number of T okens0.10
0.05
0.000.050.10Value
MATH
Base
GRPO
BARL
200 400 600 800 1000 1200
Number of T okens0.1
0.00.10.2Value
CollegeMath
Base
GRPO
BARL
200 400 600 800 1000 1200
Number of T okens0.06
0.04
0.02
0.000.020.04Value
OlympiadBench
Base
GRPO
BARL
Figure 7|Ablation on how effective the CoTs explore and exploit, measured by the Bayesian values.
The results are reported in Figure 7. We observe that the actions from the BARL model exhibit consis-
tently higher Bayesian values compared to those of the GRPO and base models, indicating more effective
exploration and exploitation. On more challenging benchmarks such as OlympiadBench, exploratory
gains peak midway through the CoTs after an early phase of uncertainty reduction. Moreover, the result
also explains our earlier observations on token efficiency and reflective behaviors. Although the base
model exhibits more self-reflections, these are likely superficial or stylistic patterns due to their low
exploration efficiency for gathering informative contexts during evaluation.
Markovian RL Optimality. We train a length-controlled (LC) GRPO with a maximum 32response
length over multiple epochs. Figure 8 shows the evolution of the training accuracy and length. The
rapidly increasing accuracy and decreasing response length of GRPO LC indicate that it learns to skip
CoT generation and emit only the final answer. Its asymptotic training accuracy matches that of GRPO
(max length 1024). This result supports Theorem 4.1: Markovian RL can achieve optimality by merely
memorizing solutions without reflective reasoning. Such policies, however, generalize poorly during
evaluation. We refer readers to Appendix B for additional experimental results.
0 25 50 75 100 125 150
Iteration0.00.10.20.30.40.50.6Training AccuracyGRPO LC
GRPO
0 25 50 75 100 125 150
Iteration10210410610Response LengthGRPO LC
GRPO
GSM8K MATH College Olympiad Average
Benchmark020406080Eval AccuracyGRPO LC
GRPO
Figure 8|(Left)Training accuracy. (Middle) Training response length. (Right) Final evaluation results.
11Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
Key Experiment Findings
BARL consistently outperforms Markovian RL baselines with superior token efficiency. Performance
correlates with the effectiveness of reflective explorations, rather than their frequency. Optimality
in Markovian RL can be attained by policies that memorize training solutions yet fail to generalize,
with no guarantees on the emergence of self-reflections.
8. Conclusion
Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited emergent
behaviors such as self-reflective reasoning. Yet in conventional Markovian RL, exploration is confined
to the training phase to identify action sequences that maximize cumulative reward, and resorts to
pure exploitation at test time. Besides, the Markov assumption indicates the dependency on history
only through the state. Thus, Markovian RL neither ensures the emergence of reflective exploration
nor explains its benefits during testing. We propose to fill this gap with Bayes-Adaptive RL, which
explicitly considers test-time performance by maximizing the expected return under a posterior of MDPs.
Within this framework, we propose BARL, a novel algorithm for LLM reasoning that provides principled
guidance for when and how to engage in reflective exploration. BARL enables efficient exploration
through hypothesis elimination and strategy switching. Our experiments are conducted on both synthetic
and mathematical reasoning tasks, where we show that BARL outperforms Markovian RL algorithms at
test time and its exploration is more efficient. As future work, we plan to extend our approach to broader
domains, such as coding and agentic tasks.
Acknowledgements
We thank Aviral Kumar, Chu-Cheng Lin, and Ziyu Ye for the insightful discussions and valuable feedback.
References
[1]RenatAksitov,SobhanMiryoosefi,ZonglinLi,DaliangLi,SheilaBabayan,KavyaKopparapu,Zachary
Fisher, Ruiqi Guo, Sushant Prakash, Pranesh Srinivasan, et al. Rest meets react: Self-improvement
for multi-step reasoning llm agent. arXiv preprint arXiv:2312.10003 , 2023.
[2]Alon Albalak, Duy Phung, Nathan Lile, Rafael Rafailov, Kanishk Gandhi, Louis Castricato, Anikait
Singh, Chase Blagden, Violet Xiang, Dakota Mahan, et al. Big-math: A large-scale, high-quality
math dataset for reinforcement learning in language models. arXiv preprint arXiv:2502.17387 ,
2025.
[3]Dilip Arumugam and Satinder Singh. Reducing the information horizon of bayes-adaptive markov
decision processes via epistemic state abstraction.
[4]Richard Bellman and Robert Kalaba. On adaptive control processes. IRE Transactions on Automatic
Control, 4(2):1–9, 1959.
[5]Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V Le, Christopher Ré, and Azalia
Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv
preprint arXiv:2407.21787 , 2024.
12Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
[6]Jiayu Chen, Wentse Chen, and Jeff Schneider. Bayes adaptive monte carlo tree search for offline
model-based reinforcement learning. arXiv preprint arXiv:2410.11234 , 2024.
[7]KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word
problems. arXiv preprint arXiv:2110.14168 , 2021.
[8]Hanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao,
Jipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative
foundation model alignment. arXiv preprint arXiv:2304.06767 , 2023.
[9]Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. Rl2: Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779 , 2016.
[10]Michael O Duff. Monte-carlo algorithms for the improvement of finite-state stochastic controllers:
Application to bayes-adaptive markov decision processes. In International Workshop on Artificial
Intelligence and Statistics , pages 93–97. PMLR, 2001.
[11]Michael O’Gordon Duff. Optimal Learning: Computational procedures for Bayes-adaptive Markov
decision processes . University of Massachusetts Amherst, 2002.
[12]Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng, Archit Sharma, and Noah D
Goodman. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683 ,
2024.
[13]Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, Aviv Tamar, et al. Bayesian reinforcement
learning: A survey. Foundations and Trends ®in Machine Learning , 8(5-6):359–483, 2015.
[14]Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline rl policies should be trained
to be adaptive. In International Conference on Machine Learning , pages 7513–7530. PMLR, 2022.
[15]Dibya Ghosh, Jad Rahme, Aviral Kumar, Amy Zhang, Ryan P Adams, and Sergey Levine. Why
generalization in rl is difficult: Epistemic pomdps and implicit partial observability. Advances in
neural information processing systems , 34:25502–25515, 2021.
[16]Arthur Guez, David Silver, and Peter Dayan. Efficient bayes-adaptive reinforcement learning using
sample-based search. Advances in neural information processing systems , 25, 2012.
[17]Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via
reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.
[18]Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International conference
on machine learning , pages 1861–1870. Pmlr, 2018.
[19]Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos Nalmpantis, Jane Dwivedi-Yu,
Maksym Zhuravinskyi, Eric Hambro, Sainbayar Sukhbaatar, and Roberta Raileanu. Teaching large
language models to reason with reinforcement learning. arXiv preprint arXiv:2403.04642 , 2024.
13Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
[20]Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu,
XuHan,YujieHuang,YuxiangZhang,etal. Olympiadbench: Achallengingbenchmarkforpromoting
agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008 ,
2024.
[21]Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv
preprint arXiv:2103.03874 , 2021.
[22]Jian Hu, Xibin Wu, Zilin Zhu, Xianyu, Weixun Wang, Dehao Zhang, and Yu Cao. Openrlhf: An
easy-to-use, scalable and high-performance rlhf framework. arXiv preprint arXiv:2405.11143 , 2024.
[23]Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec
Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint
arXiv:2412.16720 , 2024.
[24]Amirhossein Kazemnejad, Milad Aghajohari, Eva Portelance, Alessandro Sordoni, Siva Reddy, Aaron
Courville, and Nicolas Le Roux. Vineppo: Unlocking rl potential for llm reasoning through refined
credit assignment. arXiv preprint arXiv:2410.01679 , 2024.
[25]Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large lan-
guage models are zero-shot reasoners. Advances in neural information processing systems , 35:22199–
22213, 2022.
[26]Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul Mcvay, Michael Rabbat, and
Yuandong Tian. Beyond a*: Better planning with transformers via search dynamics bootstrapping.
arXiv preprint arXiv:2402.14083 , 2024.
[27]Aly Lidayan, Michael Dennis, and Stuart Russell. Bamdp shaping: a unified theoretical framework
for intrinsic motivation and reward shaping. arXiv preprint arXiv:2409.05358 , 2024.
[28]Aly Lidayan, Michael D Dennis, and Stuart Russell. Bamdp shaping: a unified framework for
intrinsic motivation and reward shaping. In The Thirteenth International Conference on Learning
Representations .
[29]Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan
Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let’s verify step by step. In The Twelfth
International Conference on Learning Representations , 2023.
[30]Zhihan Liu, Hao Hu, Shenao Zhang, Hongyi Guo, Shuqi Ke, Boyi Liu, and Zhaoran Wang. Reason
for future, act for now: A principled framework for autonomous llm agents with provable sample
efficiency. arXiv preprint arXiv:2309.17382 , 2023.
[31]Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min
Lin. Understanding r1-zero-like training: A critical perspective. arXiv preprint arXiv:2503.20783 ,
2025.
[32]Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan Li, Lei Shu, Yun
Zhu, Lei Meng, Jiao Sun, et al. Improve mathematical reasoning in language models by automated
process supervision. arXiv preprint arXiv:2406.06592 , 2, 2024.
14Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
[33]James John Martin. Some Bayesian decision problems in a Markov chain. PhD thesis, Massachusetts
Institute of Technology, 1965.
[34]Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In Icml, volume 99, pages 278–287. Citeseer, 1999.
[35]Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin
Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing inductive reason-
ing capabilities of language models with hypothesis refinement. arXiv preprint arXiv:2310.08559 ,
2023.
[36]Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, and Sjoerd van Steenkiste. Bayesian teaching
enables probabilistic reasoning in large language models. arXiv preprint arXiv:2503.17523 , 2025.
[37]Yuxiao Qu, Matthew YR Yang, Amrith Setlur, Lewis Tunstall, Edward Emanuel Beeching, Ruslan
Salakhutdinov, and Aviral Kumar. Optimizing test-time compute via meta reinforcement fine-tuning.
arXiv preprint arXiv:2503.07572 , 2025.
[38]JohnSchulman,PhilippMoritz,SergeyLevine,MichaelJordan,andPieterAbbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438 , 2015.
[39]Amrith Setlur, Chirag Nagpal, Adam Fisch, Xinyang Geng, Jacob Eisenstein, Rishabh Agarwal, Alekh
Agarwal, Jonathan Berant, and Aviral Kumar. Rewarding progress: Scaling automated process
verifiers for llm reasoning. arXiv preprint arXiv:2410.08146 , 2024.
[40]Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Yu Wu,
and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language
models. arXiv preprint arXiv:2402.03300 , 2024.
[41]Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J
Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al. Beyond human data: Scaling self-training for
problem-solving with language models. arXiv preprint arXiv:2312.06585 , 2023.
[42]Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally
can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314 , 2024.
[43]Zhengyang Tang, Xingxing Zhang, Benyou Wang, and Furu Wei. Mathscale: Scaling instruction
tuning for mathematical reasoning. arXiv preprint arXiv:2403.02884 , 2024.
[44]Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia
Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and
outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022.
[45]Huaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. Offline
reinforcement learning for llm multi-step reasoning. arXiv preprint arXiv:2412.16145 , 2024.
[46]Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui.
Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv preprint
arXiv:2312.08935 , 2023.
15Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
[47]Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah D Goodman.
Hypothesis search: Inductive reasoning with language models. arXiv preprint arXiv:2309.05660 ,
2023.
[48]Yibin Wang, Haizhou Shi, Ligong Han, Dimitris Metaxas, and Hao Wang. Blob: Bayesian low-rank
adaptation by backpropagation for large language models. arXiv preprint arXiv:2406.11675 , 2024.
[49]Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, and Jingbo Shang.
Multi-step problem solving through a verifier: An empirical analysis on model-induced process
supervision. arXiv preprint arXiv:2402.02658 , 2024.
[50]Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny
Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in
neural information processing systems , 35:24824–24837, 2022.
[51]Yuxiang Wei, Olivier Duchenne, Jade Copet, Quentin Carbonneaux, Lingming Zhang, Daniel
Fried, Gabriel Synnaeve, Rishabh Singh, and Sida I Wang. Swe-rl: Advancing llm reasoning via
reinforcement learning on open software evolution. arXiv preprint arXiv:2502.18449 , 2025.
[52]Violet Xiang, Charlie Snell, Kanishk Gandhi, Alon Albalak, Anikait Singh, Chase Blagden, Duy
Phung, Rafael Rafailov, Nathan Lile, Dakota Mahan, et al. Towards system 2 reasoning in llms:
Learning how to think with meta chain-of-though. arXiv preprint arXiv:2501.04682 , 2025.
[53]An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong
Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren,
and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via
self-improvement. arXiv preprint arXiv:2409.12122 , 2024.
[54]Edward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-
of-thought reasoning in llms. arXiv preprint arXiv:2502.03373 , 2025.
[55]LonghuiYu, WeisenJiang, HanShi, JinchengYu, ZhengyingLiu, YuZhang, JamesTKwok, Zhenguo
Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for
large language models. arXiv preprint arXiv:2309.12284 , 2023.
[56]Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong
Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale.
arXiv preprint arXiv:2503.14476 , 2025.
[57]Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou,
and Jingren Zhou. Scaling relationship on learning mathematical reasoning with large language
models. arXiv preprint arXiv:2308.01825 , 2023.
[58]Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen.
Mammoth: Building math generalist models through hybrid instruction tuning. arXiv preprint
arXiv:2309.05653 , 2023.
[59]Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah D Goodman.
Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint
arXiv:2403.09629 , 2024.
16Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
[60]Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with
reasoning. Advances in Neural Information Processing Systems , 35:15476–15488, 2022.
[61]Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-
zoo: Investigating and taming zero reinforcement learning for open base models in the wild. arXiv
preprint arXiv:2503.18892 , 2025.
[62]Shenao Zhang, Donghan Yu, Hiteshi Sharma, Han Zhong, Zhihan Liu, Ziyi Yang, Shuohang Wang,
Hany Hassan, and Zhaoran Wang. Self-exploring language models: Active preference elicitation for
online alignment. arXiv preprint arXiv:2405.19332 , 2024.
[63]Han Zhong, Yutong Yin, Shenao Zhang, Xiaojun Xu, Yuanxin Liu, Yifei Zuo, Zhihan Liu, Boyi
Liu, Sirui Zheng, Hongyi Guo, et al. Brite: Bootstrapping reinforced thinking process to enhance
language model reasoning. arXiv preprint arXiv:2501.18858 , 2025.
A. Proof of Theorem 4.2
Proof.Consider a full binary tree of depth 𝑇∗, with leaf setLof size|L|=2𝑇∗. The states are the tree
nodes, with the initial state fixed as the root node. The actions include the moves from the parent nodes
to the child nodes as well as a resetting action from the leaf node to the root node. The rewards are not
known and differ in different MDP hypotheses. Specifically, the reward is defined as 𝑟M𝑖(𝑠)= 1(𝑠=𝑠𝑖),
where𝑠𝑖is a unique leaf node for M𝑖. The prior of MDPs is 𝑝(M𝑖)=1/|L|, where𝑖=1,···,|L|. The
cumulative return is undiscounted with the minimum traverse steps as the horizon 𝑇. The episode
terminates once the agent receives a 1reward.
For any Markovian policy 𝜋, define𝑓(𝑠)=𝑝(∃𝑡≥0 :𝑠𝑡=𝑠|𝜋)and denote 𝑝𝑠as the probability
that𝜋goes left at an internal node 𝑠. For any left and right children 𝑠𝐿and𝑠𝑅of node𝑠, it holds that
𝑓(𝑠𝐿)=𝑓(𝑠)𝑝𝑠and𝑓(𝑠𝑅)=𝑓(𝑠)(1−𝑝𝑠). Thus,𝑓(𝑠𝐿)+𝑓(𝑠𝑅)=𝑓(𝑠). By an induction on depth, it follows
that at every depth 𝑑,Í
𝑠:depth(𝑠)=𝑑𝑓(𝑠)=1. In particular,Í
𝑙∈L𝑓(𝑙)=1. Since the total return under M𝑖
is𝑉(𝜋|M𝑖)=𝑝(𝜋ends at leaf 𝑙𝑖)=𝑓(𝑙𝑖), the expected return for the optimal Markovian policy is
1
|L||L|∑︁
𝑖=1𝑉(𝜋|M𝑖)=1
2𝑇∗∑︁
𝑙∈L𝑓(𝑙)=1
2𝑇∗.
Consider the following deterministic Bayes-adaptive policy. At the root node, the policy picks any
leaf𝑙with positive posterior, i.e., 𝑝(M𝑙|ℎ𝑡)>0, then follows the unique shortest path to 𝑙. Two
possible outcomes can occur: if the ground-truth reward 𝑟(𝑙)=1, then the episodes terminates with the
collected reward; if 𝑟(𝑙)=0, then the agent eliminates the hypothesis M𝑙from the posterior by setting
𝑝(M𝑙|ℎ𝑡:𝑡+𝑇∗)=0, and returns to the root to repeat the process on the remaining leaves. By construction,
the expected return of this Bayes-Adaptive policy is 1, which is an exponential improvement in 𝑇∗over
the1/2𝑇∗return of the optimal Markovian policy. □
B. Experiment Details
B.1. Evaluation Results: Accuracies
In this section, we provide the evaluation accuracy results for Qwen2.5-Math-1.5B, Qwen2.5-Math-7B,
and DeepSeek-R1-Distill-Llama-8B in Figure 9, 10, and 11, respectively. In addition to the benchmark
17Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
scores in Table 1, we also report the performance of the models on AIME 2024 and AMC 2023. It can be
observed that BARL outperforms Markovian RL baselines in terms of both accuracy and convergence
rate on most of the reported benchmarks. Again, the performance on AIME and AMC benchmarks may
be further enhanced by choosing harder training data with increased response length, especially for
R1-Distill-Llama-8B fine-tuned models whose initial average response lengths on AIME and AMC ( 2008
and1886, respectively) exceed the maximum training length ( 1024).
0 20 40 60 80 100
Iteration76788082848688Accuracy
GSM8K
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration646668707274Accuracy
MATH
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration35.037.540.042.545.047.5Accuracy
CollegeMath
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration2426283032343638Accuracy
OlympiadBench
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration5101520Accuracy
AIME 2024
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration30405060Accuracy
AMC 2023
GRPO
Progress
BARL
Figure 9|Average evaluation accuracies over training iterations for Qwen2.5-Math-1.5B models.
0 20 40 60 80 100
Iteration8486889092Accuracy
GSM8K
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration7274767880Accuracy
MATH
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration4042444648Accuracy
CollegeMath
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration30323436384042Accuracy
OlympiadBench
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration1015202530Accuracy
AIME 2024
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration455055606570Accuracy
AMC 2023
GRPO
Progress
BARL
Figure 10|Average evaluation accuracies over training iterations for Qwen2.5-Math-7B models.
18Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
0 10 20 30 40 50 60
Iteration818283848586Accuracy
GSM8K
GRPO
Progress
BARL
0 10 20 30 40 50 60
Iteration646668707274Accuracy
MATH
GRPO
Progress
BARL
0 10 20 30 40 50 60
Iteration363738394041Accuracy
CollegeMath
GRPO
Progress
BARL
0 10 20 30 40 50 60
Iteration25.027.530.032.535.037.5Accuracy
OlympiadBench
GRPO
Progress
BARL
0 10 20 30 40 50 60
Iteration5.07.510.012.515.017.520.0Accuracy
AIME 2024
GRPO
Progress
BARL
0 10 20 30 40 50 60
Iteration404550556065Accuracy
AMC 2023
GRPO
Progress
BARL
Figure 11|Average evaluation accuracies over training iterations for R1-Distill-Llama-8B models.
B.2. Evaluation Results: Response Lengths
In this section, we present the evolution of evaluation response lengths over training iterations for
Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, and DeepSeek-R1-Distill-Llama-8B, shown in Figures 12, 13,
and14, respectively. Acrossmostbenchmarks, responselengthstendtodecreaseastrainingprogressesfor
all algorithms. The response length during training has a very similar trend to that during evaluation. This
trend arises because all three models exhibit reflective behaviors, such as self-evaluation and backtracking,
that introduce redundant tokens and lengthen responses. As shown in Figure 7, these behaviors are
likely superficial or stylistic patterns with limited effectiveness. An exception is AIME, where some
models maintain consistently long responses due to the benchmark’s intrinsic requirement for extended
reasoning, even under optimal non-reflective policies.
0 20 40 60 80 100
Iteration100200300400500600700Response Length
GSM8K
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration300400500600Response Length
MATH
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration300400500600700Response Length
CollegeMath
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration425450475500525550Response Length
OlympiadBench
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration500550600650700750Response Length
AIME 2024
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration350400450500550Response Length
AMC 2023
GRPO
Progress
BARL
Figure 12|Average evaluation response lengths over training iterations for Qwen2.5-Math-1.5B models.
19Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
0 20 40 60 80 100
Iteration200225250275300325Response Length
GSM8K
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration300320340360380400Response Length
MATH
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration280300320340360380400Response Length
CollegeMath
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration440460480500520540Response Length
OlympiadBench
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration550600650700750Response Length
AIME 2024
GRPO
Progress
BARL
0 20 40 60 80 100
Iteration400425450475500525Response Length
AMC 2023
GRPO
Progress
BARL
Figure 13|Average evaluation response lengths over training iterations for Qwen2.5-Math-7B models.
10 20 30 40 50 60
Iteration350400450500Response Length
GSM8K
GRPO
Progress
BARL
10 20 30 40 50 60
Iteration650700750800850900Response Length
MATH
GRPO
Progress
BARL
10 20 30 40 50 60
Iteration550600650700750800Response Length
CollegeMath
GRPO
Progress
BARL
10 20 30 40 50 60
Iteration1050110011501200Response Length
OlympiadBench
GRPO
Progress
BARL
10 20 30 40 50 60
Iteration122012401260128013001320Response Length
AIME 2024
GRPO
Progress
BARL
10 20 30 40 50 60
Iteration900950100010501100Response Length
AMC 2023
GRPO
Progress
BARL
Figure 14|Average evaluation response lengths over training iterations for R1-Distill-Llama-8B models.
20Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning
Since the models used in the main experiments already exhibit lengthy CoTs with reflective patterns,
we further implement BARL on the Llama-3.2-3B-Instruct model, which displays fewer self-reflections.
The results are presented in Figure 15. Initially, the response length decreases as the base model tends
to produce excessively long reasoning traces, often exceeding ten steps, which are pruned during early
training. Subsequently, the response length increases, as a result of plausible strategy stitching.
0 10 20 30 40 50 60 70
Iteration0.250.300.350.400.45Training Accuracy
0 10 20 30 40 50 60 70
Iteration360380400420440460Response Length
BaseBARL
GSM8K 69.473.8
MATH 47.250.5
CollegeMath 30.833.6
Olympiad 16.419.1
AIME 24 3.313.3
AMC 23 27.535.0
Figure 15|Results of BARL fine-tuned on Llama-3.2-3B-Instruct. (Left)Training accuracy and (Middle)
response length. (Right) Evaluation results.
B.3. Token Efficiency without Greedy Decoding Outputs
In Figure 16, we present the pass@k accuracies from an ablation similar to Section 7.3, except that
greedy decoding outputs are excluded when computing token counts and accuracies. We observe that
the base and GRPO models are less robust under a sampling temperature of 1.0, resulting in significantly
lower pass@1 accuracies compared to greedy decoding. This degradation may stem from the fragility of
their CoTs, which often exhibit stylistic but unproductive self-reflection and backtracking behaviors.
500 1000 1500 2000 2500 3000
T otal Numbers of T okens7580859095Accuracy
GSM8K
GRPO
Progress
BARL
1000 2000 3000 4000
T otal Numbers of T okens606570758085Accuracy
MATH
GRPO
Progress
BARL
1000 2000 3000 4000 5000
T otal Numbers of T okens35404550Accuracy
CollegeMath
GRPO
Progress
BARL
1000 2000 3000 4000 5000
T otal Numbers of T okens253035404550Accuracy
OlympiadBench
GRPO
Progress
BARL
Figure 16|Ablation on token efficiency and pass@k accuracies with sampling temperature =1. GRPO
and the base models are less robust to temperatures.
B.4. Some Unsuccessful Attempts
As a straightforward implementation of the Bayes-Adaptive RL policy gradient in (5.1), we explored
using value ensembles to estimate the posterior-weighted value 𝔼M∼𝑝(M|ℎ𝑡)[𝑄𝜋𝜃
M(𝑠𝑡,𝑎𝑡)]. Specifically, we
trained an ensemble of state-action value functions to capture epistemic uncertainty. We experimented
with two approaches to constructing the ensemble: (1) fine-tuning multiple linear value heads on
disjoint data subsets, each paired with chain-of-thought (CoT) trajectories and outcome rewards; and (2)
applying Bayesian LoRA in a way similar to [ 48]. However, both methods failed to effectively capture
epistemic uncertainty, likely because different ensembles still share a large fraction of parameters. While
maintaining independent value models may better capture this uncertainty, doing so incurs substantial
computational cost. We leave the development of more efficient implementations to future work.
21