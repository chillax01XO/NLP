ArVoice: A Multi-Speaker Dataset for Arabic Speech Synthesis
Hawau Olamide Toyin, Rufael Marew, Humaid Alblooshi, Samar M. Magdy, Hanan Aldarmaki
Mohamed Bin Zayed University of Artificial Intelligence, UAE
hawau.toyin@mbzuai.ac.ae, hanan.aldarmaki@mbzuai.ac.ae
Abstract
We introduce ArV oice, a multi-speaker Modern Standard Ara-
bic (MSA) speech corpus with diacritized transcriptions, in-
tended for multi-speaker speech synthesis, and can be useful
for other tasks such as speech-based diacritic restoration, voice
conversion, and deepfake detection. ArV oice comprises: (1) a
new professionally recorded set from six voice talents with di-
verse demographics, (2) a modified subset of the Arabic Speech
Corpus; and (3) high-quality synthetic speech from two com-
mercial systems. The complete corpus consists of a total of
83.52 hours of speech across 11 voices; around 10 hours consist
of human voices from 7 speakers. We train three open-source
TTS and two voice conversion systems to illustrate the use cases
of the dataset. The corpus is available for research use.
1. Introduction
Speech synthesis technologies, such as Text-to-Speech (TTS),
and V oice conversion (VC), have shown remarkable progress
in quality and naturalness, particularly with the availability of
large-scale, high-quality datasets. However, for mid-low re-
source languages, such as Arabic, the availability of clean and
well-curated speech corpora remains limited, posing challenges
for developing robust open-source speech synthesis models.
Although there are several speech data sets for Arabic ASR
[1, 2, 3], these sets were mainly sourced from news chan-
nels, podcasts, and other sources that are naturally contaminated
with noise, overlapping speech, inconsistent recording quality,
and expressive or dialectal speech, making them poorly suited
for developing robust speech synthesis systems with acceptable
quality. Another limitation of these existing resources is the
lack of diacritics /ta Àásk¬Øƒ±l/, which represent important vowel in-
formation in Arabic orthography.1The absence of diacritics re-
sults in under-specified text inputs with multiple possible pro-
nunciations, leading to unintelligible synthesized speech. While
text-based diacritic restoration models for Arabic could be uti-
lized, these systems were mainly trained on Classical Arabic
(CA) text and have been shown to result in high diacritic error
rates when applied to Modern Standard Arabic (MSA) speech
transcripts [4, 5].
Several prior works have been published to improve the
availability of speech resources for MSA. We group the ap-
proaches into two classes: (1) Recording scenarios : In this
approach, researchers employ voice artists and record them ei-
ther reading pre-defined texts in carefully set-up environments
1For instance, the following forms of the same root letters /skr/ have
three distinct pronunciations and meaning: Q¬∫¬É/suk:ar/ : ‚Äòsugar‚Äô, Q¬∫¬É
/sukr/ : ‚ÄòDrunkenness‚Äô, Q¬∫¬É/sak:ara/ : ‚Äòhe closed‚Äô.[6, 7] or engaging in spontaneous interview-like conversations
[8], and (2) Re-purposing : this approach involves extracting
speech from massive online sources like TV recordings, Pod-
casts, or YouTube [1, 9]. The first approach results in clean and
high-quality speech suitable for training speech synthesis mod-
els, but the process is expensive and requires intense manual
effort. As a result, most available resources of this kind are lim-
ited in size. The second approach exploits available resources,
eliminating the need for manual recording, and resulting in large
speech corpora, but introduces noise and other sources of vari-
ability that often lead to poor synthesis quality.
In this paper, we introduce ArVoice , a multi-speaker
dataset consisting of high-quality speech with fully diacritized
transcripts for MSA. ArVoice consists of: (1) professionally
recorded audio by two male and two female voice artists from
diacritized transcripts, (2) professionally recorded audio by one
male and one female voice artists from undiacritized transcripts,
(3) repurposed audio from existing single-speaker TTS datasets,
and (4) synthesized speech using commercial TTS systems. The
latter is used mainly for voice conversion and TTS data augmen-
tation, and is based on the same text used by human speakers.
Overall, the dataset contains 7human voices of about 10to-
tal hours of non-parallel speech, and 8√ó ‚àº 9hours of parallel
synthesized speech.
We demonstrate the use of the dataset for TTS and voice
conversion. The dataset can also be used for other speech-
related tasks, including ASR and speech-based diacritization,
making it a valuable resource for the broader research com-
munity. The data set accessible to researchers in two forms:
the ASC and the synthetic parts of the dataset are available
under the Creative Commons Attribution 4.0 International Li-
cense (CC BY 4.0) through a public link2. The professionally
recorded subsets are granted only to qualified researchers upon
signing a formal Data Usage Agreement.
2. Related Works
Large and diverse Arabic datasets have been developed for
speech tasks; yet, most of them are suitable for ASR, and few
are designed for TTS. The main open-source speech datasets
developed specifically for TTS are: ASC and ClArTTS. Both
are single-speaker datasets featuring a male speaker.
ASC (Arabic Speech Corpus) [10] is the most widely
known Arabic TTS corpus, and contains around 4hours of
recorded speech by a single male speaker. Prior to the record-
ing of the corpus, a lot of effort went into curating the text tran-
scripts with complete coverage of the characters and phonemes
in the Arabic vocabulary. This was achieved by including many
non-sense sentences to maximize the coverage of all phone-
2https://huggingface.co/datasets/MBZUAI/ArVoicearXiv:2505.20506v1  [cs.CL]  26 May 2025mic sequences. The realized transcripts for the speech corpus
were further modified to reflect the phones, leading to the re-
moval/addition of characters that are silenced/pronounced. In
total, the training set has 1.8K sentences, the test set has 100
sentences. ClArTTS (Classical Arabic Text-to-Speech) [6]
consists of classical Arabic (CA) speech sourced from an open-
source digital audiobook. The dataset was developed specifi-
cally for end-to-end TTS systems that typically need more data
to be effective, resulting in 12hours of speech. While it pro-
vides consistent and high-quality recordings, it lacks diversity
in speakers and Arabic variants, making it less effective for
multi-speaker MSA speech synthesis. Table 1 summarizes the
features of these dataset compared to ArVoice .
Table 1: Comparison of ArVoice with existing Arabic TTS data.
ASC ClArTTS ArV oice
MSA ‚úì - ‚úì
Number of Human V oices 1 1 7
Open-source ‚úì ‚úì ‚úì
Human Speech Duration (hrs) 4.1 12 10
Total Duration (hrs) 4.1 12 83.52
3. Dataset Construction
ArV oice comprises both human and synthetic voices. In this
section, we describe each part of ArV oice and provide justifica-
tion for design decisions where applicable.
3.1. ArVoice Part 1 (Human)
Text Sources: Modern Standard Arabic text was sourced from
theTashkeela Corpus [11], which consists of fully diacritized
Arabic text, mostly in Classical Arabic. Within this corpus, we
extracted texts from the Al Jazeera split, as manual inspection
confirmed it to be the only part of the corpus containing MSA.
Text Pre-processing: Since the Tashkeela corpus was origi-
nally scraped from the web, it contained numerous markdown
elements and extraneous punctuation marks, which we removed
as part of preprocessing. The corpus primarily consisted of
articles, which we subsequently tokenized into sentences
using the PyArabic library3. Sentences containing English
characters or digits (e.g., dates) were initially filtered out. To
ensure a realistic audio segment length, sentences exceeding
50characters were iteratively split at commas, ensuring that
each resulting segment contained at least 20characters. After
applying these preprocessing steps, we obtained a total of
‚âà1.8Kdiacritized MSA pseudo-sentences; we added 200
additional pseudo-sentences that included digits for more
vocabulary coverage.
Recording: Professional native Arabic voice talents were
employed to record the processed text, consisting of two male
and two female speakers, ensuring no overlap in recorded texts
among the speakers. The talents were fairly compensated and
informed about the intended use of their voice. We obtained
prior permission for their recordings to be used in Arabic
speech synthesis research4. The speech samples were recorded
3https://pypi.org/project/PyArabic/
4We specified that the requested recordings are intended for research
and development of Arabic text-to-speech systems, and that the datain quiet rooms using high-quality condenser microphones, with
minor post-hoc noise reduction in some instances.
Text Post-Processing: The transcripts were manually inspected
to match the actual recordings, as we discovered word repeti-
tions and omissions in some recorded samples. All transcripts
were corrected to reflect these repetitions or omissions. For
transcripts with digits, the Arabic numerals were first automat-
ically verbalized using the PyArabic library. We carried out
manual proofreading and additional annotation mainly to ver-
ify diacritic placement and to correct number conversions from
digits to words, as the written transcripts often diverged from
the pronunciation of the speakers. An example is ‚Äò2020‚Äô, which
was vocalized as	√°K
Q√•¬Ñ¬´	√°K
Q√•¬Ñ¬´: ‚Äòtwenty twenty‚Äô, in audio but pre-
dicted by PyArabic as	√°K
Q√•¬Ñ¬´√∞	√†A	¬Æ√ã@: ‚Äòtwo thousand and twenty‚Äô.
3.2. ArVoice Part 2 (Human)
Following the same collection and processing methodology
in section 3.1, we extracted 0.9Knon-diacritized MSA
transcripts from the Khaleej Corpus [12]. The Khaleej Corpus
contains news articles on various domains written in MSA, but
the text contains no diacritics.
Recording: The Khaleej text was divided into two parts,
each recorded by a different voice talent‚Äîone male and one
female‚Äîdistinct from the voices in Part 1. Since the text did
not contain diacritics, the talents were instructed to read and
record it according to what seemed most natural to them. The
same prior agreements and recording standards were followed
as in Part 1.
3.3. ArVoice Part 3 (Human)
In this part, we utilize the Arabic Speech Corpus (ASC) dataset,
which is distributed under the Creative Commons License CC
BY 4.05. The text transcripts in ASC were originally modified
to match word pronunciation; for instance, nunation , which is
supposed to be spelled as a diacritic, is instead spelled out as
the letter	√†/n/. Another modification is the omission of Alef
/a/ before the /l/ in the definite article /al/. These modifications,
which were meant to make the dataset better normalized for
concatenative TTS systems, are inconvenient for end-to-end
systems as they are inconsistent with the standard spelling
used in the other parts of the dataset, and require additional
pre-processing steps. We used GPT-4 (using prompts proposed
in [13]) to first automatically fix these modifications back to
standard spelling, followed by manual correction. Half of the
train set of the original ASC corpus [10], which are duplicates
of non-sense sentences (included for phone coverage), were
removed completely to obtain 0.9K sentences. The full test and
train set were manually inspected and corrected.
Validation: We validated the corrections made above by evalu-
ating ASR performance on the test set using three open-source,
state-of-the-art Arabic ASR models [14, 15, 16]. Table 2 shows
WER calculated using the original ASC transcripts vs. our (cor-
rected) version; we observe a ‚âà40% drop in Word Error Rate
may be shared with other researchers for research use only; the license
excludes broadcast and resale rights.
5https://en.arabicspeechcorpus.com/ - last accessed
on Feb 19, 2025.(WER), highlighting the importance of this step for standardiz-
ing the transcripts in our set.
Table 2: Word error rate (%) and character error rate (%) of
zero-shot ASR using SOTA models on ArVoice Part 3 and the
equivalent subset of ASC. All models are evaluated without di-
acritics as these models do not generate diacritics at inference.
Model WER ‚ÜìCER‚Üì
ASC [10]
ArTST [15] 45.86 9.65
Nvidia CTC-large[16] 45.70 8.59
Whisper-large[14] 45.22 9.94
ArVoice Part 3 (Ours)
ArTST [15] 5.57 2.04
Nvidia CTC-large[16] 5.10 0.96
Whisper-large[14] 7.48 2.18
3.4. ArVoice Part 4 (Synthetic)
The final processed transcripts from Parts 1 and 3 were used for
synthetic speech generation to create a parallel corpus. We ex-
cluded Part 2 as it contains no diacritics. Google‚Äôs commercial
TTS models6were used to generate varying quality of synthetic
MSA speech from all transcripts processed. The audio was gen-
erated using both the Standard and premium Wavenet models
for two male and two female voices. We recommend the use of
this datasets for data augmentation and voice conversion only
as audio quality is not guaranteed.
3.5. Dataset Statistics
Table 3 gives a summary of recording information of individual
human speakers in ArVoice . This part of ArVoice consists
in total of about 9hours of training and 0.86hours of test data.
Although ASC has an average number of words, the speaker has
the highest duration as a result of the slower speaking rate [10].
Table 3: Data statistics and demographic details per human
speaker in ArVoice
Part ID Gender Origin #words (K) Duration (hrs)
1 m aa m Egypt 14.84 1.17
1 f ab f Jordan 17.32 1.45
1 m ac m Egypt 21.07 1.58
1 f ad f Morocco 15.37 1.23
2 m ae m Palestine 6.02 0.93
2 f af f Egypt 6.07 0.95
3 m asc m Syria 13.21 2.69
4. Baselines
In this section, we describe two speech synthesis tasks:
multi-speaker text-to-speech (TTS) and voice conversion (VC),
trained using ArVoice .
4.1. TTS Synthesis Experiments
Models: Text-to-Speech synthesis experiments were carried
out using three open-source models: ArTST-tts [15], VITS [17]
6https://cloud.google.com/text-to-speechandFish-Speech [18]. ArTST-tts is an Arabic transformer-based
model pre-trained on large amounts of unlabeled speech
and subsequently fine-tuned on Classical Arabic data from
ClArTTS [6]. ArTST-tts allows for multispeaker training using
speaker embeddings, x-vectors , extracted using the Speech
Brain toolkit7. The model is trained to produce mel spectro-
grams, and a pre-trained HiFi-GAN vocoder is used to generate
the waveform from the generated mel spectrograms. VITS
(Variational Inference Text-to-Speech) integrates variational
autoencoders, normalizing flows, and adversarial training. It
is an end-to-end architecture that uses a monotonic alignment
search mechanism. Fish-Speech employs a serial fast-slow
Dual Autoregressive (Dual-AR) architecture to enhance
the stability of Grouped Finite Scalar Vector Quantization
(GFSQ) in sequence generation tasks. Additionally, the system
leverages LLMs for linguistic feature extraction and a new
neural vocoder architecture, FF-GAN, achieving superior
compression ratios.
Experimental Settings: VITS and Fish-Speech originally did
not support Arabic TTS, but ArTST-tts was already trained
on non-diacritized Arabic text. For all models, we fine-tuned
a multi-speaker checkpoint using Parts 1,2,3 of ArV oice (the
human voices with diacritized text). For fine-tuning, we used
the default hyperparameter settings as predefined for each
model and their pre-trained checkpoints.8For VITS, slight
modifications were made to its text cleaner functions to handle
Arabic text.
Evaluation Methodology: We first compare the performance
of the three TTS models in two variants: with/without diacritics
in text. We conducted pairwise subjective preference tests,
where we randomly sampled 25transcripts from the test set and
presented synthesized speech from the two model variants. We
used the Prolific9crowd-sourcing platform to employ native
Arabic speakers for the preference test, who were presented
with two synthesized samples of the same underlying text from
systems A and B, and asked to select their preference: A, B,
or No preference. For each preference test, a minimum of
10different evaluators rated each of the 25 audio pairs. We
aggregated the ratings per sample and calculated the mean and
standard error for each category: A, B, and No Preference, then
estimated the 95% confidence interval using the t-distribution.
Evaluation Results: Figure 1 presents the results for the three
TTS models, where pairwise preference tests were conducted
to compare the versions trained with and without diacritics for
each model. The only model with statistically significant pref-
erence is VITS, where the version with diacritics was clearly
preferred over the one without. For ArTST and Fish Speech, al-
though a slight preference for the version without diacritics was
observed, the difference was not statistically significant.
Following these results, we further evaluated the intelligi-
bility of the synthesized speech using an ASR model as an au-
tomatic metric for sanity check. The results are presented in
Table 4. The results indicate that speech synthesized by Fish-
Speech is of extremely low quality, as WER is above 100%.
Subjective listening tests also confirmed the low intelligibility
of this model. We exclude this model from further analysis.
7huggingface.co/speechbrain/spkrec-xvect-voxceleb
8https://github.com/jaywalnut310/vits
https://github.com/mbzuainlp/ArTST
https://github.com/fishaudio/fishspeech
9https://app.prolific.com/Figure 1: Mean average preference with/without diacritics for
each TTS model, with 95% confidence intervals.
As VITS with diacritics performed on a par with ArTST
in the intelligibility metric, but ArTST without diacritics was
preferred in the previous subjective test, we performed another
subjective test comparing VITS with diacritics against ArTST
without diacritics. The results show a strong and statistically
significant preference for VITS (See Figure 2 (a)), which indi-
cates that VITS is the best among the tested three models on
this dataset. Finally, we compared the performance of VITS in
multi-speaker vs. single-speaker settings (Figure 2 (b)) using
the data for m asc speaker with 2.69hours of speech for train-
ing the single-speaker model. Both models are evaluated on the
same speaker. A strong preference for the multi-speaker model
is observed, illustrating the advantage of more data in training
TTS systems even for improving the same target voice.
Table 4: Intelligibility evaluation using WER(%) of ASR predic-
tions. We use the ASR model from [15] to obtain WER
ArTST-tts Fish-Speech VITS
with diacritics
‚úì 21.81 162.04 20.67
‚úó 21.24 115.58 35.69
Effect of Synthetic Data Augmentation: We trained VITS on
the full ArV oice corpus (synthetic and human speech) for half
the number of epochs as in section 4.1; the intelligibility of
the synthesized speech improved with an 8.2% drop in absolute
WER compared to the version trained on human speech alone.
Subjective preference tests also indicated a preference for mod-
els with synthetic data augmentation (about 44% of raters pre-
ferred the version with data augmentation, compared with 30%
and 26% for the baseline and No Preference, respectively).
4.2. Voice Conversion (VC) Models
We experimented with AAS-VC [19], a parallel VC model
based on a non-autoregressive sequence-to-sequence architec-
ture. To train this model, we utilized 4 speakers from Part 4 of
ArV oice (see Section 3.4), which consists of synthetic parallel
speech. We trained 4 parallel VC models and averaged their
performance. We also fine-tuned a ParallelWaveGAN vocoder
[20] on the same dataset to improve waveform synthesis qual-
ity. We also fine-tuned KNN-VC [21], a non-parallel VC model
that converts source into target speech by replacing each frame
(a)VITS (w. diac) vs.
ArTST (w.o. diac)
(b) Mutli-Speaker vs.
Single-Speaker VITS (w.
diac)
Figure 2: Mean average preference with 95% confidence inter-
vals. We compare (a) the best version of VITS against. the best
version of ArTST, and (b) Multi-speaker vs. single-speaker TTS
with the VITS model.
of the source representation with its nearest neighbor from the
target. To train this model, we used human voices in Arvoice
Parts 1 (section 3.1) and 3 (section 3.3). All the fine-tuning pro-
cesses were conducted using the default parameters specified
in the original studies10. To evaluate the quality of the con-
verted speech, we used the pre-trained ECAPA-TDNN speaker
verification system from SpeechBrain11to test whether the con-
verted speech is indistinguishable from real speech. We report
the average Speaker Similarity (SS) scores between the target
and generated audios; we also report False Acceptance Rate
(FAR) at a 0.5 similarity threshold. FAR is correlated with SS.
Results: The KNN-VC model achieved a FAR of 0.81 with
an average SS score of 0.69, indicating fair similarity between
the converted voice and the target speaker. The AAS-VC model
achieved a higher FAR of 0.95 with an average SS score of 0.72,
indicating relatively high similarity12.
5. Conclusion
We described ArV oice, a multi-speaker dataset for speech syn-
thesis in Modern Standard Arabic. The dataset consists of 11
voices in total, 7 of which are human voices, and 4 are syn-
thetic with parallel text. We illustrated the usability of ArV oice
in multi-speaker TTS, demonstrating the advantage of using di-
acritized transcripts. In addition, we demonstrated the benefit
of the synthetic speech for TTS data augmentation, as well as
parallel voice conversion. ArV oice is the largest open-source
Arabic dataset curated specifically for speech synthesis, both in
terms of speech duration (83.5 hrs) and number of voices (11).
6. Acknowledgments
We thank the five hired voice artists and our co-author, Samar
Magdy, for contributing their voice to research. This work was
partially funded by a Google research award (11/2023).
10https://github.com/bshall/knn-vc
https://github.com/rufaelfekadu/seq2seq-vc
11huggingface.co/speechbrain/spkrec-ecapa-voxceleb
12The ground truth SS score was 0.81.7. References
[1] A. Ali, S. V ogel, and S. Renals, ‚ÄúSpeech recognition challenge in
the wild: Arabic mgb-3,‚Äù in 2017 IEEE Automatic Speech Recog-
nition and Understanding Workshop (ASRU) . IEEE, 2017, pp.
316‚Äì322.
[2] H. Mubarak, A. Hussein, S. A. Chowdhury, and A. Ali,
‚ÄúQASR: QCRI aljazeera speech resource a large scale annotated
Arabic speech corpus,‚Äù in Proceedings of the 59th Annual
Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language
Processing (Volume 1: Long Papers) , C. Zong, F. Xia, W. Li,
and R. Navigli, Eds. Online: Association for Computational
Linguistics, Aug. 2021, pp. 2274‚Äì2285. [Online]. Available:
https://aclanthology.org/2021.acl-long.177/
[3] M. Al-Fetyani, M. Al-Barham, G. Abandah, A. Alsharkawi, and
M. Dawas, ‚ÄúMasc: Massive arabic speech corpus,‚Äù in 2022 IEEE
Spoken Language Technology Workshop (SLT) . IEEE, 2023, pp.
1006‚Äì1013.
[4] H. Aldarmaki and A. Ghannam, ‚ÄúDiacritic recognition perfor-
mance in arabic asr,‚Äù in Interspeech 2023 , 2023, pp. 361‚Äì365.
[5] S. Shatnawi, S. Alqahtani, and H. Aldarmaki, ‚ÄúAutomatic restora-
tion of diacritics for speech data sets,‚Äù in Proceedings of the 2024
Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies (Vol-
ume 1: Long Papers) , 2024, pp. 4166‚Äì4176.
[6] A. Kulkarni, A. Kulkarni, S. A. M. Shatnawi, and H. Aldarmaki,
‚ÄúClartts: An open-source classical arabic text-to-speech corpus,‚Äù
inInterspeech 2023 , 2023, pp. 5511‚Äì5515.
[7] A. W. Black, ‚ÄúCmu wilderness multilingual speech dataset,‚Äù in
ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP) . IEEE, 2019, pp. 5971‚Äì
5975.
[8] I. Hamed, N. T. Vu, and S. Abdennadher, ‚ÄúArzEn: A
speech corpus for code-switched Egyptian Arabic-English,‚Äù in
Proceedings of the Twelfth Language Resources and Evaluation
Conference , N. Calzolari, F. B ¬¥echet, P. Blache, K. Choukri,
C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard,
J. Mariani, H. Mazo, A. Moreno, J. Odijk, and S. Piperidis,
Eds. Marseille, France: European Language Resources
Association, May 2020, pp. 4237‚Äì4246. [Online]. Available:
https://aclanthology.org/2020.lrec-1.523
[9] Y . Lin, X. Qin, G. Zhao, M. Cheng, N. Jiang, H. Wu,
and M. Li, ‚ÄúV oxblink: A large scale speaker verification
dataset on camera,‚Äù ICASSP 2024 - 2024 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , pp. 10 271‚Äì10 275, 2023. [Online]. Available: https:
//api.semanticscholar.org/CorpusID:260927071
[10] N. Halabi, ‚ÄúModern standard arabic phonetics for speech
synthesis,‚Äù Ph.D. dissertation, University of Southampton, July
2016. [Online]. Available: https://eprints.soton.ac.uk/409695/
[11] T. Zerrouki and A. Balla, ‚ÄúTashkeela: Novel corpus of arabic vo-
calized texts, data for auto-diacritization systems,‚Äù Data in Brief ,
vol. 11, 02 2017.
[12] M. Abbas and K. Sma ¬®ƒ±li, ‚ÄúComparison of topic identification
methods for arabic language,‚Äù in International Conference on Re-
cent Advances in Natural Language Processing-RANLP 2005 , no.
14-17, 2005.
[13] H. O. Toyin, H. Li, and H. Aldarmaki, ‚ÄúSTTATTS: Unified
speech-to-text and text-to-speech model,‚Äù in Findings of the
Association for Computational Linguistics: EMNLP 2024 , Y . Al-
Onaizan, M. Bansal, and Y .-N. Chen, Eds. Miami, Florida,
USA: Association for Computational Linguistics, Nov. 2024, pp.
6853‚Äì6863. [Online]. Available: https://aclanthology.org/2024.
findings-emnlp.401/[14] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey,
and I. Sutskever, ‚ÄúRobust speech recognition via large-scale
weak supervision,‚Äù in International Conference on Machine
Learning , 2022. [Online]. Available: https://api.semanticscholar.
org/CorpusID:252923993
[15] H. O. Toyin, A. Djanibekov, A. Kulkarni, and H. Aldarmaki,
‚ÄúArTST: Arabic text and speech transformer,‚Äù in Proceedings
of ArabicNLP 2023 . Singapore (Hybrid): Association for
Computational Linguistics, Dec. 2023, pp. 41‚Äì51. [Online].
Available: https://aclanthology.org/2023.arabicnlp-1.5/
[16] D. Rekesh, N. R. Koluguri, S. Kriman, S. Majumdar, V . Noroozi,
H. Huang, O. Hrinchuk, K. Puvvada, A. Kumar, J. Balam et al. ,
‚ÄúFast conformer with linearly scalable attention for efficient
speech recognition,‚Äù in 2023 IEEE Automatic Speech Recognition
and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1‚Äì8.
[17] J. Kim, J. Kong, and J. Son, ‚ÄúConditional variational autoencoder
with adversarial learning for end-to-end text-to-speech,‚Äù in Inter-
national Conference on Machine Learning . PMLR, 2021, pp.
5530‚Äì5540.
[18] S. Liao, Y . Wang, T. Li, Y . Cheng, R. Zhang, R. Zhou,
and Y . Xing, ‚ÄúFish-Speech: Leveraging large language
models for advanced multilingual Text-to-Speech synthesis,‚Äù
arXiv (Cornell University) , 11 2024. [Online]. Available:
http://arxiv.org/abs/2411.01156
[19] W.-C. Huang, K. Kobayashi, and T. Toda, ‚ÄúAas-vc: On the
generalization ability of automatic alignment search based non-
autoregressive sequence-to-sequence voice conversion,‚Äù 2023.
[Online]. Available: https://arxiv.org/abs/2309.07598
[20] R. Yamamoto, E. Song, and J.-M. Kim, ‚ÄúParallel wavegan: A fast
waveform generation model based on generative adversarial net-
works with multi-resolution spectrogram,‚Äù in ICASSP 2020-2020
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP) . IEEE, 2020, pp. 6199‚Äì6203.
[21] M. Baas, B. van Niekerk, and H. Kamper, ‚ÄúV oice conversion
with just nearest neighbors,‚Äù in Proc. Interspeech 2023 , 2023, pp.
2053‚Äì2057.