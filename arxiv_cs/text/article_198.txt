arXiv:2505.20295v1  [cs.CL]  26 May 2025Self-reflective Uncertainties: Do LLMs Know Their
Internal Answer Distribution?
Michael Kirchhof
AppleLuca Füger
Independent ResearcherAdam Goli ´nski
Apple
Eeshan Gunesh Dhekane
AppleArno Blaas
AppleSinead Williamson
Apple
Abstract
To reveal when a large language model (LLM) is uncertain about a response,
uncertainty quantification commonly produces percentage numbers along with the
output. But is this all we can do? We argue that in the output space of LLMs, the
space of strings, exist strings expressive enough to summarize the distribution over
output strings the LLM deems possible. We lay a foundation for this new avenue
of uncertainty explication and present SelfReflect, a theoretically-motivated metric
to assess how faithfully a string summarizes an LLM’s internal answer distribution.
We show that SelfReflect is able to discriminate even subtle differences of candidate
summary strings and that it aligns with human judgement, outperforming alternative
metrics such as LLM judges and embedding comparisons. With SelfReflect, we
investigate a number of self-summarization methods and find that even state-of-
the-art reasoning models struggle to explicate their internal uncertainty. But we
find that faithful summarizations can be generated by sampling and summarizing.
Our metric enables future works towards this universal form of LLM uncertainties.
1 Introduction
When large language models (LLMs) are uncertain about a response, either because the query is
ambiguous or because they are factually unsure, they should indicate it. Consider the example in
Fig. 1. The LLM’s internal distribution comprises a variety of answers, but this variability is ignored
if we just output the greedy response. While existing uncertainty quantification approaches augment
the greedy response (or any other single sample from the distribution) with a numerical measure of
uncertainty [Aichberger et al., 2024, Fadeeva et al., 2023, Fomicheva et al., 2020, Malinin and Gales,
2020] or verbalize the confidence in the response [Lin et al., 2022, Yona et al., 2024], this offers
limited insight into the model’s beliefs: we do not see the full range of cities the LLM believes are
plausible, nor the variety of supporting information (e.g., that Paris hosts the French government).
We believe we can do better than this. As motivation, consider the following comment on Gödel’s
proof on the incompleteness of number theory.
Gödel had the insight that a statement of number theory could be about a statement of number
theory (possibly even itself), if only numbers could somehow stand for statements.
Hofstadter [1979]
Gödel’s key idea was that statements of number theory are expressive of much more than just integers.
The same holds for strings: An answer string sgenerated by an LLM is expressive enough to describe
adistribution over all answer strings the LLM could generate. We can therefore use a single string s
Preprint. Under review.LLM's internal  distribution pθ(A|q)User query : What is the main city of France?q
Numerical uncertainty: ('The capital of France is Paris.', 75%)Verbalized uncertainty: 'I'm very sure that the capital of France is Paris.'Self-reflective uncertainty: 'I'm 75% sure that it's Paris, its capital and commercial hub, but it could also be Toulouse or Marseilles.'Paris.The capital of France is Paris.It's Paris, which hosts its government  and many commercial hubs.Marseilles is one of France's most popular and vibrant cities.It's Paris.It's Toulouse.The capital of France is Paris.Its main city is Paris.Normal (greedy) answer: 'The capital of France is Paris.'Figure 1: LLMs have internal answer distributions about user queries. Rather than just sampling an
output, possibly combined with a percentage, LLMs should generate a string that is self-reflective of
their internal distribution, summarizing all possibilities and which they find the most likely.
to summarize the LLM’s distribution pθ(A|q)over responses Ato a query q. We see this in the
“self-reflective uncertainty” example of Fig. 1: A single string conveys the relative degrees of belief
in different cities, and includes additional details provided in the samples.
Our paper lays a foundation for this new avenue for uncertainty quantification. We define a metric
that evaluates whether a given self-summarization technique faithfully represents an LLM’s internal
distributions over responses to queries. The underlying challenge here is to define a distance between
a string and a distribution over strings that quantifies whether they both “carry the same information” ,
both in terms of facts and in terms of their relative likelihoods. We formulate the SelfReflect metric
based on an information-theoretic perspective on predictive sufficiency in the string space. We verify
that it discriminates good from bad (and almost-good) summaries of answer distributions on both
free-form and closed-form question datasets, and that it agrees with human judgements, in both cases
outperforming reasonable baselines such as LM judges and embedding distances.
Having defined the SelfReflect metric, we open up the possibility of evaluating whether LLMs
can be made aware of their own internal answer distributions—that is, whether they can generate
strings that explain their uncertainties. We find that such self-reflective outputs pose a hard challenge
even to modern reasoning models. It is, however, possible to give insights into the internal answer
distributions by explicitly sampling and then summarizing them. These findings mark but the start of
enabling LLMs to output honest descriptions of their internal uncertainties. We expect that future
advances along our SelfReflect metric can unlock more faithful and trustworthy LLM interactions.
2 Related Work
2.1 Uncertainty in LLMs
Most work on uncertainty in LLMs associates a single numerical expression of uncertainty to a
specific string like the greedily decoded response. Since LLMs are, in essence, probabilistic next-
token classifiers, one can attempt to read their uncertainty off their token logits [Aichberger et al.,
2024, Fadeeva et al., 2023, Fomicheva et al., 2020, Malinin and Gales, 2020]. These methods can
be extended to longer LLM answers for example by searching for fact tokens and extracting their
logits [Fadeeva et al., 2024] and made more human-readable by transforming the numeric uncertainty
into a string like “I am very sure that...” [Lin et al., 2022, Yona et al., 2024]. Still, these approaches
quantify the uncertainty of only a single element of the LLM’s internal distribution.
So how can the full uncertainty of the LLM’s distribution be captured? Farquhar et al. [2024] cluster
answers sampled from the LLM’s internal distribution semantically and calculate an entropy over the
clusters. This considers the full distribution over strings, but it still reduces the uncertainty to a single
number and presents this number alongside a single string from the distribution. Moving towards
richer uncertainty explications, Xu et al. [2024] generate multiple samples from an LLM, use GPT-4
to summarize the distribution of samples and train the LLM to output such summaries. Similarly,
2Yang et al. [2024b] train an LLM to output strings that delineate which facts it is uncertain about. This
is arguably one of the richest ways to express an LLM’s uncertainty. But both papers, focusing on the
generation of summaries rather than on evaluation, use simple LM judges to rate the summary strings.
As we show in Section 4.1, LM judges can not discern how faithfully a string reflects a distribution
over strings beyond relatively simple good vs bad cases. Our SelfReflect gives a better-founded and
more precise metric to compare whether a summary string contains the same information as the
LLM’s internal distribution, enabling to further develop this new avenue of LLM uncertainties.
2.2 Summarization
Testing whether a summary of a long document is good has a long history in natural language
processing (NLP) [Zhang et al., 2024]. Summaries are traditionally rated in terms of faithfulness to
the long document, relevance of the chosen information, and fluency and coherence of their sentences
[Särkkä and Solin, 2019], as rated by humans or recently by LM judges [Jain et al., 2023]. In modern
LLM-generated summaries, fluency and coherence are usually granted, so that the focus lays on the
faithfulness and relevance of the summary, in other words, whether it contains the same information
as the long document. This fundamental question dates back to the Cloze test [Taylor, 1953]. This
test, originally designed for human language learners, masks out words from the long document and
asks to fill them in. Summarization metrics like BLANC [Vasilyev et al., 2020] run this test twice,
once when conditioning an NLP model on the summary and once without. If the summary contains
correct information, the NLP model should fill in better words. The masked-out performance can be
quantified either as an accuracy gain [Vasilyev et al., 2020] or, more softly, as a pseudo log-likelihood
[Shin et al., 2019, Wang and Cho, 2019, Salazar et al., 2020, Kauf and Ivanova, 2023].
Since our SelfReflect metric also quantifies the quality of a summary, we base it off Cloze-like
masked-out tasks. But there is a twist: The summary string sdoes not summarize another string
but a distribution over strings pθ(A|q). This means we must go beyond comparing sto a specific
string a∼pθ(A|q), to quantifying how faithfully srepresents the density over the string space
thatpθ(A|q)defines, i.e., to all possible answers and how likely they are. To this end, we re-think
masked-out tasks from the lens of sufficient statistics in the following section.
3 Distances between summary strings and distributions over strings
Our main challenge is to find a distance that quantifies the extent to which a summary string carries
the same information as an LLM’s internal answer distribution. We build a theoretical foundation for
sufficient statistics in string spaces in Section 3.1 and develop the SelfReflect metric in Section 3.2.
3.1 Summaries as predictive sufficient statistics
BΘQ A(1:N) S
Figure 2: Graphical model for the suf-
ficiency that SelfReflect quantifies.Suppose we have an LLM (which we denote LLM θ),
prompted with a random query Q. We posit that this puts
us in a state ΘQ, which allows us to sample random re-
sponses B. We are interested in summarizing this distribu-
tion over responses. Let A(1:N):= (A(1), . . . , A(N))∈ XN
be a set of responses sampled from LLM θ, where Xis the
space of finite strings.1Consider a summarization function
ψ:XN−→ X that, given A(1:N), generates a summary
S:=ψ 
A(1:N)
. What criteria should ψsatisfy if its sum-
maries are to exactly capture LLM θ’s distribution over B?
Continuing the example from Fig. 1, we can see that an ideal summary of A(1:N)should neither omit
important details from the answer distribution nor add extra details. For example, a summary stating
“The capital of France is Paris” would ignore the LLM’s belief in Marseilles or Toulouse, whereas a
summary stating “The capital of France is Paris but for a period in history, it was Orléans” would be
adding unfaithful details. The same holds for the relative likelihood of answers: the ideal summary
should state that the capital of France is most likely Paris, and not Toulouse or Marseilles, because
1These Nsamples may be generated independently and identically to B, but we do not require this; for
example, the distribution over subsequent answers could depend on the previous answers.
3this answer has a higher probability mass in the LLM’s internal distribution. This indicates that an
ideal summary should capture exactly the same information about the answer distribution as that
contained in the sampled answers . We can formalize this in terms of mutual information,
Definition 3.1 (Ideal summary) .An ideal summary Sof answers A(1:N)of an LLM satisfies
I
A(1:N);B	
=I {S;B} (1)
Here,I {Y;Z}denotes the mutual information between YandZ. Intuitively, for any subsequent
answer Bfrom the LLM, the information about Bcontained in A(1:N)is exactly captured by S.
This definition is closely tied to the notion of predictive sufficiency [Lauritzen, 1974], whereby
a statistic T 
X(1:N)
of observations X(1:N)is called sufficient if it satisfies p 
X|X(1:N)
=
p 
X|T 
X(1:N)
for any subsequent observation X. In fact, we can reframe Definition 3.1 as
follows:
Proposition 3.1 (Connection to predictive sufficiency) .For an ideal summary Sof answers A(1:N),
I
A(1:N);B	
=I {S;B} ⇐⇒ p 
B|A(1:N)
=p(B|S) (2)
Intuitively, the ideal summary Sis a predictive-sufficient statistic of the answers A(1:N)forB.
From Definition 3.1 and Proposition 3.1, we see that a measure of how much p 
B|A(1:N)
diverges
fromp(B|S)would be a good metric for measuring how faithfully Sreflects the sampled answers
A(1:N). Towards this, we formulate a Cloze-task based on masked-token prediction that constitutes
a simple yet equivalent characterization of the desired predictive sufficiency. Let Bidenote the ith
word of Band let B−i:= (Bj)j̸=idenote all other words of the answer. We propose predicting the
missing word Bifrom the rest of the words B−iwith the extra context of either the sampled answers
A(1:N)or their summary S. Identical behavior in this masked-token prediction task turns out to be
equivalent to predictive sufficiency (and hence, Definition 3.1):
Proposition 3.2 (Informal; towards the SelfReflect metric) .For answers A(1:N)and their summary
S, under mild conditions on all involved distributions and support of B, we have:
p
B|A(1:N)
=p(B|S)⇐⇒ for all masking indices i, p
Bi|A(1:N), B−i
=p(Bi|S, B−i)(3)
Full details and proofs of Propositions 3.1 and 3.2 are given in Appendix A. Proposition 3.2 motivates
us to measure the divergence between the distributions p(Bi|S, B−i)andp 
Bi|A(1:N), B−i
as
a tractable metric for the quality of a summary, forming the basis of the SelfReflect metric.
3.2 The SelfReflect metric
Proposition 3.2 tells us we can use a sequence of masked-out tasks to quantify whether a summary
scontains the same information about LLM θ’s distribution pθ(B|q)as a sequence of Nsamples
from that distribution. We approximate this task using a second judge LLM, LLM J, to estimate the
conditional distribution over masked-out words. Intuitively, irrespective of whether we show the
sampled answers or their ideal summary, a judge LLM should predict the same masked tokens.
Concretely, we sample a new response Bat temperature 1 from LLM θ, mask out one word Bi, and
askLLM Jto predict Bigiven the remainder of the answer B−i, the query q, and either the summary
sor a sequence a(1:N)ofNsamples from pθ 
A(1:N)|q
, see Fig. 3. This yields two distributions
pJ 
Bi|Q=q, A(1:N)=a(1:N), B−i=b−i
andpJ(Bi|Q=q, S=s, B−i=b−i), over the vo-
cabulary space of LLM Jwhich we compare using the 1-Wasserstein distance.2We marginalize over
Band index ito satisfy the requirements of Proposition 3.2. To convert this into a general-purpose
metric for a summarization strategy ψ, we take the expectation over queries and sampled responses:
mSelfReflect (ψ) = E
Q,A(1:N),B,ih
W1
pJ(Bi|Q, ψ(Q), B−i), pJ
Bi|Q, A(1:N), B−ii
(4)
2IfLLM Jis a black-box model that only returns the top-predicted word, i.e., pJare one-hot vectors, our
1-Wasserstein comparison simplifies into an accuracy that tests whether the two predicted words are equal.
4<|im_start|>user Who was the first Australian prime minister? <|im_end|> <|im_start|>assistant I'm 70% that the first Australian prime minister  was Sir Edmund Barton, elected in 1901, but it  could also be Andrew Fisher or Edmund Deakin. <|im_end|> <|im_start|>user We now show a text with a missing word "_". Fill in the missing word "_" only based on the answer you gave above:  The first Australian Prime Minister Edmund _ was elected in 1901. Please provide only the missing word "_", not the whole sentence. <|im_end|> <|im_start|>assistant <|im_start|>user Who was the first Australian prime minister? Sample 50 answers to this question. <|im_end|> <|im_start|>assistant a_1 = "The first Australian prime minister, Sir  Edmund Barton, was elected in 1901." [...] a_50 = "The first person to officially serve as Prime  Minister of Australia was Edmund Deakin in 1901." <|im_end|> <|im_start|>user We now show a text with a missing word "_". Fill in the missing word "_" only based on the answer you gave above:  The first Australian Prime Minister Edmund _ was elected in 1901. Please provide only the missing word "_", not the whole sentence. <|im_end|> <|im_start|>assistant candidate summary si.i.d. samples from  pasted into contextpθ(A|q)
Predicted token vector: pJ(Bi|q,s,b−i)=(0.70,0.28,0.01,...)Predicted token vector: pJ(Bi|q,a(1:N),b−i)=(0.78,0.18,0.01,...)mSelfReflect(s)=𝒲1(pJ(Ai|q,s,a−i),pJ(Ai|q,a(1:N),a−i))pJ(Bi|q,s,b−i)pJ(Bi|q,a(1:N),b−i)question qtask with masked-out answer b−i"Barton""Deakin""Fisher""Barton""Deakin""Fisher"mSelfReflect (s) =W1
pJ(Bi|q, s, b −i), pJ
Bi|q, a(1:N), b−i
Figure 3: To test whether a summary string scontains the same information as a set of samples a(1:N),
SelfReflect prompts an LLM twice. First, it provides the summary as context; next, it provides the
concatenated samples. SelfReflect then compares the resulting distributions via a masked-out task.
Here, ψis any method that makes LLM θoutput a summary of its internal distribution in response to a
query.3We estimate Eq. (4) via Monte Carlo sampling with 1000 queries per dataset, a set of N= 50
samples A(1:N)per query, and masked-out tasks over M= 50 samples of Bfor all possible i. In
Appendix B we show that these values converge to stable estimates. Literature notes that Cloze-like
evaluations are often limited by synonyms [Kauf and Ivanova, 2023], so we post-hoc flatten pJwith
τ= 5to put likelihood on broader synonyms. We quantitatively find this improves discriminability.
We explore different choices of LLM Jand find that SelfReflect is robust to the exact choice, see the
quantitative results in Appendix C and the qualitative example in Appendix D. We find that Qwen
2.5 Instruct [Yang et al., 2024a] captures both textual details and the implicit relative certainties in
summaries or concatenated samples in its context even when they are subtle. The 7B model provides
results almost on par with the 72B model, so we choose it for efficiency. For further efficiency, we
exclude stopwords from the masked tasks and use vLLM [Kwon et al., 2023] to parallelize and prefix-
cache the computations. Overall, it takes 67 minutes to calculate the SelfReflect score over a dataset
of1000 queries and N=M= 50 answers per query on a node of 8 NVIDIA A100 GPUs.
4 Do SelfReflect scores work in practice?
We now verify that the SelfReflect metric works in practice, based on three pillars: An interventional
study with known good and bad summaries on free-form questions (Section 4.1), a simplified study
with closed-form multiple-choice QA answer distributions (Section 4.2), and a comparison to which
summaries humans deem faithful (Section 4.3). In all studies, we compare our SelfReflect metric to
several other baseline metrics, both from related fields and from ablating parts of SelfReflect.
Baselines. While developing SelfReflect, we experimented with approaches from various roots for
comparing a summary string sto a set of strings a(1:N). First, Summarization treats a(1:N)as a single
document and assesses the summary sin terms of consistency, fluency, relevance, and coherence [Jain
et al., 2023]. Second, LM Judge prompts LLM Jto rate how well smatches a(1:N), following the
chain-of-thoughts prompt of Xu et al. [2024]. Third, we turn to the neighboring field of calibration.
Wang and Holmes [2024] argue that calibration can be seen as a distance to a centroid. We implement
this in Embedding by comparing embeddings of stoa(1:N). Finally, for Opt. transport [Peyré et al.,
2019], we let LLM Jsplitsinto a “distribution” over atomic statements and likelihoods, compute a
pairwise entailment matrix and return the Earth Mover’s distance to pθ(A|q).
Ablations. We also ablate key characteristics of SelfReflect. SR-PMI forgoes the masked-out task
and directly compares the log likelihoods of full sample answers given either the summary or the
samples in terms of pairwise mutual information; this can be seen as analogous to Proposition 3.1.
SR-sampling-free uses the masked-out task, but compares the masked-out logits given the summary to
predictions of LLM θgiven qitself, without sampling answers. SR-P(True) changes from a generative
3While the link to sufficiency only holds if ψdepends only on a(1:N), the metric is well-defined whether the
summary generation involves taking samples in-between or generating a summary answer for qin other ways.
5Table 1: How well does SelfReflect, and other metrics, discriminate between good and bad summaries
of answer distributions. For each column, the second summary lacks textual details or misrepresents
relative probabilities. Mean ±95% confidence interval.
MetricGood summaries vs
bad summariesGood vs
almost-goodDetailed vs
truncatedVerbalized uncertainty vs
only majority answerVerbalized vs
or-concatenatedPercentage vs
or-concatenated
Summarization 97.40% ±0.99% 38.70% ±3.02% 53.55% ±7.85% 11.57% ±5.70% 57.02% ±8.82% 65.29% ±8.48%
LM Judge 99.00% ±0.62% 47.90% ±3.10% 65.16% ±7.50% 24.79% ±7.69% 33.88% ±8.43% 38.02% ±8.65%
Opt. Transport 91.80% ±1.70% 53.90% ±3.09% 45.16% ±7.83% 48.76% ±8.91% 47.11% ±8.89% 70.25% ±8.15%
Embedding 94.40% ±1.43% 29.90% ±2.84% 47.10% ±7.86% 5.79% ±4.61% 46.28% ±8.88% 38.02% ±8.65%
SR-PMI 90.80% ±1.79% 45.70% ±3.09% 69.68% ±7.24% 23.97% ±7.61% 9.09% ±5.12% 16.53% ±6.62%
SR-sampling-free 96.20% ±1.19% 71.00% ±2.81% 82.58% ±5.97% 39.67% ±8.72% 29.75% ±8.15% 30.58% ±8.21%
SR-P(True) 55.80% ±3.08% 71.50% ±2.80% 62.58% ±7.62% 92.56% ±4.68% 69.42% ±8.21% 85.95% ±6.19%
SelfReflect 99.80% ±0.28% 94.20% ±1.45% 98.06% ±2.17% 95.04% ±3.87% 74.38% ±7.78% 83.47% ±6.62%
to a discriminative masked-out task, asking LLM Jwhether several candidates words fit, given either
the summary or the samples. We provide prompts and implementation details in Appendix E.
4.1 Study 1: Distinguishing good from bad and almost-good summaries
We first conduct an interventional study to test whether summaries that we know are good are judged
as better (lower SelfReflect score) than summaries that we know are bad. To generate these summaries,
we use Qwen2.5 7B Instruct to sample answers A(1:50)for 1,000 open-ended questions from the
Natural Questions dataset [Kwiatkowski et al., 2019]. We then prompt Gemini 2.0 Flash to generate
good summaries (containing all possibilities, details, and relative likelihoods), and badsummaries
(which alter key facts of the good summaries, but keep their remaining style). In Appendix F, we run
the same experiments with human-written summaries, yielding similar results.
Table 1 shows that SelfReflect correctly discriminates good from bad in 99.8% of cases. But all other
baseline metrics also score over 90%. So we make the task harder by comparing good to almost-good
summaries, which only contain facts that are faithful to the answer distribution, but leave out some
possibilities and details that the good summary mentions. SelfReflect gives the good summary a better
score than the almost-good summaries in 94.2% of all questions. Most other approaches, including
the LM judge used in literature, can no longer distinguish these fine-grained quality differences.
To investigate this further, we test the metrics’ ability to score the existence of details and the relative
likelihoods of possible answers in isolation. First, we subsample questions where all answers in the
answer distribution are the same, only varying in level of detail. In 98.06% of the cases, SelfReflect
correctly gives a summary that is informative of all details mentioned in the answer distribution a
better score than a summary string that is shortened to remove details. The second-best metric is again
a SelfReflect ablation, SR-sampling-free with 82.58%. Second, we subsample questions whose answer
distributions contain multiple possible answers. SelfReflect gives a verbalized summary of the form
“It is most likely... but could also be ... ” a better score than a summary that only mentions the majority
answer in 95.04% of questions. Interestingly, Embedding consistently prefers the most likely answer,
probably because it is on average closer to the centroid. When making this harder by comparing to a
summary that does mention all possibilities, but not their relative likelihoods ( “It is ... or ... or ... ” ,
SelfReflect still gives the verbalized summary a better score in 74.38% of the cases, while all baselines
except the mSR-P(True) ablation perform at or below random chance level. The same holds when using
a numerical uncertainty summary ( “I’m X% sure that ..., but it could also be ... (Y% sure)” ) instead
of a verbalized one, at 83.47%. These tests verify that SelfReflect can correctly discern whether a
summary is faithful to an answer distribution both in terms of textual facts and relative frequencies.
4.2 Study 2: Distances of multiple-choice distributions
Next, we investigate SelfReflect in a narrower setup. We generate 1,000 answer distributions for
MMLU [Hendrycks et al., 2021], a multiple-choice dataset with choices A, B, C, and D for each
question. To give a spectrum of different-quality summaries, we create summaries of the form
“The answer is most likely C (54% sure), but it could also be B (32% sure) or A (14% sure). ” that
either match the true ratio of answers, mention the most likely answer only, are overconfident, or
give random percentages. These simple summary strings allow testing how well SelfReflect and
the baseline metrics capture distributional faithfulness. As a reference-metric for faithfulness in
this narrow setup, we compute the true Wasserstein distance between the distribution described in
6the summary and that of the test-set answers. We then calculate the correlation of the ranks that
SelfReflect assigns to the summaries of a question and that which the reference metric assigns. In
order to broaden our analysis, the answer-distributions are generated with a different LLM than
in the previous section. We use Gemma 3 12B (non-Instruct) [Gemma Team et al., 2025], whose
majority/greedy answer has a 71% accuracy on MMLU.
Table 2: Correlations between how SelfRe-
flect, and others, and a metric specialized for
MMLU rank summaries. Mean ±95% CI.
MetricRank Corr.
per questionRank Corr. over
avg. of 1k questions
Summarization 0.49 ±0.02 0.80±0.00
LM Judge 0.80±0.02 1.00±0.00
Opt. Transport 0.63 ±0.02 0.80±0.00
Embedding 0.29 ±0.03 0.18±0.02
SR-PMI -0.03 ±0.03 -0.20 ±0.00
SR-sampling-free 0.57 ±0.03 0.83±0.00
SR-P(True) 0.66 ±0.03 1.00±0.00
SelfReflect 0.66 ±0.03 1.00±0.00Table 2 shows that most metrics have a positive rank
correlation with the reference metric. The LM judge
metric even outperforms SelfReflect, indicating that
SelfReflect may be slightly noisy on individual ques-
tions when summaries contain exact probabilities.
However, as soon as we compute the average score
across all 1000 questions, as it will later be used in
the benchmark, SelfReflect, like LM Judge and the
P(True) ablation, achieves a perfect rank correlation
with the reference metric, i.e., ranks the different
types of summaries the same way as the reference
metric for this special case would.
4.3 Study 3: Do the ratings align with human ratings?
Finally, we assess whether SelfReflect scores are aligned with human judgements. We conduct a
user study using 200 open-ended questions from the TriviaQA dataset [Joshi et al., 2017]. For each
question, we generate ten sample responses using Phi-4 [Abdin et al., 2024], and four summaries: a
good summary and a badsummary, generated using Gemini 2.0 Flash as in Section 4.1; a greedy
summary, i.e., the greedy response of Phi-4; and a Chain of Thought ( CoT) summary, using Phi-4
to reason about possible answers and then summarize its reasoning. Note that the greedy and CoT
summaries are not based on the actual samples. All prompts are provided in Appendix E.
Raters were shown the question, the ten sample answers, and two of the summaries, and asked
to choose which best summarized the set of samples. Each question/summary combination was
evaluated by 5 raters. To assess agreement between human raters, we calculate Krippendorff’s α.
Alternative agreement metrics such as Cohen’s kappa or Fleiss’ kappa are not appropriate here since
each rater only rates a subset of the combinations. We then calculate Krippendorff’s αbetween the
majority human preference and that of SelfReflect and other scores. Further details are in Appendix G.
Table 3: Agreement of metrics with human preference (consensus over five raters) on a pairwise
summary preference task, using Krippendorff’s α(values in [-1, 1]; positive numbers indicate
agreement). Also shown is Krippendorff’s αbetween individual human raters. Mean ±95% CI.
all bad vs good bad vs greedy bad vs CoT good vs greedy good vs CoT greedy vs CoT
Summarization 0.480 ±0.050 0.950 ±0.046 0.910 ±0.050 0.940 ±0.046 -0.211 ±0.156 -0.067 ±0.135 0.260 ±0.121
LM Judge 0.517 ±0.046 0.940 ±0.048 0.920 ±0.058 0.930 ±0.046 -0.063 ±0.152 -0.015 ±0.151 0.267 ±0.128
Opt. Transport 0.487 ±0.047 0.850 ±0.076 0.779 ±0.085 0.679 ±0.104 0.098 ±0.155 0.265 ±0.132 0.191 ±0.146
Embeddings 0.435 ±0.047 0.750 ±0.081 0.799 ±0.087 0.477 ±0.125 -0.363 ±0.136 0.331 ±0.135 0.490 ±0.121
SR-PMI 0.436 ±0.053 0.820 ±0.081 0.890 ±0.067 0.769 ±0.080 -0.246 ±0.156 0.029 ±0.147 0.246 ±0.114
SR-sampling-free 0.530 ±0.045 0.829 ±0.076 0.870 ±0.071 0.799 ±0.080 0.025 ±0.143 0.241 ±0.131 0.340 ±0.141
SR-P(True) -0.032 ±0.052 -0.029 ±0.138 -0.335 ±0.124 -0.474 ±0.120 0.311 ±0.147 0.409 ±0.125 -0.024 ±0.143
SelfReflect 0.690 ±0.036 0.990 ±0.015 0.850 ±0.066 0.850 ±0.070 0.489 ±0.131 0.599 ±0.103 0.329 ±0.125
Human vs human 0.723 ±0.027 0.988 ±0.013 0.906 ±0.035 0.871 ±0.048 0.441 ±0.075 0.636 ±0.064 0.452 ±0.069
As we see from Table 3, SelfReflect has the highest overall alignment with the majority human
judgement ( α= 0.690). This is close to the inter-human alignment ( α= 0.723) and significantly
higher than any of the competing methods or ablations. Looking into the individual summary types,
we see all metrics other than SR-P(True) have good alignment with humans on the badvsgood ,
badvsgreedy , and badvsCoT comparisons. However, the other metrics show poor agreement
with humans on the more nuanced good vsgreedy andgood vsCoT. For all pairs of summary type,
SelfReflect is close to inter-human agreement and either the most aligned with the majority human
preference, or has overlapping 95% confidence intervals with the most aligned metric.
7Table 4: SelfReflect score ↓(×10−3, rounded for readability) averaged across TriviaQA, NQ, and
SimpleQA. Per-dataset results are in Appendix H. The results in small font are relative to Greedy .
pθ(A|q)unimodal is the proportion of questions for which the LLM always gives the same answer.
Model pθ(A|q) Single-decoding methods Sample & summarize
unimodal Greedy Basic CoT N=10 N=20
Qwen2.5 0.5B Instruct [Yang et al., 2024a] 7% 96 95 −1 94−2 96−0 96−0
Qwen2.5 1.5B Instruct [Yang et al., 2024a] 17% 94 94 −0 92−2 87−7 87−7
Qwen2.5 3B Instruct [Yang et al., 2024a] 27% 97 99 +2 99+2 91−6 89−8
Qwen2.5 7B Instruct [Yang et al., 2024a] 36% 96 99 +3 101 +5 91−5 90−6
Qwen2.5 14B Instruct [Yang et al., 2024a] 52% 92 97 +5 99+7 86−6 85−7
Qwen2.5 32B Instruct [Yang et al., 2024a] 49% 96 102 +6 105 +9 91−5 91−0
Qwen2.5 72B Instruct [Yang et al., 2024a] 50% 91 94 +3 96+5 85−6 84−7
Phi 4 14B [Abdin et al., 2024] 36% 92 92 −0 93+1 85−7 84−8
Ministral 8B Instruct 2410 [Jiang et al., 2024] 25% 107 106 −1 105−2 101−6 100−7
Llama 3.1 70B Instruct [Meta AI, 2024a] 51% 92 92 −0 95+3 87−5 87−5
Llama 3.3 70B Instruct [Meta AI, 2024b] 63% 94 98 +4 104 +10 89−5 88−6
Llama 4 Scout 17B 16e Instruct [Meta AI, 2025] 53% 91 96 +5 101 +10 88−3 87−4
Gemma 3 1B Instruct [Gemma Team et al., 2025] 26% 116 129 +13 129 +13 117 +1 111−5
Gemma 3 4B Instruct [Gemma Team et al., 2025] 52% 108 124 +16 128 +20 101−7 100−8
Gemma 3 12B Instruct [Gemma Team et al., 2025] 59% 105 116 +11 121 +16 102−3 101−4
Gemma 3 27B Instruct [Gemma Team et al., 2025] 71% 100 113 +13 120 +20 97−3 96−4
Generation time (seconds) 1.56 1.59 2.48 3.65 4.50
Length (characters) 104.79 195.12 303.09 174.70 219.22
5 Can LLMs generate self-reflective responses?
Now that we have a metric to judge how well summaries summarize the distribution of LLM answers,
we explore the performance of different summarization methods. We distinguish two broad categories
of methods: A) Sample & summarize : draw multiple independent samples from the model, and then
summarize the resulting distribution, B) Single-decoding : methods which utilize only one decoding,
without explicitly elicitating intermediate samples. Of particular interest is whether any of the single-
decoding methods are able to match the performance of the multi-sample methods. We consider
three single-decoding methods: a) Greedy : a baseline simply using a greedy-decoding answer to the
question as the summary; b) Basic : a prompt asking the LLM for a summary of all possible answer
options; c) CoT: a prompt inducing chain-of-thoughts reasoning about the possible answers and
then summarizing them. Our intuition behind Basic andCoT is that, by encouraging responses that
include multiple possible answers, we may be able to recover a reasonable approximation to the true
distribution. We evaluate these summarization methods on 1000 randomly chosen questions from
each of the three datasets: Natural Questions, SimpleQA and TriviaQA. We use the same LLM to
sample the answers to the question and generate the summaries in order to assess whether LLMs can
access and describe their own internal distributions. We provide more details in Appendix H.
As we see in Table 4, Sample & summarize is able to consistently create summaries that reflect
the model’s internal uncertainty better than the Greedy answer. In fact, its score matches that of
humans asked to summarize samples from an LLM distribution, with humans achieving 90·10−3
when summarizing Qwen 2.5 72B Instruct answer distributions and Sample & summarize achieving
88·10−3on the data-split of Appendix F. However, it is of particular interest if we can generate such
self-reflective outputs without needing to sample in-between, which would improve runtime and be
more elegant. Single-decoding methods that implement this are, however, not able to consistently
out-perform the Greedy approach, corroborating that the LLMs are not able to fully verbalize their
own uncertainty by themselves, despite our best efforts to optimize the prompts. This is because
Greedy is in fact a strong baseline: On questions where a model has a unimodal distribution on a
specific answer, Greedy is in fact the best possible summary of this distribution and in turn achieves
a competitive SelfReflect score. Table 4 reveals that this makes Greedy ’s average score a strong
baseline particularly on datasets with high pθ(A|q)unimodal percentages.
5.1 Which answers does Chain-of-Thoughts consider?
We might expect that by considering and summarizing multiple possible options, CoT can capture an
LLM’s distribution better than the single Greedy response. However, in the previous section we found
8Table 5: SelfReflect score ↓(×10−3) of RLVR models averaged over TriviaQA, NQ & SimpleQA.
Greedy is generated w/o reasoning. Basic andSample & Summarize reason and output a summary.
Model Single-decoding methods Sample & Summarize
Greedy Basic N=10 N=20
QwQ 32B [Qwen Team, 2025b] 96 105 +9 91−5 90−6
DeepSeek R1 Distill Qwen 2.5 32B [DeepSeek-AI et al., 2025] 96 108 +12 91−5 90−6
Qwen3 32B (Reasoning enabled) [Qwen Team, 2025a] 93 96 +3 86−7 85−8
Qwen3 8B (Reasoning enabled) [Qwen Team, 2025a] 103 104 +1 90−13 89−14
Generation time (seconds) 1.96 3.60 6.99 8.57
Length (characters) 107.56 224.98 287.31 350.98
this not to be the case. To understand why, we compare CoT summaries to samples from the answer
distribution, focusing on a single large-scale model, Qwen2.5 72B Instruct. To explore whether CoT
correctly captures the spread of the answer distribution, i.e., whether it focuses on a single answer
when the true distribution is unimodal and includes multiple options when the true distribution is
multimodal, we let Gemini 2.0 Flash classify whether the CoT summaries and a(1:N)are certain
(only mentioning one answer option) or uncertain (mentioning semantically different options).
Certain Uncertain
CoT SummaryCertain UncertainAnswer Distribution18% 36%
7% 39%
Figure 4: Confusion matrix
between the uncertainty of
Qwen2.5 72B Instruct answer
distributions and the uncer-
tainty of CoT summaries per
question, judged by Gemini
2.0 Flash for 500 random
questions per dataset. Per-
dataset results and details can
be found in Appendix I.The results are shown in Figure 4. We can see that CoT is often un-
derconfident . In36% of the questions, its summary is uncertain even
when the answer distribution samples are not, meaning it suggests
multiple answers options that do not have high probability under the
true distribution. The Greedy method, by contrast, would give the
ideal summary here, mentioning the only real answer option. But
Greedy isoverconfident in other cases: For 46% of the questions,
there are multiple options in the answer distribution, but Greedy still
collapses them to a single one (leading to Greedy underperforming
Sample & Summarize in Table 4). A balance is clearly needed, but
not trivial.
To study CoT-like summaries further, we turn to reasoning models,
trained with reinforcement learning with verifiable rewards (RLVR)
[DeepSeek-AI et al., 2025]. Asking them to output a summary of
all possibilities, i.e., the Basic prompt, is relatively similar to CoT
on RLHF models, since it automatically invokes a reasoning block.
Table 5 shows that RLVR models do not perform any better than
RLHF models in Table 4. Qualitatively, the summaries produced by
RLVR models with Basic prompts are similar to the ones produced
by RLHF models with CoT prompts. Generating self-reflective
summaries that are faithful to the model’s internal uncertainty thus
remains challenging.
But what awaits us at the end of this road? With improving faithfulness to the subjective distribu-
tion, and LLMs’ subjective distributions becoming more aligned with the objective ground-truth
(-distributions), we can hope to cover ground-truth answers more often than with greedy answers. Fol-
lowing the best practices of Santilli et al. [2025], we measure the RougeL-Recall on Natural Questions’
short answers, i.e., the longest substring of the true answer that appears in a summary, as percentage of
the true answer’s length. We find that Greedy answers have an average overlap of 59.5%with the true
answers. Basic summaries have 62.0%,CoT summaries 64.0%, and Sample & Summarize summaries
65.6%. An LM Judge-based evaluation shows the same trend, rating that 71.3%,72.2%,74.1%, and
76.0%of the summaries include the true answer. In other words, summaries of the LLM’s internal
distributions don’t just hallucinate extra possibilities but actually cover the true answer more often.
6 Discussion
We present SelfReflect, a metric that judges how faithfully a single string represents a distribution over
output strings. SelfReflect is intended to guide the field towards developing methods to make LLMs
honestly describe all possible answers to a question. We have seen in our benchmark that this is a
9hard task, but a solution to this problem would be a fundamental building block in many applications:
Describing internal LLM distributions in a string provides a human-interpretable measure of model
uncertainty, which can be useful in building appropriate trust in the LLM’s outputs. The string can
also be fed back to the LLM, for example to reason about follow-up questions when a user query is
ambiguous. Extracting all output possibilities could also drive development of conformal approaches,
which are popular for classification but less explored for LLMs where the output possibilities are
not immediately available. Finally, an accurate description of a distribution can also be recast into a
numeric uncertainty value.
To outline the limitations of our work, we note that 1-Wasserstein-based SelfReflect scores are
not directly interpretable without baselines. A simplified version, like the percentage of equal top-
predicted words using either summary or answer samples, would give more standardized values in
[0,1]. However, we found that such an approach is less sensitive to differences in good vs almost -
good summaries. Second, we repeat that the faithfulness we measure is with respect to an LLM’s
subjective uncertainty. We intentionally did not develop SelfReflect to quantify objective truthfulness,
with the outlook that larger LLMs approximate their training datasets better and better, such that more
faithful summaries of subjective uncertainties will ultimately lead to better objective uncertainties.
Acknowledgements
We would like to thank Seong Joon Oh for insightful and thought-inspiring discussions. We thank
Eugene Ndiaye, Preetum Nakkiran, and Lukas Aichberger for feedback on a draft of this paper.
References
M. Abdin, J. Aneja, H. Behl, S. Bubeck, R. Eldan, S. Gunasekar, M. Harrison, R. J. Hewett,
M. Javaheripi, P. Kauffmann, et al. Phi-4 technical report. arXiv preprint arXiv:2412.08905 , 2024.
L. Aichberger, K. Schweighofer, and S. Hochreiter. Rethinking uncertainty estimation in natural
language generation. arXiv preprint arXiv:2412.15176 , 2024.
J. M. Bernardo and A. F. Smith. Bayesian theory , volume 405. John Wiley & Sons, 2009.
T. M. Cover. Elements of information theory . John Wiley & Sons, 1999.
DeepSeek-AI, D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi,
X. Zhang, X. Yu, Y . Wu, Z. F. Wu, Z. Gou, Z. Shao, Z. Li, Z. Gao, A. Liu, B. Xue, B. Wang, B. Wu,
B. Feng, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan, D. Dai, D. Chen, D. Ji, E. Li, F. Lin, F. Dai,
F. Luo, G. Hao, G. Chen, G. Li, H. Zhang, H. Bao, H. Xu, H. Wang, H. Ding, H. Xin, H. Gao,
H. Qu, H. Li, J. Guo, J. Li, J. Wang, J. Chen, J. Yuan, J. Qiu, J. Li, J. L. Cai, J. Ni, J. Liang, J. Chen,
K. Dong, K. Hu, K. Gao, K. Guan, K. Huang, K. Yu, L. Wang, L. Zhang, L. Zhao, L. Wang,
L. Zhang, L. Xu, L. Xia, M. Zhang, M. Zhang, M. Tang, M. Li, M. Wang, M. Li, N. Tian, P. Huang,
P. Zhang, Q. Wang, Q. Chen, Q. Du, R. Ge, R. Zhang, R. Pan, R. Wang, R. J. Chen, R. L. Jin,
R. Chen, S. Lu, S. Zhou, S. Chen, S. Ye, S. Wang, S. Yu, S. Zhou, S. Pan, S. S. Li, S. Zhou, S. Wu,
S. Ye, T. Yun, T. Pei, T. Sun, T. Wang, W. Zeng, W. Zhao, W. Liu, W. Liang, W. Gao, W. Yu,
W. Zhang, W. L. Xiao, W. An, X. Liu, X. Wang, X. Chen, X. Nie, X. Cheng, X. Liu, X. Xie, X. Liu,
X. Yang, X. Li, X. Su, X. Lin, X. Q. Li, X. Jin, X. Shen, X. Chen, X. Sun, X. Wang, X. Song,
X. Zhou, X. Wang, X. Shan, Y . K. Li, Y . Q. Wang, Y . X. Wei, Y . Zhang, Y . Xu, Y . Li, Y . Zhao, Y . Sun,
Y . Wang, Y . Yu, Y . Zhang, Y . Shi, Y . Xiong, Y . He, Y . Piao, Y . Wang, Y . Tan, Y . Ma, Y . Liu, Y . Guo,
Y . Ou, Y . Wang, Y . Gong, Y . Zou, Y . He, Y . Xiong, Y . Luo, Y . You, Y . Liu, Y . Zhou, Y . X. Zhu, Y . Xu,
Y . Huang, Y . Li, Y . Zheng, Y . Zhu, Y . Ma, Y . Tang, Y . Zha, Y . Yan, Z. Z. Ren, Z. Ren, Z. Sha, Z. Fu,
Z. Xu, Z. Xie, Z. Zhang, Z. Hao, Z. Ma, Z. Yan, Z. Wu, Z. Gu, Z. Zhu, Z. Liu, Z. Li, Z. Xie, Z. Song,
Z. Pan, Z. Huang, Z. Xu, Z. Zhang, and Z. Zhang. Deepseek-r1: Incentivizing reasoning capability
in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948 .
K. Enevoldsen, I. Chung, I. Kerboua, M. Kardos, A. Mathur, D. Stap, J. Gala, W. Siblini,
D. Krzemi ´nski, G. I. Winata, S. Sturua, S. Utpala, M. Ciancone, M. Schaeffer, G. Sequeira,
D. Misra, S. Dhakal, J. Rystrøm, R. Solomatin, Ömer Ça ˘gatan, A. Kundu, M. Bernstorff, S. Xiao,
A. Sukhlecha, B. Pahwa, R. Po ´swiata, K. K. GV , S. Ashraf, D. Auras, B. Plüster, J. P. Harries,
10L. Magne, I. Mohr, M. Hendriksen, D. Zhu, H. Gisserot-Boukhlef, T. Aarsen, J. Kostkan, K. Woj-
tasik, T. Lee, M. Šuppa, C. Zhang, R. Rocca, M. Hamdy, A. Michail, J. Yang, M. Faysse, A. Va-
tolin, N. Thakur, M. Dey, D. Vasani, P. Chitale, S. Tedeschi, N. Tai, A. Snegirev, M. Günther,
M. Xia, W. Shi, X. H. Lù, J. Clive, G. Krishnakumar, A. Maksimova, S. Wehrli, M. Tikhonova,
H. Panchal, A. Abramov, M. Ostendorff, Z. Liu, S. Clematide, L. J. Miranda, A. Fenogenova,
G. Song, R. B. Safi, W.-D. Li, A. Borghini, F. Cassano, H. Su, J. Lin, H. Yen, L. Hansen, S. Hooker,
C. Xiao, V . Adlakha, O. Weller, S. Reddy, and N. Muennighoff. MMTEB: Massive multilingual
text embedding benchmark. arXiv preprint arXiv:2502.13595 , 2025.
E. Fadeeva, R. Vashurin, A. Tsvigun, A. Vazhentsev, S. Petrakov, K. Fedyanin, D. Vasilev, E. Gon-
charova, A. Panchenko, M. Panov, T. Baldwin, and A. Shelmanov. LM-polygraph: Uncertainty es-
timation for language models. In Y . Feng and E. Lefever, editors, Proceedings of the 2023 Confer-
ence on Empirical Methods in Natural Language Processing: System Demonstrations , Dec. 2023.
E. Fadeeva, A. Rubashevskii, A. Shelmanov, S. Petrakov, H. Li, H. Mubarak, E. Tsymbalov,
G. Kuzmin, A. Panchenko, T. Baldwin, P. Nakov, and M. Panov. Fact-checking the output of large
language models via token-level uncertainty quantification. In L.-W. Ku, A. Martins, and V . Sriku-
mar, editors, Findings of the Association for Computational Linguistics: ACL 2024 , Aug. 2024.
S. Farquhar, J. Kossen, L. Kuhn, and Y . Gal. Detecting hallucinations in large language models using
semantic entropy. Nature , 630(8017):625–630, 2024.
R. Flamary, N. Courty, A. Gramfort, M. Z. Alaya, A. Boisbunon, S. Chambon, L. Chapel, A. Corenflos,
K. Fatras, N. Fournier, L. Gautheron, N. T. Gayraud, H. Janati, A. Rakotomamonjy, I. Redko,
A. Rolet, A. Schutz, V . Seguy, D. J. Sutherland, R. Tavenard, A. Tong, and T. Vayer. Pot:
Python optimal transport. Journal of Machine Learning Research , 22(78):1–8, 2021. URL
http://jmlr.org/papers/v22/20-451.html .
M. Fomicheva, S. Sun, L. Yankovskaya, F. Blain, F. Guzmán, M. Fishel, N. Aletras, V . Chaudhary,
and L. Specia. Unsupervised quality estimation for neural machine translation. Transactions of the
Association for Computational Linguistics , 8:539–555, 2020.
Gemma Team, A. Kamath, J. Ferret, S. Pathak, N. Vieillard, R. Merhej, S. Perrin, T. Matejovicova,
A. Ramé, M. Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786 , 2025.
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring
massive multitask language understanding. Proceedings of the International Conference on
Learning Representations (ICLR) , 2021.
D. R. Hofstadter. Godel, Escher, Bach: An Eternal Golden Braid . Basic Books, Hassocks, England,
1979.
S. Jain, V . Keshava, S. M. Sathyendra, P. Fernandes, P. Liu, G. Neubig, and C. Zhou. Multi-
dimensional evaluation of text summarization with in-context learning. arXiv preprint
arXiv:2306.01200 , 2023.
A. Jiang, A. A. Chahine, A. Sablayrolles, A. Tacnet, A. Boissonnet, A. Kothari, A. Héliou, A. Lo,
A. Peronnin, A. Meunier, A. Roux, A. Faure, A. Paul, A. Darcet, A. Mensch, A. Herblin-Stoop,
A. Garreau, A. Birky, A. Sooriyarachchi, B. Rozière, B. Conklin, B. Bouillon, B. S. de Beauregard,
C. Rambaud, C. Feldman, C. de Freminville, C. Mauro, C.-K. Yeh, C. Bamford, C. Auguy,
C. Heintz, C. Dubois, D. S. Chaplot, D. L. Casas, D. Costa, E. Arcelin, E. B. Hanna, E. Metzger,
F. O. Autran, F. Lesage, G. Gourdel, G. Blanchet, G. D. Vidal, G. M. Lengyel, G. Bour, G. Lample,
G. Denis, H. Rajaona, H. Jaju, I. Mack, I. Mathew, J.-M. Delignon, J. Facchetti, J. Chudnovsky,
J. Studnia, J. Murke, K. Khandelwal, K. Chiu, K. Riera, L. Blier, L. Suslian, L. Deschaseaux,
L. Martin, L. Ternon, L. Saulnier, L. R. Lavaud, S. Yang, M. Jennings, M. Pellat, M. Torelli,
M. Janiewicz, M. Felardos, M. Darrin, M. Hoff, M. Seznec, M. J. Kenyon, N. Derwiche, N. C.
Zaragoza, N. Faurie, N. Moreau, N. Schuhl, N. Raghuraman, N. Muhs, O. de Garrigues, P. Rozé,
P. Wang, P. von Platen, P. Jacob, P. Buche, P. R. Muddireddy, P. Savas, P. Stock, P. Agrawal,
R. de Peretti, R. Sauvestre, R. Sinthe, R. Soletskyi, S. Vaze, S. Subramanian, S. Garg, S. Ghosh,
S. Regnier, S. Antoniak, T. L. Scao, T. Gervet, T. Schueller, T. Lavril, T. Wang, T. Lacroix,
V . Nemychnikova, W. Shang, W. E. Sayed, and W. Marshall. Un ministral, des ministraux. 2024.
URL https://mistral.ai/news/ministraux?utm_source=tldrai .
11M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer. Triviaqa: A large scale distantly supervised
challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics , Vancouver, Canada, July 2017. Association for
Computational Linguistics.
C. Kauf and A. Ivanova. A better way to do masked language model scoring. In A. Rogers, J. Boyd-
Graber, and N. Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 2: Short Papers) , July 2023.
T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin,
J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le,
and S. Petrov. Natural questions: A benchmark for question answering research. Transactions of
the Association for Computational Linguistics , 7, 2019.
W. Kwon, Z. Li, S. Zhuang, Y . Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.
Efficient memory management for large language model serving with pagedattention. In Proceed-
ings of the ACM SIGOPS 29th Symposium on Operating Systems Principles , 2023.
S. L. Lauritzen. Sufficiency, prediction and extreme models. Scandinavian Journal of Statistics ,
pages 128–134, 1974.
Z. Li, X. Zhang, Y . Zhang, D. Long, P. Xie, and M. Zhang. Towards general text embeddings with
multi-stage contrastive learning. arXiv preprint arXiv:2308.03281 , 2023.
S. Lin, J. Hilton, and O. Evans. Teaching models to express their uncertainty in words. arXiv preprint
arXiv:2205.14334 , 2022.
A. Malinin and M. Gales. Uncertainty estimation in autoregressive structured prediction. arXiv
preprint arXiv:2002.07650 , 2020.
Meta AI. Llama 3.1 model card. 2024a. URL https://www.llama.com/docs/
model-cards-and-prompt-formats/llama3_1/ .
Meta AI. Llama 3.3 model card. 2024b. URL https://www.llama.com/docs/
model-cards-and-prompt-formats/llama3_3/ .
Meta AI. Llama 4 model card. 2025. URL https://www.llama.com/docs/
model-cards-and-prompt-formats/llama4/ .
G. Peyré, M. Cuturi, et al. Computational optimal transport: With applications to data science.
Foundations and Trends® in Machine Learning , 11(5-6):355–607, 2019.
Qwen Team. Qwen3, April 2025a. URL https://qwenlm.github.io/blog/qwen3/ .
Qwen Team. Qwq-32b: Embracing the power of reinforcement learning, March 2025b. URL
https://qwenlm.github.io/blog/qwq-32b/ .
P. Sahoo, A. K. Singh, S. Saha, V . Jain, S. Mondal, and A. Chadha. A systematic survey of
prompt engineering in large language models: Techniques and applications. arXiv preprint
arXiv:2402.07927 , 2024.
J. Salazar, D. Liang, T. Q. Nguyen, and K. Kirchhoff. Masked language model scoring. In D. Jurafsky,
J. Chai, N. Schluter, and J. Tetreault, editors, Proceedings of the 58th Annual Meeting of the
Association for Computational Linguistics , July 2020.
A. Santilli, A. Golinski, M. Kirchhof, F. Danieli, A. Blaas, M. Xiong, L. Zappella, and S. Williamson.
Revisiting uncertainty quantification evaluation in language models: Spurious interactions with
response length bias results. arXiv preprint arXiv:2504.13677 , 2025.
S. Särkkä and A. Solin. Applied stochastic differential equations , volume 10. Cambridge University
Press, 2019.
12J. Shin, Y . Lee, and K. Jung. Effective sentence scoring method using bert for speech recognition.
In W. S. Lee and T. Suzuki, editors, Proceedings of The Eleventh Asian Conference on Machine
Learning , volume 101 of Proceedings of Machine Learning Research , pages 1081–1093. PMLR,
17–19 Nov 2019. URL https://proceedings.mlr.press/v101/shin19a.html .
W. L. Taylor. Cloze procedure: A new tool for measuring readability. Journalism quarterly , 30(4):
415–433, 1953.
O. Vasilyev, V . Dharnidharka, and J. Bohannon. Fill in the BLANC: Human-free quality estimation of
document summaries. In S. Eger, Y . Gao, M. Peyrard, W. Zhao, and E. Hovy, editors, Proceedings
of the First Workshop on Evaluation and Comparison of NLP Systems , pages 11–20, Online, Nov.
2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.eval4nlp-1.2. URL
https://aclanthology.org/2020.eval4nlp-1.2/ .
A. Wang and K. Cho. BERT has a mouth, and it must speak: BERT as a Markov random field
language model. In A. Bosselut, A. Celikyilmaz, M. Ghazvininejad, S. Iyer, U. Khandelwal,
H. Rashkin, and T. Wolf, editors, Proceedings of the Workshop on Methods for Optimizing and
Evaluating Neural Language Generation , June 2019.
Z. Wang and C. Holmes. On subjective uncertainty quantification and calibration in natural language
generation. arXiv preprint arXiv:2406.05213 , 2024.
T. Xu, S. Wu, S. Diao, X. Liu, X. Wang, Y . Chen, and J. Gao. Sayself: Teaching llms to express
confidence with self-reflective rationales. In Proceedings of the 2024 Conference on Empirical
Methods in Natural Language Processing , pages 5985–5998, 2024.
A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei, H. Lin,
J. Yang, J. Tu, J. Zhang, J. Yang, J. Yang, J. Zhou, J. Lin, K. Dang, K. Lu, K. Bao, K. Yang, L. Yu,
M. Li, M. Xue, P. Zhang, Q. Zhu, R. Men, R. Lin, T. Li, T. Xia, X. Ren, X. Ren, Y . Fan, Y . Su,
Y . Zhang, Y . Wan, Y . Liu, Z. Cui, Z. Zhang, and Z. Qiu. Qwen2.5 technical report. arXiv preprint
arXiv:2412.15115 , 2024a.
R. Yang, C. Zhang, Z. Zhang, X. Huang, S. Yang, N. Collier, D. Yu, and D. Yang. Logu: Long-form
generation with uncertainty expressions. arXiv preprint arXiv:2410.14309 , 2024b.
G. Yona, R. Aharoni, and M. Geva. Can large language models faithfully express their intrinsic
uncertainty in words? arXiv preprint arXiv:2405.16908 , 2024.
Y . Zhang, H. Jin, D. Meng, J. Wang, and J. Tan. A comprehensive survey on process-oriented auto-
matic text summarization with exploration of llm-based methods. arXiv preprint arXiv:2403.02901 ,
2024.
13A SelfReflect and predictive sufficiency: propositions and proofs
In this appendix, we provide details of the propositions from the main text and their proofs. We begin
with the definition of predictive sufficiency and provide a proof of its two equivalent characterizations
in the context of the SelfReflect metric. We then prove an equivalence between solving the masked-
token prediction task of the SelfReflect metric and the desired predictive sufficiency of the summary,
providing a theoretical foundation for the design of the SelfReflect metric.
BΘQ A(1:N) S
Figure 5: The graphical model for the setting of SelfReflect metric. The figure is reproduced from
Figure 2 of the main text for the sake of better readability of the formalization that follows.
A.1 Setup, notations, and assumptions
Recall that prompting a given LLM with question Qputs it in state ΘQ, from which we sample N
answers A(1:N). A summarization mechanism function ψgenerates the summary of these answers
asS=ψ 
A(1:N)
. For developing the SelfReflect metric, we generate another sample Bfrom the
same state ΘQand require an ideal summary Sto capture all the information about Bthat is captured
by the samples A(1:N). Now, we formalize this setup of the SelfReflect metric by setting the notation,
listing the assumptions of the setup, and providing their justifications.
Setup and notation
1.Firstly, Figure 2 shows the graphical model of this setup, which we also reproduce here in
Figure 5 for better readability. In this graphical model, observed variables are shaded gray,
which includes the sampled answers A(1:N), their summary S, and a subsequent answer B,
whereas unobserved/latent variables are unshaded, which includes the LLM state ΘQ.
2.We will use upper-case non-boldface letters (like BorS) to represent random vari-
ables/vectors and the corresponding lower-case non-boldface letters (like bors) to represent
particular samples from their underlying distributions.
3.For a random variable Y, the sampling of a particular value ywill be denoted as y∼Yor
y∈supp(Y), where supp (Y)represents the support of the random variable Y.
4.LetVdenote a finite vocabulary of words (or tokens), which is used to generate questions,
the corresponding answers, and their summaries.
5. Let Qdenote the random variable for a question.
6.Prompting the given LLM with this question Qis assumed to put it in state, which is
represented with the random variable ΘQ. From this state, we can sample multiple answers,
which are then used to define the SelfReflect metric.
7.The random variables A(1:N):= 
A(1),···, A(N)
are used to denote the Nanswers
sampled from the LLM in state ΘQ. These samples may be sampled in an i.i.d. manner
but we do not necessitate this. In fact, one can sample each answer A(n)conditioned on all
previous samples A(1:n−1)as well. We allow for this generality because throughout our
derivation, we will always consider these answers jointly as A(1:N).
8. A summarization mechanism inputs the sampled answers and generates their summary S.
9.Suppose Bdenote a subsequent sample from the LLM in the same state ΘQ. For the
SelfReflect metric, we require an idea summary Sof sampled answers A(1:N)to capture all
information about this subsequent answer B.
14assumptions
1.The support of question Qis assumed to be the set of all finite-length sentences generated
fromV, which we denote by X.
2.The support of each A(i)is also assumed to be X, the set of all finite-length sentences
generated from vocabulary V.
3.The summarization mechanism that inputs the sampled answers A(1:N)and generates their
summary Sis assumed to be a function ψ. Formally, ψ:XN−→ X inputs any Nsampled
answers A(1:N)from the LLM and generates their summary SasS:=ψ 
A(1:N)
. Note
that the support of the summary S, will be a subset of the set of all finite-length sentences,
i.e.,supp(S)⊆ X . This condition models our setup sufficiently well, where we have a
candidate summary Sper set of answers A(1:N). However, we acknowledge that it is a
restrictive condition in that it doesn’t allow for modeling a conditional distribution over all
summaries given the answers. Generalizing our SelfReflect metric for this case or proving
its generality in this case is an interesting direction for future work.
4.We define the support of the subsequent new answer Bto be the set XL:=VLof all
possible sentences from the vocabulary Vthat are of length L. Despite being slightly
restrictive, this assumption is not unreasonable; all LLMs have a maximum context length,
which can be viewed as an upper limit on the length of the answer B. Also, sentences with
smaller lengths are usually padded to achieve the maximum context length.
5.Throughout our derivations, we will assume all required marginal and conditional distribu-
tions to be strictly positive. This assumption is reasonable for our setting because in prac-
tice, we would be implementing corresponding distributions using the given LLM. For in-
stance, p(W)would represent the probability of sentence Wunder the given LLM. Further,
p(Y|Z)would represent the probability of sentence Ywhen the LLM is prompted with the
context Z. Since the LLMs generate distribution over the entire vocabulary V, all the condi-
tional distributions will have strictly positive values, albeit extremely small in certain cases.
A.2 Predictive sufficiency and equivalent characterizations
Now, having set the notations and assumptions, we define the notion of sufficiency and connect it
with the definition of an ideal summary.
Definition A.1 (Bayesian and Predictive Sufficiency [Bernardo and Smith, 2009]) .Consider a
distribution parameterized in terms of a parameter ϕ. LetX(1:M)denote M(i.i.d.) samples from
this distribution. A statistic (function) T 
X1:M
is called a Bayesian sufficient statistic of samples
X(1:M)forϕif and only if we have: p 
ϕ|X(1:M)=x(1:M)
=p 
ϕ|T 
X(1:M)
=t 
x(1:M)
.
On the other hand, it is called a predictive sufficient statistic of samples X(1:M)if and only if we
have: p 
X=x|X(1:M)=x(1:M)
=p 
X=x|T 
X(1:M)
=t 
x(1:M)
for any subsequent
sample X(with concrete value x∈supp(X)) from the same distribution.
Note that our Definition 3.1 of an ideal summary is closely related to predictive sufficiency as defined
in Definition A.1. However, it turns out that Bayesian and predictive sufficiency notions are not
exactly equivalent. In light of this, our reason for defining an ideal summary to be predictive sufficient,
rather than Bayesian sufficient, is as follows. An LLM trained on a huge corpus of data contains
information about a wide array of aspects. However, through the summary, we are interested in
capturing only those aspects of the state ΘQof the LLM that are related to answering the given
question Q. For this, requiring the summary to be predictive sufficient serves the purpose precisely.
Now, in the context of the Definition A.1 of predictive sufficiency, Definition 3.1 of ideal summary,
and the graphical model of Figure 5, we prove Proposition 3.1, which asserts the equivalence in the
information theoretic and conditional distribution based formulations of the ideal summary. We begin
by proving a lemma about the graphical model of Figure 5.
Lemma A.1 (Conditioning on A(1:N)andS).Under the graphical model given in Figure 5, we have:
p
B|A(1:N), S
=p
B|A(1:N)
15Proof. Consider the following manipulations:
p
B|A(1:N), S
=(1)Z
θdθ p
B,ΘQ=θ|A(1:N), S
=(2)Z
θdθp 
ΘQ=θ, B, A(1:N), S
p 
A(1:N), S
=(3)Z
θdθp(ΘQ=θ)·p(B|ΘQ=θ)·p 
A(1:N)|ΘQ=θ
·p 
S|A(1:N)
p 
A(1:N)
·p 
S|A(1:N)
=(4)Z
θdθp(ΘQ=θ)·p(B|ΘQ=θ)·p 
A(1:N)|ΘQ=θ
p 
A(1:N)
=(5)Z
θdθp 
ΘQ=θ, B, A(1:N)
p 
A(1:N)
=(6)Z
θdθ p
B,ΘQ=θ|A(1:N)
=(7)p
B|A(1:N)
(5)
Here, steps (2),(5),(6)follow from chain rule. Step (4)follows by cancellation of the common
terms. Steps (1),(7)follows from integrating out variable ΘQ. Step (3)follows from the graphical
model of Figure 5. Finally, an analogous derivation would follow by replacing integration with
summation in the case of ΘQbeing a discrete variable.
Now, we prove Proposition 3.1 establishing the equivalence of the information theoretic and condi-
tional distribution based formulations of the desired predictive sufficiency.
Theorem A.1 (Connection of SelfReflect to Predictive Sufficiency) .Consider the graphical model
given in Figure 5. Under this graphical model, for ideal summary Sof answers A(1:N),
In
A(1:N);Bo
=I {S;B} ⇐⇒ p
B|A(1:N)
=p(B|S)
Proof. Consider following steps:
In
A(1:N);Bo
=I {S;B} ⇐⇒(1)EA(1:N),B"
logp 
A(1:N), B
p 
A(1:N)
·p(B)#
=ES,B
logp(S, B)
p(S)·p(B)
⇐⇒EB,A(1:N),S"
logp 
A(1:N), B
·p(S)
p(S, B)·p 
A(1:N)#
= 0⇐⇒(2)EB,A(1:N),S"
logp 
B|A(1:N)
p(B|S)#
= 0
⇐⇒(3)EB,A(1:N),S"
logp 
B|A(1:N), S
p(B|S)#
= 0⇐⇒(4)In
A;A(1:N)|So
= 0
⇐⇒(5)p
B, A(1:N)|S
=p(B|S)·p
A(1:N)|S
⇐⇒(6)p
B|A(1:N), S
=p(B|S)⇐⇒(7)p
B|A(1:N)
=p(B|S) (6)
Here, step (1)follows from the definition of mutual information, steps (2)and(6)from chain rule,
steps (3)and(7)from Lemma A.1, step (4)from the definition of conditional mutual information,
and step (5)from the equality condition of conditional mutual information. For details on mutual
information and conditional mutual information, we refer the reader to Cover [1999].
A.3 SelfReflect metric and equivalence to predictive sufficiency
Now, we demonstrate that the masked-token prediction task of SelfReflect is equivalent to the above
notion of predictive sufficiency. For the SelfReflect metric, we consider the random variable B
for a new subsequent sample from the LLM in state ΘQand dissect it in terms of its words. In
particular, we have: B≡(B1,···, BL), where Lis length of the sentence B(which, as we saw,
could be chosen to be the maximum context length for the LLM). Here, Birepresents the random
variable for the i−th word of the sentence Bfor each value of i∈ {1,···, L}. For each i, we use
16the shorthand notation B−ito represent the variable for all the words in the sentence Bexcept for Bi,
i.e.,B−i:= (B1,···, Bi−1, Bi+1,···, BL) = ( Bj)j̸=i. Note that Bℓ, which represents the ℓ−th
word of sentence B, is not to be confused with A(k), which represents the k−th sampled answer from
the LLM. For each Bi, its support is going to be the vocabulary Vand the supports of B−iandB
areVL−1andVL≡ XLrespectively. With this setup, we can prove Proposition 3.2, which asserts
that under assumptions from subsection A.1, SelfReflect metric provides an equivalent formulation
of the desired predictive sufficiency of ideal summary S. This is done as follows.
Theorem A.2 (SelfReflect Metric and Predictive Sufficiency) .Suppose all involved conditionals are
modeled via the given LLM and hence, are strictly positive. Then, we have:
p
B|A(1:N)
=p(B|S)⇐⇒ for all masking indices i, p
Bi|A(1:N), B−i
=p(Bi|S, B−i)(7)
Proof. (=⇒)Suppose we are given that p 
B|A(1:N)
=p(B|S). Consider the following steps:
p
B|A(1:N)
=p(B|S) =⇒p
B1,···, BL|A(1:N)
=p(B1,···, BL|S)
=⇒X
bi∈Vp
B1,···, Bi=bi,···, BL|A(1:N)
=X
bi∈Vp(B1,···, Bi=bi,···, BL|S)
=⇒(1)p
B−i|A(1:N)
=p(B−i|S) (8)
Here, step (1)follows from integrating out variable Bi. Combining this result with the premise gives:
p
B|A(1:N)
=p(B|S), p
B−i|A(1:N)
=p(B−i|S)
=⇒p 
B|A(1:N)
p 
B−i|A(1:N)=p(B|S)
p(B−i|S)=⇒(1)p
Bi|A(1:N), B−i
=p(Bi|S, B−i)(9)
Here, step (1)follows because Bis formed of the i−th word Biand the rest of the words B−i. Since
we can carry out these steps for any index i, we prove the forward direction of the theorem.
(⇐=)Now, to prove the converse, suppose we are given that for all masking indices i, we have:
p 
Bi|A(1:N), B−i
=p(Bi|S, B−i)and we have to prove that p 
B|A(1:N)
=p(B|S).
Since this is an equality of the random variables, we prove the equality of random variables by
proving it for any and all choices of the samples of those random variables. Note that this works
because of the assumption of summary mechanism Sbeing a function of A(1:N), which allows us to
use the given condition as well as prove the desired result by assuming particular instantiations of
A(1:N)= ¯a(1:N)and using the corresponding summary S= ¯s:=ψ 
¯a(1:N)
. Pick any instantiations
of sampled answers from their support as a(1:N)∼A(1:N). Since the summary mechanism is a
function, it gives us a concrete sample s=ψ 
a(1:N)
∈ X. Now, suppose we want to prove the
desired result for any particular given sample b∼Bwithb:= (b1,···, bL)∈VL. Consider a fixed
sentence b∗∈VLwithb∗:= (b∗
1,···, b∗
L). Now, we define a sequence of sentences as follows:
x(0):= (b1, b2,···, bL) =b∈VL
x(1):= (b∗
1, b2,···, bL)∈VL
x(2):= (b∗
1, b∗
2,···, bL)∈VL
...
x(L):= (b∗
1, b∗
2,···, b∗
L) =b∗∈VL(10)
Intuitively, we create a sequence of sentences where each subsequent sentence x(i)differs from
the previous sentence and the next sentence in exactly one word and as we go from sentence x(0)
tox(L), we change the given sentence bto the fixed sentence b∗. Now, we consider the following
17manipulations for p 
B=b|A(1:N)=a(1:N)
:
p
B=b|A(1:N)=a(1:N)
=p
B=x(0)|A(1:N)=a(1:N)
=(1)p
B=x(0)|A(1:N)=a(1:N)
·YL
ℓ=1p 
B=x(ℓ)|A(1:N)=a(1:N)
p 
B=x(ℓ)|A(1:N)=a(1:N)
=(2) YL
ℓ=1p 
B=x(ℓ−1)|A(1:N)=a(1:N)
p 
B=x(ℓ)|A(1:N)=a(1:N)!
·p
B=b∗|A(1:N)=a(1:N)
(11)
In an exactly analogous way, we get following manipulations for p(B=b|S=s):
p(B=b|S=s) =p
B=x(0)|S=s
=(1)p
B=x(0)|S=s
·YL
ℓ=1p 
B=x(ℓ)|S=s
p 
B=x(ℓ)|S=s
=(2) YL
ℓ=1p 
B=x(ℓ−1)|S=s
p 
B=x(ℓ)|S=s!
·p(B=b∗|S=s) (12)
Note that in both Equation 11 and Equation 12 above, step (1)follows from multiplying and dividing
by the same terms and step (2)follows from rearranging the terms and recognizing x(L)=b∗by
definition. Now, we consider the ℓ−th term from the Equation 11 and simplify it as follows:
p 
B=x(ℓ−1)|A(1:N)=a(1:N)
p 
B=x(ℓ)|A(1:N)=a(1:N)
=(1)p 
B1=b∗
1,···, Bℓ−1=b∗
ℓ−1, Bℓ=bℓ, Bℓ+1=bℓ+1,···, BL=bL|A(1:N)=a(1:N)
p 
B1=b∗
1,···, Bℓ−1=b∗
ℓ−1, Bℓ=b∗
ℓ, Bℓ+1=bℓ+1,···, BL=bL|A(1:N)=a(1:N)
=(2)p 
B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
|A(1:N)=a(1:N)
p 
B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
|A(1:N)=a(1:N)
×p 
Bℓ=bℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
p 
Bℓ=b∗
ℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
=(3)p 
Bℓ=bℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
p 
Bℓ=b∗
ℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL (13)
Again, in an exactly analogous way, we simplify the ℓ−th terms of Equation 12 as follows:
p 
B=x(ℓ−1)|S=s
p 
B=x(ℓ)|S=s
=(1)p 
B1=b∗
1,···, Bℓ−1=b∗
ℓ−1, Bℓ=bℓ, Bℓ+1=bℓ+1,···, BL=bL|S=s
p 
B1=b∗
1,···, Bℓ−1=b∗
ℓ−1, Bℓ=b∗
ℓ, Bℓ+1=bℓ+1,···, BL=bL|S=s
=(2)p 
B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
|S=s
p 
B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
|S=s
×p 
Bℓ=bℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
p 
Bℓ=b∗
ℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
=(3)p 
Bℓ=bℓ|S=s, A−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
p 
Bℓ=b∗
ℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL (14)
In both these simplifications, step (1)follows from the definition of the sentences x(ℓ−1), x(ℓ), step
(2)follows from chain rule, and step (3)follows from canceling the common terms. However, given
18equality p 
Bi|A(1:N), B−i
=p(Bi|S, B−i)for all masking locations iimplies that for all ℓ:
p
Bℓ=bℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
=p 
Bℓ=bℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
and (15)
p
Bℓ=b∗
ℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
=p 
Bℓ=b∗
ℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
(16)
=⇒p 
Bℓ=bℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
p 
Bℓ=b∗
ℓ|A(1:N)=a(1:N), B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
=p 
Bℓ=bℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
p 
Bℓ=b∗
ℓ|S=s, B−ℓ= 
b∗
1,···, b∗
ℓ−1, bℓ+1,···, bL
=⇒p 
B=x(ℓ−1)|A(1:N)=a(1:N)
p 
B=x(ℓ)|A(1:N)=a(1:N)=p 
B=x(ℓ−1)|S=s
p 
B=x(ℓ)|S=sfor all ℓ∈ {1,···, L}. (17)
Combining this with Equation 11 and Equation 12, we get an interesting result:
p 
B=b|A(1:N)=a(1:N)
p(B=b|S=s)
=QL
ℓ=1p(B=x(ℓ−1)|A(1:N)=a(1:N))
p(B=x(ℓ)|A(1:N)=a(1:N))
·p 
B=b∗|A(1:N)=a(1:N)
QL
ℓ=1p(B=x(ℓ−1)|S=s)
p(B=x(ℓ)|S=s)
·p(B=b∗|S=s)
=(1)p 
B=b∗|A(1:N)=a(1:N)
p(B=b∗|S=s)(18)
Here, step (1)follows from canceling equal terms in both the numerator and the denominator. What
Equation 18 implies is that given A(1:N)=a(1:N), thereby giving S=s:=ψ 
a(1:N)
, the ratio
p(B=b|A(1:N)=a(1:N))
p(B=b|S=s)equals the ratiop(B=b∗|A(1:N)=a(1:N))
p(B=b∗|S=s)for any and all values of b∈VL, thereby
making it a constant c:=c 
a(1:N)
(a constant that dependents on a(1:N)). Now, we can integrate
outBand obtain the value of this constant as follows:
For all b∈VL,p 
B=b|A(1:N)=a(1:N)
p(B=b|S=s)=c
a(1:N)
=⇒1 =X
b∈VLp
B=b|A(1:N)=a(1:N)
=X
b∈VLc
a(1:N)
·p(B=b|S=s)
=c
a(1:N)
·X
b∈VLp(B=b|S=s) =c
a(1:N)
·1 =c
a(1:N)
(19)
This proves that in fact c 
a(1:N)
= 1 , which gives that for all b∼B, we have:
p 
B=b|A(1:N)=a(1:N)
=p(B=b|S=s). Since this result holds for all b∼B, we can
write the corresponding result with the underlying random variable as: p 
B|A(1:N)=a(1:N)
=
p(B|S=s). However, since this result holds for any sample choice of A(1:N)=a(1:N)(and corre-
sponding S=s:=ψ 
a(1:N)
), we get the desired results involving all underlying random variables:
p 
B|A(1:N)
=p(B|S). This proves the reverse direction of the equivalence.
A.4 Modeling with LLM: From derivation to implementation
Now, having proved the equivalence of the basis of the SelfReflect metric and the desired predictive
sufficiency of summary, we show the connection with the exact definition of the SelfReflect metric.
Suppose we are given with a question Q=q∈ X, which is shown to an LLM labeled LLM θ. This
puts LLM θin a state ΘQ=θq, from which we sample answers A(1:N)=a(1:N), and a subsequent
sample B=b∈VL. Now, to calculate the SelfReflect metric, the core idea is that conditional
distributions of the form p(Y|Z)involved in the theoretical considerations above are modeled by
19prompting the judge LLM Jwith context Zand checking the probability of Y. In our implementation,
thisLLM Jwill be temperature-scaled with temperature τ= 5 as mentioned in the main text in
order to flatten its distribution and make it consider more synonyms. Then, we build the prompt of
LLM Jby including the question Q=qand either the samples A(1:N)=a(1:N)or their summary
S=s:=ψ 
a(1:N)
, along with a description tof the masked-token prediction task to tell the LLM J
judge what it needs to do. We then mask each word of B=bone by one to obtain the masked word
Bm=bm∈Vand the rest of the sentence B−m=b−m∈VL−1. Then, we model the required
conditional distributions that appear in the derivation using the LLM Jjudge as follows:
p
Bm=bm|A(1:N)=a(1:N), B−m=b−m
:=pLLMJ
Bm=bm|Q=q, A(1:N)=a(1:N), t, B−m=b−m
,and
p(Bm=bm|S=s, B−m=b−m)
:=pLLMJ(Bm=bm|Q=q, S=s, t, B −m=b−m)(20)
This modeling along with Theorem A.2 demonstrates the efficacy of SelfReflect metric:
Corollary A.1 (Efficacy of SelfReflect Metric) .For any question Q, for all masking indices m,
W1
pLLMJ
Bm|Q, A(1:N), t, B−m
, pLLMJ(Bm|Q, S, t, B −m)
= 0
⇐⇒(1)pLLMJ
Bm|Q, A(1:N), t, B−m
=pLLMJ(Bm|Q, S, t, B −m)
⇐⇒(2)p
Bm|A(1:N), B−m
=p(Bm|S, B−m)
⇐⇒(3)p
B|A(1:N)
=p(B|S)
⇐⇒(4)In
A(1:N);Bo
=I {S;B}(21)
Proof. Step(4)follows from Theorem A.1, step (3)follows from Theorem A.2, step (2)follows
from modeling in Equation 20, and step (1)follows from the fact that the W1(1−Wasserstein)
distance between two distributions is 0if and only if the distributions are identical.
Discussion
We conclude this section by discussing two important points about our derivation.
1.Firstly, LLMs are known to behave significantly better with careful design of prompts [Sa-
hoo et al., 2024]. Thus, in our modeling of Equation 20, one may try to optimize the prompt-
ing template and the task description tin order to further obtain sharper versions of the
SelfReflect metric. In this aspect, note that our derivation does not provide a mechanism for
optimizing for the prompt template or task description t. In fact, irrespective of this detail,
the derivation holds true.
2.Secondly, we state the assumptions required for the derivation, as stated in Appendix A.1,
are needed for establishing the connection of SelfReflect metric with the notion of predic-
tive sufficiency. However, these are not needed for defining, implementing, or using the
SelfReflect metric. Users may find our SelfReflect metric useful even in cases where one or
more of the assumptions are loosened. Also, further generalizing the SelfReflect metric in
cases where the assumptions are loosened or proving that the current formulation holds in
those scenarios remains an interesting direction for future theoretical work.
20B Convergence of the SelfReflect metric
In the main paper, we evaluate SelfReflect on 1000 questions per dataset with N=M= 50
conditioning and masked-out answers. This is based on a convergence analysis that we present in this
section. We use Qwen 2.5 72B Instruct and Natural Questions as an example and calculate the average
SelfReflect score across an increasing number of questions and conditioning and masked-out answers
in Figs. 6 to 10. The question is how many questions are needed to arrive at a stable average score.
It can be seen in Fig. 6 that at N=M= 50 , the SelfReflect score converges at 1000 questions,
our setup for the paper. One can of course reduce NandM, which will roughly linearly reduce
the runtime required to compute the score. However, when for example reducing to N=M= 20
questions in Fig. 7, convergence to the final value sets in only at about 2500 questions, which linearly
increases the runtime, so that the runtime advantage vanishes. If one allows the score to be a bit
less converged, for example in development rather than in reporting test results, we suggest to use
N=M= 10 and 500 questions. This reduces the runtime to calculate SelfReflect to 9 minutes on a
node with 8 A100 GPUs, compared to the 67 minutes of N=M= 50 and 1000 questions.
The only real outlier to these trends is N=M= 1. Here, it is especially important that N= 1, i.e.,
in the context of the answer distribution prompt, there is only a single response. In this case, the ideal
summary is actually to return exactly this response rather than a summary of the distribution. Hence,
in Fig. 10, Greedy obtains a better SelfReflect score than Sample & Summarize . This underlines the
importance of why SelfReflect uses multiple samples from the answer distribution.
0 1000 2000 3000 4000 5000
Number of Questions/Answer Distributions Averaged Over0.0850.0900.0950.1000.1050.1100.115Average SelfReflect ScoreCoT
Basic
Greedy
Sample & Summarize (n=50)
Figure 6: Convergence of the SelfReflect score with N=M= 50 and an increasing number of
queries we evaluate on. Answer Distributions of Qwen 2.5 72B Instruct on Natural Questions.
210 1000 2000 3000 4000 5000
Number of Questions/Answer Distributions Averaged Over0.0800.0850.0900.0950.100Average SelfReflect ScoreCoT
Basic
Greedy
Sample & Summarize (n=50)Figure 7: Convergence of the SelfReflect score with N=M= 20 and an increasing number of
queries we evaluate on. Answer Distributions of Qwen 2.5 72B Instruct on Natural Questions.
0 1000 2000 3000 4000 5000
Number of Questions/Answer Distributions Averaged Over0.07750.08000.08250.08500.08750.09000.09250.09500.0975Average SelfReflect ScoreCoT
Basic
Greedy
Sample & Summarize (n=50)
Figure 8: Convergence of the SelfReflect score with N=M= 10 and an increasing number of
queries we evaluate on. Answer Distributions of Qwen 2.5 72B Instruct on Natural Questions.
220 1000 2000 3000 4000 5000
Number of Questions/Answer Distributions Averaged Over0.0750.0800.0850.0900.0950.100Average SelfReflect ScoreCoT
Basic
Greedy
Sample & Summarize (n=50)Figure 9: Convergence of the SelfReflect score with N=M= 5 and an increasing number of
queries we evaluate on. Answer Distributions of Qwen 2.5 72B Instruct on Natural Questions.
0 1000 2000 3000 4000 5000
Number of Questions/Answer Distributions Averaged Over0.0700.0750.0800.0850.0900.095Average SelfReflect ScoreCoT
Basic
GreedySample & Summarize (n=50)
Figure 10: Convergence of the SelfReflect score with N=M= 1 and an increasing number of
queries we evaluate on. Answer Distributions of Qwen 2.5 72B Instruct on Natural Questions.
23C Which LLM Jjudge to use to generate SelfReflect logits
Table 6: To find out which LLM judge produces the best logits, we test how often SelfReflect correctly
distinguishes a good (top) from a bad (bottom) summarywith different possible judges LLM Jthat
calculate the SelfReflect metric, across different LLM’s LLM θwhose answer distributions are being
summarized. Automatically generated summaries on Natural Questions, following Table 1. Results
for Phi 4 14B as a judge for Llama 3.1 8B Instruct are pending and will be added.
LLM θ LLM JGood summaries vs
bad summariesGood vs
almost-goodDetailed vs
truncatedVerbalized uncertainty vs
only majority answerVerbalized vs
or-concatenatedPercentage vs
or-concatenated
Llama 3.1 8B Instruct Llama 3.1 8B Instruct 99.73% ±0.37% 96.13% ±1.38% 94.92% ±3.96% 97.39% ±2.91% 80.00% ±7.31% 87.83% ±5.98%
Phi 4 14B Llama 3.1 8B Instruct 99.75% ±0.49% 97.50% ±1.53% 100.00% ±0.00% 96.30% ±5.03% 51.85% ±13.33% 66.67% ±12.57%
Qwen2.5 7B Instruct Llama 3.1 8B Instruct 99.70% ±0.34% 94.10% ±1.46% 100.00% ±0.00% 97.52% ±2.77% 47.93% ±8.90% 80.99% ±6.99%
Llama 3.1 8B Instruct Phi 4 14B % 99.87±0.26% %94.93±1.57% 94.92% ±3.96% 99.13% ±1.70% 87.83% ±5.98% 86.09% ±6.32%
Phi 4 14B Phi 4 14B 100.00% ±0.00% 94.25% ±2.28% 94.44% ±5.33% 48.15% ±13.33% 59.26% ±13.11% 59.26% ±13.11%
Qwen2.5 7B Instruct Phi 4 14B 99.70% ±0.34% 93.10% ±1.57% 98.71% ±1.78% 95.04% ±3.87% 59.50% ±8.75% 75.21% ±7.69%
Llama 3.1 8B Instruct Qwen2.5 7B Instruct 100.00% ±0.00% 95.73% ±1.45% 95.76% ±3.64% 95.65% ±3.73% 80.87% ±7.19% 85.22% ±6.49%
Phi 4 14B Qwen2.5 7B Instruct 99.25% ±0.85% 96.75% ±1.74% 98.59% ±2.74% 94.44% ±6.11% 70.37% ±12.18% 77.78% ±11.09%
Qwen2.5 7B Instruct Qwen2.5 7B Instruct 99.80% ±0.28% 94.20% ±1.45% 98.06% ±2.17% 95.04% ±3.87% 74.38% ±7.78% 83.47% ±6.62%
Llama 3.1 8B Instruct Qwen2.5 72B Instruct 99.87% ±0.26% 96.13% ±1.38% 97.46% ±2.84% 99.13% ±1.70% 86.96% ±6.15% 78.26% ±7.54%
Phi 4 14B Qwen2.5 72B Instruct 98.75% ±1.09% 97.50% ±1.53% 98.59% ±2.74% 96.30% ±5.03% 72.22% ±11.95% 55.56% ±13.25%
Qwen2.5 7B Instruct Qwen2.5 72B Instruct 99.80% ±0.28% 94.40% ±1.43% 99.35% ±1.27% 99.17% ±1.62% 75.21% ±7.69% 66.94% ±8.38%
A mandatory component to calculate the SelfReflect metric is a judge LLM Jthat predicts which
masked-out words are possible, given either a summary or a concatenation of samples. This judge
needs to be able to "understand" both the details of the answer and the probabilistic aspect of this task,
all the while not overwriting its context information with its own world knowledge when making the
prediction. The choice of the judge can thus be seen as a hyperparameter to be optimized to produce
SelfReflect scores that are as discriminative as possible between good and bad and almost-good
summaries. We test four different judges in this section, Llama 3.1 8B Instruct, Phi 4 14B, Qwen 2.5
7B Instruct (which we ultimately use in the paper), and Qwen 2.5 72B Instruct. We generate answer
distributions on Natural Questions for different LLM θ(Llama 3.1 8B Instruct, Phi 4 14B, and Qwen
2.5 7B Instruct), then use Gemini 2.0 to generate summaries like in Section 4.1, and calculate how
often SelfReflect correctly tells apart good from bad (or almost-good) summaries.
Table 6 shows that SelfReflect is very robust to the choice of the judge LLM: All judges can tell apart
good from bad summaries in almost all cases. In particular, there is also no indication of a “home-
bias”, i.e., that a judge would perform better in judging answer distributions that it sampled itself.
This, along with the fact that especially bad summaries, which explicitly introduce statements that
are wrong and go against the judge’s world knowledge, are almost always judged as worse than good
summaries, shows that there is no world-knowledge leakage. We attribute this to LLMs’ abilities to
predict from their context, and to the fact that SelfReflect runs its prediction both conditional on the
summary and conditional on the answer distribution, so that should there be any world knowledge
leakage, it would likely be equal and removed.
To make the choice of which LLM judge to use, we pay particular attention to the last three columns
of Table 6: Comparing a verbalized or percentage uncertainty answer to an or-concatenated answer is
among the most subtle challenges and tests whether the judge correctly infers the relative probabilities
in both the answer distributions and the summaries, even when they are not explicit. Here we see that
the Qwen family sets itself slightly off Phi 4 and Llama 3.1. Within the Qwen family, the 7B model
is within the confidence interval of the 72B model (with a mean result better for percentage vs or-
concatenated, and worse for the other two), so we use it in the main paper due to its lower inference
cost. We note that we also tried using a Qwen 2.5 0.5B Instruct judge, however, this small model
was not able to tell apart good from bad summaries. Finally, we note that there exists a research
opportunity in developing an LLM judge specialized to perform the SelfReflect judging, either to
compress the 7B model into a smaller and faster one, or to improve the last bits of performance on
challenging cases. However, we decide against this in this paper, since a specialized model would
increase the complexity of our method and add a dependency on a particular model (-checkpoint),
which is likely to be outdated soon in the fast-moving field of LLMs.
24D Example of SelfReflect scores per masked-out word
To deepen the understanding of how the SelfReflect score judges summaries, we provide a worked
example. We break down the SelfReflect score to the penalty it gives to each masked-out word. To
simplify this educational example, we use only N=M= 7samples and make the answers in the
conditioning of the prompt equal to the masked-out test answers.
The question posed to the LLM is “Who received the first Nobel Prize in physics?” . As can be seen
below, the LLM’s answer distribution includes Wilhelm Conrad Röntgen as most likely answer, as
well as Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel as additional possibilities,
and details on their work. Let us now first look at how SelfReflect judges a relatively bad summary
of this distribution which just returns the greedy answer “Wilhelm Conrad Röntgen received the first
Nobel Prize in Physics. ” . Overall, SelfReflect assigns this bad summary a distance of 0.102(or taken
×1000 like in Table 4: 102). This score is due to SelfReflect detecting that Hendrik Antoon Lorentz
and Pieter Zeeman or Henri Becquerel are not predictable from the summary, and neither the details
of the works, as we can see in the per-word penalties below (darker red = higher penalty).
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
The first Nobel Prize in Physics was awarded to Wilhelm Conrad Röntgen.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics for his discovery of X-rays.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics in recognition of his discovery of X-rays which
are now named after him.
It was Henri Becquerel who received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics for their work on the
effect of magnetic fields on the spectrum of light emitted by atoms, known as the Zeeman effect.bad summary: Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
Figure 11: SelfReflect per-word penalties on how far the prediction of each masked-out word based
on the summary “Wilhelm Conrad Röntgen received the first Nobel Prize in Physics. ” differs from
the prediction based on the samples from the internal distribution. Total penalty: 0.102.
We can now improve this summary by adding the two other possibilities, namely “It’s most likely that
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics. But the laureates could also have
been Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel. ” . With this better summary,
SelfReflect correctly removes the penalty on Hendrik Antoon Lorentz, Pieter Zeeman, and Henri
Becquerel. But it correctly still penalizes the summary for not mentioning the details of any of the
works. This results in an overall score of 0.084(or 84).
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
The first Nobel Prize in Physics was awarded to Wilhelm Conrad Röntgen.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics for his discovery of X-rays.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics in recognition of his discovery of X-rays which
are now named after him.
It was Henri Becquerel who received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics for their work on the
effect of magnetic fields on the spectrum of light emitted by atoms, known as the Zeeman effect.mid summary: It's most likely that Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
But the laureates could also have been Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel.
Figure 12: SelfReflect per-word penalties on how far the prediction of each masked-out word based on
the summary “It’s most likely that Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
But the laureates could also have been Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel. ”
differs from the prediction based on the samples from the internal distribution. Total penalty: 0.084.
Having added all answer possibilities, we can now add details mentioned in the individual answers.
As a good summary, we give “It’s most likely that Wilhelm Conrad Röntgen received the first Nobel
Prize in Physics in recognition of his discovery of X-rays which are now named after him. But the
laureates could also have been Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel. ” .
SelfReflect removes the penalty on X-rays, which the summary mentions. The remaining penalty of
250.078(or 78) is due to the summary still not mentioning the details on the Zeeman effect, plus some
remaining noise mostly on the names.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
The first Nobel Prize in Physics was awarded to Wilhelm Conrad Röntgen.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics for his discovery of X-rays.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics in recognition of his discovery of X-rays which
are now named after him.
It was Henri Becquerel who received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics for their work on the
effect of magnetic fields on the spectrum of light emitted by atoms, known as the Zeeman effect.good summary: It's most likely that Wilhelm Conrad Röntgen received the first Nobel Prize in Physics
in recognition of his discovery of X-rays which are now named after him. But the laureates could
also have been Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel.
Figure 13: SelfReflect per-word penalties on how far the prediction of each masked-out word based
on the summary “It’s most likely that Wilhelm Conrad Röntgen received the first Nobel Prize in
Physics in recognition of his discovery of X-rays which are now named after him. But the laureates
could also have been Hendrik Antoon Lorentz and Pieter Zeeman or Henri Becquerel. ” differs from
the prediction based on the samples from the internal distribution. Total penalty: 0.078.
These examples demonstrate that SelfReflect punishes summaries for the correct reasons: Either
when they don’t mention all possibilities or all details of the actual internal answer distribution. We
have seen in Sections 4.1 and 4.2 that SelfReflect also correctly punishes deviations from the relative
frequencies. To this end, let us modify the second summary which previously had a score of 0.084(or
84) and state that Henri Becquerel was the most likely first Nobel laureate, which is in conflict with
the LLM’s internal answer distribution: “It’s most likely that Henri Becquerel received the first Nobel
Prize in Physics. But the laureates could also have been Hendrik Antoon Lorentz and Pieter Zeeman
or Wilhelm Conrad Röntgen. ” . This correctly leads to higher penalties on Wilhelm Conrad Röntgen
and Henri Becquerel because both of their implied probabilities are off (while keeping the same
penalties on Hendrik Antoon Lorentz and Pieter Zeeman, as well as the in both cases unmentioned
details on their works) and worsens the SelfReflect score to 0.092(or 92).
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics.
The first Nobel Prize in Physics was awarded to Wilhelm Conrad Röntgen.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics for his discovery of X-rays.
Wilhelm Conrad Röntgen received the first Nobel Prize in Physics in recognition of his discovery of X-rays which
are now named after him.
It was Henri Becquerel who received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics.
Hendrik Antoon Lorentz and Pieter Zeeman received the first Nobel Prize in Physics for their work on the
effect of magnetic fields on the spectrum of light emitted by atoms, known as the Zeeman effect.flipped summary: It's most likely that Henri Becquerel received the first Nobel Prize in Physics.
But the laureates could also have been Hendrik Antoon Lorentz and Pieter Zeeman or Wilhelm Conrad
Röntgen.
Figure 14: SelfReflect per-word penalties on how far the prediction of each masked-out word based
on the summary “It’s most likely that Henri Becquerel received the first Nobel Prize in Physics. But
the laureates could also have been Hendrik Antoon Lorentz and Pieter Zeeman or Wilhelm Conrad
Röntgen. ” (note that Henri Becquerel is in fact not the most likely; it is Wilhelm Conrad Röntgen)
differs from the prediction based on the samples from the internal distribution. Total penalty: 0.092.
This demonstrates that SelfReflect’s score works as intended, not only on the dataset or question level
as studied in the main paper, but also on a word-level granularity. This example is a regular case, one
of the 95%+ (see Table 1) where SelfReflect correctly scores the summaries. We note, however, that
there are around 5% of questions where it does not score correctly. In most of these cases, the scores
of a good and a slightly worse summary are very close to one another and the mis-decision is mostly
due to noise. We thus recommend to run SelfReflect over 1000 questions per dataset, as noted in
Appendix B and the main paper, in order to smoothen out some of the remaining noise.
26E Implementation details
E.1 SelfReflect score
To calculate the SelfReflect score, in every masked-out task we run the two prompts in Fig. 3 through
a judge LLM, which is by default Qwen 2.5 7B-Instruct. This makes the judge predict the logits
over the vocabulary size for the current token of the fill-in word. If a fill-in word consists of multiple
tokens, where we add the tokens of the true fill-in word one after another into the autoregressive
context of the assistant answer. Given the two fill-in token vectors conditioned either on the summary
or on the concatenated answers, we apply a temperature of 5 to flatten it. This is in order to give some
weight to synonyms, since instruct-tuned LM judges otherwise would give nearly probability 1 to
only one possible token (in which case SelfReflect would still be valid, but simplify into comparing
whether the two contexts lead to predicting the exactly same word). We found that a temperature of
τ= 5improves the SelfReflect score, making it able to discern good from almost-good summaries
more often on a validation dataset. We then softmax the flattened logit vectors and calculate the 1-
Wasserstein distance between the log probability vectors. Since these are categorical vectors, the
1-Wasserstein distance simplifies into the L1distance, times 0.5. We repeat this over all tokens of
a masked-out word, then over all masked-out words of each of the M= 50 answers (that are not
stopwords), then across all 1000 questions of a dataset. The global average gives the SelfReflect score.
E.2 SR sampling-free score
The sampling-free ablation of the SelfReflect metric also gives two prompts to a judge LLM to
calculate the masked-in task. The difference is that the prompt which in SelfReflect contains the
sampled answers does not contain sampled answers. Instead it just gives the question and then the
masked-out task.
E.3 SR-PMI score
The PMI ablation of the SelfReflect metric uses no masked out task. Instead it poses the question,
gives either the summary or the sampled answers as background information in context, and then
measures the logit vectors assigned to each token of each of the M= 50 answers. In other words,
the answers are not given word-by-word with masked-out tasks, but measured as one full answer. As
for SelfReflect, we then calculate the 1-Wasserstein distance between the flattened logit vectors and
average.
E.4 SR-P(True) score
In the P(True) ablation of SelfReflect, we turn the generative masked-out task into a discriminative
one. We first generate three candidate words to fill in the masked word: One is the true masked word,
one is a word sampled from the masked-out task prompt given the summary and the last is a word
sampled from the masked-out task prompt given the distribution samples. With these candidate fill-
in words, we then run two prompts, one conditional on the summary and one on the answers, to let
the judge LLM predict how likely they fit in, see Fig. 15. As in the normal SelfReflect, this gives a
distribution over the vocabulary size, concentrated on "True" and "False" tokens. We then compare the
two flattened logit vectors via the 1-Wasserstein distance and average as in the original SelfReflect.
E.5 Embedding score
We compare the gte-Qwen2-7B-instruct [Li et al., 2023] embedding of the summary to the embedding
of the samples of the distribution, normalize them and take the inner product to form cosine distances.
We average over all samples. The reason why we select this particular embedding model is that
at the time of submission it was the best-performing open-source model on the MTEB benchmark
[Enevoldsen et al., 2025].
27<|im_start|>user Who was the first Australian prime minister? <|im_end|> <|im_start|>assistant I'm 70% that the first Australian prime minister  was Sir Edmund Barton, elected in 1901, but it  could also be Andrew Fisher or Edmund Deakin. <|im_end|> <|im_start|>user Here is a text with a missing word, denoted as "_":  The first Australian Prime Minister Edmund _ was elected in 1901. Here is a candidate word to fill in the missing word "_": Deakin Respond whether the candidate word would fit in as the missing word (True) or not (False), based on the answer you gave above. Respond only with True or False. <|im_end|> <|im_start|>assistant <|im_start|>user Who was the first Australian prime minister? Sample 50 answers to this question. <|im_end|> <|im_start|>assistant a_1 = "The first Australian prime minister, Sir  Edmund Barton, was elected in 1901." [...] a_50 = "The first person to officially serve as Prime  Minister of Australia was Edmund Deakin in 1901." <|im_end|> <|im_start|>user Here is a text with a missing word, denoted as "_":  The first Australian Prime Minister Edmund _ was elected in 1901. Here is a candidate word to fill in the missing word "_": Deakin Respond whether the candidate word would fit in as the missing word (True) or not (False), based on the answer you gave above. Respond only with True or False. <|im_end|> <|im_start|>assistant candidate summary si.i.d. samples from  pasted into contextpθ(A|q)
Predicted token vector: pJ(X|q,s,a−i,cj)=(0.69,0.30,...)Predicted token vector: pJ(X|q,a(1:N),a−i,cj)=(0.82,0.16,...)mSelfReflect(s)=𝒲1(pJ(Ai|q,s,a−i),pJ(Ai|q,a(1:N),a−i))pJ(Ai|q,s,a−i)pJ(Ai|q,a(1:N),a−i)question q
discriminative task with masked-out answer  and candidate word b−icj"True""False""True""False"mSR-P(True) (s) =W1
pJ(X|q, s, a −i, cj)1/τ, pJ
X|q, a(1:N), a−i, cj1/τ
Figure 15: The P(True) ablation of SelfReflect adds a candidate word cjinto the context and asks the
judge LLM to classify whether this word fits as masked-out word or not. It compares the probability
vectors predicted given either the summary or the concatenated samples.
E.6 Summarization score
We follow the prompts of Jain et al. [2023] that prompt an LLM to judge a summary in terms of
consistency, fluency, relevance, and coherence with a few-shot example. We then normalize all scores
to [0, 1] and average them to get the summarization score.
Prompt for the ’Summarization’ metric in Table 1 to judge fluency.
Fluency measures the quality of individual sentences, and whether they are
well-written and grammatically correct. Rate the summary of a given text on
a scale of 0 to 1 on fluency.
Here are some examples: <4 few-shot examples>
Now here is the summary whose fluency you are supposed to rate:
Summary: {summary}
Fluency:
Prompt for the ’Summarization’ metric in Table 1 to judge coherence.
Rate the following summaries on a scale from 0 to 1 on coherence, with a
higher value corresponding to higher coherence. Coherence is a collective
quality of all sentences. To score highly on it, the summary should be
well-structured and well-organized. It should not just be a heap of related
information, but should build from sentence to sentence to form a coherent
body of information about the topic.
Here are some examples: <4 few-shot examples>
Now here is the summary whose coherence you are supposed to rate:
Summary: {summary}
Coherence:
Prompt for the ’Summarization’ metric in Table 1 to judge consistency.
Consistency measures whether the details in the summary reproduce the facts
present in the text accurately. Rate the summary of given text on a scale
from 0 to 1 on consistency.
28Here are some examples: <4 few-shot examples>
Now here is the text and summary whose consistency you are supposed to rate:
Text: We received many answers to our question ’{question}’. Here they are:
x_1 = ’{answer}’
...
x_{n_answers} = ’{answer}’
Summary: {summary}
Consistency:
Prompt for the ’Summarization’ metric in Table 1 to judge relevance.
Relevance is the quality of a summary to capture important information from a
reference text. Rate the summary on a scale from 0 to 1 on relevance.
Here are some examples: <4 few-shot examples>
Now here is the text and summary whose relevance you are supposed to rate:
Text: We received many answers to our question ’{question}’. Here they are:
x_1 = ’{answer}’
...
x_{n_answers} = ’{answer}’
Summary: {summary}
Relevance:
E.7 LM Judge score
We follow Xu et al. [2024] to build a metric that asks an LM judge to chain-of-thoughts think and rate
how well a summary matches a distribution of answers, including a few-shot example. The prompt is
shown below.
Prompt for the ’LM Judge’ metric in Table 1.
Your task is to analyze whether a summarized answer correctly contains all
the possibilities that len(answers) individual answers to a question mention.
Note that some individual answers occur more often than other individual
answers. You should output a score from 0 to 10, indicating whether the
summarized answer mentions all possibilities and whether it correctly
outlines which are the most often appearing individual answers and which
appear less often. A higher score is means the summarized answer matches the
distribution of individual answers better.
Also note that some individual answers may be factually wrong. Do not
correct those, just report how good the summarized answer matches the
individual answers.
You should first provide your reasoning for how well the summarized answer
matches the distribution over individual answers, and then assign a score
based on this reasoning. The output should be in the following format:
Reason: [REASON]
Score: [SCORE]
Here is an example:
Question: <Example question>
Individual answers:
<Example answer samples>
Summarized answer: <Example summary>
29Then your output can be:
Reason: The summarized answer mentions the most likely possibility, and
it also correctly mentions that this is the most likely one. For other
possibilities, it mentions Wilhelm Conrad Röntgen, but does not mention that
he got the award for his discovery of x-rays, which the individual answers do
mention. It also does not mention the possibility of Hendrik Antoon Lorentz
and Pieter Zeeman, which the individual answers mention.
Score: 8
Now consider the following case:
{question}
Individual answers:
x_1 = ’{answer}’
...
x_{n_answers} = ’{answer}’
Summarized answer: ’{summary}’
Please provide the reason and the score of how good the summarized answer
matches the distribution of individual answers.
E.8 Optimal Transport score
The optimal transport metric consists of two steps. First, we break down a summary into a distribution
of statements and their probabilities. This is done with the following prompt.
Prompt for the ’Optimal transport’ metric in Table 1 to split a summary into core statements
and their probabilities.
Question: {question}
Here is some background information. This background information defines a
distribution of possible answers you can later sample from:
{summary}
Now, split this distribution up into its mutually fundamental statements and
the explicitly or implicitly connected probabilities.
Split it up such that each statement is mutually exclusive and the
probabilities sum to 1.
Include an ’I don’t know’ statement with the remaining percentage if the
background information explicitly mentions not being certain.
Return a json file with a list of dictionaries, where in every dictionary the
first key is called ’prob’ and includes the numerical probability and the
second key is ’statement’ and includes a string of the fundamental statement.
In the second step, we use an NLI model to calculate an entailment probability in [0, 1] be-
tween how much each sample answer entails each statement and vice versa. We multiply (1−
entailment probability )of both directions to get an distance score for each sample answer and state-
ment. This defines a distance matrix with the statements as rows and the sample answers as columns.
Besides a distance matrix, optimal transport also requires marginals for both rows and columns. For
the rows, we use the probabilities assigned to the statements in the above prompt. For the columns,
we assign each individual sample answer a uniform probability. We then compute the earth movers
distance using Flamary et al. [2021]. This matches sample answers to summary statements in such a
manner that the marginals are preserved and that overall all pairs in sum have the smallest possible
distance. The resulting overall distance then tells how far the answer samples are from the summary.
E.9 Licensing information
Table 7 contains licensing information for models used in this paper.
30Table 7: Licencing information for models used in this work
LLM License Reference
DeepSeek R1 Distill Qwen
2.5 32BMIT DeepSeek-AI et al. [2025]
Gemma 3 family https://ai.google.dev/
gemma/termsGemma Team et al. [2025]
Gemini 2.0 Flash Apache 2.0
Ministral 8B Instruct 2410 https://mistral.ai/static/
licenses/MRL-0.1.mdJiang et al. [2024]
Llama 3.1 70B Instruct https://www.llama.com/
llama3_1/license/Meta AI [2024a]
Llama 3.3 70B Instruct https://www.llama.com/
llama3_3/license/Meta AI [2024b]
Llama 4 Scout 17b 16e In-
structhttps://www.llama.com/
llama4/license/Meta AI [2025]
Phi-4 MIT Abdin et al. [2024]
gte Qwen 2 7B Instruct Apache-2.0 Li et al. [2023]
Qwen 2.5 family Apache-2.0 Yang et al. [2024a]
Qwen 3 family Apache-2.0 Qwen Team [2025a]
QwQ 32B Apache-2.0 Qwen Team [2025b]
F Rating good and bad summaries written by humans
Table 8: Mirroring Table 1, we compare how often our SelfReflect metric, and other possible metrics,
discriminates good from bad summaries of answer distributions. In this version, the summaries
are written by humans rather than by Gemini, on a disjoint set of questions. Mean ±95% interval.
Confidence intervals are larger than in Table 1 because we have less manually written summaries of
answer distributions than the automated ones in Table 1.
MetricGood summaries vs
bad summariesGood vs
almost-goodDetailed vs
truncatedVerbalized uncertainty vs
only majority answerVerbalized vs
or-concatenatedPercentage vs
or-concatenated
Summarization 98.20% ±1.43% 60.18% ±5.25% 70.00% ±20.08% 24.14% ±7.93% 55.17% ±18.10% 51.72% ±18.19%
LM Judge 98.50% ±1.30% 54.19% ±5.34% 55.00% ±21.80% 34.48% ±17.30% 37.93% ±17.66% 24.14% ±15.58%
Opt. Transport 78.14% ±4.43% 57.19% ±5.31% 10.00% ±13.15% 58.62% ±17.93% 79.31% ±14.74% 72.41% ±16.27%
Embedding 74.85% ±4.65% 44.01% ±5.32% 60.00% ±21.47% 20.69% ±14.74% 41.38% ±17.93% 13.79% ±12.55%
SR-PMI 85.33% ±3.79% 60.18% ±5.25% 75.00% ±18.98% 3.45% ±6.64% 24.14% ±15.58% 31.03% ±16.84%
SR-sampling-free 92.22% ±2.87% 80.24% ±4.27% 75.00% ±18.98% 62.07% ±17.66% 62.07% ±17.66% 58.62% ±17.93%
SR-P(True) 47.90% ±5.36% 58.38% ±5.29% 35.00% ±20.90% 96.55% ±6.64% 82.76% ±13.75% 79.31% ±14.74%
SelfReflect 99.70% ±0.59% 94.61% ±2.42% 95.00% ±9.55% 86.21% ±12.55% 93.10% ±9.22% 82.76% ±13.75%
In Table 1 in the main paper, we use Gemini 2.0 Flash to generate various types of good and bad
summaries from sampled answers. We choose an automated LLM approach because it is more
scalable (with accordingly preciser 95% intervals) and reproducible than manual annotation. For
reproducibility, we also report the prompts below. To ensure the quality of the results, we have,
however, also replicated the experiments where we wrote the summaries manually for 334 questions
of the Natural Questions dataset, on a disjoint split from those in Table 1. Table 8 reports the
results on how often SelfReflect, and other metrics, rated good summaries as better than their worse
counterparts. The results are analogous to those in the main paper in that SelfReflect scores the
highest on all metrics except on one where the P(True) ablation achieves a slightly better result.
Gemini 2.0 Flash prompt to generate ’good’ summaries in Table 1.
Below, you are given {n_answers} individual answers to the question
’{question}’.
Your goal is to summarize the {n_answers} answers into one answer.
31•The summarized answer should mention the main possibilities mentioned
by the {n_answers} answers. If a possibility is mentioned only once,
it can be skipped so that the summary remains concise.
•If some possibilities are mentioned much more often than others,
delineate which possibilities are more often found in the others by
using words like "most likely" and "could also be".
•The format of the summarized answer should be the same as each
individual answer. Provide only the answer, as if it were part
of the {n_answers} answers, without statements like "The answers
include...".
•Similarly, the summarized answer should use the same wording as the
original answers. If the original answer always uses "is situated",
then use "is situated" and not "is located".
•The summarized answer should reflect what the {n_answers} answers
deem possible. They can contain factually wrong options. Do not
correct those, just report the possibilities as they are given in the
answers.
Here are the {n_answers} answers:
x_1 = ’{answer}’
...
x_{n_answers} = ’{answer}’
Please provide the summarized answer.
Gemini 2.0 Flash prompt to generate an ’almost-good’ summary from a ’good’ summary in
Table 1. Also used to generate ’truncated’ from ’detailed’ summaries.
Below, you are given an answer to the question ’{question}’.
Your goal is to shorten the answer.
•If the answer mentions multiple possibilities, only return the main
possibility.
•If the answer includes a main answer and details, remove the details.
•The shortened answer should have the same format as the original
answer. If the original answer uses full sentences, the shortened
answer should also use a full sentence.
•The shortened answer should use the same wording as the original
answers. If the original answer always uses "is situated", then use
"is situated" and not "is located".
•The answer can contain factually wrong options. Do not correct those,
just shorten what the answer says, even if it is factually wrong.
Original answer: {good_summary}
Please provide the shortened answer.
Gemini 2.0 Flash prompt to generate a ’bad’ summary from a ’good’ summary in Table 1.
Below, you are given a response to the question ’{question}’.
Your goal is to change the answer.
•The answer should generally stay close to the original answer, with
only some key factual terms changed.
•The answer might already be factually wrong. But the goal is still
to change the key facts, so that the changed answer is different from
the original one.
32•The changed answer should have the same format as the original
answer. The structure should remain the same, only keywords should
be exchanged.
•The changed answer should also use the same wording as the original
answers for any non-factual words. If the original answer always
uses "is situated", then use "is situated" and not "is located".
Original answer: {good_summary}
Please provide the changed answer.
For verbalized, percentage, or-concatenated and majority answers, we first use a prompt to cluster the
answer distribution into clusters of statements and which answers belong to which cluster statement:
Gemini 2.0 Flash prompt to cluster samples from an answer distribution into a list of cluster
representatives and cluster memberships.
Below, you are given {n_answers} individual answers to the question
’{question}’. These {n_answers} answers can be seen as samples from an
answer distribution. Your goal is to cluster the distribution in two steps:
First step: Find the clusters and their representatives.
•Each cluster contains a set of answers that are essentially the same.
This means they may vary in the level of detail, but their primary
answer should be the same.
•Different clusters should be mutually exclusive answers.
•There are at least two clusters.
•The answer can contain factually wrong options. Do not correct them,
just cluster the answers as they are.
•Output a json file with each entry giving the "cluster_id" (cluster_1,
cluster_2, ...), and a "representative_answer", copy-pasted from the
answers below.
Second step: Match the answers to their clusters.
•Match each of the {n_answers} individual answers to one cluster
representative.
•Output a json file with each entry giving the "cluster_id" (cluster_1,
cluster_2, ...) and the "cluster_members", a list of [x_1, x_26,
...].
Here are the {n_answers} answers:
x_1 = ’{answer}’
...
x_{n_answers} = ’{answer}’
Please output the two json files, one after another. Each json file should
start with “‘json
We then count how many member each cluster has (manually in code as opposed to asking the LLM
since this increases accuracy), and provide lists of the representative answers of each cluster and their
relative frequencies to build the percentage and or-concatenated summaries. We sort the resulting list
frequency. For the majority answer, we directly return the representative answer of the highest-likely
cluster. The verbalized uncertainty summary is built by removing the percentages in their brackets
from the percentage summary.
Gemini 2.0 Flash prompt to generate ’percentage-uncertainty’ summaries in Table 1.
Below, you are given list of answers with their probabilities to the question
’{question}’.
33Your goal is to stitch these answers together into one sentence.
•The sentence should have the structure ’It is most likely that
<Answer A> (<probability of Answer A>% sure), but it could also
be <Answer B> (<probability of Answer B>% sure) or <Answer C>
(<probability of Answer C>% sure) or ...’
•Stick to the original wording of the answers as much as possible, but
you can add small words so that the sentence becomes a grammatically
coherent sentence.
•The answer can contain factually wrong options. Do not correct those,
just stitch together the answer options, even if it is factually
wrong.
List of answers:
[
{
’prob’: 0.72,
’statement’: ...
},
{
’prob’: 0.22,
’statement’: ...
}
]
Please provide the coherent sentence.
Gemini 2.0 Flash prompt to generate ’or-concatenated’ summaries in Table 1.
Below, you are given list of answers with their probabilities to the question
’{question}’.
Your goal is to stitch these answers together into one sentence.
•The sentence should have the structure ’Either <Answer A> or <Answer
B> or <Answer C> or ...’
•The sentence should be grammatically coherent.
•Stick to the original wording of the answers as much as possible.
•The answer can contain factually wrong options. Do not correct those,
just stitch together the answer options, even if it is factually
wrong.
List of answers:
[
{
’prob’: 0.72,
’statement’: ...
},
{
’prob’: 0.22,
’statement’: ...
}
]
Please provide the coherent sentence.
34G User study details
User studies were carried out using TryRating, with five raters per task. Raters were allowed to rate
as many tasks as they wanted. All raters were US-based English speakers, and were paid $18/hr.
The users were presented with the instructions shown in Fig. 16, which included two examples with
hand-crafted summaries (for space reasons, we include only one summary here).
Figure 16: Instructions for user study (truncated; actual instructions contained a second example,
which we have cut here for space).
To ensure quality responses, we constructed twenty “golden answer” tasks, where the summaries were
manually constructed to either fit the definition of “good”, “nearly good”, or “bad” summaries, as
described in Section 4.1. Ten of these questions were given as an entrance exam, with raters required to
answer the golden answer in 80% of tasks to proceed. The remaining ten questions were periodically
included as verification checks. A total of 215 raters passed the entrance exam and contributed ratings.
Confidence intervals were calculated using 100 bootstrapped samples.
35H Automatic summary generation
H.1 Experimental details
In this section, we denote sampling parameters as {T=1, topp=1, topk=None, minp=0} .
Non-reasoning/RLHF models For the models in Table 4, the same model is used for both gen-
erating the answers, and generating the summaries. We sample answers with {T=1, topp=1,
topk=None, minp=0} . For summaries generation, we use greedy decoding, i.e., {T=0, topp=1,
topk=None, minp=0} .
Reasoning/RLVR models For the models in Table 5, we also want to sample the answers
without reasoning and make use of the reasoning only for the summaries generation. For
Qwen3, it is possible to suppress the reasoning with tokenizer.apply_chat_template(...,
enable_thinking=False) . For QwQ-32B and DeepSeek-R1-Distill-Qwen2.5-32B, we did not
find a way to suppress reasoning and hence we sample the answers from Qwen2.5-32B-Instruct,
which is the RLHF model which served as a base model for the RLVR training. We sample answers
with {T=1, topp=1, topk=None, minp=0} .
For the Greedy summary generation, we use non-reasoning greedy decoding with the same model as
for the answers generation, i.e., for rows QwQ-32B and DeepSeek-R1-Distill-Qwen2.5-32B, we use
Qwen2.5-32B-Instruct.
For the Basic andSample & Summarize summaries generation, we use the reasoning mode of each
respective model. Unlike for RLHF models, we stray away from using greedy decoding for summary
generation, because the creators of the reasoning models we use warn that the use of greedy decoding
with reasoning “as it can lead to performance degradation and endless repetitions”. Hence, for
each model we use the respective recommended sampling parameters available on their respective
HuggingFace model card.
H.2 Prompts used
Prompt for the basic summary generation method in Section 5.
Please respond to the following question ’{question}’.
Your goal is to summarize all possible answers to this question:
•If there are multiple possible answers, the summarized answer should
mention the main possible answers. However, you do not have to list
possibilities that are too unlikely.
•If some possibilities are more likely than others, delineate which
possibilities are more more likely by using words like "most likely"
and "could also be".
•The format of the summarized answer should be the same as a normal
answer.
•If there is only clear answer to the question, just provide that
answer, without hedging across possibilities.
Please provide the summarized answer.
Prompt for the CoT summary generation method in Section 5.
Please respond to the following question ’{question}’.
Your goal is to first reason about all possible answers to this question and
then summarize them into a final answer:
•Reflect on whether there are multiple possible answers to this
question.
36•If there are multiple possible answers, the summarized answer should
mention the main possible answers. However, you do not have to list
possibilities that are too unlikely.
•If some possibilities are more likely than others, delineate which
possibilities are more more likely by using words like "most likely"
and "could also be".
•The format of the summarized answer should be the same as a typical
answer and be stand-alone.
•If there is only clear answer to the question, just provide that
answer, without hedging across possibilities.
The output should be in the following format:
Reasoning: [REASONING ABOUT WHICH POSSIBLITIES THERE ARE AND HOW LIKELY THEY
ARE]
Summary: [SUMMARIZED ANSWER]
Please provide the reasoning and then the summarized answer.
H.3 Results per dataset
Table 9: SelfReflect score ↓(×10−3, rounded for readability) for the SimpleQA dataset, averaged
across 1000 questions. The results in small font are relative to Greedy .
Model pθ(A|q) Single-decoding methods Sample & summarize
unimodal Greedy Basic CoT N= 10 N= 20
Qwen2.5 0.5B Instruct [Yang et al., 2024a] 1% 98 97 −1 95−3 99+1 99+1
Qwen2.5 1.5B Instruct [Yang et al., 2024a] 2% 98 97 0 93−5 90−8 89−8
Qwen2.5 3B Instruct [Yang et al., 2024a] 5% 101 101 0 99−1 93−8 91−9
Qwen2.5 7B Instruct [Yang et al., 2024a] 15% 98 102 +4 102 +3 93−5 92−6
Qwen2.5 14B Instruct [Yang et al., 2024a] 23% 96 101 +5 102 +7 88−8 87−9
Qwen2.5 32B Instruct [Yang et al., 2024a] 17% 103 108 +5 110 +8 95−8 94−8
Qwen2.5 72B Instruct [Yang et al., 2024a] 18% 95 99 +3 100 +5 88−8 87−9
Phi 4 [Abdin et al., 2024] 3% 99 99 0 97−2 89−10 87−12
Ministral 8B Instruct 2410 [Jiang et al., 2024] 1% 117 116 0 114−2 109−7 107−9
Llama 3.1 70B Instruct [Meta AI, 2024a] 16% 97 97 0 97+1 90−6 90−7
Llama 3.3 70B Instruct [Meta AI, 2024b] 29% 100 103 +3 113 +13 92−8 91−9
Llama 4 Scout 17B 16e Instruct [Meta AI, 2025] 20% 95 100 +5 103 +8 89−6 87−7
Gemma 3 1B Instruct [Gemma Team et al., 2025] 17% 123 134 +11 135 +11 124 0 118−6
Gemma 3 4B Instruct [Gemma Team et al., 2025] 25% 118 135 +16 138 +20 108−10 106−12
Gemma 3 12B Instruct [Gemma Team et al., 2025] 35% 117 129 +12 135 +18 112−5 112−5
Gemma 3 27B Instruct [Gemma Team et al., 2025] 49% 109 124 +16 134 +25 103−5 101−7
Table 10: SelfReflect score ↓(×10−3, rounded for readability) for the Natural Questions dataset,
averaged across 1000 questions. The results in small font are relative to Greedy .
Model pθ(A|q) Single-decoding methods Sample & summarize
unimodal Greedy Basic CoT N= 10 N= 20
Qwen2.5 0.5B Instruct [Yang et al., 2024a] 5% 92 90 −1 90−2 92 0 92 0
Qwen2.5 1.5B Instruct [Yang et al., 2024a] 13% 90 91 +1 89−1 85−5 84−6
Qwen2.5 3B Instruct [Yang et al., 2024a] 30% 95 96 +1 97+2 88−7 87−8
Qwen2.5 7B Instruct [Yang et al., 2024a] 32% 94 98 +4 101 +7 90−5 89−5
Qwen2.5 14B Instruct [Yang et al., 2024a] 56% 91 97 +6 100 +9 86−5 85−6
Qwen2.5 32B Instruct [Yang et al., 2024a] 50% 94 100 +6 104 +10 89−5 89−5
Qwen2.5 72B Instruct [Yang et al., 2024a] 48% 89 94 +5 98+9 84−5 83−6
Phi 4 [Abdin et al., 2024] 36% 89 88 −1 92+3 83−6 82−7
Ministral 8B Instruct 2410 [Jiang et al., 2024] 17% 101 99 −1 99−2 95−6 94−7
Llama 3.3 70B Instruct [Meta AI, 2024b] 68% 91 96 +5 104 +13 86−5 85−6
Llama 4 Scout 17B 16e Instruct [Meta AI, 2025] 59% 90 96 +6 104 +14 88−2 87−4
Gemma 3 1B Instruct [Gemma Team et al., 2025] 22% 113 126 +13 127 +14 113 +1 108−5
Gemma 3 4B Instruct [Gemma Team et al., 2025] 56% 106 123 +17 128 +22 100−6 99−7
Gemma 3 12B Instruct [Gemma Team et al., 2025] 57% 103 118 +14 121 +17 99−5 99−5
Gemma 3 27B Instruct [Gemma Team et al., 2025] 72% 100 116 +15 121 +21 98−3 97−3
37Table 11: SelfReflect score ↓(×10−3, rounded for readability) for the TriviaQA dataset, averaged
across 1000 questions. The results in small font are relative to Greedy .
Model pθ(A|q) Single-decoding methods Sample & summarize
unimodal Greedy Basic CoT N= 10 N= 20
Qwen2.5 0.5B Instruct [Yang et al., 2024a] 15% 97 96 −1 96−1 98+1 98 0
Qwen2.5 1.5B Instruct [Yang et al., 2024a] 36% 93 94 +1 93 0 87−6 87−7
Qwen2.5 3B Instruct [Yang et al., 2024a] 45% 97 99 +2 100 +2 91−6 90−7
Qwen2.5 7B Instruct [Yang et al., 2024a] 60% 95 98 +3 100 +5 91−3 91−4
Qwen2.5 14B Instruct [Yang et al., 2024a] 76% 88 93 +5 94+6 85−3 84−4
Qwen2.5 32B Instruct [Yang et al., 2024a] 79% 92 98 +6 101 +8 89−3 89−3
Qwen2.5 72B Instruct [Yang et al., 2024a] 85% 87 89 +3 90+4 84−3 83−4
Phi 4 [Abdin et al., 2024] 69% 89 88 −1 89 0 84−5 83−6
Ministral 8B Instruct 2410 [Jiang et al., 2024] 56% 104 103 0 103−1 99−4 98−5
Llama 3.1 70B Instruct [Meta AI, 2024a] 82% 89 88 −1 89 0 85−4 85−5
Llama 3.3 70B Instruct [Meta AI, 2024b] 92% 91 93 +2 95+4 89−2 88−3
Llama 4 Scout 17B 16e Instruct [Meta AI, 2025] 81% 89 92 +2 96+6 87−2 86−3
Gemma 3 1B Instruct [Gemma Team et al., 2025] 40% 112 127 +14 125 +12 113 0 108−4
Gemma 3 4B Instruct [Gemma Team et al., 2025] 76% 101 114 +13 118 +17 96−5 94−6
Gemma 3 12B Instruct [Gemma Team et al., 2025] 84% 95 103 +8 107 +13 94−1 93−1
Gemma 3 27B Instruct [Gemma Team et al., 2025] 93% 91 99 +8 104 +13 91 0 91 0
Table 12: Runtime of generating different summaries in seconds per prompt per GPU, averaged across
all three datasets with 1000 questions each. Note that some models are sharded across multiple GPUs
– in this case, to represent their total computational requirements, we report the summed runtime of
all GPUs, i.e., although a prompt may run through in one second using four GPUs, we will count it
as four seconds total. In small font, relative comparisons w.r.t. Greedy .
Model Single-decoding methods Sample & summarize
Greedy Basic CoT N=10
Qwen2.5 0.5B Instruct [Yang et al., 2024a] 0.93 1.26 ×1.35 1.26×1.36 1.79×1.93
Qwen2.5 1.5B Instruct [Yang et al., 2024a] 0.94 0.91 ×0.96 1.29×1.37 1.89×2.01
Qwen2.5 3B Instruct [Yang et al., 2024a] 0.84 0.85 ×1.02 1.00×1.20 1.82×2.18
Qwen2.5 7B Instruct [Yang et al., 2024a] 0.80 0.84 ×1.05 1.09×1.36 1.91×2.38
Qwen2.5 14B Instruct [Yang et al., 2024a] 0.96 1.01 ×1.05 1.33×1.38 2.44×2.53
Qwen2.5 32B Instruct [Yang et al., 2024a] 1.11 1.22 ×1.09 1.72×1.54 3.42×3.07
Qwen2.5 72B Instruct [Yang et al., 2024a] 1.68 1.63 ×0.97 3.48×2.07 4.41×2.62
Phi 4 14B [Abdin et al., 2024] 1.09 1.02 ×0.94 1.43×1.31 2.96×2.71
Ministral 8B Instruct 2410 [Jiang et al., 2024] 0.91 0.91 ×1.00 1.04×1.15 1.82×2.00
Llama 3.1 70B Instruct [Meta AI, 2024a] 4.16 4.45 ×1.07 10.58 ×2.54 10.59 ×2.55
Llama 3.3 70B Instruct [Meta AI, 2024b] 3.54 3.17 ×0.89 4.25×1.20 7.23×2.04
Llama 4 Scout 17B 16e Instruct [Meta AI, 2025] 3.45 3.92 ×1.13 5.67×1.64 9.05×2.62
Gemma 3 1B Instruct [Gemma Team et al., 2025] 0.89 0.86 ×0.97 0.93×1.04 1.74×1.96
Gemma 3 4B Instruct [Gemma Team et al., 2025] 0.95 0.93 ×0.98 1.09×1.15 1.96×2.06
Gemma 3 12B Instruct [Gemma Team et al., 2025] 1.12 1.14 ×1.02 1.54×1.38 2.43×2.17
Gemma 3 27B Instruct [Gemma Team et al., 2025] 1.32 1.36 ×1.03 2.26×1.71 3.09×2.34
38Table 13: Character length of different summaries in seconds per prompt per GPU, averaged across
all three datasets with 1000 questions each. In small font, relative comparisons w.r.t. Greedy .
Model Single-decoding methods Sample & summarize
Greedy Basic CoT N=10
Qwen2.5 0.5B Instruct [Yang et al., 2024a] 130.10 699.93 ×5.38 896.85 ×6.89 155.39 ×1.19
Qwen2.5 1.5B Instruct [Yang et al., 2024a] 152.44 151.44 ×0.99 528.68 ×3.47 310.16 ×2.03
Qwen2.5 3B Instruct [Yang et al., 2024a] 81.55 168.60 ×2.07 254.03 ×3.11 274.03 ×3.36
Qwen2.5 7B Instruct [Yang et al., 2024a] 93.16 149.61 ×1.61 198.33 ×2.13 178.72 ×1.92
Qwen2.5 14B Instruct [Yang et al., 2024a] 92.93 170.49 ×1.83 245.97 ×2.65 203.53 ×2.19
Qwen2.5 32B Instruct [Yang et al., 2024a] 64.53 145.30 ×2.25 186.80 ×2.89 138.33 ×2.14
Qwen2.5 72B Instruct [Yang et al., 2024a] 97.30 157.04 ×1.61 247.99 ×2.55 177.61 ×1.83
Phi 4 14B [Abdin et al., 2024] 124.85 203.35 ×1.63 281.96 ×2.26 273.05 ×2.19
Ministral 8B Instruct 2410 [Jiang et al., 2024] 49.15 100.54 ×2.05 179.57 ×3.65 130.08 ×2.65
Llama 3.1 70B Instruct [Meta AI, 2024a] 168.64 267.63 ×1.59 499.54 ×2.96 225.05 ×1.33
Llama 3.3 70B Instruct [Meta AI, 2024b] 113.47 229.45 ×2.02 277.31 ×2.44 152.70 ×1.35
Llama 4 Scout 17B 16e Instruct [Meta AI, 2025] 132.81 275.57 ×2.07 220.68 ×1.66 214.80 ×1.62
Gemma 3 1B Instruct [Gemma Team et al., 2025] 22.66 38.91 ×1.72 183.15 ×8.08 61.71 ×2.72
Gemma 3 4B Instruct [Gemma Team et al., 2025] 26.45 80.66 ×3.05 113.46 ×4.29 101.16 ×3.83
Gemma 3 12B Instruct [Gemma Team et al., 2025] 28.51 102.76 ×3.60 174.70 ×6.13 70.67 ×2.48
Gemma 3 27B Instruct [Gemma Team et al., 2025] 37.77 137.62 ×3.64 192.39 ×5.09 70.97 ×1.88
39I Experiment details of CoT deep dive
I.1 Results per dataset
Certain Uncertain
CoT SummaryCertain UncertainAnswer Distribution36% 49%
3% 13%TriviaQA
Certain Uncertain
CoT SummaryCertain UncertainAnswer Distribution16% 40%
9% 35%Natural Questions
Certain Uncertain
CoT SummaryCertain UncertainAnswer Distribution2% 20%
9% 69%SimpleQA
Figure 17: Confusion matrices between certainty of CoT summaries vs. actual answer distributions
for Qwen2.5 72B Instruct. Judged by Gemini 2.0 Flash for 500random questions per dataset.
We show the results per dataset in Fig. 17. By just looking at the marginals of the answer distribution,
we can infer that the question difficulty increases from TriviaQA to Natural Questions to SimpleQA,
as the number of questions with uncertain answer distributions increases from 16% to44% to78%
of questions. However, the majority of the CoT summaries are uncertain for all three datasets (for
62%,75%, and 89% of questions respectively), meaning that CoT isunderconfident on TriviaQA
and Natural Quesions—i.e., it suggests answers that do not have high probability under the true
distribution. A balance is clearly needed. The Greedy method, by contrast, is overconfident. While it
outperforms CoT on average, it underperforms Sample & Summarize since it will only present a single
option. In a similar way, simply making CoT more confident would likely lead to detrimental effects
on harder questions. Already as-is, there are relevant shares of questions in SimpleQA and Natural
Questions where CoT is certain while the actual answer distribution is not ( 9%each). Generating self-
reflective summaries that are truthful to the model’s internal uncertainty thus remains a challenging
task.
I.2 Prompts used to classify certainty vs. uncertainty
We used Gemini 2.0 Flash to judge if a summary mentions multiple semantically different answers
or not, and likewise if samples from the distributions do so. In particular, we provide it with the
corresponding context, and ask it to classify into three classes: A) Fully certain, B) not fully certain,
C) Completely uncertain. We then aggregate B and C to form the general “uncertain” category (we
found this to give better results than to directly give a binary task to Gemini, because it allows us
to explicitly tell how to handle the “not fully certain” edge case). We manually verified that the
classification is qualitatively correct. We show the prompts below.
Gemini 2.0 Flash prompt to classify CoT summary in Fig. 4. We call A. ’Certain’ and group
B. and C. into ’Uncertain’.
Below, you are given an answer to the question ’{question}’.
Your goal is to classify which type of answer this is:
A. The answer is certain, it only mentions one answer option.
B. The answer is not fully certain. It might mention one or two further
answer options but judges them as less likely.
C. The answer is very uncertain. It mentions many mutually exclusive answer
options, without a clear single most likely answer.
Ignore differences in form and style. You are only supposed to judge the
answer semantically.
Here is the answer: {CoT summary}
40Please respond with the category of what type of this answer this is.
Respond only with A, B, or C.
Gemini 2.0 Flash prompt to classify answer distribution samples in Fig. 4. We call A. ’Certain’
and group B. and C. into ’Uncertain’
Below, you are given {n_answers} individual answers to the question
’{question}’. These {n_answers} answers can be seen as samples from an
answer distribution.
Your goal is to classify which type of distribution this is:
A. The answers all do not contradict each other, up to one or two that differ
from the majority answer.
B. The answers give multiple mutually exclusive answer options, but there is
one answer option that is given in the majority of cases.
C. The answers give multiple mutually exclusive answer options, and they are
almost all different, without a clear majority answer.
The answers will have some natural variability. Ignore differences in form
and style. You are only supposed to judge if answer options are semantically
different.
Here are the {n_answers} answers:
x_1 = ’{answer}’
...
x_{n_answers} = ’{answer}’
Please respond with the category of what type of this distribution this is.
Respond only with A, B, or C.
41