arXiv:2505.21329v1  [cs.IR]  27 May 2025Something’s Fishy In The Data Lake:
A Critical Re-evaluation of Table Union Search Benchmarks
Allaa Boutaleb, Bernd Amann, Hubert Naacke and Rafael Angarita
Sorbonne Université, CNRS, LIP6, F-75005 Paris, France
{firstname.lastname}@lip6.fr
Abstract
Recent table representation learning and data
discovery methods tackle table union search
(TUS) within data lakes, which involves iden-
tifying tables that can be unioned with a given
query table to enrich its content. These meth-
ods are commonly evaluated using benchmarks
that aim to assess semantic understanding in
real-world TUS tasks. However, our analysis
of prominent TUS benchmarks reveals several
limitations that allow simple baselines to per-
form surprisingly well, often outperforming
more sophisticated approaches. This suggests
that current benchmark scores are heavily in-
fluenced by dataset-specific characteristics and
fail to effectively isolate the gains from seman-
tic understanding. To address this, we propose
essential criteria for future benchmarks to en-
able a more realistic and reliable evaluation of
progress in semantic table union search.
1 Introduction
Measurement enables scientific progress. In com-
puter science and machine learning, this requires
the creation of efficient benchmarks that provide
a stable foundation for evaluation, ensuring that
observed performance scores reflect genuine capa-
bilities for real-world tasks.
Table Union Search (TUS) aims to retrieve tables
Cfrom a corpus that are semantically unionable
with a query table Q, meaning they represent the
same information type and permit vertical concate-
nation (row appending) (Nargesian et al., 2018; Fan
et al., 2023a). As a top- kretrieval task, TUS ranks
candidate tables Cby a table-level relevance score
R(Q, C). This score is typically obtained by ag-
gregating column-level semantic relevance scores
R(CQ, CC)computed for each column CQof the
query table Qand each column CCof the candidate
tableC. The aggregation often involves finding an
optimal mapping between the columns of QandC,
for instance via maximum bipartite matching (Fanet al., 2023b). Successful TUS facilitates data inte-
gration and dataset enrichment (Khatiwada et al.,
2023; Castelo et al., 2021).
Recent research has introduced sophisticated
TUS methods with complex representation learn-
ing (Fan et al., 2023b; Khatiwada et al., 2024; Chen
et al., 2023) designed to capture deeper semantics.
However, current benchmarks often exhibit exces-
sive schema overlap, limited semantic complexity,
and potential ground truth inconsistencies, which
raises questions about whether they provide a re-
liable environment to evaluate advanced TUS ca-
pabilities. While state-of-the-art methodologies
leverage semantic reasoning to reflect the task spe-
cific challenges, observed high performance may
be significantly attributed to model adaptation to
specific statistical and structural properties inherent
within the benchmark datasets. This phenomenon
can confound the accurate assessment and poten-
tially underestimate the isolated contribution of im-
provements specifically targeting semantics-aware
TUS.
In this paper, we examine prominent TUS bench-
marks1, using simple baselines to assess the bench-
marks themselves. Our research questions are:
1.Do current TUS benchmarks necessitate deep se-
mantic analysis, or can simpler features achieve
competitive performance?
2.How do benchmark properties and ground truth
quality impact TUS evaluation?
3.What constitutes a more realistic and discrimi-
native TUS benchmark?
Our analysis2reveals that simple baseline meth-
ods often achieve surprisingly strong performance
by leveraging benchmark characteristics rather than
demonstrating sophisticated semantic reasoning.
1Preprocessed benchmarks used in our evaluation are avail-
able at https://zenodo.org/records/15499092
2Our code is available at: https://github.com/
Allaa-boutaleb/fishy-tusOur contributions include:
•A systematic analysis identifying limitations in
current TUS benchmarks.
•Empirical evidence showing simple embedding
methods achieve competitive performance.
•An investigation of ground truth reliability issues
across multiple TUS benchmarks.
•Criteria for developing more realistic and discrim-
inative benchmarks.
2 Related Work
We review existing research on TUS methods and
the benchmarks used for their evaluation, with a
focus on how underlying assumptions about table
unionability have evolved to become increasingly
nuanced and complex.
2.1 Methods and Their Assumptions
2.1.a) Foundational Approaches: Following early
work on schema matching and structural similar-
ity (Sarma et al., 2012), Nargesian et al. (2018)
formalized TUS by assessing attribute unionability
via value overlap, ontology mappings, and natural
language embeddings. Bogatu et al. (2020) incor-
porated additional features (e.g., value formats, nu-
merical distributions) and proposed a distinct aggre-
gation method based on weighted feature distances.
Efficient implementations of these methods rely
on Locality Sensitive Hashing (LSH) indices and
techniques like LSH Ensemble (Zhu et al., 2016)
for efficient table search.
2.1.b) Incorporating Column Relationships: Be-
yond considering columns individually, Khatiwada
et al. (2023) proposed SANTOS, which evaluates
the consistency of inter-column semantic relation-
ships (derived using an existing knowledge base
like YAGO (Pellissier Tanon et al., 2020) or by
synthesizing one from the data itself) across tables
to improve TUS accuracy.
2.1.c) Deep Table Representation Learning: Re-
cent approaches use deep learning for tabular un-
derstanding. Pylon (Cong et al., 2023) and Starmie
(Fan et al., 2023b) use contrastive learning for con-
textualized column embeddings. Hu et al. (2023)
propose AutoTUS, employing multi-stage self-
supervised learning. TabSketchFM (Khatiwada
et al., 2024) uses data sketches to preserve se-
mantics while enabling scalability. Graph-based
approaches like HEARTS (Boutaleb et al., 2025)
leverage HyTrel (Chen et al., 2023), representingtables as hypergraphs to preserve structural proper-
ties.
2.2 Benchmarks and their Characteristics
Benchmark creators make design choices at ev-
ery stage of the construction process that reflect
their understanding and assumptions about how
and when tables can and should be meaningfully
combined. We identify three primary construction
paradigms applied for building TUS benchmarks:
2.2.a) Partitioning-based: TUSSmall andTUSLarge
(Nargesian et al., 2018), as well as the SANTOS
benchmark (referring to SANTOS Small, as SAN-
TOS Large is not fully labeled) (Khatiwada et al.,
2023) partition seed tables horizontally or verti-
cally, labeling tables from the same original seed as
unionable with the seed table. This approach likely
introduces significant schema and value overlap,
potentially favoring methods that detect surface-
level similarity rather than deeper semantic align-
ment.
2.2.b) Corpus-derived: ThePYLON benchmark
(Cong et al., 2023) curates tables from GitTables
(Hulsebos et al., 2023) on specific topics. While
this avoids systematic partitioning overlap, the fo-
cus on common topics may result in datasets with
a general vocabulary that is well-represented in
pre-trained models. This can reduce the compara-
tive advantage of specialized table representation
learning and data discovery methods.
2.2.c) LLM-generated: UGEN (Pal et al., 2024)
leverages Large Language Models (LLMs) to gen-
erate table pairs, aiming to overcome limitations
of previous methods by crafting purposefully chal-
lenging scenarios, including hard negatives. How-
ever, this strategy introduces the risk of ground
truth inconsistency, as LLMs may interpret the cri-
teria for unionability differently across generations,
affecting label reliability.
2.2.d) Hybrid approaches: LAKEBENCH (Deng
et al., 2024) uses tables from OpenData3and
WebTable corpora4alongside both partitioning-
based synthetic queries and real queries sampled
from the corpus. However, such hybrid approaches
can inherit the limitations of their constituent meth-
ods: partitioning still risks high overlap, candidate-
based labeling may yield incomplete ground truth,
3https://data.gov/
4https://webdatacommons.org/webtables/Benchmark Overall Statistics Column Type (%) Size (MB)
Files Rows Cols Avg Shape Missing% Str Int Float Other
SANTOSNQ 500 2,736,673 5,707 5473 × 11 9.96 65.39 17.00 11.46 6.15 ∼422
Q 50 1,070,085 615 21402 × 12 5.79 73.17 15.93 8.46 2.44
TUS SmallNQ 1,401 5,293,327 13,196 3778 × 9 6.77 85.43 5.93 4.77 3.86 ∼1162
Q 125 577,900 1,610 4623 × 13 6.86 82.05 7.08 5.84 5.03
TUS LargeNQ 4,944 8,416,415 53,133 1702 × 11 12.53 90.12 5.10 3.57 1.21 ∼1459
Q 100 213,229 1,792 2132 × 18 14.87 90.46 3.68 4.13 1.73
PYLONNQ 1,622 85,282 16,802 53 × 10 0.00 58.74 25.36 15.90 0.00 ∼22
Q 124 11,207 880 90 × 7 0.00 75.68 22.95 1.36 0.00
UGEN V1NQ 1,000 7,609 10,315 8 × 10 5.79 91.68 3.27 4.29 0.76 ∼4
Q 50 405 546 8 × 11 5.87 90.48 4.58 4.21 0.73
UGEN V2NQ 1,000 18,738 13,360 19 × 13 8.16 82.40 11.71 5.50 0.39 ∼8
Q 50 5,363 665 107 × 13 4.14 84.96 10.23 2.41 2.41
LB-OpenDataNQ 4,832 351,067,113 89,757 72655 × 19 3.44 52.50 22.56 22.37 2.57 ∼80834
Q 3,138 238,576,481 61,815 76028 × 20 2.90 40.60 26.28 27.60 5.53
LB-WebtableNQ 29,686 1,039,347 387,432 35 × 13 0.01 61.07 26.28 12.64 0.01 ∼170
Q 5,488 335,187 56,174 61 × 10 0.00 40.43 43.06 16.51 0.01
Table 1: Table Union Search Benchmarks Summary. NQ = Non-query table, Q = Query table.
and the large scale of these benchmarks can intro-
duce practical evaluation challenges.
3 Methodology
As TUS methods become increasingly sophisti-
cated, the benchmarks used for their evaluation
may contain inherent characteristics that hinder the
accurate assessment of progress in semantic under-
standing. This section outlines our approach to ex-
amining prominent TUS benchmarks through anal-
ysis of their construction methods and strategic use
of simple baselines as diagnostic tools. The goal of
advanced TUS methods is to capture deep semantic
compatibility between tables, beyond simple lexi-
cal or structural similarity. Our investigation first
analyzes the various benchmark construction pro-
cesses to identify potential structural weaknesses,
then employs computationally inexpensive baseline
methods to reveal how these characteristics enable
alternative pathways to high performance, thereby
influencing evaluation outcomes.
3.1 Analyzing Benchmark Construction
We examine five prominent families of TUS bench-
marks and formulate hypotheses about their poten-
tial limitations based on their construction method-
ologies (Table 1). We identify three issues stem-
ming from these methodologies: (1) excessive
overlap ,(2) semantic simplicity , and (3) ground
truth inconsistencies , which we detail below:
3.1.a) Excessive Overlap: Benchmarks like
TUSSmall,TUSLarge,SANTOS , and the synthetic
query portion of the LAKEBENCH derivatives are
created by partitioning seed tables horizontally andvertically, with tables derived from the same orig-
inal seed designated as unionable pairs. We hy-
pothesize that this methodology inherently leads to
significant overlap in both schema (column names)
and content (data values) between query tables and
their ground truth unionable candidates.
To quantify this, we measure overlap using the
Szymkiewicz–Simpson coefficient for exact col-
umn names ( Overlapc, Eq. 1) and for values of
a given data type d(Overlapv, Eq. 2) between
ground truth pairs.
Overlapc(Q, C) =|Cols Q∩Cols C|
min(|Cols Q|,|Cols C|)(1)
Overlapv(Q, C) =|Vd
Q∩Vd
C|
min(|Vd
Q|,|Vd
C|)(2)
where Cols QandCols Cdenote the sets of col-
umn names in the query table Qand candidate
table Crespectively, and Vd
Q,Vd
Crepresent the
sets of unique values of data type din each ta-
ble. The coefficient equals 1.0 when one set is
fully contained within the other. Figure 1 shows
the distribution of overlap coefficients, with val-
ues≥50% indicating substantial overlap. As
expected, partitioning-based benchmarks exhibit
high overlap: over 90% of ground truth pairs share
≥50% of exact column names. For value over-
lap, we focus on string data types, which dominate
the benchmarks (Table 1). Here too, 45–60% of
query-candidate pairs share ≥50% of string tokens.
LAKEBENCH derivatives ( LB-O PENDATA,LB-
WEBTABLE ) show similar trends. Appendix A pro-
vides a detailed breakdown by data type. This high
surface similarity favors simple lexical methods/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000026/uni00000052/uni00000048/uni00000049/uni00000049/uni0000004c/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000057/uni0000002f/uni00000025/uni00000010/uni0000003a/uni00000048/uni00000045/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000002f/uni00000025/uni00000010/uni00000032/uni00000053/uni00000048/uni00000051/uni00000027/uni00000044/uni00000057/uni00000044/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000015/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000014/uni00000033/uni0000003c/uni0000002f/uni00000032/uni00000031/uni00000037/uni00000038/uni00000036/uni00000010/uni0000002f/uni00000037/uni00000038/uni00000036/uni00000036/uni00000024/uni00000031/uni00000037/uni00000032/uni00000036
/uni00000051/uni00000020/uni0000001b/uni0000001c/uni00000018/uni0000001a/uni0000001a/uni00000013/uni00000011/uni00000019/uni00000019/uni00000018/uni00000051/uni00000020/uni00000018/uni0000001b/uni00000013/uni00000019/uni0000001a/uni00000013/uni00000011/uni00000019/uni00000014/uni00000016/uni00000051/uni00000020/uni00000018/uni00000013/uni00000013/uni00000013/uni00000011/uni00000015/uni00000018/uni0000001a/uni00000051/uni00000020/uni00000018/uni00000013/uni00000013/uni00000013/uni00000011/uni00000017/uni00000015/uni00000014/uni00000051/uni00000020/uni00000018/uni00000013/uni0000001b/uni00000019/uni00000013/uni00000011/uni00000014/uni00000018/uni00000018/uni00000051/uni00000020/uni00000016/uni00000015/uni00000013/uni0000001c/uni0000001b/uni00000013/uni00000011/uni00000019/uni0000001b/uni00000013/uni00000051/uni00000020/uni00000014/uni0000001c/uni00000016/uni00000015/uni00000013/uni00000013/uni00000011/uni0000001b/uni00000016/uni0000001a/uni00000051/uni00000020/uni00000019/uni00000017/uni00000015/uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000014/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053
/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000026/uni00000052/uni00000048/uni00000049/uni00000049/uni0000004c/uni00000046/uni0000004c/uni00000048/uni00000051/uni00000057/uni0000002f/uni00000025/uni00000010/uni0000003a/uni00000048/uni00000045/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni0000002f/uni00000025/uni00000010/uni00000032/uni00000053/uni00000048/uni00000051/uni00000027/uni00000044/uni00000057/uni00000044/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000015/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000014/uni00000033/uni0000003c/uni0000002f/uni00000032/uni00000031/uni00000037/uni00000038/uni00000036/uni00000010/uni0000002f/uni00000037/uni00000038/uni00000036/uni00000036/uni00000024/uni00000031/uni00000037/uni00000032/uni00000036
/uni00000051/uni00000020/uni0000001b/uni00000018/uni00000018/uni0000001a/uni00000013
/uni00000013/uni00000011/uni00000017/uni0000001b/uni00000014/uni00000051/uni00000020/uni00000018/uni00000018/uni00000019/uni00000014/uni00000019
/uni00000013/uni00000011/uni00000017/uni00000018/uni00000016/uni00000051/uni00000020/uni00000018/uni00000013/uni00000013
/uni00000013/uni00000011/uni00000017/uni00000015/uni00000015/uni00000051/uni00000020/uni00000018/uni00000013/uni00000013
/uni00000013/uni00000011/uni00000016/uni00000016/uni00000017/uni00000051/uni00000020/uni00000018/uni00000013/uni0000001b/uni00000019
/uni00000013/uni00000011/uni00000015/uni00000018/uni00000015/uni00000051/uni00000020/uni00000016/uni00000015/uni00000013/uni0000001c/uni0000001b
/uni00000013/uni00000011/uni00000018/uni00000013/uni0000001c/uni00000051/uni00000020/uni00000014/uni0000001c/uni00000016/uni00000015/uni00000013
/uni00000013/uni00000011/uni00000019/uni00000013/uni00000014/uni00000051/uni00000020/uni00000019/uni00000017/uni00000015
/uni00000013/uni00000011/uni00000019/uni0000001a/uni00000017/uni00000036/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000039/uni00000044/uni0000004f/uni00000058/uni00000048/uni00000056/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053Figure 1: Distribution of Exact Column Name Overlap
(Top) and String Value Overlap (Bottom) Coefficients
for Ground Truth Unionable Pairs Across Benchmarks.
Colored circles represent mean values; numbers on the
right indicate total pairwise relationships considered.
and also influences advanced models by introduc-
ing repeated patterns in serialized inputs (Starmie),
data sketches (TabSketchFM), and graph structures
(HEARTS). Though designed for deeper semantics,
these models are affected by strong benchmark-
induced surface signals, making it hard to attribute
performance gains purely to nuanced reasoning.
3.1.b) Semantic Simplicity: Benchmarks derived
directly from large corpora, such as PYLON (Cong
et al., 2023) using GitTables (Hulsebos et al., 2023)
or the realquery portions of LAKEBENCH deriva-
tives using diverse public datasets, avoid the sys-
tematic overlap introduced by partitioning. How-
ever, we hypothesize that this construction method
introduces other limitations since (1) it often fo-
cuses on relatively common topics with simpler se-
mantics, reducing the need for specialized domain
knowledge, and (2) it generally draws from public
data sources likely included in the pre-training cor-
pora of large foundation models. Evidence from
specific benchmarks supports this concern. PY-
LON’s construction indeed avoids high overlap (Fig-
ure 1 shows lower overlap than partitioning-based
benchmarks). For LAKEBENCH , while the dis-
tinction between real andsynthetic queries was
unavailable during our analysis5, the significant
overall observed overlap suggests that synthetic,
partitioning-based queries constitute a large por-
tion of the benchmark. The semantic simplicity
evident in PYLON ’s topics and the public origins
5https://github.com/RLGen/LakeBench/issues/9of data in both PYLON andLAKEBENCH could
favor general-purpose models like BERT (Devlin
et al., 2019) or SBERT (Reimers and Gurevych,
2019), which have with a high, however unverifi-
able, probability encountered similar content dur-
ing pre-training. Consequently, the semantic chal-
lenge presented by these benchmarks might be
relatively low for models with strong general lan-
guage understanding – a contrast to documented
LLM struggles with non-public, enterprise-specific
data characteristics (Bodensohn et al., 2025), po-
tentially allowing off-the-shelf embedding models
to achieve high performance without fine-tuning.
3.1.c) Noisy Ground Truths: Ensuring accurate
and complete ground truth labels is challenging, es-
pecially with automated generation or large-scale
human labeling efforts as used in LLM-generated
benchmarks ( UGEN) and large human-labeled ones
(LAKEBENCH derivatives). We hypothesize that
ground truth in these benchmarks may suffer from
reliability issues, including incorrect labels (false
positives/negatives) or incompleteness (missed true
positives). For UGEN, generating consistent, ac-
curate positive and negative pairs (especially hard
negatives) is difficult. LLMs might interpret union-
ability rules inconsistently across generations, lead-
ing to noisy labels. For large-scale human labeling
with LB-O PENDATAandLB-W EBTABLE , the pro-
cess introduces two risks: incompleteness , if the
initial retrieval misses true unionable tables; and
incorrectness , if human judgments vary or contain
errors despite validation efforts. Evaluating perfor-
mance on UGEN andLAKEBENCH derivatives thus
requires caution. Scores are affected by label noise
or incompleteness; low scores reflect ground truth
issues and are therefore not solely attributable to
benchmark difficulty, while the maximum achiev-
able recall is capped by unlabeled true positives.
3.2 Baseline Methods for Benchmark Analysis
Based on the hypothesized benchmark issues iden-
tified above, we select some simple baseline meth-
ods to test benchmark sensitivity to different in-
formation types. While the (1) overlap and (2)
general semantics limitations can be directly exam-
ined through baseline performance, (3) the ground
truth integrity issue requires separate validation of
labels, which we address in Section 5.2. Detailed
implementation choices for all baseline methods
are in Appendix B.1.3.2.a) Bag-of-Words Vectorizers: To test whether
theExcessive Overlap enables methods sensi-
tive to token frequency to perform well on
partitioning-based benchmarks, we employ stan-
dard lexical vectorizers ( HashingVectorizer ,
TfidfVectorizer , and CountVectorizer ) from
scikit-learn6. These generate column embeddings
based on sampled string values, with a single ta-
ble vector obtained via max pooling across column
vectors. These baselines test whether high per-
formance can be achieved primarily by exploiting
surface signals without semantic reasoning.
3.2.b) Pre-trained Sentence Transformers: To ex-
amine whether the Semantic Simplicity allows
benchmarks from broad corpora to be effectively
processed by pre-trained language models, we use
a Sentence-BERT model ( all-mpnet-base-v27)
with three column-to-text serializations: (1)
SBERT (V+C): input includes column name and
sampled values; (2) SBERT (C): input is only
the column name; and (3) SBERT (V): input is
only concatenated sampled values. Column em-
beddings are aggregated using mean pooling to
produce a single table vector. These baselines as-
sess whether general semantic embeddings, with-
out task-specific fine-tuning, suffice for high per-
formance on benchmarks with general vocabulary.
4 Experimental Setup
To evaluate our hypotheses about benchmark limi-
tations, we employ both simple baseline methods
(Section 3.2) and advanced SOTA methods in a
controlled experimental framework. This section
details the benchmark datasets used, any necessary
preprocessing, the comparative methods, and our
standardized evaluation approach.
4.1 Benchmarks
Our analysis uses the benchmarks described in Sec-
tion 2.2, with post-preprocessing statistics sum-
marized in Table 1. Most benchmarks were used
as-is, but the large-scale LAKEBENCH derivatives
(LB-O PENDATA andLB-W EBTABLE ) required
additional preprocessing for feasibility and repro-
ducibility. The original datasets were too large
to process directly and included practical issues,
such as missing files, as well as characteristics that
complicated evaluation, such as many unreferenced
tables. We removed ground truth entries pointing
6Scikit-Learn Vectorizers Documentation
7all-mpnet-base-v2 on Hugging Faceto missing files (58 in LB-W EBTABLE ), and ex-
cluded unreferenced tables from the retrieval cor-
pus (removing ∼5,300 and >2.7M files from LB-
OPENDATA and LB-W EBTABLE , respectively).
This latter step was done purely for computational
feasibility; as a side effect, it simplifies the bench-
mark by eliminating tables that would otherwise
be false positives if retrieved. We also ensured that
each query table was listed as a candidate for itself.
These steps substantially reduced corpus size while
preserving evaluation integrity. The LAKEBENCH
variants considered in our study are those available
as of May 20, 20258. Future updates to the origi-
nal repository may modify dataset contents, which
yield different evaluation results.
Additionally, for LB-O PENDATA, we created a
smaller variant with tables truncated to 1,000 rows,
which we use in experiments alongside the original
version (Table 2). For TUSSmall andTUSLarge, we
followed prior work (Fan et al., 2023b; Hu et al.,
2023), sampling 125 and 100 queries, respectively.
For the other benchmarks, all queries were used.
4.2 Comparative Methods
To evaluate our baseline methods (Section 3.2), we
compare them against key TUS models previously
discussed in Section 2.1, focusing on SOTA meth-
ods. For each method, we optimize implementation
using publicly available code for fairness:
•Starmie (Fan et al., 2023b): We retrained the
RoBERTa-based model for 10 epochs on each
benchmark using recommended hyperparameters
and their “Pruning” bipartite matching search
strategy for generating rankings, which achieves
optimal results according to the original paper.
•HEARTS (Boutaleb et al., 2025): We utilized
pre-trained HyTrel embeddings (Chen et al.,
2023) with a contrastively-trained checkpoint.
For each benchmark, we adopted the best-
performing search strategy from the HEARTS
repository: Cluster Search for SANTOS ,PYLON ,
andUGEN benchmarks, and ANN index search
with max pooling for the TUSandLAKEBENCH
benchmarks.
•TabSketchFM (Khatiwada et al., 2024): Results
for the TUSSmall andSANTOS were reported di-
rectly from the original paper, as the pretrained
checkpoint was unavailable at the time of our
experiments.
8LakeBench commit df7559d used in our studyThese methods represent significant advance-
ments in table representation learning. AutoTUS
(Hu et al., 2023) wasn’t included due to code un-
availability at the time of writing. We provide fur-
ther implementation details in Appendix B.2.
4.3 Evaluation Procedure
We use a consistent evaluation procedure for all
baseline and SOTA methods to ensure fair com-
parison. Table vectors are generated per method
(Section 3.2 for baselines; SOTA-specific proce-
dures otherwise) and L2-normalized for similarity
via inner product. For similarity search, baseline
methods use the FAISS library (Douze et al., 2024)
with an exact inner product index ( IndexFlatIP );
each query ranks all candidate tables by simi-
larity. SOTA methods use FAISS or alternative
search strategies (Appendix B.2). Following prior
work (Fan et al., 2023b; Hu et al., 2023), we report
Precision@k (P@k) and Recall@k (R@k), aver-
aged across queries. Values of kfollow prior works
and are shown in results tables (e.g., Table 2). We
also evaluate computational efficiency via offline
(training, vector extraction, indexing) and online
(query search) runtimes, with hardware details in
Appendix B.3.
5 Results and Discussion
Our empirical evaluation revealed significant pat-
terns across benchmarks that expose fundamental
limitations in their ability to measure progress in
semantic understanding. Tables 2 and 3 present
effectiveness and efficiency metrics respectively.
5.1 Evidence of Benchmark Limitations
The most compelling evidence for our bench-
mark limitation hypotheses emerges from the un-
expectedly strong performance of simple base-
lines. On partitioning-based benchmarks ( TUSSmall,
TUSLarge,SANTOS ), lexical methods achieve near-
perfect precision, matching or exceeding sophis-
ticated models at a fraction of the cost. This di-
rectly validates our overlap issue hypothesis: the
high schema and value overlap (Figure 1) creates
trivial signals that simple lexical matching can ex-
ploit. While advanced methods like Starmie or
HEARTS also achieve high scores here, the fact
that much simpler, non-semantic methods perform
nearly identically leads us to conclude that the
benchmark itself does not effectively differentiate
methods based on deep semantic understanding.
This phenomenon, where simpler approaches canachieve comparable or even better results than more
complex counterparts, especially when computa-
tional costs are considered, has also been observed
in related data lake tasks such as table augmenta-
tion via join search (Cappuzzo et al., 2024).
ForPYLON , a different pattern emerges: lexical
methods perform considerably worse due to the
much lower exact overlap, but general-purpose se-
mantic embeddings excel. SBERT variants, partic-
ularly SBERT(V+C) combining column and value
information, outperform specialized SOTA models
like Starmie. This confirms our general semantics
hypothesis that these benchmarks employ vocab-
ulary well-represented in standard pre-trained em-
beddings, diminishing the advantage of specialized
tabular architectures for the TUS task.
LB-O PENDATA and LB-W EBTABLE exhibit
both limitations despite their scale. Simple lexi-
cal methods remain surprisingly competitive, while
SBERT variants consistently outperform special-
ized models. The computational demands of so-
phisticated models create additional practical barri-
ers: Starmie requires substantial offline costs (train-
ing and inference) plus over 16 hours to process the
queries on the truncated LB-O PENDATA, and over
21 hours to evaluate the queries of LB-W EBTABLE .
HEARTS performs better computationally by lever-
aging a pre-trained checkpoint without additional
training, resulting in a shorter offline processing
time, but still under-performs SBERT variants.
5.2 Ground Truth Reliability Issues
A notable observation across UGEN and
LAKEBENCH derivatives is the significant gap
between the R@k achieved by all methods and
the IDEAL recall (Table 2). This discrepancy led
us to question the reliability of the benchmarks’
ground truth labels. We hypothesized that such
gaps might indicate not only the limitations of
the search methods or the inherent difficulty of
the benchmarks but also potential incompleteness
or inaccuracies within the ground truth itself.
Examining discrepancies at small values of k
is particularly revealing, as this scrutinizes the
highest-confidence predictions of a system. If a
high-performing method frequently disagrees with
the ground truth at these top ranks, it may signal
issues with the ground truth labels.
To investigate this, we defined two heuristic met-
rics designed to help identify potential ground truth
flaws. Let Q={Q1, . . . , Q N}beNquery tables.
ForQi∈ Q,CQi,kis the set of top- kcandidatesMethodSANTOS TUS TUS Large PYLON UGEN V1 UGEN V2 LB-O PENDATA 1kLB-O PENDATA LB-WebTable
P@10 R@10 P@60 R@60 P@60 R@60 P@10 R@10 P@10 R@10 P@10 R@10 P@50 R@50 P@50 R@50 P@20 R@20
IDEAL 1.00 0.75 1.00 0.34 1.00 0.23 1.00 0.24 1.00 1.00 1.00 1.00 0.39 1.00 0.39 1.00 0.81 0.95
Non-specialized Embedding Methods
HASH 0.98 0.74 0.99 0.33 0.99 0.23 0.64 0.15 0.59 0.59 0.43 0.43 0.21 0.60 0.21 0.60 0.21 0.25
TFIDF 0.99 0.74 1.00 0.34 0.99 0.23 0.70 0.17 0.58 0.58 0.50 0.50 0.21 0.61 0.21 0.61 0.23 0.27
COUNT 0.99 0.74 1.00 0.34 0.99 0.23 0.68 0.17 0.58 0.58 0.50 0.50 0.21 0.60 0.21 0.60 0.23 0.27
SBERT (V+C) 0.98 0.74 1.00 0.34 0.99 0.23 0.91 0.22 0.61 0.61 0.68 0.68 0.23 0.66 0.23 0.66 0.26 0.31
SBERT (V) 0.94 0.71 1.00 0.34 0.99 0.23 0.84 0.20 0.58 0.58 0.58 0.58 0.22 0.62 0.22 0.62 0.25 0.29
SBERT (C) 0.98 0.74 1.00 0.34 0.98 0.23 0.85 0.21 0.60 0.60 0.65 0.65 0.22 0.64 0.22 0.64 0.16 0.20
Specialized Table Union Search Methods
Starmie 0.98 0.73 0.96 0.31 0.93 0.21 0.81 0.20 0.57 0.57 0.58 0.58 0.18 0.51 ‡ ‡ 0.25 0.30
HEARTS 0.98 0.74 1.00 0.34 0.99 0.23 0.65 0.16 0.56 0.56 0.37 0.37 0.19 0.61 0.19 0.60 0.23 0.28
TabSketchFM 0.92 0.69 0.97 0.32 * * * * * * * * * * * * * *
Table 2: Precision and Recall across benchmarks. Highest values in bold , second highest underlined . IDEAL
represents the maximum possible P@k and R@k achievable for each benchmark at the specified k. *: Results
unavailable as checkpoint was not publicly accessible. ‡: Not reported due to excessive computational requirements.
MethodSANTOS TUS TUS Large PYLON UGEN V1 UGEN V2 LB-O PENDATA 1K LB-O PENDATA LB-WebTable
Offline Online Offline Online Offline Online Offline Online Offline Online Offline Online Offline Online Offline Online Offline Online
Non-specialized Embedding Methods
HASH 0m 15s 0m 0s 0m 43s 0m 1s 1m 45s 0m 2s 0m 19s 0m 1s 0m 12s 0m 0s 0m 14s 0m 0s 7m 56s 0m 31s 12m 4s 0m 22s 6m 3s 0m 21s
TFIDF/COUNT 0m 53s 0m 0s 1m 45s 0m 1s 3m 10s 0m 2s 0m 22s 0m 1s 0m 9s 0m 0s 0m 12s 0m 0s 22m 22s 0m 31s 37m 14s 0m 21s 6m 21s 0m 22s
SBERT 1m 45s 0m 0s 3m 30s 0m 0s 9m 21s 0m 15s 3m 18s 0m 0s 1m 41s 0m 0s 2m 20s 0m 0s 27m 47s 0m 4s 82m 13s 0m 4s 30m 45s 0m 3s
Specialized Table Union Search Methods
STARMIE 19m 3s 1m 2s 4m 24s 8m 59s 14m 43s 20m 29s 7m 56s 3m 27s 2m 8s 1m 0s 2m 45s 1m 45s 131m 48s 1220m 53s – – 48m 11s 1311m 43s
HEARTS 0m 21s 0m 34s 1m 1s 0m 0s 3m 10s 0m 0s 0m 57s 0m 36s 0m 23s 0m 40s 0m 30s 0m 35s 21m 33s 0m 3s 76m 12s 0m 5s 29m 28s 0m 3s
Table 3: Computational efficiency across benchmarks. Times are averaged over 5 runs due to runtime variability.
Offline includes vector generation, indexing, and training times where applicable; Online is total query search time.
retrieved by a search method for Qi, andGQiis the
set of ground truth candidates labeled unionable
withQi.
1.GTFP@k (Ground Truth False Positive Rate) :
This measures the fraction of top- kcandidates
retrieved by a search method that are not labeled
as unionable in the original ground truth. A
high GTFP@k, especially at small k, suggests
the method might be identifying valid unionable
tables missing from the ground truth, thereby
helping us pinpoint its possible incompleteness .
It is calculated as:
PN
i=1|CQi,k\GQi|
N·k
Here,|CQi,k\GQi|counts retrieved candidates
forQithat are absent from its ground truth set
GQi. The denominator is the total top- kslots
considered across all queries.
2.GTFN@k (Ground Truth False Negative
Rate) : This quantifies the fraction of items la-
beled as positives in the ground truth that a well-
performing search method fails to retrieve within
its top- kresults (considering a capped expecta-
tion up to kitems per query). It is calculated
as:
PN
i=1(min( k,|GQi|)− |GQi∩CQi,k|)PN
i=1min(k,|GQi|)
The term min(k,|GQi|)represents the capped
ideal number of ground truth items we wouldexpect to find in the top kforQi. The numerator
sums the "misses" for each query: the differ-
ence between this capped ideal and the number
of ground truth items actually retrieved. The
denominator sums this capped ideal across all
queries. A high GTFN@k at small kis particu-
larly insightful when investigating ground truth
integrity. If we trust the method’s ability to dis-
cern relevance, a high GTFN@k implies that
the method correctly deprioritizes items that, de-
spite being in the ground truth, might be less
relevant or even incorrectly labeled as positive.
Thus, it can signal potential incorrectness within
the ground truth. GTFN@k is equivalent to
"1−CappedRecall@k" (Thakur et al., 2021).
These metrics assume discrepancies between a
strong search method and the ground truth may in-
dicate flaws in the latter. While not highly accurate,
they helped us identify a smaller, focused subset
of query-candidate pairs with disagreements for
deeper manual or LLM-based inspection. Results
are shown in Table 4.
Beyond heuristic metrics, we also conduct
a more direct–though still imperfect–assessment
ofUGEN’s ground truth using an LLM-as-a-
judge approach. While this method may not
capture the same conflicts identified by the
cheaper GTFP/GTFN heuristics, it provides a
complementary perspective that can offer more
precise insights in certain cases. We usegemini-2.0-flash-thinking-exp-01-219, cho-
sen for its 1M-token context window, baked-in rea-
soning abilities, and low hallucination rate10. This
LLM-as-a-judge approach has become increasingly
common in recent works (Gu et al., 2024; Wolff
and Hulsebos, 2025). We gave the LLM both ta-
bles in each query-candidate pair, along with a de-
tailed prompt including curated unionable and non-
unionable examples from UGEN (see Appendix D)
to condition the LLM’s understanding of unionabil-
ity based on the benchmark. Each pair was evalu-
ated in 5 independent runs with temperature=0.1 .
A sample of 20 LLM outputs was manually val-
idated and showed strong alignment with human
judgment. Comparison with original U GEN labels
(Table 5) revealed substantial inconsistencies. Our
manual inspection (Appendix C.1) suggested the
LLM often provided more accurate assessments, in-
dicating notable noise in the original ground truth.
Given the scale of LB-O PENDATA and LB-
WEBTABLE , full LLM adjudication was imprac-
tical. Instead, we used SBERT(V+C) as our refer-
ence search method to compute GTFP@k, focus-
ing on top-ranked pairs not labeled as unionable
in the ground truth. As shown in Table 4, such
cases were frequent even at top ranks ( 2< k < 5).
To assess ground truth completeness, we manually
inspected 20 randomly sampled top-2 and top-3
disagreements. Of these, 19 were genuinely union-
able but missing from the ground truth; the remain-
ing pair was correctly non-unionable, with SBERT
likely misled by its numeric-only columns. These
results suggest non-negligible incompleteness in
theLAKEBENCH ground truth. Example cases are
shown in Appendix C.2.
In summary, our investigations, combining
heuristic metrics, LLM-based adjudication, and
manual inspection, reveal the presence of non-
negligible noise and incompleteness within the
original benchmark labels for both UGEN and
LAKEBENCH . Consequently, performance metrics
reported on these benchmarks may be influenced
by these underlying ground truth issues, potentially
misrepresenting true task difficulty or method ca-
pabilities.
5.3 Implications for Measuring Progress
Our experiments reveal several critical issues.
Benchmark scores often fail to measure true seman-
tic capabilities, as simple lexical or general embed-
9Gemini 2.0 Flash Thinking Model Card
10Vectara Hallucination LeaderboardBenchmark (Metric) @1 @2 @3 @4 @5
UGEN V1(GTFP) 0.160 0.210 0.247 0.275 0.308
UGEN V1(GTFN) 0.160 0.210 0.247 0.275 0.308
UGEN V2(GTFP) 0.060 0.080 0.093 0.140 0.156
UGEN V2(GTFN) 0.060 0.080 0.093 0.140 0.156
LB-O PENDATA(GTFP) 0.000 0.059 0.092 0.123 0.154
LB-O PENDATA(GTFN) 0.000 0.054 0.080 0.105 0.132
LB-W EBTABLE (GTFP) 0.000 0.110 0.198 0.296 0.377
LB-W EBTABLE (GTFN) 0.000 0.110 0.197 0.295 0.376
Table 4: Disagreement rates of top- kretrieved results
between SBERT and the ground truth across different
benchmarks. For UGEN, the query table is not consid-
ered a candidate to itself, so values at @1 reflect actual
disagreement. For LAKEBENCH variants, the ground
truth is normalized to include the query table as a valid
candidate for itself. Therefore, the top-1 match is al-
ways correct by construction, yielding no disagreement
@1.
GT Label LLM Judge UGEN V1 UGEN V2
Unionable Non-unionable 24.8% 0.0%
Non-unionable Unionable 33.8% 23.6%
Non-unionable Non-unionable 16.2% 76.4%
Unionable Non-unionable 25.2% 0.0%
Table 5: Breakdown of agreement and disagreement
between ground truth labels and LLM-based judgments.
ding methods can match or outperform specialized
models by exploiting excessive domain overlap,
semantic simplicity, or ground truth inconsistency.
This suggests that current benchmarks may inad-
vertently reward adaptation to these characteristics,
making it difficult to quantify the practical benefits
of progress on sophisticated TUS methods capabil-
ities within these settings. These persistent issues
also point to a fundamental challenge, the lack of a
precise, operational definition for unionability, mir-
roring broader difficulties in dataset search (Hulse-
bos et al., 2024) and highlighting the need to ad-
dress the subjective, context-dependent nature of
table compatibility in practice.
6 Towards Better TUS Benchmarks
In industry practice, unionability judgments are in-
herently subjective, depending on analytical goals,
domain contexts, data accessibility constraints
(Martorana et al., 2025), and user preferences
(Mirzaei and Rafiei, 2023). Yet current benchmarks
impose fixed definitions, creating a disconnect with
practical utility: methods excelling on benchmarks
often falter in real-world scenarios demanding dif-
ferent compatibility thresholds. Addressing this
requires benchmark designs that embrace contex-
tual variability and provide a stable foundation forevaluation, lest even advanced methods fall short
in practice.
Rethinking Benchmark Design Principles:
Overcoming current benchmark limitations re-
quires a shift in design focusing on three key prin-
ciples: (1) actively reducing artifactual overlap
while introducing controlled semantic heterogene-
ity to better reflect real-world schema and value
divergence; (2) incorporating realistic domain com-
plexity beyond general vocabularies, addressing
challenges like non-descriptive schemas and pro-
prietary terms where LLMs struggle (Bodensohn
et al., 2025), thus emphasizing domain-specific
training that may require industry collaboration;
and (3) rethinking ground truth representation by
replacing brittle binary labels with richer, nuanced
formats validated through multi-stage adjudication
to improve completeness and consistency.
Exploring Implementation Pathways: Trans-
lating these principles into practice requires con-
crete strategies for benchmark design and evalu-
ation. One approach is to develop (1) scenario-
driven micro-benchmarks targeting specific chal-
lenges such as schema drift simulation or value
representation noise, enabling more granular anal-
ysis than coarse end-to-end metrics. Another is
(2) advancing controllable synthetic data genera-
tion, following LLM-based methods like UGEN
(Pal et al., 2024), to verifiably embed semantic con-
straints or domain knowledge, supporting diverse
testbeds when real data is unavailable or sensitive.
Equally important is (3) exploring adaptive, inter-
active evaluation frameworks such as human-in-
the-loop systems, which would dynamically adjust
relevance criteria based on user feedback to better
capture the subjective nature of unionability. Tools
like LakeVisage (Hu et al., 2025) further enhance
usability and trust by recommending visualizations
that help users interpret relationships among re-
turned tables, improving transparency and inter-
pretability in union search systems.Incorporating
natural language preferences is also key. The re-
cent NLCT ABLES benchmark (Cui et al., 2025) ad-
vances this by introducing NL conditions for union
and join searches on column values and table size
constraints. However, its predicate-style conditions
may be better addressed via post-retrieval filtering
(e.g., translating NL to SQL predicates with an
LLM), avoiding early discard of unionable candi-
dates and unnecessary retrieval model complexity.
To drive further advancement, benchmarks shouldincorporate (4) natural language conditions that
capture key aspects of unionability and joinability,
including specifications about the characteristics of
the final integrated table or conditional integration
logic. For example, a challenging predicate might
require identifying tables that can be "joined with
a query table on column A, unioned on columns
B and C, and also contain an additional column
D providing specific contextual information about
a particular attribute." Such conditions would de-
mand deeper reasoning capabilities from data inte-
gration systems and encourage the development of
more sophisticated methods for Table Union and
Join Search. Finally, moving beyond binary suc-
cess metrics, future benchmarks could adopt (5)
multi-faceted evaluation frameworks using richer
ground truth representations to assess unionability
across dimensions like schema compatibility, se-
mantic type alignment, value distribution similarity,
and task-specific relevance, offering a more holistic
evaluation than current standards.
7 Conclusion
Our analysis of TUS benchmarks highlights three
major limitations: excessive overlap in partitioning-
based datasets, semantics easily captured by pre-
trained embeddings, and non-negligible ground-
truth inconsistencies. The first two allow simple
baselines to rival sophisticated models with far
lower computational cost, showing that high perfor-
mance isn’t necessarily tied to advanced semantic
reasoning. The third undermines evaluation valid-
ity, as scores may reflect misalignment with flawed
ground truth rather than actual benchmark diffi-
culty. This gap between benchmark performance
and true semantic capability suggests current eval-
uations often reward adaptation to benchmark-
specific artifacts. To address this, we propose de-
sign principles that better reflect the complex, sub-
jective nature of real-world table union search.
Limitations: Our study examined selected
benchmarks and methods, with broader evalua-
tion potentially revealing more insight. Our in-
vestigation of ground truth issues in UGEN and
LAKEBENCH , while systematic, identifies certain
patterns without exhaustive quantification.
Future Work: Developing benchmarks aligned
with our proposed criteria represents the next step
towards ensuring that measured progress translates
to meaningful real-world utility.References
Jan-Micha Bodensohn, Ulf Brackmann, Liane V ogel,
Anupam Sanghi, and Carsten Binnig. 2025. Unveil-
ing challenges for llms in enterprise data engineering.
Preprint , arXiv:2504.10950.
Alex Bogatu, Alvaro A. A. Fernandes, Norman W. Pa-
ton, and Nikolaos Konstantinou. 2020. D3L: Dataset
Discovery in Data Lakes. In 2020 IEEE 36th Inter-
national Conference on Data Engineering (ICDE) ,
pages 709–720. ArXiv:2011.10427 [cs].
Allaa Boutaleb, Alaa Almutawa, Bernd Amann, Rafael
Angarita, and Hubert Naacke. 2025. HEARTS:
Hypergraph-based related table search. In ELLIS
workshop on Representation Learning and Genera-
tive Models for Structured Data .
Riccardo Cappuzzo, Aimee Coelho, Felix Lefebvre,
Paolo Papotti, and Gael Varoquaux. 2024. Retrieve,
Merge, Predict: Augmenting Tables with Data Lakes.
arXiv preprint . ArXiv:2402.06282.
Sonia Castelo, Rémi Rampin, Aécio Santos, Aline
Bessa, Fernando Chirigati, and Juliana Freire. 2021.
Auctus: a dataset search engine for data discovery
and augmentation. Proceedings of the VLDB Endow-
ment , 14(12):2791–2794.
Pei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasub-
ramaniam Srinivasan, Sheng Zha, Ruihong Huang,
and George Karypis. 2023. HYTREL: Hypergraph-
enhanced Tabular Data Representation Learning.
arXiv preprint . ArXiv:2307.08623.
Tianji Cong, Fatemeh Nargesian, and H. V . Jagadish.
2023. Pylon: Semantic Table Union Search in Data
Lakes. arXiv preprint . ArXiv:2301.04901 [cs].
Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, and Gang
Chen. 2025. Nlctables: A dataset for marrying
natural language conditions with table discovery.
Preprint , arXiv:2504.15849.
Yuhao Deng, Chengliang Chai, Lei Cao, Qin Yuan,
Siyuan Chen, Yanrui Yu, Zhaoze Sun, Junyi Wang,
Jiajun Li, Ziqi Cao, Kaisen Jin, Chi Zhang, Yuqing
Jiang, Yuanfang Zhang, Yuping Wang, Ye Yuan,
Guoren Wang, and Nan Tang. 2024. LakeBench:
A Benchmark for Discovering Joinable and Union-
able Tables in Data Lakes. Proceedings of the VLDB
Endowment , 17(8):1925–1938.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers) , pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff
Johnson, Gergely Szilvasy, Pierre-Emmanuel Mazaré,Maria Lomeli, Lucas Hosseini, and Hervé Jégou.
2024. The faiss library.
Grace Fan, Jin Wang, Yuliang Li, and Renée J. Miller.
2023a. Table Discovery in Data Lakes: State-of-the-
art and Future Directions. In Companion of the 2023
International Conference on Management of Data ,
pages 69–75, Seattle WA USA. ACM.
Grace Fan, Jin Wang, Yuliang Li, Dan Zhang, and
Renée Miller. 2023b. STARMIE: Semantics-aware
Dataset Discovery from Data Lakes with Contextual-
ized Column-based Representation Learning. arXiv
preprint . ArXiv:2210.01922 [cs].
Daniel Gomm and Madelon Hulsebos. 2025. Metadata
matters in dense table retrieval. In ELLIS workshop
on Representation Learning and Generative Models
for Structured Data .
Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,
Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan
Shen, Shengjie Ma, Honghao Liu, and 1 others.
2024. A survey on llm-as-a-judge. arXiv preprint
arXiv:2411.15594 .
Xuming Hu, Shen Wang, Xiao Qin, Chuan Lei,
Zhengyuan Shen, Christos Faloutsos, Asterios Katsi-
fodimos, George Karypis, Lijie Wen, and Philip S. Yu.
2023. AUTOTUS: Automatic Table Union Search
with Tabular Representation Learning. In Findings of
the Association for Computational Linguistics: ACL
2023 , pages 3786–3800, Toronto, Canada. Associa-
tion for Computational Linguistics.
Yihao Hu, Jin Wang, and Sajjadur Rahman. 2025. Lake-
visage: Towards scalable, flexible and interactive vi-
sualization recommendation for data discovery over
data lakes. arXiv preprint arXiv:2504.02150 .
Madelon Hulsebos, Çagatay Demiralp, and Paul Groth.
2023. Gittables: A large-scale corpus of relational
tables. Proceedings of the ACM on Management of
Data , 1(1):1–17.
Madelon Hulsebos, Wenjing Lin, Shreya Shankar, and
Aditya Parameswaran. 2024. It Took Longer than I
was Expecting: Why is Dataset Search Still so Hard?
InProceedings of the 2024 Workshop on Human-In-
the-Loop Data Analytics , pages 1–4, Santiago AA
Chile. ACM.
Aamod Khatiwada, Grace Fan, Roee Shraga, Zixuan
Chen, Wolfgang Gatterbauer, Renée J. Miller, and
Mirek Riedewald. 2023. SANTOS: Relationship-
based Semantic Table Union Search. Proceedings of
the ACM on Management of Data , 1(1):1–25.
Aamod Khatiwada, Harsha Kokel, Ibrahim Abdelaziz,
Subhajit Chaudhury, Julian Dolby, Oktie Hassan-
zadeh, Zhenhan Huang, Tejaswini Pedapati, Horst
Samulowitz, and Kavitha Srinivas. 2024. Tab-
SketchFM: Sketch-based Tabular Representation
Learning for Data Discovery over Data Lakes. arXiv
preprint . ArXiv:2407.01619.Margherita Martorana, Tobias Kuhn, and Jacco van
Ossenbruggen. 2025. Metadata-driven table union
search: Leveraging semantics for restricted access
data integration. Preprint , arXiv:2502.20945.
Leland McInnes, John Healy, Steve Astels, and 1 others.
2017. hdbscan: Hierarchical density based clustering.
J. Open Source Softw. , 2(11):205.
Leland McInnes, John Healy, and James Melville. 2018.
Umap: Uniform manifold approximation and pro-
jection for dimension reduction. arXiv preprint
arXiv:1802.03426 .
Hamed Mirzaei and Davood Rafiei. 2023. Table union
search with preferences. In Joint Proceedings of
Workshops at the 49th International Conference
on Very Large Data Bases (VLDB 2023), Vancou-
ver, Canada, August 28 - September 1, 2023 , vol-
ume 3462 of CEUR Workshop Proceedings . CEUR-
WS.org.
Fatemeh Nargesian, Erkang Zhu, Ken Q. Pu, and
Renée J. Miller. 2018. TUS: Table union search
on open data. Proceedings of the VLDB Endowment ,
11(7):813–825.
Koyena Pal, Aamod Khatiwada, Roee Shraga, and
Renée J Miller. 2024. Alt-gen: Benchmarking table
union search using large language models. Proceed-
ings of the VLDB Endowment. ISSN , 2150:8097.
Thomas Pellissier Tanon, Gerhard Weikum, and Fabian
Suchanek. 2020. Yago 4: A reason-able knowledge
base. In The Semantic Web , pages 583–596, Cham.
Springer International Publishing.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP) , pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Anish Das Sarma, Lujun Fang, Nitin Gupta, Alon Y
Halevy, Hongrae Lee, Fei Wu, Reynold Xin, and
Cong Yu. 2012. Finding related tables. In SIGMOD
Conference , volume 10, pages 2213836–2213962.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. BEIR:
A heterogeneous benchmark for zero-shot evaluation
of information retrieval models. In Thirty-fifth Con-
ference on Neural Information Processing Systems
Datasets and Benchmarks Track (Round 2) .
Cornelius Wolff and Madelon Hulsebos. 2025. How
well do llms reason over tabular data, really? arXiv
preprint arXiv:2505.07453 .
Erkang Zhu, Fatemeh Nargesian, Ken Q. Pu, and
Renée J. Miller. 2016. LSH Ensemble: Internet-Scale
Domain Search. arXiv preprint . ArXiv:1603.07410
[cs].A Benchmark Overlap
As discussed in section 3.1.a), the degree of lex-
ical overlap (both in column names and values)
between query and candidate tables in benchmark
ground truths can significantly influence model per-
formance. Methods sensitive to surface-level sim-
ilarity might perform well on benchmarks with
high overlap without necessarily capturing deeper
semantic relationships. This section provides a
more detailed breakdown of overlap coefficients by
data type across the different benchmarks evaluated.
Figure 2 presents these distributions.
BImplementation and Evaluation Details
This appendix provides supplementary details re-
garding the implementation of baseline methods,
SOTA models, and the evaluation procedure used in
our experiments, complementing the core method-
ology described in Sections 3.2 and 4.3.
B.1 Lexical Baselines (Hashing, TF-IDF,
Count) Implementation Details
Vectorizers: We used implementations from
scikit-learn11. All vectorizers were configured with
lowercase=True .
•TfidfVectorizer and CountVectorizer :
Used an ngram_range=(1, 2) . Their vocabu-
lary was constructed by first collecting unique
tokens from all columns across the entire corpus
(query tables included), ensuring a consistent
feature space.
•HashingVectorizer : Used
an ngram_range=(1, 1) and
alternate_sign=False .
Input Data: For each table, we randomly sam-
pled up to 1000 unique non-null cell values per
column.
Vectorization: Each column’s sampled values
were treated as a document and vectorized into a
4096-dimensional vector using the appropriately
fitted or configured vectorizer.
B.2 SOTA Method Implementation Details
B.2.a) Starmie:12We utilized the implemen-
tation and recommendations from the original
Starmie paper (Fan et al., 2023b).
11https://scikit-learn.org/stable/api/sklearn.
feature_extraction.html
12Starmie GitHub Repository/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000033/uni0000003c/uni0000002f/uni00000032/uni00000031/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000033/uni0000003c/uni0000002f/uni00000032/uni00000031/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000037/uni00000038/uni00000036/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000037/uni00000038/uni00000036/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000008/uni00000014/uni00000013/uni00000011/uni00000013/uni00000008/uni00000015/uni00000013/uni00000011/uni00000013/uni00000008/uni00000016/uni00000013/uni00000011/uni00000013/uni00000008/uni00000017/uni00000013/uni00000011/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000037/uni00000038/uni00000036/uni00000010/uni0000002f/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000037/uni00000038/uni00000036/uni00000010/uni0000002f/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000014/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000016/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000018/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000036/uni00000024/uni00000031/uni00000037/uni00000032/uni00000036/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000036/uni00000024/uni00000031/uni00000037/uni00000032/uni00000036/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000011/uni00000013/uni00000008/uni00000014/uni00000013/uni00000011/uni00000013/uni00000008/uni00000015/uni00000013/uni00000011/uni00000013/uni00000008/uni00000016/uni00000013/uni00000011/uni00000013/uni00000008/uni00000017/uni00000013/uni00000011/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000014/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000014/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000015/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni0000001b/uni00000013/uni00000008/uni00000014/uni00000013/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni00000038/uni0000002a/uni00000028/uni00000031/uni00000010/uni00000039/uni00000015/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000014/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000016/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000018/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni0000002f/uni00000025/uni00000010/uni00000032/uni00000053/uni00000048/uni00000051/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni0000002f/uni00000025/uni00000010/uni00000032/uni00000053/uni00000048/uni00000051/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000014/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000016/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000018/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni0000002f/uni00000025/uni00000010/uni0000003a/uni00000048/uni00000045/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000010/uni00000003/uni00000026/uni00000052/uni0000004f/uni00000058/uni00000050/uni00000051/uni00000003/uni00000031/uni00000044/uni00000050/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000027/uni0000004c/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000045/uni00000058/uni00000057/uni0000004c/uni00000052/uni00000051
/uni00000013/uni00000010/uni00000015/uni00000018 /uni00000015/uni00000018/uni00000010/uni00000018/uni00000013 /uni00000018/uni00000013/uni00000010/uni0000001a/uni00000018 /uni0000001a/uni00000018/uni00000010/uni00000014/uni00000013/uni00000013
/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000035/uni00000044/uni00000051/uni0000004a/uni00000048/uni00000003/uni0000000b/uni00000008/uni0000000c/uni00000013/uni00000008/uni00000015/uni00000013/uni00000008/uni00000017/uni00000013/uni00000008/uni00000019/uni00000013/uni00000008/uni00000008/uni00000003/uni00000052/uni00000049/uni00000003/uni00000035/uni00000048/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000056/uni0000004b/uni0000004c/uni00000053/uni00000056/uni0000002f/uni00000025/uni00000010/uni0000003a/uni00000048/uni00000045/uni00000037/uni00000044/uni00000045/uni0000004f/uni00000048/uni00000003/uni00000010/uni00000003/uni00000037/uni00000058/uni00000053/uni0000004f/uni00000048/uni00000003/uni00000032/uni00000059/uni00000048/uni00000055/uni0000004f/uni00000044/uni00000053/uni00000003/uni00000045/uni0000005c/uni00000003/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni00000027/uni00000044/uni00000057/uni00000044/uni00000003/uni00000037/uni0000005c/uni00000053/uni00000048
/uni0000004c/uni00000051/uni00000057/uni00000048/uni0000004a/uni00000048/uni00000055
/uni00000049/uni0000004f/uni00000052/uni00000044/uni00000057
/uni00000056/uni00000057/uni00000055/uni0000004c/uni00000051/uni0000004a
/uni00000052/uni00000057/uni0000004b/uni00000048/uni00000055Figure 2: Distribution of exact column name and tuple overlap across different benchmarks, broken down by data
type (String, Numeric, Datetime, Other). Each subplot represents a benchmark, showing the percentage of ground
truth pairs falling into different overlap ranges.
Training Setup: The provided RoBERTa-based
model was retrained for 10 epochs on each bench-
mark. Key hyperparameters included: batch size
32, projection dimension 768, learning rate 5e-5,
max sequence length 256, and fp16 precision.
Sampling and Augmentation Strategies:
Starmie employs specific strategies during con-
trastive pre-training to generate positive pairs
(views of the same column). The strategies, based
on the definitions in the original paper, are:
•TF-IDF Entity Sampling (‘tfidf_entity‘): Samples
cells in columns that have the highest average
TF-IDF scores calculated over their tokens.
•Alpha Head Sampling (‘alphaHead‘): Samples
the first N tokens sorted alphabetically.
•Column Dropping Augmentation (‘drop_col‘):
Creates augmented views by dropping a random
subset of columns from the table.
•Drop Cell Augmentation (‘drop_cell‘): Creates
augmented views by dropping random cells
within the table.
We followed the paper’s recommendations for
each benchmark, detailed in Table 6. For bench-
marks not explicitly mentioned in the original pa-
per (PYLON ,UGEN,LAKEBENCH derivatives), we
applied the same strategies recommended for the
SANTOS benchmark.Evaluation: We used the "Pruning" search strat-
egy described in the Starmie paper, also referred to
as "bounds" in the original implementation. This
involves a maximum bipartite matching approach
on a pruned set of candidate column pairs to cal-
culate table similarity, offering higher efficiency
compared to naive matching, while remaining more
precise than approximate search approaches.
Benchmark Sampling Augmentation
SANTOS tfidf_entity drop_col
TUS (Small) alphaHead drop_cell
TUS Large tfidf_entity drop_cell
PYLON tfidf_entity drop_col
UGEN V1 tfidf_entity drop_col
UGEN V2 tfidf_entity drop_col
LB-O PENDATA tfidf_entity drop_col
LB-W EBTABLE tfidf_entity drop_col
Table 6: Starmie sampling and augmentation strategies
applied per benchmark.
B.2.b) HEARTS:13
Model: Employs pre-trained HyTrel embeddings
(Chen et al., 2023), utilizing a publicly available
checkpoint trained with a contrastive learning ob-
jective14. No further finetuning was performed.
13HEARTS GitHub Repository
14https://github.com/awslabs/
hypergraph-tabular-lm/tree/main/checkpointsEvaluation Strategy: We adopted the best-
performing search strategy reported in the
HEARTS repository for each benchmark:
•Cluster Search (for SANTOS ,PYLON ,
UGEN V1, UGEN V2):This strategy first reduces
the dimensionality of the pre-trained HyTrel
column embeddings using UMAP (McInnes
et al., 2018) and then performs clustering using
HDBSCAN (McInnes et al., 2017). Default
parameters provided in the HEARTS repository
were used for both UMAP and HDBSCAN
within this search method. Table similarity is
derived based on cluster assignments.
•FAISS + Max Pooling (for TUS Small, TUS Large,
LB-O PENDATA,LB-W EBTABLE ):This strat-
egy uses FAISS (Douze et al., 2024) for efficient
similarity search. Table vectors are computed by
max-pooling the embeddings of their constituent
columns before indexing and searching.
B.3 Hardware
Our experiments were conducted using the follow-
ing setup:
•CPU: Intel Xeon Gold 6330: 4 cores / 8 threads
@ 2.00 GHz.
•GPU: 40GB MIG partition of NVIDIA A100
(used for SBERT embedding generation and
SOTA models training/inference).
• 64 Go DDR4 RAM.
C Inconsistent Ground Truth Examples
This section provides illustrative examples of the
ground truth inconsistencies identified in the UGEN
andLAKEBENCH benchmarks during our analy-
sis (Section 5.2). We categorize these into False
Positives (pairs incorrectly labeled as unionable)
and False Negatives (pairs incorrectly labeled as
non-unionable or missed).
C.1 UGEN Benchmark Inconsistencies
Figures 3 and 4 showcase examples from UGEN
variants.
C.2 Lakebench Benchmark Inconsistencies
This subsection presents examples of GTFPs from
theLAKEBENCH benchmarks, where semantically
and structurally compatible tables were not labeled
as unionable in the ground truth but were correctly
retrieved by search methods. Figures 5 and 6 show
such cases from the WebTable and OpenData sub-
sets, respectively.Query: Anthropology_FGTNBDWF.csv
Candidate: Anthropology_N30U114M.csv
Age Culture Arena Domain Meaning Origin Activity
1 Neo. Arch. Past Prim. Africa Hunt.
2 Islam. Artif. Hist. Cplx. Asia Farm.
Artifact Language Technology Education Society
1 English GPS Political Communal
2 Latin Smartphone Scientific Global
(a) UGEN V1Example: Tables discussing structurally and se-
mantically distinct aspects of Anthropology (historical cul-
tures vs. social technology), originally labeled unionable
despite conceptual incompatibility.
Query: Anthropology_N7BS08I4.csv
Candidate: Anthropology_VS4SJ2VH.csv
Site Name Location Period Culture
Olduvai Gorge Tanzania, Africa Pliocene Hominin
Teotihuacan Central Mexico Early Classic Teotihuacanos
Age Group Clothing Food Housing
Children (0-12) Tunics, hides Porridge, roots Huts (branch)
Teenagers (13-19) Garments, beads Grains, stews Huts (woven)
(b) UGEN V2Example: Tables about archaeological sites ver-
sus demographic lifestyles, representing fundamentally differ-
ent entity types despite the shared Anthropology topic.
Figure 3: Examples of UGEN where pairs labeled
unionable in the original ground truth exhibit signif-
icant semantic/structural divergence suggesting non-
unionability.
D LLM Adjudicator
D.1 Prompt Details
To systematically re-evaluate potential ground truth
inconsistencies in the UGEN benchmarks, we em-
ployed an LLM-based adjudicator. This process tar-
geted disagreements identified during our analysis,
specifically Ground Truth False Positives (GTFPs,
pairs retrieved as potentially unionable within a
rank threshold k′but not labeled as unionable in
the ground truth, k′< k) and Ground Truth False
Negatives (GTFNs, pairs labeled as unionable in
the ground truth but retrieved within a rank thresh-
oldk′,k′> k, or not retrieved at all).
For each query-candidate pair under review,
we provided the LLM with the full content
of both tables. The table data was se-
rialized into a Markdown format using the
MarkdownRawTableSerializer recipe from the
Table Serialization Kitchen library15(Gomm and
Hulsebos, 2025). This serialized data was in-
serted into specific placeholders (‘<Query Table
Data>‘, ‘<Candidate Table Data>‘) within
15Table Serialization Kitchen Github RepositoryQuery: Archeology_2LWSQ5A2.csv
Candidate: Archeology_3ML53C0M.csv
Discovery Item Artifact Date Culture Region
Giza Pyramid Scroll Diamond ~2500 BC Anc. Egypt N. Africa
Tut. Tomb Knife Stone Tab. 1323 BC Anc. Egypt N. Africa
Item Discovery Artifact Date Culture Region
Scroll Giza Pyramid Diamond ~2500 BC Anc. Egypt N. Africa
Knife Tut. Tomb Stone Tab. 1323 BC Anc. Egypt N. Africa
(a) UGEN V1Example: Two archaeology tables with identical
information and permuted but perfectly alignable columns,
incorrectly labeled non-unionable despite clear semantic com-
patibility.
Query: Veterinary-Science_YP1NJGLN.csv
Candidate: Veterinary-Medicine_GVNM098Q.csv
Animal Type Breed Age Health Status Symptoms Diagnosis
Dog Labrador Retr. 3 years Healthy No symptoms Routine check-up
Cat Domestic SH 5 years Overweight Lethargy... Obesity
Animal Type Breed Age Gender Symptoms Diagnosis
Dog Labrador 3 years Male Aggression... Rabies
Cat Siamese 8 years Female Limping... Arthritis
(b) UGEN V2Example: Two veterinary case tables with highly
alignable core columns (Animal Type, Breed, Age, Symptoms,
Diagnosis) representing the same fundamental entity type
(animal patients).
Figure 4: Examples of UGEN Pairs explicitly labeled
as non-unionable in the original ground truth exhibiting
strong compatibility suggesting unionability.
Query: csvData10212811.csv
Candidate: csvData1066748.csv
Player Team POS G AB H HR ... OPS
B Dean GL 1B 96 350 83 7 ... 0.657
Y Arbelo SB 1B 134 461 114 31 ... 0.877
Player Team POS G AB H HR ... OPS
J Colina WS 2B 59 216 66 3 ... 0.832
B Friday LYN SS 85 341 98 2 ... 0.752
(a) WebTable Example 1: Baseball player statistics tables with
identical, rich schemas (including Player, Team, POS, G, AB,
H, HR, OPS, etc.). These tables represent the same entity type
(player season stats) and are highly unionable, but were not
labeled as such in the ground truth.
Query: csvData10025189.csv
Candidate: csvData20099586.csv
Player Team POS A VG G AB R ... OPS
A Ramirez MIL 3B 0.285 133 494 47 ... 0.757
E Chavez ARI 3B 0.246 44 69 6 ... 0.795
Player Team POS A VG G AB R ... OPS
L Castillo NYM 2B 0.245 87 298 46 ... 0.660
R Durham MIL 2B 0.289 128 370 64 ... 0.813
(b) WebTable Example 2: More baseball player statistics tables
with identical schemas, clearly unionable but not labeled as
such.
Figure 5: Examples of LB-W EBTABLE Ground Truth
Incompleteness.Source: OpenData (Canada)
Query: CAN_CSV0000000000000659.csv
Candidate: CAN_CSV0000000000000562.csv
REF_DATE GEO Age group Sex ... V ALUE
2003 Canada Total, 12 years and over Both sexes ... 20723896.0
2003 Canada Total, 12 years and over Both sexes ... 20632799.0
REF_DATE GEO Age group Sex ... V ALUE
2003 Canada Total, 12 years and over Both sexes ... 26567928.0
2003 Canada Total, 12 years and over Both sexes ... 26567928.0
(a) OpenData Example 1: Canadian health survey tables shar-
ing key demographic columns (REF_DATE, GEO, Age group,
Sex) for the same population. This pair represents unionable
statistics about that population but was not labeled as union-
able in the ground truth.
Source: OpenData (Canada)
Query: CAN_CSV0000000000000686.csv
Candidate: CAN_CSV0000000000005304.csv
Sex Type of work Hourly wages UOM UOM_ID ... V ALUE
Both Both full- and
part...Total employees,
all wagesPersons 249 ... 10921.0
Males Both full- and
part...Total employees,
all wagesPersons 249 ... 5645.4
Sex Type of work Weekly wages UOM UOM_ID ... V ALUE
Both Both full- and
part...Total employees,
all wagesPersons 249 ... 11364.5
Males Both full- and
part...Total employees,
all wagesPersons 249 ... 5954.5
(b) OpenData Example 2: Canadian employment statistics.
The query table (data related to ’Hourly wages’) and candidate
table (data related to ’Weekly wages’) share key dimensions
like Sex, Type of work, and UOM. The cell values within their
respective ’Hourly wages’/’Weekly wages’ columns (e.g., ’To-
tal employees, all wages’) describe similar employee groups.
This pair, differing mainly in wage aggregation period (hourly
vs. weekly) and slightly in REF_DATE format (YYYY vs
YYYY-MM), is potentially unionable for comprehensive wage
analysis but was not labeled as such in the ground truth.
Figure 6: Examples of LB-O PENDATAGround Truth
Incompleteness.the prompt detailed below. Crucially, the original
table names were notincluded in the prompt. This
decision was made to avoid potentially biasing the
LLM by providing explicit hints about the table’s
topic beforehand, thereby ensuring the adjudica-
tion relies solely on the semantic and structural
information present in the table content itself.
The prompt utilizes few-shot learning, incorpo-
rating hand-selected positive and negative exam-
ples of unionability from the UGEN benchmarks
themselves to guide the LLM’s judgment (these
examples are represented by a placeholder in the
verbatim prompt below for brevity). The prompt
defines the LLM’s role, outlines core principles for
assessing conceptual coherence and semantic col-
umn alignment, and specifies the required output
format.
The complete prompt structure provided to the
LLM adjudicator is shown below:
You are an experienced data curator evaluating
if two database tables can be meaningfully
combined vertically (unioned). The goal of
unioning is to create a single, larger
dataset containing the same kind of
information or describing the same type of
entity/event.
Your task is to determine if TABLE 1 and TABLE
2 are conceptually compatible enough for a
union operation.
CORE PRINCIPLES FOR UNIONABILITY:
1. Conceptual Coherence: Do both tables
fundamentally describe the same type of
entity (e.g., customers, products, logs) or
record the same type of event (e.g., sales,
website visits)? Appending rows from one
table to the other should result in a
dataset that makes logical sense.
2. Meaningful Column Alignment: There must be a
reasonable set of columns across the two
tables that represent the same underlying
attributes or concepts.
* These columns can have DIFFERENT NAMES
(e.g., "Cust_ID" vs.
"ClientIdentifier").
* They can have DIFFERENT FORMATS (e.g.,
"2023-01-15" vs. "1/15/2023").
* They may have LITTLE TO NO OVERLAP in
actual data values.
* Focus on the semantic meaning of the
columns in the context of their
respective tables.
3. Sufficient Column Matching: The alignment
shouldn 't rely on just one incidental or
minor column. There should be enough
matching among key columns to confidently
conclude that the tables represent the same
underlying information. More aligned
columns representing core attributesincrease confidence.
4. Distinction from Joins: We are NOT looking
for keys to join tables horizontally. We
are assessing if they can be stacked
vertically.
[EXAMPLES OF UNIONABILITY/NON-UNIONABILTY:]
(Hand-selected positive/negative examples from
\textsc{Ugen} benchmark were inserted here
in the actual prompt)
YOUR TASK:
Examine the following two tables based on the
principles and examples above:
TABLE 1:
<Query Table Data (Serialized)>
TABLE 2:
<Candidate Table Data (Serialized)>
PROVIDE YOUR ANSWER IN THE FOLLOWING FORMAT:
1. First line: `UNIONABLE: Yes `or`UNIONABLE:
No`
2. Second line: `EXPLANATION: `followed by a
brief justification focusing on the
conceptual coherence and the sufficiency of
semantic column alignment. Explain why they
are or are not the same kind of data.
D.2 Example LLM Outputs
Below are illustrative examples of the outputs gen-
erated by the LLM adjudicator based on the prompt
above for some table pairs showcased in Appendix
C.1.
Example 1: Archeology_2LWSQ5A2.csv vs.
Archeology_3ML53C0M.csv (UGEN-V1)
UNIONABLE: Yes
EXPLANATION: Both tables describe archeological
artifacts and their attributes (discovery
location, item type, date, culture,
material, etc.). They represent the same
kind of data - records of archeological
findings. Unioning them would create a
consolidated dataset of such findings.
There is strong semantic alignment across
almost all columns, even though the column
order differs.
Example 2: Anthropology_FGTNBDWF.csv vs.
Anthropology_N30U114M.csv (UGEN-V1)
UNIONABLE: No
EXPLANATION: TABLE 1 categorizes
anthropological concepts based on
dimensions like 'Age','Culture ','Arena ',
'Domain ','Meaning ','Origin ', and
'Activity '. TABLE 2, on the other hand,
lists categories such as 'Artifact ',
'Language ','Technology ','Education ', and
'Society 'as columns. While both tables are
related to Anthropology, they representdifferent perspectives and categorizations
within the field. There is insufficient
semantic overlap in the columns to suggest
they describe the same type of entity or
event. Unioning them would likely result in
a table that is not conceptually coherent,
as the rows from each table represent
different kinds of anthropological
classifications.