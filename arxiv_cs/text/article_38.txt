Beyond Chemical QA: Evaluating LLMâ€™s Chemical
Reasoning with Modular Chemical Operations
Hao Li1âˆ—, He Cao2âˆ—, Bin Feng2, Yanjun Shao3, Xiangru Tang3, Zhiyuan Yan1,
Li Yuan1â€¡,Yonghong Tian1â€¡,Yu Li2â€¡,
1Peking University,2International Digital Economy Academy,3Yale University
lihao1984@pku.edu.cn, caohe@idea.edu.cn, liyu@idea.edu.cn
Abstract
While large language models (LLMs) with Chain-of-Thought (CoT) reasoning
excel in mathematics and coding, their potential for systematic reasoning in chem-
istry, a domain demanding rigorous structural analysis for real-world tasks like
drug design and reaction engineering, remains untapped. Current benchmarks
focus on simple knowledge retrieval, neglecting step-by-step reasoning required for
complex tasks such as molecular optimization and reaction prediction. To address
this, we introduce ChemCoTBench, a reasoning framework that bridges molecular
structure understanding with arithmetic-inspired operations, including addition,
deletion, and substitution, to formalize chemical problem-solving into transpar-
ent, step-by-step workflows. By treating molecular transformations as modular
"chemical operations", the framework enables slow-thinking reasoning, mirroring
the logic of mathematical proofs while grounding solutions in real-world chemical
constraints. We evaluate models on two high-impact tasks: Molecular Property
Optimization and Chemical Reaction Prediction. These tasks mirror real-world
challenges while providing structured evaluability. By providing annotated datasets,
a reasoning taxonomy, and baseline evaluations, ChemCoTBench bridges the gap
between abstract reasoning methods and practical chemical discovery, establishing
a foundation for advancing LLMs as tools for AI-driven scientific innovation.1 2
1 Introduction
With the rapid advancement of large language models (LLMs), reasoning capabilities have become a
defining measure of performance. Techniques like chain-of-thought [ 64] prompting enable LLMs
to decompose complex problems into structured, human-like reasoning steps ( system-II [29]),
achieving breakthroughs in mathematics [ 47,54,67], coding [ 14,23], and even Olympiad-level
challenges [ 17,22,61]. Despite recent advances in LLM reasoning capabilities, chemistry, a discipline
fundamental to areas like drug discovery and materials science, still lacks a benchmark that assesses
whether these improvements extend to its complex, domain-specific problem-solving needs. While
several benchmarks have been proposed for LLMs in chemistry [ 16,34,38,43,69], they primarily
focus on domain-specific question answering, which suffers from several key limitations:
1. Lack of Structured, Stepwise Reasoning and Real-World Relevance: Current evaluations often
reduce chemistry assessment to factual recall (e.g., naming compounds or reactions), neglecting
the need for operational reasoning akin to arithmetic or coding. Unlike mathematical problems,
where solutions demand explicit, verifiable steps, chemistry QA tasks fail to simulate how experts
1ChemCoTBench: https://huggingface.co/datasets/OpenMol/ChemCoTBench
2ChemCoTDataset: https://huggingface.co/datasets/OpenMol/ChemCoTBench-CoT
Preprint. Under review.arXiv:2505.21318v1  [cs.AI]  27 May 20251Add
4Delete3
Scaffold
Understand
Chemical Operations: 123â€¦â€¦Previous Chemical QAs
+ð‘²ð‘¶ð‘¯2 ReactionScientific Benchmark
Reasoning LLMs
Chem
 CoT
Bench
1
11
1Functional Groups
2Murcko Scaffold2
1
1Add the hydroxyl 2
2Delete the methyl 
+
1
1Mol-Understanding Mol-Editing Mol-Optimization
Reaction Prediction
Given two parts of Products, Reagents, and Reactants , predict 
the rest of Reaction.
Optimize the molecule for better chemical properties,  e.g. QED, 
LogP, Solubility, DrD -2, JNK, GSK -3ðœ·Help me find the Functional 
Group, extract the Murcko 
Scaffold , distinguish if the 
molecule is Mutated or
Permutated.Add/Delete/Substitute 
the Functional Group of 
this Molecule for me.
2
2Optimization: add & delete 1
1ReactionQ: What is the physical property of 
beryllium?
A: Beryllium is highly toxic if inhaled, 
requiring careful handlingFigure 1: Previous chemical benchmarks focus on factual recall with domain knowledge, while our
ChemCoTBench focuses on the evaluation of step-wise reasoning for complex chemical problems by
defining a set of modular chemical operations.
decompose challenges. For instance, they donâ€™t capture the process of iteratively refining a moleculeâ€™s
substructure to optimize properties, considering crucial real-world factors like synthesizability or
toxicity, or deducing reaction mechanisms through intermediate transformations. This gap means
weâ€™re not fully evaluating the analytical depth required in real-world chemistry. Therefore, evaluations
must shift from these textbook-like problems to challenges that better reflect practical applications.
2. Ambiguous Skill Attribution in Hybrid Evaluations: Existing benchmarks [ 37,50,63]
often conflate reasoning, knowledge recall, and numerical computation into single "exam-style"
metricsâ€”for instance, asking LLMs to calculate reaction yields while simultaneously recalling
reagent properties. This obscures whether strong performance stems from structured reasoning (e.g.,
analyzing reaction pathways) or memorized facts (e.g., solvent boiling points). Such ambiguity
hinders targeted model improvement and misaligns evaluations with downstream tasks like drug
discovery, where success depends on modular reasoning (e.g., decoupling molecular design from
synthesizability checks) rather than monolithic problem-solving.
To address these limitations, we introduce ChemCoTBench , astep-by-step ,application-oriented ,
andhigh-quality benchmark for evaluating LLM reasoning in chemical applications. A core
innovation of ChemCoTBench is its formulation of complex chemical tasks, specifically targeting
molecular modeling and design (Fig.1), into explicit sequences of verifiable modular chemical
operations on SMILES structures (e.g., substructure addition, deletion, or substitution). This approach
allows for a granular assessment of an LLMâ€™s ability to execute and chain together fundamental
chemical transformations. The benchmark features progressively challenging tasks, spanning from
basic molecular understanding and editing to property-guided structure optimization and complex
multi-molecule chemical reactions. High-quality evaluation is ensured through a dual validation
process combining LLM judgment with expert review from 13 chemists.
We employ quantitative assessments for all subtasks in ChemCoTBench to evaluate the chemical
reasoning ability across reasoning-enhanced and non-reasoning LLMs. Experimental results reveal
room for improvement in reasoning LLMs, particularly open-source and distilled-reasoning LLMs,
when addressing complex chemical problems. While these models demonstrate strong performance
in complex mathematical and coding tasks, they are unable to organize chemical knowledge and
establish step-wise modular chemical operations due to the scarcity of chemical reasoning data.
Notably, ChemCoTDataset, the large chemical CoT dataset provided by ChemCoTBench, is shown
to enhance chemical reasoning performance, effectively addressing the reasoning data scarcity issue
in Chemistry.
To summarize, our key contributions in this work are as follows: Firstly, to address the lack of reason-
ing and application-oriented tasks in existing chemical benchmarks, we propose ChemCoTBench,
which evaluates the chemical capabilities of reasoning-LLMs through step-by-step tasks centered on
molecular structure modification. Secondly, ChemCoTDataset is provided by ChemCoTBench to
facilitate LLMs on chemical reasoning. Finally, extensive experiments demonstrate the effectiveness
of ChemCoTBench and its corresponding ChemCoTDataset.
22 Related Works
LLM Chain-of-Thoughts. LLMs have progressed from text generators to reasoning systems, with
[64]â€™s Chain-of-Thought enabling stepwise problem decomposition via "slow-thinking" paradigms.
These reasoning-enhanced LLMs have shown impressive performance in domains requiring system-
atic problem-solving skills, particularly in mathematics and coding. Models like DeepSeek-R1 [ 13],
Gemini [ 56], and Anthropic Claude [ 53] have achieved notable results on mathematical bench-
marks like MATH [ 19] and GSM8K [ 6], while also excelling at programming. Recent studies
have begun exploring LLMs for chemical tasks, such as synthesis planning [ 4] and computational
chemistry [ 26,45,51]. However, these efforts lack a systematic evaluation of LLMsâ€™ chemical reason-
ing capabilities, spanning spatial reasoning, domain-specific knowledge integration, and multi-step
logical inference.
Chemical Benchmarks. Current chemical benchmarks primarily focus on assessing discrete
knowledge retrieval or simple prediction tasks, rather than evaluating the step-by-step reasoning
processes crucial for complex chemical problem-solving. Most existing benchmarks [ 37,38,50,63]
concentrate on question-answering formats that test factual recall and precise calculation but offer
limited insight into a modelâ€™s ability to reason through multi-step chemical problems. Studies like [ 3,
15,43] have begun exploring LLMsâ€™ chemical capabilities but typically focus on isolated tasks rather
than comprehensive reasoning scenarios. Recent work by [ 69] introduces ChemLLM, a chemistry-
specialized LLM framework with supporting datasets, but its benchmark focuses on knowledge recall
rather than complex reasoning. Similarly, [ 15] introduces MolPuzzle, a benchmark for molecular
structure elucidation that advances spatial reasoning evaluation but remains limited to spectral
interpretation rather than broader chemical reasoning. ChemCoTBench advances chemical reasoning
evaluation by using molecular structure to guide step-by-step reasoning, featuring core chemical
arithmetic tasks and advanced cross-context applications for more thorough LLM assessment.
3 ChemCoTBench Construction
Molecule 
OptimizationðŸ‘ðŸ–%
Reaction
Prediction
ðŸ‘ðŸ•%
Molecule 
UnderstandingðŸðŸ—%
Molecule 
EditingðŸ”%Molecule Understanding
SMILES -level Understanding
Scaffold -level Understanding
Functional Group Understanding
Molecule Editing
Substitute Functional Group
Add Functional Group
Delete Functional Group
Molecule Optimization
Physicochemical Properties (QED, LogP, Solubility)
Protein Activation (DRD2, JNK3, GSK -3ð›½)
Reaction Predictions
Retrosynthesis Prediction
Forward Prediction (Major & Byproduct Prediction)
Condition Prediction
Mechanism PredictionFunc -Group100%
90%
0%Scaffold SMILES Add Delete Substitute
100%
0%80%
60%
40%
PhyChem Protein -Act Retro Fwd Condition MechðŸ—ðŸ%ðŸ—ðŸ’%ðŸðŸŽðŸŽ% ðŸðŸŽðŸŽ% ðŸðŸŽðŸŽ% ðŸðŸŽðŸŽ%ðŸðŸŽðŸŽ% ðŸðŸŽðŸŽ%
ðŸ—ðŸ”%
ðŸ–ðŸ—%ðŸ—ðŸ•%
ðŸ—ðŸŽ%
(a)Entity correct rate in understanding & editing(b)
Entity correct rate in optimization & reaction prediction(c)
Figure 2: (a). Distribution analysis for ChemCoTBench. (b). Samples from both molecular
understanding and editing tasks achieved exceptionally high accuracy in chemical expert evaluations
of chemical entities, including function group names, molecule names, chemical operation names,
reaction information, etc. (c).Samples from molecule optimization and reaction prediction also show
high accuracy (> 89%) in chemical expert evaluations.
ChemCoTBench contains 1,495 samples across 22 chemical tasks as the benchmark dataset, as
shown in Fig 2(a). 14,000 high-quality samples with chain-of-thoughts annotations are further
sampled to form the ChemCoTDataset. ChemCoTBench was constructed through over 1,800 hours
of combined expert and LLM-assisted annotation. It comprises four main tasks and 22 subtasks,
covering a broad spectrum of chemical challenges. We define the reasoning steps of each task as
modular chemical operations, as shown in the bottom two lines of Fig. 3. ChemCoTBench is guided
by two core principles: Diversity andQuality . Molecular diversity is ensured by systematically
selecting compounds with varied scaffolds and functional groups, enabling broad coverage of real-
world chemical scenarios. To ensure high data quality, all benchmark samples undergo multi-stage
hybrid review by LLMs and expert chemists, with prompt templates iteratively refined to meet
subtask-specific requirements.
33.1 Task Construction
To evaluate the capabilities of LLMs in chemistry, we constructed a comprehensive suite of tasks.
Foundation Task: Molecule-Understanding. We begin with the recognition and counting of two
fundamental elements of molecules: (1) Functional groups (FGs) , which are critical clusters of atoms
that determine the physicochemical properties and reactivity of organic molecules; (2) Rings , which
maintain fixed conformations and serve as stable building blocks in drug design, crystal engineering,
and polymer synthesis. The recognition and counting of FGs and rings, which require syntactic
and lexical understanding of SMILES, remain challenging for LLMs due to their limited chemical
topology awareness. Next, we evaluate the recognition of two more complex scaffolds: (1) Murcko
scaffolds , which are molecular frameworks obtained by systematically removing side chains and
serve as a foundation for structural analysis in medicinal chemistry; (2) Ring systems , which include
fused and bridged ring systems and pose a significant challenge for molecular synthesis. These
tasks assess deeper hierarchical comprehension. Finally, we introduce SMILES equivalence tasks,
involving permutations and mutations, to test whether LLMs can recognize chemically equivalent
structures despite surface-level variations. This probes the modelsâ€™ robustness to SMILES variability.
Foundation Task: Molecule-Editing. This task assesses whether LLMs can perform basic molecular
editing operations, such as adding, deleting, and substituting functional groups, when guided by
natural language instructions. Analogous to basic arithmetic in mathematics, these editing operations
form the building blocks of molecular manipulation. Complex tasks like molecular optimization or
synthesis can be translated into specific editing operations. For example, a molecular optimization
task can be treated as a series of molecule-editing tasks aimed at improving chemical or biological
properties. This task evaluates two core capabilities: the capacity to maintain chemical validity after
editing operations and the ability to correctly execute the modifications based on textual instructions.
Application Task: Molecule-Optimization. This task evaluates whether LLMs can generate
optimized molecules given a source molecule and target property. We consider two levels of
molecular properties: At the physicochemical level , we aim to improve LogP, solubility, and QED
for improved drug-likeness. At the target level , we aim to improve binding affinity for the DRD2,
GSK3- Î², and JNK3 target, which poses a more challenging task as it requires the understanding
of drug-target interactions. Solving these problems necessitates in-depth analysis and reasoning
capabilities, as LLMs must not only parse the molecular structure but also infer how specific structural
modifications influence target properties through complex chemical and biological interactions.
Application Task: Reaction Prediction. This task evaluates LLMsâ€™ chemical reasoning ability
across four tasks: (1) Forward Prediction : Predict major products and by-products from reactants and
reagents, requiring knowledge of reactivity, reaction rules, and stability. By-product prediction aids
reaction optimization and purification by reflecting kinetics and thermodynamics. (2) Single-Step Ret-
rosynthesis : Given a product and reagents, predict reactants by identifying key bond disconnections
and functional group transformations under constraints. (3) Reaction Condition Recommendation :
Suggest catalysts, solvents, and reagents for given reactants and products, relying on understanding
of solvent effects, catalyst mechanisms, and their impact on yield and selectivity. (4) Reaction
Mechanism Understanding : Includes Next Elementary-Step Product Prediction (predicting intermedi-
ates stepwise, testing electron flow modeling) and Mechanism Route Selection (choosing the most
plausible pathway from alternatives, assessing mechanistic reasoning). Together, these tasks span
from overall product prediction to detailed mechanistic insight, providing a comprehensive test of
LLMs as chemical reasoning agents.
3.2 Benchmark Construction
Data Collection. Raw molecular structures for understanding, editing, and optimization are sourced
from published datasets, including PubChem [ 30], ChEMBL [ 11], ZINC [ 25], and Deep-Mol-
Opt [ 18]. Chemical reactions are collected from patent databases such as USPTO [ 21], Pistachio [ 42],
and Reaxys [ 8]. For reaction mechanism annotation, we refer to the processing pipeline proposed in
[28]. The complete data collection protocols are archived in the Appendix B.
Data Filtering and Sampling. An initial filtration step removed specimens exhibiting: metal-
containing compounds, excessive molecular complexity (defined by the presence of multiple sophis-
ticated functional groups and polycyclic architectures), and factually inconsistent data. To ensure
both high data diversity and broad coverage, we systematically curate diverse chemical features
4Raw Data Collection
Gather raw molecules and reaction equations 
from PubChem, ChEMBL, ZINC ,etc.1
 Filter & Sampling2
Filter & Stratified sampling over different 
weights , complexity , and functional groups / 
reaction types
1Contain metals
 2Is ambiguousRemove invalid molecules that â€¦â€¦
3Is too complexCoT Annotation3
Summarize the step-wise reasoning from models 
and perform CoT distillation with DeepSeek -r1.Question Step-wise CoT
Reasoning models 
Distilled CoT1I need to analyze â€¦â€¦
2The molecule â€¦â€¦
3It is not feasible â€¦â€¦
Distillation
Review & Refinement4
Have Chemisty  Ph.Ds evaluate samples 
and correct flawed reasoning chains.
Count methyl groups number  in this molecule: CC1=CC=C(C)C=C1( p-Xylene) Problem:
Identify fragment: A methyl group  is â€“CHâ‚ƒ, SMILES=C.
Parse molecule SMILES: CC1=CC=C(C)C=C1 shows two â€œCâ€ substituents 
attached to the aromatic ring .
Match fragment occurrences: Each â€œCâ€ outside the ring core represents 
one methyl.Step-wise Reasoning
21
3
Solution: 2
Add a nitro group  and remove the chlorine substituent from Clc1ccc(C)cc1 Problem:
Functional group analysis: Locate Cl at C-4 and CHâ‚ƒ at C-1 on the benzene .
Delete strategy: Remove chlorine substituent to get Cc1ccccc1 ( toluene ).
Add strategy: Add a nitro group  at the former Cl site to get 
CC1=CC=C(C=C1)[N+](=O)[O -](4-nitrotoluene ).
Feasibility: Conversion of toluene to 4-nitrotoluene is well-established.Step-wise Reasoning
21
3
Solution: CC1=CC=C(C=C1)[N+](=O)[O -]
4
Optimize the benzene  to improve the compoundâ€™s solubility property Problem:
Structure analysis: Benzene is fully hydrophobic with no polar groups.
Property analysis: Negligible solubility ; no H-bond donors/acceptors.
1. Add â€“OH at C1 to form phenol;
2. Add â€“COOH at C4 to boost polarity ;
3. Confirm aromaticity  and valence .Step-wise Reasoning
21
3
Solution:  O=C(O)c1ccc(O)cc1
Predict the product  from benzene and chloroethane with AlCl3 reagent Problemï¼š
Reactant analysis: C6H6 (Benzene), CHâ‚ƒCHâ‚‚Cl  (alkyl halide), AlClâ‚ƒ (catalyst).
Identify function group: Locate H at benzene and CHâ‚ƒ at chloromethane.
Reaction type predict: friedel-crafts alkylation , The electrophile is the 
methyl carbocation. Benzene attacks the carbocation
Product predict: Ethylbenzene ( Câ‚†Hâ‚…CHâ‚‚CHâ‚ƒ )..Step-wise Reasoning
21
3
Solution: c1cccc(CC)c1 
4
Dataset Construction
Molecule Understanding Molecule Editing
Molecule Optimization Reaction Prediction
+Samples
Updated Samples
+
Chemical ExpertsFigure 3: The dataset construction pipeline of ChemCoTBench contains four steps, including raw
data collection, molecule filtering and sampling, chain-of-thoughts annotation, and chemical expert
review & refinement. We also visualize the samples from the four main tasks and their corresponding
modular chemical operations during the reasoning process.
across tasks. For molecular understanding, the dataset includes 38 functional groups and 9 ring types.
For editing, we cover 57 functional group transformations. Optimization tasks span 4 molecular
weight-based structural scales. For reaction tasks, we include 100 common reaction classes, 175
distinct reaction conditions, and 123 annotated reaction mechanisms. Together, these components
offer a rich and representative benchmark dataset for evaluating chemical reasoning in LLMs.
Chain-of-Thoughts Annotation for Modular Chemical Operations. To derive intermediate
reasoning steps for complex chemical problems, we distill the chain-of-thought annotations from
LLMs and arrange them as modular chemical operations for systematic evaluation and supervised
fine-tuning of reasoning models. Specifically, we analyze the problem-solving strategies of state-of-
the-art reasoning models, including Gemini-2.5-pro, DeepSeek-R1, and Claude-3.7-sonnet-thinking,
to extract step-wise reasoning patterns. These are distilled into a structured training corpus using
DeepSeek-R1 via CoT prompting. As illustrated in Fig. 3, our distilled CoT samples span key
chemical tasks including molecular understanding, editing, optimization, and reaction prediction.
3.3 Quality Review & Refinement
To ensure the high quality of our benchmark and its large-scale dataset, we performed iterative
evaluation and optimization of the molecules, results, and distilled Chain-of-Though reasoning
processes from DeepSeek-R1-0324 [ 13] in ChemCoTBench. Our hybrid assessment approach
combines automated LLM-based evaluation for scalability with manual expert review by chemists to
guarantee scientific rigor, enabling comprehensive dataset refinement while maintaining efficiency.
LLM-based CoT Evaluation. To improve the quality of CoT annotations in Deepseekâ€™s process,
we focused on two key elements: (1) Task-Specific Prompt Design : We discovered that providing
detailed task descriptions and prior knowledge within prompts significantly enhances the modelâ€™s
performance on chemical tasks. (2) Incorporation of IUPAC name : We found that including IUPAC
names helps LLMs better understand complex molecular structures, as these names offer precise
details about functional groups. Leveraging these insights, we iteratively refined our prompt designs.
We then employed GPT-4o as an LLM verifier to ensure each CoT annotation was consistent with its
corresponding prompt template and the provided IUPAC names.
Chemical Expert Review & Refinement As a rigorous benchmark evaluation, we engaged 13
chemistry PhD candidates from Top Universities to assess the accuracy of chemical entities, including
5Table 1: Experiments for the foundational tasks, including molecule understanding, molecule edit-
ing, and their correlated subtasks. For the functional-group counting task (FG) and ring counting
task (Ring) in the functional-group level molecule understanding, we apply the mean absolute er-
ror (MAE) as the evaluation metric. Tanimoto molecule similarity is applied as the evaluation for the
Murcko scaffold extraction task (Murcko). The accuracy (%) metric is applied to other subtasks.
ModelsFunc-Group Scaffold SMILES Molecule-Edit
FGâ†“Ringâ†“Murcko â†‘Ring-sys â†‘ Eq.â†‘ Add Delete Sub
W/ Thinking
Gemini-2.5-pro-think 0.11 0.60 0.51 87.5 82 100 85 81.7
Claude3.7-sonnet-think 0.21 1.60 0.40 80.0 84 85 80 83.4
DeepSeek-R1 0.27 1.55 0.34 45.0 65 70 70 68.3
o3-mini@20250103 0.13 0.60 0.39 75.0 78 65 55 80.0
o1-mini@20240912 0.21 1.25 0.25 61.7 66 55 80 58.3
Qwen3-235B-A22B-think 0.42 1.00 0.38 82.5 72 40 75 71.7
Qwen3-32B-think 0.25 0.95 0.21 75.0 68 20 55 20.0
Llama-Nemo-49B-think 0.80 1.90 0.09 86.8 46 0 80 8.0
W/o Thinking
GPT-4o@20241120 0.17 1.35 0.21 80.0 72 80 80 65.0
Deepseek-V3 0.15 1.50 0.24 76.7 77 70 75 76.7
Gemini-2.0-flash 0.19 1.65 0.43 75.0 76 65 75 66.7
Qwen3-235B-A22B 0.42 1.00 0.34 82.5 75 40 75 66.7
Qwen3-32B 0.26 0.95 0.22 68.3 67 30 55 25.0
Qwen2.5-72B-Instruct 0.26 0.60 0.24 70.0 61 70 80 56.7
Qwen2.5-32B-Instruct 0.36 0.65 0.12 53.3 62 50 50 48.3
Llama-3.1-70B-Instruct 0.52 1.80 0.12 68.3 67 60 80 50.0
Llama-Nemo-49B 0.72 1.77 0.11 65.0 54 30 55 30.5
Gemma-2-27b-it 0.19 1.65 0.43 66.7 76 75 70 35.0
Phi-4-14B 0.28 1.65 0.15 70.0 65 60 80 38.3
OLMo2-32B-Instruct 0.19 1.05 0.07 63.3 50 15 30 11.7
BioMedGPT-7B 1.6 2.43 0.18 53.3 39 10 12 10
BioMistral-7B 1.0 1.85 0.04 32.5 50 0 10 0
functional groups, molecular names, reaction types, and operation names, in ChemCoTBenchâ€™s CoT
annotations. As shown in Fig. 2 (b), the evaluation revealed near-perfect accuracy for molecule
understanding and editing tasks, while more challenging tasks like molecule optimization and reaction
prediction maintained over 90% accuracy (as shown in Fig. 2 (c)). Furthermore, we corrected these
errors to enhance ChemCoTBenchâ€™s quality.
4 Experiments
4.1 Evaluation Metrics
For understanding tasks, functional group (FG) and ring recognition are treated as counting problems,
with mean absolute error (MAE) used to measure precision. Scaffold-level understanding includes
extracting Murcko scaffolds, evaluated by Tanimoto similarity, and identifying whether complex ring
systems are present, evaluated by accuracy. The SMILES equivalence task is formulated as a binary
decision problem, determining whether the target and source SMILES represent the same molecule,
and is also evaluated using accuracy. For molecule editing, we use Pass@1 to assess whether the
edited molecule meets the instructions. Mechanism route selection is framed as a multiple-choice task
and evaluated by accuracy. Other reaction tasks are modeled as SMILES generation problems, where
evaluation is based on both Top-1 accuracy and fingerprint-based similarity (FTS), using Morgan [ 46],
MACCS [7], and RDKit [32] fingerprints to reflect correctness and structural similarity.
4.2 Evaluated LLMs
Our evaluation includes three model categories: (1) Reasoning LLMs with explicit step-by-step
reasoning, including Deepseek-R1 [ 13], o1-mini [ 58], o3-mini [ 59], Gemini-2.5-pro [ 55], Claude-3.7-
Sonnet-thinking [ 53], Qwen-3-thinking [ 60], Llama-Nemotron-thinking [ 2]; (2) General-purpose
6Table 2: Baseline Performance on Molecule Optimization. The optimized targets are categorized into
physicochemical properties (QED, LogP, solubility) and protein activity-related properties (JNK3,
DRD2, GSK-3 Î²), with the latter posing greater challenges to the modelâ€™s chemical knowledge and
reasoning capabilities. âˆ†is the mean property improvement, where a negative âˆ†indicates that most
optimizations are property degradations. SR% is the success rate that brings property increase.
ModelsLogP Solubility QED DRD2 JNK3 GSK3- Î²
âˆ† SR% âˆ† SR% âˆ† SR% âˆ† SR% âˆ† SR% âˆ† SR%
W/ Thinking
Gemini-2.5-pro-think -0.28 81 1.91 92 0.21 84 0.35 74 -0.04 35 0.04 68
Claude3.7-sonnet-think 0.41 81 0.59 77 0.09 73 0.18 66 -0.01 49 0.01 57
DeepSeek-R1 0.36 74 1.48 97 0.05 72 0.10 62 -0.06 29 -0.02 41
o3-mini@20250103 0.29 68 1.15 85 0.17 86 0.18 69 -0.08 23 -0.03 45
o1-mini@20240912 -0.42 52 1.78 95 0.07 70 -0.03 37 -0.10 15 -0.08 31
Qwen3-235B-A22B-think -0.01 41 0.27 42 0.01 24 0.03 31 -0.01 23 0.01 31
Qwen3-32B-think 0.0 2 0.11 23 0.02 14 0.0 6 -0.02 6 -0.02 5
Llama-Nemo-49B-think -0.64 24 0.20 24 -0.16 41 -0.05 30 -0.15 7 -0.12 11
W/o Thinking
GPT-4o@20241120 -0.20 42 0.82 80 0.05 70 0.05 48 -0.05 30 -0.04 39
DeepSeek-V3 0.08 34 0.47 93 0.08 46 0.02 28 0.0 18 0.0 29
Gemini-2.0-flash 0.35 75 0.19 54 0.10 79 0.15 63 0.03 34 0.0 38
Qwen3-235B-A22B 0.02 41 0.51 45 0.01 26 0.01 31 -0.01 23 0.0 34
Qwen3-32B -0.03 2 0.17 23 0.02 14 -0.01 6 -0.02 6 -0.02 5
Qwen2.5-72B-Instruct -0.12 42 0.28 60 0.03 57 0.04 40 -0.02 26 -0.01 40
Qwen2.5-32B-Instruct 0.03 47 0.42 66 -0.01 54 0.04 32 -0.04 19 -0.02 31
Llama-3.3-70B-Instruct -0.16 42 0.61 80 0.07 61 -0.02 31 -0.04 30 -0.02 40
Llama-Nemo-Super-49B -0.14 27 0.31 41 0.02 50 -0.02 18 -0.04 16 -0.03 27
Gemma-2-27b-it -0.03 34 0.34 66 0.05 56 -0.03 15 0.0 16 -0.01 17
Phi-4-14B -0.10 45 0.28 54 0.11 74 -0.04 18 -0.05 14 -0.04 22
OLMo2-32B-Instruct -2.03 22 1.03 46 -0.13 40 -0.11 7 -0.12 8 -0.11 12
BioMedGPT-7B -0.36 17 0.25 63 -0.29 7 -0.09 5 -0.11 6 -0.08 1
BioMistral-7B 0.01 1 0.24 6 0.0 0 0.0 1 -0.01 1 -0.01 0
non-reasoning LLMs without specialized reasoning mechanisms including GPT-4o [ 24], Qwen-
2.5/3 [ 66], Llama-3.3 [ 12], Gemma-2 [ 57], Phi-4 [ 1], OLMo2 [ 44] (3) Biomolecular LLMs
BioMedGPT [ 39], BioMistral [ 31], and Text+Chem T5 [ 5]. This comprehensive comparison eval-
uates whether reasoning-specific capabilities provide advantages over domain-specific models in
challenging chemical reasoning tasks. Details of evaluation implementation and prompt design are
available in Appendix C.2.
4.3 LLMsâ€™ Performance on Solving ChemCoTBench
We evaluated reasoning LLMs, their non-reasoning counterparts, and task-specific models [ 5,31,39]
on foundational (molecule understanding and editing, Table 1) and application (molecule optimization,
Table 2; reaction prediction, Table 3) tasks within ChemCoTBench. Key findings include:
Hierarchical Skill Transfer. Strong performance in foundational molecular understanding and edit-
ing tasks directly translates to success in complex application tasks. This validates ChemCoTBenchâ€™s
design, where fundamental chemical knowledge underpins advanced problem-solving. For example,
Claude-3.7-sonnet and Gemini-2.5-pro, top performers in foundational tasks (Table 1), also lead in
molecule optimization and reaction prediction.
Efficacy of Advanced Reasoning in Commercial LLMs: Commercial LLMs equipped with
sophisticated reasoning mechanisms (e.g., Deepseek-R1, o3-mini) significantly outperform their
non-reasoning counterparts on ChemCoTBenchâ€™s challenging applied tasks. In molecule optimization
(Table 2), Deepseek-R1 shows a >30% improvement over Deepseek-V3, and o3-mini gains >20%
over GPT-4o. Similar trends are observed for reaction prediction (Table 3). This suggests that RL-
honed "slow thinking" capabilities [ 40,48,62], when combined with sufficient domain knowledge,
enable superior abstraction and problem-solving beyond mere knowledge retrieval.
7Table 3: The chemical reaction task contains forward prediction (Fwd major: major-product prediction,
and Fwd by: by-product prediction), resynthesis prediction (Retro), reaction condition prediction (Con-
dition), and reaction mechanism prediction (NEPP: next element-step product prediction, MechSel:
reaction mechanism selection prediction). FTS: molecule fingerprint similarity with reference.
ModelsFwd major Fwd by Retro Condition NEPP MechSel
Top-1 FTS â†‘Top-1 FTS â†‘Top-1 FTS â†‘Top-1 FTS â†‘Top-1 FTS â†‘Acc.â†‘
W/ Thinking
Gemini-2.5-pro-think 0.72 0.89 0.20 0.51 0.20 0.45 0.20 0.33 0.58 0.53 0.62
Claude3.7-sonnet-think 0.73 0.87 0.25 0.31 0.12 0.27 0.14 0.22 0.24 0.79 0.49
DeepSeek-R1 0.48 0.71 0.21 0.45 0.07 0.41 0.23 0.30 0.15 0.55 0.46
o3-mini@20250103 0.52 0.71 0.20 0.27 0.11 0.39 0.19 0.19 0.18 0.58 0.49
o1-mini@20240912 0.26 0.31 0.11 0.17 0.02 0.15 0.08 0.22 0.09 0.33 0.44
Qwen3-235B-A22B-think 0.03 0.54 0.0 0.07 0.01 0.42 0.20 0.27 0.09 0.63 0.41
Qwen3-32B-think 0.11 0.33 0.09 0.18 0.02 0.24 0.14 0.20 0.08 0.67 0.46
Llama-Nemo-49B-think 0.09 0.18 0.04 0.18 0.0 0.05 0.18 0.19 0.04 0.21 0.47
W/o Thinking
GPT-4o@20241120 0.28 0.58 0.04 0.20 0.03 0.43 0.0 0.08 0.12 0.71 0.43
DeepSeek-V3 0.36 0.62 0.04 0.30 0.03 0.44 0.08 0.16 0.20 0.70 0.45
Gemini-2.0-flash 0.19 0.56 0.01 0.07 0.05 0.41 0.07 0.08 0.13 0.68 0.53
Qwen3-235B-A22B 0.04 0.57 0.0 0.06 0.0 0.30 0.07 0.14 0.07 0.59 0.40
Qwen3-32B 0.06 0.57 0.0 0.13 0.0 0.43 0.01 0.10 0.08 0.67 0.46
Qwen2.5-72B-Instruct 0.04 0.49 0.0 0.13 0.01 0.35 0.01 0.07 0.06 0.60 0.46
Qwen2.5-32B-Instruct 0.01 0.43 0.0 0.12 0.0 0.29 0.02 0.10 0.05 0.50 0.45
Llama-3.3-70B-Instruct 0.02 0.35 0.0 0.08 0.0 0.34 0.06 0.13 0.06 0.41 0.39
Llama-Nemo-49B 0.04 0.40 0.0 0.08 0.0 0.30 0.03 0.05 0.05 0.41 0.46
Gemma-2-27b-it 0.01 0.55 0.0 0.04 0.0 0.48 0.03 0.10 0.04 0.53 0.43
Phi-4-14B 0.01 0.27 0.03 0.10 0.0 0.39 0.0 0.03 0.05 0.57 0.39
OLMo2-32B-Instruct 0.0 0.10 0.0 0.07 0.0 0.10 0.0 0.03 0.01 0.13 0.32
Text+Chem T5 0.44 0.74 0.0 0.07 0.06 0.24 0.0 0.09 0.0 0.0 0.10
Unrealized Promise of Hybrid Thinking in Open-Source Models for Chemistry without Domain-
Specific Data: Current open-source models featuring hybrid thinking modes, such as Llama-3.3-
Nemotron [ 2] and Qwen3 [ 72], achieve substantial, often efficient, performance in general domains
like code and mathematics. However, their advanced reasoning capabilities, intended to be general,
do not effectively transfer to specialized scientific fields like chemistry. We attribute this shortfall to a
critical lack of domain-specific reasoning training data. Our empirical results are stark (Tables 1-3):
enabling the reasoning modes in these models yields no significant performance improvement on
chemical tasks compared to their non-reasoning counterparts. This finding strongly suggests that
general reasoning architectures require specialized data to adapt to new domains.
4.4 Evaluating Strategies to Enhance Chemical Reasoning in Open-Source LLMs
Our preceding analyses underscored the critical role of advanced reasoning capabilities (or "slow
thinking") for tackling complex chemical tasks. This motivates an investigation into efficient methods
for bolstering these capabilities within open-source LLMs.
Challenges in Distilling Chemical Reasoning: Distilling CoT capabilities from advanced LLMs
(e.g., using DeepSeek-R1-generated samples [ 13,71]) is a common strategy to enhance reasoning
in smaller models. However, this approach proves significantly limited for specialized chemical
reasoning. Our experiments (Fig.4) show that Qwen2.5-Instruct models distilled for CoT exhibit
little to no improvement on ChemCoTBench chemical subtasks compared to their non-distilled
counterparts; indeed, smaller base models (1.5B-32B) often perform comparably or better. While
effective for general domains like code and math (Fig.4), this distillation strategy falters in chemistry,
likely due to insufficient volume or specificity of chemical CoT samples in the distillation process,
hindering the development of robust step-by-step chemical reasoning. Moreover, smaller distilled
models (<7B) frequently produce lengthy, repetitive, and irrelevant (hallucinatory) thought processes.
These findings suggest that direct CoT distillation, without substantial domain-specific adaptation, is
an ineffective standalone method for improving chemical reasoning in open-source models.
8Mol Optimization 
PhysChem -LevelMol Optimization 
Target -LevelReaction Mechanism
Route PredictionMath -500 and
LiveCodeBenchMol Understanding 
SMILES Equivalence Mol Editing
W/O CoT CoT Template CoT Process
 Qwen -2.5-Instruct
1.5B 7B 14B 32B010203040Success Rate (%)
Mol Optimization 
Target -LevelDistilled -Qwen -2.5
1.5B 7B 14B 32B10305070
Mol EditingPass@1(%)60
40
20
1.5B 7B 14B 32B1.2
0.8
0.4Mol UnderstandingMean Absolute Error1.0
0.6
0.2Figure 4: The top two rows compare the reasoning performance of the Qwen-2.5-Instruct series
against its DeepSeek-R1-distilled versions. The bottom row illustrates performance improvements in
Qwen-2.5-Instruct when enhanced with the CoT template and detailed CoT process.
Domain-Specific Chemical CoT Data Augmentation Boosts Reasoning: Given the limitations of
direct distillation, we explored enhancing chemical reasoning using our high-quality, domain-specific
ChemCoTDataset via prompting. This dataset was meticulously curated to minimize hallucinations
and align with expert thought processes, which we posited would be vital for chemical reasoning
tasks. We tested this by evaluating two CoT prompting strategies: one providing only coarse strategic
guidance (CoT templates), and another augmented with detailed step-by-step reasoning processes
from our dataset. The results in the bottom line of Fig. 4 consistently demonstrate that our large-scale
chemical CoT dataset significantly enhances the chemical reasoning capabilities of Qwen-2.5 models
across various scales (1.5B to 32B) when used in this way. Augmentation with detailed CoT processes
yielded stable and substantial performance gains across all evaluated tasks. Notably, while CoT
templates stably benefited small models (<14B), larger LLMs develop rigid code & math reasoning
templates that constrain their ability to effectively leverage the chemical patterns for performance
improvement. This highlights an interplay between model scale and the granularity of CoT guidance
required for optimal benefit.
5 Conclusion and Discussion
This paper introduces ChemCoTBench, a new chemical reasoning benchmark to evaluate the complex
chemical problem-solving ability of LLMs. Compared to existing Scientific benchmarks that focus
on simple knowledge retrieval, our ChemCoTBench establishes a step-by-step, application-oriented,
and high-quality benchmark by gathering samples from both foundational and applicational chemical
tasks, including molecule understanding, editing, optimization, and reaction prediction. Furthermore,
a 14k large chemical CoT dataset is also provided for enhancing chemical reasoning ability of
LLMs. Extensive experiments across 22 chemical tasks in ChemCoTBench demonstrate that current
open-source and distillation-based reasoning LLMs still have significant room for improvement in
complex chemical reasoning, while also validating the boosting effect of our large chemical CoT
dataset on chemical reasoning capabilities. ChemCoTBench bridges the gap between LLM reasoning
capabilities and real-world chemical problem-solving needs, offering researchers a standardized
9evaluation platform for complex chemical reasoning. Future works could continue with designing
policy optimization and distillation strategies to enhance the chemical reasoning capability of LLMs.
Chemical-aware reward mechanisms warrant further exploration. We also focus on extending
ChemCoTBench and its chemical CoT dataset to larger biochemical domains and scale.
References
[1]Marah Abdin, Jyoti Aneja, Harkirat Behl, SÃ©bastien Bubeck, Ronen Eldan, Suriya Gunasekar,
Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat
Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa,
Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril
Zhang, and Yi Zhang. Phi-4 technical report, 2024.
[2]Akhiad Bercovich, Itay Levy, Izik Golan, Mohammad Dabbah, Ran El-Yaniv, Omri Puny, Ido
Galil, Zach Moshe, Tomer Ronen, Najeeb Nabwani, et al. Llama-nemotron: Efficient reasoning
models. arXiv preprint arXiv:2505.00949 , 2025.
[3]Andres M Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe
Schwaller. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint
arXiv:2304.05376 , 2023.
[4]Andres M Bran, Theo A Neukomm, Daniel P Armstrong, Zlatko Jon Ë‡cev, and Philippe Schwaller.
Chemical reasoning in llms unlocks steerable synthesis planning and reaction mechanism
elucidation. arXiv preprint arXiv:2503.08537 , 2025.
[5]Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Mat-
teo Manica. Unifying molecular and textual representations via multi-task language modelling.
InInternational Conference on Machine Learning , pages 6140â€“6157. PMLR, 2023.
[6]Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.
[7]Joseph L. Durant, Burton A. Leland, Douglas R. Henry, and James G. Nourse. Reoptimization
of MDL keys for use in drug discovery. Journal of Chemical Information and Computer
Sciences , 42(6):1273â€“1280, 2002.
[8] Elsevier. Reaxys, 2024.
[9]Chaoran Feng, Wangbo Yu, Xinhua Cheng, Zhenyu Tang, Junwu Zhang, Li Yuan, and Yonghong
Tian. Ae-nerf: Augmenting event-based neural radiance fields for non-ideal conditions and
larger scene. arXiv preprint arXiv:2501.02807 , 2025.
[10] Hanyu Gao, Thomas J. Struble, Connor W. Coley, Yuran Wang, William H. Green, and Klavs F.
Jensen. Using machine learning to predict suitable conditions for organic reactions. ACS Central
Science , 4:1465 â€“ 1476, 2018.
[11] Anna Gaulton, Anne Hersey, MichaÅ‚ Nowotka, A Patricia Bento, Jon Chambers, David Mendez,
Prudence Mutowo, Francis Atkinson, Louisa J Bellis, Elena CibriÃ¡n-Uhalte, et al. The chembl
database in 2017. Nucleic acids research , 45(D1):D945â€“D954, 2017.
[12] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,
Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama
3 herd of models. arXiv preprint arXiv:2407.21783 , 2024.
[13] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,
Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-R1: Incentivizing reasoning capability in
LLMs via reinforcement learning. arXiv preprint arXiv:2501.12948 , 2025.
[14] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen,
Xiao Bi, Y . Wu, Y . K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When
the large language model meets programming â€“ the rise of code intelligence, 2024.
10[15] Kehan Guo, Bozhao Nan, Yujun Zhou, Taicheng Guo, Zhichun Guo, Mihir Surve, Zhenwen
Liang, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Can llms solve molecule puzzles? a
multimodal benchmark for molecular structure elucidation. Advances in Neural Information
Processing Systems , 37:134721â€“134746, 2024.
[16] Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xi-
angliang Zhang, et al. What can large language models do in chemistry? a comprehensive
benchmark on eight tasks. Advances in Neural Information Processing Systems , 36:59662â€“
59688, 2023.
[17] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu,
Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: A challenging benchmark for
promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint
arXiv:2402.14008 , 2024.
[18] Jiazhen He, Huifang You, Emil SandstrÃ¶m, Eva Nittinger, Esben Jannik Bjerrum, Christian Tyr-
chan, Werngard Czechtizky, and Ola Engkvist. Molecular optimization by capturing chemistâ€™s
intuition using deep neural networks. Journal of cheminformatics , 13:1â€“17, 2021.
[19] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn
Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.
arXiv preprint arXiv:2103.03874 , 2021.
[20] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu,
and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language
models. arXiv preprint arXiv:2503.06749 , 2025.
[21] Zan Huang, Hsinchun Chen, Zhi-Kai Chen, and Mihail C Roco. International nanotechnology
development in 2003: Country, institution, and technology field analysis based on uspto patent
database. Journal of nanoparticle Research , 6:325â€“354, 2004.
[22] Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan,
Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. Olympicarena: Benchmarking multi-discipline
cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems ,
37:19209â€“19253, 2024.
[23] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jia-
jun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint
arXiv:2409.12186 , 2024.
[24] Aaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,
AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. GPT-4o system card. arXiv
preprint arXiv:2410.21276 , 2024.
[25] John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc:
a free tool to discover chemistry for biology. Journal of chemical information and modeling ,
52(7):1757â€“1768, 2012.
[26] Yunhui Jang, Jaehyung Kim, and Sungsoo Ahn. Chain-of-thoughts for molecular understanding,
2024.
[27] Joonyoung F Joung, Mun Hong Fong, Nicholas Casetti, Jordan P Liles, Ne S Dassanayake,
and Connor W Coley. Electron flow matching for generative reaction mechanism prediction
obeying conservation laws. arXiv preprint arXiv:2502.12979 , 2025.
[28] Joonyoung F Joung, Mun Hong Fong, Jihye Roh, Zhengkai Tu, John Bradshaw, and Connor W
Coley. Reproducing reaction mechanisms with machine-learning models trained on a large-scale
mechanistic dataset. Angewandte Chemie International Edition , 63(43):e202411296, 2024.
[29] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011.
[30] Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi
Han, Jane He, Siqian He, Benjamin A Shoemaker, et al. Pubchem substance and compound
databases. Nucleic acids research , 44(D1):D1202â€“D1213, 2016.
11[31] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-antoine Gourraud, MickaÃ«l Rouvier,
and Richard Dufour. BioMistral: A collection of open-source pretrained large language models
for medical domains. In 62th Annual Meeting of the Association for Computational Linguistics
(ACLâ€™24) , 2024.
[32] Greg Landrum, Paolo Tosco, Brian Kelley, Ricardo Rodriguez, David Cosgrove, Ric-
cardo Vianello andSriniker, Gedeck, Gareth Jones, Nadine Schneider, Eisuke Kawashima,
Dan Nealschneider, Andrew Dalke, Matt Swain, Brian Cole, Samo Turk, Aleksandr Savelev,
Alain Vaucher, Maciej WÃ³jcikowski, Ichiru Take, Vincent F. Scalfani, Rachel Walker, Kazuya
Ujihara, Daniel Probst, Guillaume Godin, Axel Pahl, Tadhurst-cdd, Juuso Lehtivarjo, Francois
Berenger, and Jason D Biggs. RDKit: Open-source cheminformatics and machine learning,
May 2024.
[33] Hao Li, Da Long, Li Yuan, Yu Wang, Yonghong Tian, Xinchang Wang, and Fanyang Mo.
Decoupled peak property learning for efficient and interpretable electronic circular dichroism
spectrum prediction. Nature Computational Science , pages 1â€“11, 2025.
[34] Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, and Qing Li. Tomg-bench: Evaluating
llms on text-based open molecule generation. arXiv preprint arXiv:2412.14642 , 2024.
[35] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and
Jiaqi Wang. Visual-rft: Visual reinforcement fine-tuning. arXiv preprint arXiv:2503.01785 ,
2025.
[36] Daniel Mark Lowe. Extraction of chemical structures and reactions from the literature . PhD
thesis, 2012.
[37] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought
chains for science question answering. Advances in Neural Information Processing Systems ,
35:2507â€“2521, 2022.
[38] Xingyu Lu, He Cao, Zijing Liu, Shengyuan Bai, Leqing Chen, Yuan Yao, Hai-Tao Zheng, and
Yu Li. Moleculeqa: A dataset to evaluate factual accuracy in molecular comprehension. arXiv
preprint arXiv:2403.08192 , 2024.
[39] Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu Qiao, and Zaiqing Nie.
BioMedGPT: Open multimodal generative pre-trained transformer for biomedicine. arXiv
preprint arXiv:2308.09442 , 2023.
[40] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft:
Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967 , 3, 2024.
[41] Liuzhenghao Lv, Hao Li, Yu Wang, Zhiyuan Yan, Zijun Chen, Zongying Lin, Li Yuan, and
Yonghong Tian. Navigating chemical-linguistic sharing space with heterogeneous molecular
encoding. arXiv preprint arXiv:2412.20888 , 2024.
[42] J. Mayfield, D. Lowe, and R. Sayle. Pistachio - search and faceting of large reaction databases.
ACS Fall 2017 , 2017.
[43] Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, MartiÃ±o RÃ­os-GarcÃ­a, Benedict Emoek-
abu, Aswanth Krishnan, Tanya Gupta, Mara Schilling-Wilhelmi, Macjonathan Okereke,
Anagha Aneesh, et al. Are large language models superhuman chemists? arXiv preprint
arXiv:2404.01475 , 2024.
[44] Team OLMo. 2 olmo 2 furious, 2025.
[45] Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei Han, and Lianhui Qin.
Structured chemistry reasoning with large language models. arXiv preprint arXiv:2311.09656 ,
2023.
[46] Nadine Schneider, Roger A. Sayle, and Gregory A. Landrum. Get your atoms in orderâ€”an open-
source implementation of a novel and robust molecular canonicalization algorithm. Journal of
Chemical Information and Modeling , 55(10):2111â€“2120, 2015.
12[47] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical
reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.
[48] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,
Mingchuan Zhang, YK Li, Y Wu, et al. Deepseekmath: Pushing the limits of mathematical
reasoning in open language models. arXiv preprint arXiv:2402.03300 , 2024.
[49] Haozhan Shen, Peng Liu, Jingcheng Li, Chunxin Fang, Yibo Ma, Jiajia Liao, Qiaoli Shen, Zilun
Zhang, Kangjia Zhao, Qianqian Zhang, et al. Vlm-r1: A stable and generalizable r1-style large
vision-language model. arXiv preprint arXiv:2504.07615 , 2025.
[50] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai
Yu. Scieval: A multi-level large language model evaluation benchmark for scientific research. In
Proceedings of the AAAI Conference on Artificial Intelligence , volume 38, pages 19053â€“19061,
2024.
[51] Xiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu
Zhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large
language models improves chemical reasoning. arXiv preprint arXiv:2501.06590 , 2025.
[52] Zhenyu Tang, Junwu Zhang, Xinhua Cheng, Wangbo Yu, Chaoran Feng, Yatian Pang, Bin Lin,
and Li Yuan. Cycle3d: High-quality and consistent image-to-3d generation via generation-
reconstruction cycle. arXiv preprint arXiv:2407.19548 , 2024.
[53] Anthropic Team. Claude-3.7-sonnet: Hybrid reasoning model.
[54] CodeGemma Team, Heri Zhao, Jeffrey Hui, Joshua Howland, Nam Nguyen, Siqi Zuo, Andrea
Hu, Christopher A. Choquette-Choo, Jingyue Shen, Joe Kelley, Kshitij Bansal, Luke Vilnis,
Mateo Wirth, Paul Michel, Peter Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham
Agrawal, Zhitao Gong, Jane Fine, Tris Warkentin, Ale Jakse Hartman, Bin Ni, Kathy Korevec,
Kelly Schaefer, and Scott Huffman. Codegemma: Open code models based on gemma, 2024.
[55] DeepMind Team. Gemini 2.5 pro preview: even better coding performance.
[56] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,
Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly
capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.
[57] Gemma Team. Gemma 2: Improving open language models at a practical size, 2024.
[58] OpenAI Team. O1-mini: advancing cost-efficient reasoning.
[59] OpenAI Team. Openai o3-mini.
[60] Qwen Team. Qwen3: Think deeper, act faster.
[61] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry
without human demonstrations. Nature , 625(7995):476â€“482, 2024.
[62] Peiyi Wang, Lei Li, Zhihong Shao, RX Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang
Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. arXiv
preprint arXiv:2312.08935 , 2023.
[63] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R
Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level
scientific problem-solving abilities of large language models. arXiv preprint arXiv:2307.10635 ,
2023.
[64] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,
Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.
Advances in neural information processing systems , 35:24824â€“24837, 2022.
[65] Guowei Xu, Peng Jin, Li Hao, Yibing Song, Lichao Sun, and Li Yuan. Llava-o1: Let vision
language models reason step-by-step. arXiv preprint arXiv:2411.10440 , 2024.
13[66] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan
Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint
arXiv:2412.15115 , 2024.
[67] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng
Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2.5-math technical report: Toward
mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122 , 2024.
[68] Wangbo Yu, Chaoran Feng, Jiye Tang, Xu Jia, Li Yuan, and Yonghong Tian. Evagaussians:
Event stream assisted gaussian splatting from blurry images. arXiv preprint arXiv:2405.20224 ,
2024.
[69] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang,
Xiangyu Yue, Wanli Ouyang, et al. Chemllm: A chemical large language model. arXiv preprint
arXiv:2402.06852 , 2024.
[70] Junwu Zhang, Zhenyu Tang, Yatian Pang, Xinhua Cheng, Peng Jin, Yida Wei, Xing Zhou,
Munan Ning, and Li Yuan. Repaint123: Fast and high-quality one image to 3d generation
with progressive controllable repainting. In European Conference on Computer Vision , pages
303â€“320. Springer, 2025.
[71] Han Zhao, Haotian Wang, Yiping Peng, Sitong Zhao, Xiaoyu Tian, Shuaiting Chen, Yunjie Ji,
and Xiangang Li. 1.4 million open-source distilled reasoning dataset to empower large language
model training. arXiv preprint arXiv:2503.19633 , 2025.
[72] Xingyu Zheng, Yuye Li, Haoran Chu, Yue Feng, Xudong Ma, Jie Luo, Jinyang Guo, Haotong
Qin, Michele Magno, and Xianglong Liu. An empirical study of qwen3 quantization. arXiv
preprint arXiv:2505.02214 , 2025.
14Appendix
A Full Related Works 1
A.1 LLM Chain-of-Thoughts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
B Data Construction Details 1
B.1 Data Collection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
B.2 Dataset Composition and Filtering Strategies . . . . . . . . . . . . . . . . . . . . 2
B.3 Rationale for Task Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
C Experimental Details 3
C.1 Hardware Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
C.2 Evaluation Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
C.3 Count Distribution Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
D Case Study for Tasks in ChemCoTBench 5
D.1 Case Study for Molecule Understanding . . . . . . . . . . . . . . . . . . . . . . . 5
D.2 Case Study for Molecule Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
D.3 Case Study for Molecule Optimization . . . . . . . . . . . . . . . . . . . . . . . . 6
E Task Example 9
A Full Related Works
A.1 LLM Chain-of-Thoughts.
The evolution of large language models (LLMs) has transitioned from basic text generation to
sophisticated reasoning systems, exemplified by [ 64] Chain-of-Thought methodology that facili-
tates systematic problem decomposition through deliberate cognitive paradigms. These advanced
reasoning architectures demonstrate exceptional proficiency in domains demanding structured ana-
lytical capabilities, particularly in mathematical computation and programming tasks. Benchmark
evaluations on MATH [ 19] and GSM8K [ 6] reveal significant achievements by models including
DeepSeek-R1 [13], Gemini [56], and Anthropic Claude.
LLM Reasoning on Multimodal Domain. With the rapid development of vision-language domain,
reasoning on images and videos are increasingly important [ 65]. Visual-RFT [ 35], VLM-R1 [ 49]
establish the visual chain-of-thoughts data construction pipeline and RL-based post-training strategies.
Vision-R1 [ 20] further propose the cold start strategy for better multimodal reasoning. In the 3D
domain, [ 9,52,68,70] apply chain-of-thoughts to point clouds and 3D objects to achieve LLM
reasoning.
LLM Reasoning on Chemical Domain. Emerging applications in chemical sciences demonstrate
LLM capabilities in spectra analysis [ 33], synthesis planning [ 4], and computational chemistry [ 26,
45,51]. Also, LLMs [ 41] show outstanding multi-task generalization ability on the molecule domain
and protein domain. However, current research lacks a comprehensive assessment of chemical
reasoning capacities encompassing spatial cognition, domain knowledge assimilation, and complex
logical inference processes.
B Data Construction Details
In this section, we propose the detailed information during our benchmark and dataset construction
process, including the data source description, dataset composition, filtering strategies, and the
1Table 4: The Dataset Statistics of ChemCoTBench and its Large CoT Dataset. We visualize
the sample numbers for every subtask in ChemCoTBench. The data distribution of molecule
understanding & editing, molecule optimization, and reaction prediction is nearly average.
#Mol-Understanding Mol-Edit Mol-Optimization Reaction
Func-Group Scaffold SMILES Add Del Sub Physico Protein Fwd Retro Cond Mech
Bench
mark120 100 100 20 20 60 300 300 200 100 90 275
CoT
Dataset6400 4500 3000
rationale for dataset construction. In Table. 4, we also visualize the data distribution of subtasks in
ChemCoTBench.
B.1 Data Collection
The raw molecular structures used for understanding, editing, and optimization are obtained from sev-
eral published datasets, including PubChem [ 30], ChEMBL [ 11], ZINC [ 25], and Deep-Mol-Opt [ 18].
Chemical reaction data are separately collected from patent databases, including USPTO [ 21], Pista-
chio [ 42], and Reaxys [ 8]. For reaction mechanism annotation, we followed the processing pipeline
described in [28].
B.2 Dataset Composition and Filtering Strategies
Molecular Samples (25% of Benchmark): Although the ZINC database contains 250,000
molecules, we observed that its molecular weight distribution is relatively concentrated. To en-
sure diversity, we carefully selected molecules from PubChem, ChEMBL, and ZINC based on
molecular weight and structural complexity. This filtering process resulted in a smaller but more
representative molecular subset for our benchmark.
Molecular Optimization Pairs (38% of Benchmark): The Deep-Mol-Opt dataset provided 198,559
molecular pairs with property annotations. However, we excluded pairs with minimal property
improvement ( âˆ†< 0.3) or those containing complex polycyclic structures that might challenge LLM
comprehension. The remaining high-quality pairs were retained for molecular optimization tasks.
Chemical Reaction Samples (19% of Benchmark): Reaction equations (including reactants,
products, conditions, and catalysts) were sourced from USPTO, Pistachio, and Reaxys. To avoid
redundancy, we balanced the selection across these databases by reaction type and catalyst diversity.
For reaction mechanism annotation, we incorporated 275 manually curated examples from [ 28],
which were chosen for their high quality and balanced distribution.
B.3 Rationale for Task Construction
Molecular Understanding and Editing Tasks: Molecular understanding and editing tasks are
designed as closed-ended problems with deterministic answers. Since these tasks rely on well-defined
chemical properties and structures, we directly sampled molecules from PubChem, ChEMBL, and
ZINC as the source data. The corresponding ground-truth answers, including molecular properties
and SMILES transformations, are programmatically extracted using RDKit, ensuring accuracy and
reproducibility.
Molecular Optimization Task Design: Unlike fixed-answer tasks, molecular optimization is
inherently open-ended, where multiple valid optimization paths may exist for a given input molecule.
To construct this dataset, we considered two sampling strategies:
â€¢Baseline Model-Generated Optimizations: Advantage : Enables sampling large-scale and multi-step
optimization paths for source molecules; Limitation : Existing models often fail to preserve scaffold
consistency, a critical requirement in drug design.
2â€¢Predefined Molecular Pairs: Advantage : Ensures chemically meaningful transformations with
verified property improvements; Limitation : limited molecule samples.
To maintain the scaffold consistency, we adopt the second strategy for our ChemCoTBench, sourcing
molecular pairs from Deep-Mol-Opt [ 18]. We perform Murcko scaffold similarity analysis to validate
scaffold consistency, confirming that the selected pairs maintain structural integrity while optimizing
target properties.
Reaction Prediction Task Design: Reaction prediction is a cornerstone of chemical research and
industrial applications. From an academic standpoint, it is fundamental to understanding chemical
reactivity, discovering novel transformations, and advancing the design of new molecules. In practical
applications, accurate reaction prediction accelerates drug discovery, facilitates materials science
innovation, optimizes chemical manufacturing processes, and enables the automation of chemical
synthesis. Our benchmark aims to evaluate LLMsâ€™ capabilities in this multifaceted domain rigorously.
â€¢Forward Reaction Prediction : This task, pivotal for academic discovery and industrial applications
like drug development, evaluates an LLMâ€™s ability to predict both major products and, uniquely in
our benchmark, byproducts from given reactants and reagents. Data is sourced from 100 distinct
reaction classes from Pistachio. To enhance difficulty and assess deeper reasoning, the reaction
type is deliberately omitted, requiring the model to first infer the plausible reaction type and then
deduce potential products, thereby providing a comprehensive understanding of reaction outcomes
crucial for optimization.
â€¢Retrosynthesis Prediction : Essential for planning the synthesis of novel compounds, this task
assesses an LLMâ€™s understanding of reverse chemical logic, specifically its capacity to identify
strategic bond disconnections and propose valid precursor structures. We focus on single-step
retrosynthesis, considering multi-step planning a more complex hybrid task, to directly evaluate
core retrosynthetic reasoning. Data comprises 100 reaction classes from Pistachio, and problem
formulation includes providing reagents alongside the target product to help narrow the solution
space and guide the LLM towards chemically relevant disconnections.
â€¢Reaction Condition Prediction : Predicting optimal reaction conditions (catalysts, solvents, reagents)
is critical for synthesis success, efficiency, and selectivity. This task tests an LLMâ€™s knowledge of
how these components influence reaction pathways. Following Gao et al. [ 10] for data construction
from USPTO [ 36] (retaining reactions with at most one catalyst, two solvents, and two reagents),
we uniquely model this as a SMILES sequence generation task for catalyst, solvent, and reagent
prediction, offering a more rigorous challenge than simple MCQ formats by requiring specific
chemical structure (In SMILES) generation.
â€¢Mechanism Prediction : Understanding reaction mechanismsâ€”the step-by-step sequence of elemen-
tary reactionsâ€”is fundamental to chemistry, providing the "why" and "how" behind transformations
and enabling rational design and optimization. This task evaluates an LLMâ€™s grasp of core mecha-
nistic principles such as electron flow, intermediate stability, bond-making/breaking sequences, and
the influence of conditions on pathways, addressing a significant gap in current LLM assessments,
which often treat reactions as black boxes. Inspired by prior works [ 27,28] but aiming for a more
holistic probe, we introduce two subtasks: "Next Elementary Step Product Prediction," where the
LLM, given a sequence of annotated elementary steps, predicts the subsequent product, testing
its ability to comprehend and extrapolate mechanistic progression; and "Reaction Mechanism
Selection (MCQ type)," where the LLM chooses the most plausible mechanism from several
alternatives for a given reaction (reactants, conditions, reagents), assessing its capacity to discern
how subtle changes in reagents or conditions dictate specific mechanistic routes, thereby evaluating
both sequential understanding and discriminative judgment of mechanistic pathways.
C Experimental Details
C.1 Hardware Requirements
The experimental workload was supported by a dedicated GPU cluster comprising three high-
performance computing nodes: an NVIDIA RTX A6000 (48GB VRAM) and an RTX 3090 (24GB
VRAM) for LLM API scheduling and deployment of smaller models (1.5B/7B parameters), comple-
mented by an NVIDIA A100 (80GB VRAM) node dedicated to large-scale LLM inference. This
3heterogeneous configuration achieved optimal resource allocation, with the A100â€™s tensor cores and
high-bandwidth memory handling memory-intensive model inferences while the A6000/3090 pair
efficiently managed concurrent API requests and lighter workloads. Storage requirements remained
modest at approximately 1GB, encompassing benchmark datasets (SMILES strings and annotations),
quantized model checkpoints, and evaluation logs, all hosted on an NVMe-backed filesystem for
rapid data access.
C.2 Evaluation Metrics
To comprehensively assess model performance, we employ the following metrics:
Accuracy: The proportion of correctly predicted outcomes, providing a baseline measure of overall
correctness. For reaction prediction tasks (e.g., forward reaction prediction), we choose the Top-1
accuracy, which specifically means the modelâ€™s highest-ranked prediction exactly matches the true
product(s).
Mean Absolute Error: Quantifies the average magnitude of errors in continuous predictions, offering
insight into precision for regression tasks (e.g., molecular property prediction).
Scaffold Similarity: Measured via the Tanimoto coefficient of molecular scaffolds, this evaluates
structural conservation between generated and reference molecules. Values range from 0 to 1,
representing scaffolds without similarity to correct scaffolds, with higher scores indicating better
preservation of core frameworks.
Improvement: Absolute gains in target properties, reported as: Mean improvement: Average
uplift across all samples. Max/min improvement: Extreme cases highlighting model potential and
limitations.
Success Rate: The fraction of generated molecules exceeding a predefined threshold (e.g., > 0.8 for
solubility), reflecting practical utility.
Validity: Measures the proportion of generated SMILES strings that are syntactically correct and can
be successfully parsed into a chemical structure by RDKit [32].
C.3 Count Distribution Analysis
Error Distribution for Ring Counting Task Error Distribution for Functional -Group Counting TaskAbsolute Error Per Sample
Absolute Error Per Sample
Figure 5: Error distribution analysis for ring counting and functional-group counting tasks.
For the two counting tasks under molecule understandingâ€”ring counting and functional-group
countingâ€”we evaluated model performance using the Mean Absolute Error in the main experimental
section to quantify overall accuracy. To provide a more granular analysis of LLMsâ€™ capabilities in
these molecule-specific counting tasks, we further examined the error distribution across different
models.
As illustrated in Fig. 5, the ring counting task proves significantly more challenging than the functional-
group counting task. This is evident from the error distributions: For functional-group counting, the
majority of errors fall within the 0.0â€“1.0 range, indicating relatively high accuracy. In contrast, ring
counting exhibits higher errors, with most models (except Gemini-2.5-pro) showing an average MAE
> 1.0. Gemini-2.5-pro stands out as the only model achieving consistently low errors in this task,
suggesting superior structural reasoning capabilities. This disparity highlights the inherent difficulty
of ring counting, which requires precise identification of cyclic structuresâ€”a more complex task
4than detecting localized functional groups. The results underscore the need for further refinement of
LLMs in handling intricate molecular topologies.
D Case Study for Tasks in ChemCoTBench
To provide a more detailed analysis of the performance of different types of LLMs across various
tasks in ChemCoTBench, we supplement the quantitative findings in the Experiment section with
visualizations of model outputs. In the following three subsections, we present case visualizations
from distinct subtasks: molecule understanding, molecule editing, and molecule optimization.
Table 5: This is a case study for molecule understanding. We visualize the Murcko Scaffold generation
task in molecule understanding because it can provide detailed information compared to number
prediction tasks and correction distinguishing tasks.
Source Molecule GT-Scaffold Gemini-2.5-pro Llama3.3-70B
100% 41.8% 27.8%
100% 38.6% 0.0%
100% 56.8% 15.4%
100% 33.3% 13.3%
D.1 Case Study for Molecule Understanding
The molecule understanding task in ChemCoTBench contains three types of subtasks, including
number prediction subtasks (functional-group counting and ring counting), distinguish subtasks (ring
system distinguish, SMILES consistency distinguish), and scaffold generation subtask (murcko
scaffold generation). To visualize the detailed molecule structure generated by different types of
LLMs, we select the Murcko scaffold generation subtask as the case visualization source.
Table. 5 presents four examples featuring distinct ring structures and functional groups. Through
comparative analysis, we identify two key advantages of commercial LLMs over smaller open-source
LLMs:
5Superior SMILES Parsing Accuracy. Commercial LLMs(e.g., Gemini-2.5-Pro) correctly interpret
molecular SMILES structures, with predicted structures closely matching the source molecules (only
1â€“2 bond position errors). In contrast, open-source models like LLaMA-3.1 generate structures
largely inconsistent with the source molecules.
Robust Instruction-Following for Murcko Scaffolds. When tasked with extracting Murcko
scaffoldsâ€”defined as the maximal connected framework retaining ring systems while removing
non-critical functional groupsâ€”commercial LLMs adhere to the provided instructions and generate
connected scaffolds. Llama-3.1, however, often outputs fragmented substructures, highlighting its
limitations in instruction comprehension.
D.2 Case Study for Molecule Editing
The molecule editing task in ChemCoTBench contains three parts: adding a target functional group
to the molecule, removing a target functional group from the molecule, and substituting a functional
group with a target functional group from the molecule. In Table. 6, we visualize samples from
each subtask with different types of target functional groups. Two key observations emerge from the
analysis:
Functional Group Recognition Directly Impacts Task Performance. Gemini-2.5-Pro demon-
strates high precision in functional group identification, enabling accurate molecular editing. While
Qwen3-235B correctly identifies functional groups, it frequently fails to execute valid molecular
modifications. LLaMA-3.1 struggles with basic functional group recognition, severely limiting its
task completion capability. This trend aligns with the modelsâ€™ performance in the functional-group
counting subtask under molecule understanding, confirming a strong correlation between recognition
accuracy and downstream success.
2D Molecular Structure Parsing Poses a Significant Challenge. Due to the inherently linear nature
of SMILES notation, LLMs generally perform well on molecules with extended one-dimensional
chains. However, their accuracy declines sharply when processing complex polycyclic systems with
intricate 2D topologies.
D.3 Case Study for Molecule Optimization
Molecular Optimization Tasks involve improving three physicochemical properties (QED, Solubility,
LogP) and three protein-related activation capabilities (DRD2, JNK3, GSK3- Î²). Since large language
models perform poorly in optimizing protein-related activations, we focus on their ability to optimize
physicochemical properties. Table 3 presents the optimization results of three LLMs, including
Gemini-2.5-pro, Qwen3-235B, and llama3.3-70B, revealing two key observations:
LLMs exhibit significant potential in this task. Despite the inherent difficulty of molecular
optimization, LLMs exhibit significant potential in this task. We observed that these models introduce
diverse functional groups, including halogens, aldehydes, hydroxyls, and amines, indicating broad
chemical adaptability. However, some modifications led to negative optimization, likely due to limited
understanding of the underlying physicochemical principlesâ€”a gap that could be addressed through
targeted training.
Commercial LLMs demonstrate bolder optimization strategies compared to open-source models .
For instance, Gemini-2.5-pro frequently performs skeleton-level modifications (e.g., additions or
deletions), whereas Qwen3-235B and llama3.3 tend toward conservative insertions with minimal
structural changes. This contrast highlights the greater flexibility and potential of commercial LLMs
in molecular optimization.
6Table 6: The case study for functional-group addition, deletion, and substitution in the molecule
editing task. For better comparison, we visualize the predicted results from Gemini-2.5-pro (reasoning
LLM), Qwen3-235B (non-reasoning LLM), and llama3.3-70B (non-reasoning LLM) and show the
outstanding chemical reasoning ability of Gemini compared to other open-sourced LLMs.
Instruction Source Molecule Gemini-2.5-pro Qwen3-235B Llama3.3-70B
Add Functional Groups
Add the amide
group while
keeping the
molecule scaf-
fold unchanged.
Add the amine
group while
keeping the
molecule scaf-
fold unchanged.
Invalid SMILES
Add the benzene
ring group while
keeping the
molecule scaf-
fold unchanged.
Delete Functional Groups
Delete aldehyde
group while
keeping the
molecule scaf-
fold unchanged.
Delete hydroxyl
group while
keeping the
molecule scaf-
fold unchanged.
Delete nitro
group while
keeping the
molecule scaf-
fold unchanged.
Invalid SMILES
Substitute Functional Groups
Remove
aldehyde group
and add halo
group for the
molecule.
Invalid SMILES
Remove
aldehyde group
and add halo
group for the
molecule.
7Table 7: The case study for Molecule Optimizations.
Source Molecule Gemini-2.5-pro QWen3-235B Llama3.3-70B
LogP Optimization
âˆ† = 1 .16 âˆ† = 0 .51 âˆ† =âˆ’3.76
âˆ† = 1 .68 âˆ† = 0 .68 âˆ† =âˆ’0.39
âˆ† = 0 .68 âˆ† = 0 .01 âˆ† = 0 .0
QED Optimization
âˆ† = 0 .38 âˆ† = 0 .01 âˆ† =âˆ’0.03
âˆ† = 0 .34 âˆ† = 0 .0 âˆ† =âˆ’0.03
Solubility Optimization
âˆ† = 3 .47 âˆ† = 0 .87 âˆ† = 0 .48
âˆ† = 1 .08 âˆ† = 0 .87 âˆ† = 0 .52
8E Task Example
To better demonstrate the data structure of ChemCoTBench and the large-scale CoT dataset, we
conducted visualizations of representative samples from four distinct tasks: molecule understanding,
molecule editing, molecule optimization, and reaction prediction. As illustrated in Figure. 6, Figure. 7,
Figure. 8, and Figure. 10, each figure presents sample cases from different tasks, with text highlighted
in red indicating the chemical-specific prompt design.
Question example for Molecule Understanding
You are a chemical assistent . Please Determine whether the ring_system_scaffold  is in 
the Molecule. Input: a molecule's SMILES string, a Ring System Scaffold. Output: yes / 
no.
Definition: The ring system scaffold consists of one or more cyclic (ring -shaped) 
molecular structures
Source Molecule: CC(C)n1cnc2c(NCc3ccc( -c4ccccc4)cc3)nc(N(CCO)CCO)nc21, IUPAC 
of Source Molecule: 2 -[2-hydroxyethyl -[6-[(4-phenylphenyl)methylamino] -9-propan -2-
ylpurin -2-yl]amino]ethanol.  Ring system scaffold: c1ccc( -c2ccccc2)cc1.
Your response must be directly parsable  JSON format: 
{{
    "input_structure ": "original input structure",
         
    "molecule_structure_analysis ": "describe the structure of the input Molecule",
            
    "scaffold_analysis ": "describe the ring system scaffold",
            
    "matching_analysis ": "matching the scaffold with the molecule",
            
    "output": "Yes / No"
}}
DO NOT output other text except for the answer. If your response includes ``` json ```, 
regenerate it and output ONLY the pure JSON content.
Figure 6: Task example for molecule understanding subtask: Ring System Counting Task.
9Question example for Molecule Editing
You are a chemical assistant. Given the SMILES structural formula of a molecule, help 
me add a specified functional group and output the improved SMILES sequence of the 
molecule. Input: Molecule SMILES string, Functional Group Name. Output: Modified 
Molecule SMILES string.
Source Molecule: O=S(=O)(Cc1nc( -c2cccs2)no1)c1ccc2ccccc2n1, Instrcution:  Modify the 
molecule by adding a aldehyde.
Your response must contain the step -by-step reasoning, and must be directly parsable  
JSON format:
{{
            "molecule_analysis ": "[your reasoning] Analyze the functional groups and other 
components within the molecule",
            "function_group_introduce_strategy ": "[your reasoning] Determine how and at 
which site the new group can be most reasonably added",
            "feasibility_analysis ": "[your reasoning] Assess the chemical viability of the 
proposed modification",
            "output": "Modified Molecule SMILES"
}}
DO NOT output other text except for the answer. If your response includes ``` json ```, 
regenerate it and output ONLY the pure JSON content..Figure 7: Task example for molecule editing subtask: Functional-Group Adding Task.
Question example for Molecule Optimization
You are a chemical assistent ,  Optimize the Source Molecule to improve the GSK3 -beta 
property (Glycogen Synthase Kinase 3 -beta Inhibition) while following a structured 
intermediate optimization process. IUPAC names are provided to resolve ambiguities 
in SMILES. For functional groups, IUPAC takes priority over SMILES. Note these key 
group distinctions which are difficult to distinguish (1) Piperazine (1,4 -
diazacyclohexane): C1CNCCN1 (2) Piperidine (azinane): C1CCNCC1 (3) Pyrrole 
(azole): C1=CC=CN1
Source Molecule: c1ccc( -c2cc(NCc3cccnc3)n3nccc3n2)cc1, IUPAC of Source Molecule: 
5-phenyl -N-(pyridin -3-ylmethyl)pyrazolo[1,5 -a]pyrimidin -7-amine.
Always output in strict, raw JSON format. Do NOT include any Markdown code block 
wrappers (e.g., ``` json ``` or ```). Your response must be directly passable JSON 
format: \n
                {{
                    "Structural Analysis of Source Molecule": "",
                    "Property Analysis": "",
                    "Limitation in Source Molecule for Property": ""
                    "Optimization for Source Molecule": "",
                    "Final Target Molecule": "SMILES",
                }}
DO NOT output other text except for the answer. If your response includes ``` json ```, 
regenerate it and output ONLY the pure JSON content.
Figure 8: Task example for molecule optimization subtask: Optimizing GSK-3 Î²Task.
10Question example for Next Elementary-step Product PredictionWe have one typical reaction (reaction class: 'Bromo Sonogashiracoupling', starting reactants:'CCOC(=O)C(OC(C)(C)C)c1c(C)cc2ccc(Br)cc2c1-c1ccc(Cl)cc1.C#CC(C)(C)O', reagents:'CCN(CC)CC.C1CCOC1.CCOC(=O)C(OC(C)(C)C)c1c(C)cc2ccc(Br)cc2c1-c1ccc(Cl)cc1.C#CC(C)(C)O.[Cl-].[Cu]I.[NH4+]', reaction condition: 'Reaction with Pd coordinated with 3 or 4 ligands'). Here are the previous elementary reaction steps: Elementary Step 1: { "reactants": c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1, "products": c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1.c1ccc(P(c2ccccc2)c2ccccc2)cc1, "step annotation": Ligand leaving, } Elementary Step 2: { "reactants": c1ccc([PH](c2ccccc2)(c2ccccc2)[Pd]([PH](c2ccccc2)(c2ccccc2)c2ccccc2)[PH](c2ccccc2)(c2ccccc2)c2ccccc2)cc1, "products": c1ccc([PH]([Pd][PH](c2ccccc2)(c2ccccc2)c2ccccc2)(c2ccccc2)c2ccccc2)cc1.c1ccc(P(c2ccccc2)c2ccccc2)cc1, "step annotation": Ligand leaving, } Now, we want to predict the next elementary reaction step.Currently we know the basic information: "current_step_info": { "reactants": [Cu]I.C#CC(C)(C)O, "step annotation": Copper activation, } Under the same reaction condition and reagents, please give me the products of the next step element reaction. Just return the SMILES of prediction. Your response must contains directly parsableJSON format: { "pred_smi": str }Figure 9: Task example for mechanism prediction subtask: Next Elementary-step Product Prediction.
11Question example for Mechanism Route SelectionFor reaction class: 'Carboxylic acid + amine condensation', under the condition of 'Condensation using BOP' and given reagents (written in SMARTS format) '[#8]=[#6]-[#8].[#7,#16,#8].[#7]-[#8]-[P+]', which following description is the correct elementary reaction stages description, considering the mechanism of this type of reaction?Choices: A: Carboxylic acid deprotonation â†’Reaction of carboxylic acid and HATU/HBTU â†’Addition of HOBt(1-hydroxybenzotriazole) into carboxylic acid-HATU/HBTU â†’Amine attacks HOBt-carboxylic acid complex â†’Proton exchange between amide and HOBtB: Proton exchange â†’Formation of a single bond between carboxylic acid and protonated DCC â†’Addition of amine (thiol) into carboxylic acid-DCC complex â†’Cleavage into amide and urea â†’Proton exchange between amide and urea C: Carboxylic acid deprotonation â†’Reaction of carboxylic acid and CDI â†’Addition of imidazole into carboxylic acid-CDI â†’Amine attacks imidazole-carboxylic acid complex â†’Proton exchange between amide and imidazole D: Addition of alcohol under the acidic conditions / deprotonation of alcohol â†’Neutralization of protonated ester / Addition of alcohol under the basic conditions E: Proton exchange â†’Formation of a single bond between carboxylic acid and protonated DCC â†’Addition of HOBt(1-hydroxybenzotriazole) into carboxylic acid-DCC complex â†’Amine attacks HOBt-carboxylic acid complex â†’Proton exchange between amide and HOBtF: Deprotonation of carboxylic acid â†’Nucleophilic substitution G: Carboxylic acid deprotonation â†’Reaction of carboxylic acid and BOP â†’Addition of HOBt(1-hydroxybenzotriazole) into carboxylic acid-HATU/HBTU â†’Amine attacks HOBt-carboxylic acid complex â†’Proton exchange between amide and HOBtH: Addition of amine into carboxylic acid â†’Deprotonation of amine â†’Hydroxide ion leaves I: Addition to thionyl chloride â†’Addition of chloride â†’Pseudo-pericyclic expulsion of SO2, HCl â†’Nucleophilic addition â†’Nucleophilic addition â†’Deprotonation J: Protonation of carbonyl or deprotonation of alcohol â†’Alcohol addition to carbonyl â†’Protonation or deprotonation of complex â†’Water or hydroxide ion leaving â†’Proton exchange.Return the choice (capital letter) in JSON format: { "choice": str # (e.g. 'A'/'B') }Figure 10: Task example for mechanism prediction subtask: Mechanism Route Selection (MechSel).
12