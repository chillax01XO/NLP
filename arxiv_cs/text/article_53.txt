arXiv:2505.21212v1  [cs.AI]  27 May 2025Interpretable DNFs
Martin C. Cooper1,Imane Bousdira2,Cl´ement Carbonnel3
1IRIT, University of Toulouse, France
2IRIT, INP Toulouse, France
3LIRMM, CNRS, University of Montpellier, France
{cooper, imane.bousdira }@irit.fr, clement.carbonnel@lirmm.fr
Abstract
A classifier is considered interpretable if each of its
decisions has an explanation which is small enough
to be easily understood by a human user. A DNF
formula can be seen as a binary classifier κover
boolean domains. The size of an explanation of a
positive decision taken by a DNF κis bounded by
the size of the terms in κ, since we can explain a
positive decision by giving a term of κthat evalu-
ates to true. Since both positive and negative de-
cisions must be explained, we consider that inter-
pretable DNFs are those κfor which both κandκ
can be expressed as DNFs composed of terms of
bounded size. In this paper, we study the family of
k-DNFs whose complements can also be expressed
ask-DNFs. We compare two such families, namely
depth- kdecision trees and nested k-DNFs, a novel
family of models. Experiments indicate that nested
k-DNFs are an interesting alternative to decision
trees in terms of interpretability and accuracy.
1 Introduction
Interpretable models are critical in machine learning applica-
tions requiring accountability of decisions [24; 23 ]. In par-
ticular, there is a growing interest in models whose decisions
can always be explained in a way that is comprehensible by
a human user. In recent work on formal explainability [25;
17; 4; 3; 20 ], two notions of explanation of decisions have
emerged. An abductive explanation corresponds to a minimal
set of features that caused the decision, whereas a contrastive
explanation corresponds to a means of changing the decision
with changes to a minimal set of features. A theoretical line
of research, starting from a list of desirable properties rather
than a particular definition, has identified abductive explana-
tions as the basis for determining what constitutes a sufficient
reason for a decision [1; 10 ].
In this paper, we deem a model to be interpretable if each
of its decision has both a short abductive explanation and a
short contrastive explanation. Observe that we are consid-
ering interpretability as an orthogonal question to explain-
ability , which depends on whether we can efficiently find
an explanation of each decision. There is a considerable
literature on the question of which families of models areexplainable, whether explainability means the existence of
polynomial-time or efficient-in-practice algorithms to find ex-
planations [21; 22; 14; 18; 11; 8; 19; 16; 15 ].
This criterion for interpretability is very restrictive. The
only commonly used family of models that are interpretable
in this sense are decision trees whose depth is bounded by a
small constant. In contrast, linear classifiers, random forests,
decision lists and neural networks may all require a linear
number of features in an explanation. However, it is theo-
retically possible that very different families of interpretable
models exist. The purpose of this paper is to study the struc-
ture of interpretable models in order to find a competitive al-
ternative to decision trees.
We restrict our attention to classifiers which are functions
of boolean features only. (However, most of our results can
be extended to non-boolean features through binarisation.)
In Section 2, we observe that a boolean classifier κis in-
terpretable if and only if both κand its complement κare
expressible as k-DNF formulas, where kis the upper bound
on the size of explanations. In Section 3, we show that such
classifiers can always be expressed by short k-DNF formu-
las composed of at most kkterms. For small enough k, this
shows that direct representation of interpretable classifiers as
DNF formulas is always possible. Then, we describe in Sec-
tion 4 a simple graph-based condition which guarantees that
the complement of a k-DNF formula is also expressible in k-
DNF and use this property to define nested k-DNFs, a new
family of interpretable classifiers that is orthogonal to deci-
sion trees. We study the expressivity of nested k-DNFs in
Section 5. Finally, we present in Section 6 a practical algo-
rithm for learning nested k-DNFs, and show empirically that
classifiers constructed this way are competitive with decision
trees on various datasets.
2 Preliminaries
We denote by Fthe feature space, which for most of the paper
will be {0,1}n, and by Fthe set of features {1, . . . , n }.
Definition 1. Given a function κ:F→ {0,1}and an input
v= (v1, . . . , v n)∈F, a weak abductive explanation (wAXp)
of(κ, v)is a subset AofFsuch that ∀x= (x1, . . . , x n)∈
F,(∧i∈A(xi=vi))⇒κ(x) =κ(v). A weak contrastive
explanation (wCXp) of (κ, v)is a subset CofFsuch that
∃x∈F,(∧i∈F\C(xi=vi))∧κ(x)̸=κ(v). An abductiveexplanation (AXp) is a subset-minimal wAXp. A contrastive
explanation (CXp) is a subset-minimal wCXp.
In order to give a formal definition of interpretability of a
family of models, we first give a parameterized definition of
interpretability of a classifier based on AXps/CXps.
Definition 2. Letkbe a natural number. A function κ:F→
{0,1}isk-AXp-interpretable if for each v∈F, there is an
AXp of ( κ, v)of size at most k. A non-constant function is
k-CXp-interpretable if for each v∈F, there is a CXp of size
at most k. By convention, a constant function is deemed to be
k-CXp-interpretable.
To see that k-AXp-interpretability and k-CXp-
interpretability do not coincide, consider the parity function
κwhich returns 1 if the sum of its nboolean features is
even and 0 otherwise. For any v∈F, changing one feature
changes the parity, which implies both that (κ, v)has a CXp
of size 1and that, on the other hand, the only AXp is of size
n. Thus the existence of a small CXp does not guarantee
the existence of a small AXp. On the other hand, for any
κ, the existence of a small AXp (for all inputs) implies the
existence of a small CXp, as we now show.
Lemma 1. A function κthat is k-AXp-interpretable is also
k-CXp-interpretable.
Proof. Suppose that κisk-AXp-interpretable. The case of
constant functions is trivial, so we assume that κis non-
constant. Thus, given an arbitrary input v∈F, there is
another input v′∈Fsuch that κ(v′)̸=κ(v). By k-AXp-
interpretability, (κ, v′)has an AXp Aof size at most k. Let
yi=viifi∈ F \ Aandyi=v′
iifi∈A. By definition,
κ(y) =κ(v′)̸=κ(v). Therefore, Ais a wCXp of (κ, v)and
hence some subset of Ais a CXp of size at most k.
Since k-CXp-interpretability follows from k-AXp-
interpretability, this leads to a natural definition of inter-
pretable models in terms of k-AXp-interpretability.
Definition 3. A family Mof models is interpretable if there
is a constant ksuch that every classifier κ∈ M isk-AXp-
interpretable.
We now focus on the case where the feature space Fis
boolean. Given a boolean function κover boolean variables
(x1, . . . , x n), aliteral is either a variable xior its negation xi.
A boolean formula is in disjunctive normal form (DNF) if it is
a disjunction of terms , which are conjunctions of literals. For
simplicity of presentation, we freely interpret terms as either
sets or conjunctions of literals depending on context. A DNF
formula is in k-DNF if each of its terms has size at most k. We
say that a conjunction (or set) of literals is consistent if it does
not contain both a variable and its negation. An implicant of
κis a consistent conjunction of literals Qsuch that κmaps
to1all assignments to (x1, . . . , x n)for which Qevaluates to
true. An implicant of κisprime if it is subset-minimal.
Given a DNF formula Dwith variables Xand a consistent
set of literals QoverX, we denote by D[Q]the DNF formula
with variables {xi∈X:xi/∈Qandxi/∈Q}obtained from
Dby removing all the terms that contain the negation of a
literal in Qand replacing each remaining term t=V
l∈Sl
witht[Q] =V
l∈S\Ql. IfD1andD2are DNF formulas thatexpress respectively a boolean function and its complement,
then for any choice of Qthe formulas D1[Q]andD2[Q]also
express functions that are complements to each other. The
sizeofD, denoted by |D|, is the number of terms in Dand its
length ||D||is the sum of the sizes of its terms. Throughout
the paper we will use L(D)(resp. T(D)) to denote the sets
of literals (resp. terms) that appear in the formula D.
For a boolean classifier κ, the prime implicants of κ(resp.
κ) are in one-to-one correspondence with AXps for positive
(resp. negative) decisions. The relationship between inter-
pretability and expressibility as a k-DNF formula is made ex-
plicit by the following proposition.
Proposition 1. A binary boolean classifier κ:{0,1}n→
{0,1}isk-AXp-interpretable if and only if both κand its
complement are expressible as k-DNFs.
Proof. The ‘if’ direction follows from the fact that a term that
evaluates to true is a wAXp (of size k), and hence some subset
will be an AXp. The ‘only if’ direction follows from the fact
thatκ(resp. κ) is equivalent to the disjunction of terms corre-
sponding to the AXps of its positive (negative) decisions.
Using Proposition 1, it is straightforward to verify that a
boolean function κisk-AXp-interpretable if and only if both
κandκare equivalent to the disjunction of their prime im-
plicants of size at most k. The standard double-DNF expres-
sion of ak-AXp-interpretable classifier is the pair (Dκ, Dκ),
where Dκis the DNF formula whose terms are the prime im-
plicants of κof size at most kandDκis the DNF formula
whose terms are the prime implicants of κof size at most k.
The smallest integer ksuch that a boolean function κand
its complement can be expressed as k-DNF formulas is called
thecertificate complexity ofκ[2, Chapter 11 ]. This measure
is well studied in theoretical computer science and computa-
tional learning theory [9; 6], but little appears to be known
about the structure of functions whose certificate complexity
is bounded by a small constant.
Example 1. Decision trees are a well-known family of clas-
sifiers which have the reputation of being interpretable. In-
deed, if kis the depth of a decision tree, then the correspond-
ing classifier κDTand its complement κDTcanboth be ex-
pressed as k-DNFs. Given a path πfrom the root to a leaf,
letL(π)denote the set of literals labelling the edges in the
pathπ. We assume a binary classifier, so each leaf is labelled
0 or 1. Let P0andP1denote the sets of paths from the root
to, respectively, leaves labelled 0 and leaves labelled 1. Then
the classifier κDTcorresponding to the decision tree can be
expressed as the following DNF:
κDT(x) =_
π∈P1^
ℓ∈L(π)ℓ
Furthermore, κDTcan also be expressed as a DNF:
κDT(x) =_
π∈P0^
ℓ∈L(π)ℓ
Observe that both these DNFs are k-DNFs since the length
of paths is at most k.As seen in Example 1, if κcan be represented as a decision
tree of depth kthenκisk-AXp-interpretable. However, the
converse implication does not hold. In this paper, we are in-
terested in identifying new families of interpretable classifiers
that are orthogonal to those derived from decision trees.
Example 2. Fork= 2, a characterisation of 2-DNF formu-
las whose complement is expressible in 2-DNF can be derived
from a recent result [8, Corollary 2 ]. Together with Propo-
sition 1, this characterisation implies that a classifier κis2-
AXp-interpretable if and only if it is equivalent to a DNF with
one of the following forms (where the literals a, b, c, d are ar-
bitrary and not necessarily distinct): (i) (a∧b)∨(c∧d), (ii)
(a∧b)∨(b∧c)∨(c∧d), and (iii) (a∧b)∨(b∧c)∨(c∧d)∨(d∧a).
Interestingly, certain DNFs of this kind cannot be represented
as decision trees of depth 2(see Example 4 for more details).
However, they all satisfy a different combinatorial criterion
for2-AXp-interpretability that we describe in Section 4.
3 Short explanations imply few explanations
In this section, we show that every k-AXp-interpretable clas-
sifier is expressible as a k-DNF consisting of at most kkterms
(independently of the number nof features). This result gives
further justification to work directly with DNF representa-
tions of k-AXp-interpretable classifiers when kis small. In
particular, this implies that if a classifier can provide an expla-
nation of size at most kfor every decision, then all decisions
can be explained using only 2kkdistinct explanations.
Theorem 1. Every k-AXp-interpretable classifier is express-
ible as a k-DNF formula that contains at most kkterms.
Proof. Letκbe a k-AXp-interpretable classifier and
(Dκ, Dκ)be the standard double-DNF expression of κ. We
will show that Dκcontains at most kkterms. If κis constant
then the theorem obviously holds, so let us assume that it is
not. (This assumption implies in particular k >0,|Dκ|>0,
and|Dκ|>0.) We claim that for all integers j≥0, either
|Dκ|< kjor there exists a consistent set Qofjliterals that
is contained in at least (1/k)j· |Dκ|terms of Dκ. We will
prove this claim by induction on j.
The base case j= 0 is immediate because every term in
Dκcontains the empty set of literals. Now, let jbe such that
1≤j≤kand suppose that the claim holds for j−1. If
|Dκ|< kj−1then|Dκ|< kjand we are done. Otherwise,
there exists a consistent set Q′ofj−1literals and a set Sof
at least (1/k)j−1· |Dκ|terms of Dκsuch that every term in
Scontains Q′. We distinguish two cases.
Case 1: Q′={l:l∈Q′}has non-empty intersection with
every term in Dκ. Then, Q′is an implicant of κ. The terms of
Dκare prime implicants of κandQ′is contained in at least
one term of Dκ, soQ′is contained in exactly one term of Dκ.
This implies (1/k)j−1· |Dκ| ≤1and hence |Dκ|< kj.
Case 2: there exists a term tinDκwhose intersection with
Q′is empty. Consider the DNF formulas Dκ[Q′]andDκ[Q′].
Observe that t[Q′]is a term of Dκ[Q′], ands[Q′]is a term of
Dκ[Q′]for all s∈S. (This last observation follows from
the fact that every term in Sis a prime implicant of κ: these
terms are consistent and contain Q′, so they cannot intersect
Q′.) Ift[Q′]is the empty term, then Q′is an implicant of κ;this is not possible because at least one term in Dκcontains
Q′. In addition, as Dκ[Q′]andDκ[Q′]express functions that
are complements of each other, the set {l:l∈t[Q′]}must
have non-empty intersection with every term in Dκ[Q′]and
in particular with every term in {s[Q′]|s∈S}. The term
t[Q′]contains at most kliterals, so there exists l∈t[Q′]such
that at least (1/k)· |S|terms in Scontain l. Then, the set of
literals Q=Q′∪ {l}is contained in at least (1/k)· |S| ≥
(1/k)·(1/k)j−1· |Dκ|= (1/k)j· |Dκ|terms of Dκand the
claim holds by induction.
We can now finish the proof of the theorem. Every term in
Dκis a prime implicant of κsoDκcannot contain the same
term twice. In addition, every term in Dκhas size at most k.
Then, for j=kwe have either |Dκ|< kkor(1/k)k·|Dκ| ≤
1and the theorem follows.
The specific bound of Theorem 1 is sharp as there exist
k-AXp-interpretable classifiers that cannot be expressed as a
DNF formula with fewer than kkterms. A concrete example
is the complement of a classifier κcorresponding to a DNF
formula Dwithkterms of size exactly k, with all literals
negative and no literal occurring twice. This function κis
k-AXp-interpretable, has kkprime implicants, and by mono-
tonicity these implicants must be contained in distinct terms
in any DNF expression of κ.
Corollary 1. Letκ:{0,1}n→ { 0,1}be a k-AXp-
interpretable classifier over a set of features F. There ex-
ists a set Eof at most 2kksubsets of Fsuch that for every
v∈ {0,1}n,Econtains an AXp of size at most kof(κ, v).
Proof. Applying Theorem 1, we derive that κandκcan be
expressed as k-DNF formulas of size at most kk. The terms of
these formulas are implicants of κandκrespectively, and we
can further assume that they are prime implicants. Let Ebe
the set of all subsets of Fwhose features correspond exactly
to a term. (Note that multiple terms may correspond to the
same set of features, so Ecan be strictly smaller than the
sum of the sizes of these formulas.) Then, for any choice of
vat least one term evaluates to true and the corresponding set
inEconstitutes a wAXp of size at most kof(κ, v). Finally,
this term corresponds to a prime implicant (of either κorκ)
so no strict subset can be a wAXp.
Another interesting consequence of Theorem 1 is that it
provides an explicit characterisation of interpretable families
of models (as per Definition 3).
Corollary 2. A family Mof models is interpretable if and
only if there exists a constant ksuch that every classifier κ∈
Mis expressible as a DNF formula of length at most k.
Proof. For the forward direction, if every classifier in Mis
j-AXp-interpretable then by Theorem 1 they are expressible
as DNF formulas of length at most k=j·jj. Conversely,
the complement of a DNF formula of length at most k >
0is always expressible as a k-DNF of length at most kk+1.
Therefore, if every classifier in Mis expressible as a DNF
formula of length at most kthenMis interpretable.4 Induced matchings and nested k-DNF
In this section we describe a simple criterion for a classifier
described by a k-DNF formula to be k-AXp-interpretable.
This criterion is orthogonal to expressibility as a decision tree
of depth k, and we will show in the subsequent section that it
defines a remarkably expressive family of classifiers.
LetDbe a DNF formula that expresses a boolean function
κ. Atransversal ofDis a subset of L(D)that intersects every
term in D. If we let TDdenote the set of all minimal transver-
sals of D, then κhas the following canonical expression as a
DNF:
κ(x) =_
T∈TD^
l∈Tl
Note that the canonical DNF expression of κmay include in-
consistent terms. If Ddoes not contain two terms t1, t2such
thatt1⊂t2, then the canonical complement of the canoni-
cal complement of DisDitself1. From this perspective, it
is clear that the function expressed by a given k-DNF for-
mula Disk-AXp-interpretable if all minimal transversals of
Dhave cardinality at most k.
LetGD= (V, E)be the bipartite graph with V=L(D)∪
T(D)and{l, t} ∈Eif and only if l∈t. An induced match-
ingofGDis a subset M⊆Esuch that no two edges in M
share an endpoint and no edge in Eintersects two distinct
edges in M. We denote by mim (GD)the maximum number
of edges in an induced matching of GD.
Lemma 2. LetDbe ak-DNF formula expressing a boolean
function κ. If mim( GD)≤k, then κisk-AXp-interpretable.
Proof. We show that every minimal transversal of Dhas car-
dinality at most k. Suppose for the sake of contradiction that
Dhas a minimal transversal Tof size q > k . By minimality,
for every literal l∈Tthere exists a term tl∈T(D)such that
tl∩T={l}. Then, the set of edges {{l, tl} |l∈T}is an
induced matching of GDof size q > k . This is not possible
because mim( GD)≤k.
Example 3. Consider the majority function on 2k−1argu-
ments defined by κmaj(x1, . . . , x 2k−1)≡(P2k−1
i=1xi≥k).
This function κmajisk-AXp-interpretable since it is the dis-
junction of all terms composed of exactly kpositive literals
and its complement is the disjunction of all terms composed
of exactly knegative literals.
The graphs associated with these formulas do not contain
induced matchings of size larger than k, soκmajsatisfies the
criterion for k-AXp-interpretability given by Lemma 2. How-
ever, it is well known that any decision tree representing κmaj
must have depth at least 2k−1, as any path starting from
the root that alternates between positive and negative literals
cannot reach a leaf before all variables have been assigned.
The simple condition provided by Lemma 2 already de-
fines a new family of k-AXp-interpretable classifiers, those
expressible by k-DNF formulas with no induced matchings of
1This is a well-known property of hypergraph dualisation, see
e.g.[5, Chapter 2 ].sizek+1. From a practical viewpoint, the interest of this fam-
ily is limited because its definition is not constructive: with-
out a clear structure, it is difficult to design efficient heuristics
for learning formulas of this kind directly from data.
We address this issue by defining a smaller family of clas-
sifiers whose structure is more explicit. Consider k2literals
ℓi,j(1≤i, j≤k). We can view {ℓi,j}as ak×kmatrix:
L=
ℓ1,1ℓ1,2. . . ℓ 1,k
...
ℓk,1ℓk,2. . . ℓ k,k

We will define a k-DNF Dcomposed of mterms (where mis
arbitrary) whose complement is also expressible as a k-DNF.
For each p= 1, . . . , m , letrpi(i= 1, . . . , k ) bekintegers
between 0 and ksuch thatPk
i=1rpi≤k. Then define Das
follows:
D=m_
p=1k^
i=1rpi^
j=1ℓi,j
The condition thatPk
i=1rpi≤kfor each p= 1, . . . , m
ensures that Dis ak-DNF. We call such a DNF a nested k-
DNF. The term
k^
i=1rpi^
j=1ℓi,j
ofDis the conjunction of, for each i= 1, . . . , k , the rpi
leftmost elements in row iof the matrix L.
Proposition 2. Every boolean function expressible as a
nested k-DNF formula is k-AXp-interpretable.
Proof. LetD=Wm
p=1Vk
i=1Vrpi
j=1ℓi,jbe a nested k-DNF
formula. Towards a contradiction, suppose that there exists
an induced matching Mof size k+ 1inGD. By the pigeon-
hole principle, at least two literals that appear in Mbelong
to the same row iMofL. Two terms are matched with these
two literals, and the term with the largest value for rpiMmust
contain both. This is impossible because Mis an induced
matching. Applying Lemma 2, the function expressed by D
is therefore k-AXp-interpretable.
Example 4. Observe that all k-DNF formulas with qterms
are nested if q≤k. Indeed, for any such formula Dwe can
setLto be a k×kmatrix of literals whose ith row contains
the literals of the ith term of D(possibly with repetition if
the term has fewer than kliterals). Then, for p≤qwe set
rpi=kifp=i, and rpi= 0 otherwise. These parameters
will produce exactly the formula D. In general, such formulas
are not expressible as decision trees of depth smaller than
k2[13].
On the other hand, the function κmajof Example 3 is not
expressible as a nested k-DNF . We proceed again by contra-
diction. If κmajcould be represented by a nested k-DNF gen-
erated from a k×kmatrix L, thenLwould contain only pos-
itive literals. Let L1be set of the literals in the first column
ofL, and Jthe other positive literals. All terms generated
fromLmust contain at least one literal from L1. If|J| ≥k,
then there is a term consisting of kpositive literals not oc-
curring in the first column, and which therefore could not begenerated. Hence, we must have |L1|=kand|J|=k−1.
Without loss of generality, assume L1={x1, . . . , x k}and
J={xk+1, . . . , x 2k−1}. For each i= 1, . . . , k , the term
xiV
xj∈Jxjmust be generated from Lby taking literals from
a single row, since it contains a single literal from the first col-
umn of L. It follows that the columns 2,3, . . . , k ofLcontain
only elements of J. Since |J|=k−1, the second column of
Lmust contain at least one repeated element. Without loss
of generality, assume that this repeated element is xk+1and
that it occurs in the two rows whose first elements are x1and
x2. But then, for k≥3, it is impossible to generate the
termx1x2xk+2. . . x 2k−1, since all terms containing x1and
x2must also contain xk+1.
5 Expressivity of nested k-DNFs
In machine learning, it is important that the language of mod-
elsMused in the learning phase be sufficiently rich to cap-
ture all functions we might wish to learn. Consider a classi-
fierκthat is a function of only kvariables x1, . . . , x k. Both
κand its complement κcan be expressed as k-DNFs. This is
because κ(respectively, κ) is the disjunction of the terms cor-
responding to the assignments to the variables x1, . . . , x kfor
which κ(x1, . . . , x k) = 1 (respectively, κ(x1, . . . , x k) = 1 ).
All functions of kvariables can be expressed as depth- kdeci-
sion trees (with xiassociated with all decision nodes at depth
i−1), so an obvious question is whether the same is true
for nested k-DNFs. We answer this question positively in the
following proposition.
Proposition 3. Every boolean function κofkboolean vari-
ables can be expressed as a nested k-DNF .
Proof. Ifκis the constant function 1, then it can be trivially
expressed as a nested k-DNF that contains a single term with
zero literals. We can therefore assume that κis equal to 0 for
some assignment to the kvariables x1, . . . , x k. Without loss
of generality, we assume that κ(0, . . . , 0) = 0 . Let the k×k
matrix of literals {ℓij}be
L=
x1x2x3. . . xk
x2x3x4. . . x1
x3x4x5. . . x2
...
xkx1x2. . .xk−1

The classifier κcan be expressed as the disjunction of terms
corresponding to assignments for which κis equal to 1. Any
such term tcontains hpositive literals, where h≥1(since
a DNF satisfying κ(0, . . . , 0) = 0 cannot contain the term
x1···xk). Let xij(j= 1, . . . , h ) be these positive literals.
Letrij=ij+1−ij(j= 1, . . . , h −1),rih=k+i1−ih
andri= 0for all i /∈ {i1, . . . , i h}. Then tis the conjunction
of the leftmost rijliterals in row i(fori= 1, . . . , k ) of the
above matrix L. Since each term tofκcan be constructed in
this way, κis a nested k-DNF.
A consequence of Proposition 3 is that nested k-DNF for-
mulas can always be constructed to fit any consistent dataset
provided that kis large enough. In particular, the least in-
teger ksuch that a boolean function or its complement can'
&$
%#
" 
!'
&$
%'
&$
%depth- k
DTsfunctions of
kvariablesκorκis
a nested
k-DNFκorκis ak-DNFs with
induced matchings of size ≤k
Figure 1: The landscape of k-AXp-interpretable classifiers κ
be represented as a nested k-DNF formula is a well-defined
measure that cannot exceed the number of variables (as is the
case for decision trees of depth k).
One criterion for comparing families of models Mis to
estimate the number of distinct functions that can be repre-
sented by M. Let NDT(k, n)andNnested (k, n)be, respec-
tively, the number of functions representable by a depth- kde-
cision tree or by a nested k-DNF, where nis the total number
of variables. Recall that nested k-DNF formulas can be func-
tion of at most k2variables, whereas decision trees of depth
kmay depend on (up to) 2k−1variables. For this reason,
it is expected that if nis large enough compared to kthen
Nnested (k, n)will necessarily be smaller than NDT(k, n).
We show that the opposite is true when nis not much larger
thank. Informally, nested k-DNF formulas involve fewer fea-
tures than decision trees of depth kbut can express a greater
variety of dependencies between those features.
Proposition 4. Ifk≥4andk2≤n≤22k−1/k−1, then
Nnested (k, n)> N DT(k, n).
Proof. Every function representable by a decision tree of
depth kcan be represented by a complete tree with 2kleaves.
Each of the 2k−1internal nodes is associated with a variable
and each of the 2kleaves is associated with a class. There are
n2k−122ksuch decision trees, so NDT(k, n)≤n2k−122k.
We consider a fixed matrix Lcomposed of k2distinct posi-
tive literals ℓi,j. By the stars and bars theorem, the number of
distinct terms of the formVk
i=1Vri
j=1ℓi,jwherePk
i=1ri=k
is exactly C2k−1
k−1= 1/2·C2k
k. Using the inequality C2k
k≥
22k/p
π(k+ 1/3), we deduce that Nnested (k, n)is bounded
below by 222k−1/kfork≥4, since each of the 1/2·C2k
kterms
may or may not occur in the nested k-DNF formula. It fol-
lows that Nnested (k, n)> N DT(k, n)ifn≤22k−1/k−1.
Figure 1 provides a summary of the relationship between
the major classes of k-AXp-interpretable classifiers.
6 Experiments
In this section, we present a heuristic algorithm for finding
nested k-DNFs, distinguished by its intuitive and straightfor-
ward design2. It is worth noting that alternative algorithms
2The code is available in this GitHub repositorycould also be considered. Next, we provide an experimen-
tal comparison with the depth- kdecision trees obtained by
CART [7].
6.1 Heuristic algorithm
The heuristic consists of three steps: constructing the matrix,
constructing the nested k-DNF, and a pruning phase. In Al-
gorithm 1, we show how to construct the k×kmatrix Lby
proceeding row by row, where kis less than or equal to the
total number of features n. The idea is to create a matrix that
will allow us, in the next step, to generate a large number of
distinct and consistent terms. To achieve this, the literal ℓi,j
(0≤i, j≤k−1) is selected such that the j+ 1 leftmost
elements in row iof the matrix Lare highly representative of
class 1 while being minimally representative of class 0. A key
condition is that ℓi,jmust differ from the jpreceding literals
in row iand their negations (to avoid redundancy or inconsis-
tency). Additionally, to encourage diversity between different
rows, we exclude all literals in the first limit =k−jcolumns
(of the already-chosen rows) from the list of candidate literals
forℓi,j, provided that at least one literal remains available for
selection. The value of limit is reduced accordingly if the
number 2(n−j)of available literals is less than or equal to
the number i×(k−j)of literals we would like to forbid.
Secondly, we construct the nested k-DNF by evaluating
one term at a time, starting with terms of size kand decreas-
ing down to size 1. A term is considered for evaluation if
it is consistent (i.e. it does not contain both a literal and its
negation). We decide to select a term if P̸= 0 andQ < P ,
where P(respectively, Q) represents the number of exam-
ples in class 1 (respectively, class 0) that satisfy this term and
are not already covered by the selected terms. Furthermore, a
term is also chosen if it covers at least one example from class
1 and does not cover any example from class 0 (irrespective of
whether examples have already been covered). The process
stops when either all examples in class 1 are covered or there
are no more terms to evaluate. Finally, we perform pruning,
where we determine whether to retain each term. The same
evaluation as before is applied using P and Q (i.e. we remove
a term if P= 0orQ≥P). This time, we compare each term
against all other terms, not just the previously selected terms.
6.2 Datasets
A collection of datasets from the UCI repository and Kag-
gle are considered, which have been used to evaluate a wide
range of learning algorithms. These datasets contain various
feature types, which are converted into boolean features for
binary classification as in [12]. We employ the datasets in
their original form, without any preprocessing techniques ap-
plied. Table 1 shows, for each dataset, the number of data
examples and the number of boolean features.
6.3 Results
As a first test, the proposed heuristic successfully found the
2-DNF with 2 terms that perfectly match the full truth-table
generated from κ(a, b, c, d ) = (a∧b)∨(c∧d). In contrast,
the CART algorithm required a depth of 4 to create a decision
tree that fits the data exactly, as mentioned in Example 2.Algorithm 1 Construct matrix
Input : k, dataset
Output : matrix L
1:fori= 0to k−1do
2: forj= 0to k−1do
3: ifi= 0then
4: limit = 0
5: else
6: limit = min( k−j,⌈(2(n−j)/i)−1⌉)
7: end if
\\Ec1(t): nb. examples in class 1 that satisfy t
\\Ec0(t): nb. examples in class 0 that satisfy t
8: Calculate G=Ec1(ℓi,0...ℓi,j) –Ec0(ℓi,0...ℓi,j)for
each literal not in Li,0:j∪Li,0:j∪ L0:i,0:limit
9: Take as ℓi,jthe literal that gives the greatest G
10: end for
11:end for
12:return matrix L
Dataset Size Nb. boolean features
Balance-scale 625 16
Banknote 1372 28
Car-evaluation 1728 14
Compas discretized 6167 25
Indians Diabetes 768 43
Iris 150 12
Lymph 148 68
Monks-1 124 11
Monks-2 169 11
Monks-3 122 11
Tic-tac-toe 958 27
Table 1: Description of the datasets used in the experiments.
The rest of our experimental assessment was performed on
the datasets described above. For a given dataset, 80% of
the dataset was used for training and 20% for testing, except
for the Monks datasets, where the test set is provided sepa-
rately and consists of 432 examples, consisting of all possi-
ble combinations of the feature-values. The average perfor-
mance across five split experiments is reported. For each of
the two training algorithms, the experiment is run 10 times
and the average accuracy is computed on the test set. Table 2
shows the accuracy of our nested k-DNFs (column DNF) and
the decision trees generated by CART with a fixed maximum
depth of k(column DT). Given the asymmetry of nested k-
DNFs with respect to complementation, we repeated the ex-
periment, learning a nested k-DNF model for κrather than κ:
results are reported in column DNF.
The aim in using different datasets for experimentation is
to assess whether the proposed heuristic can actually find a
nested k-DNF that accurately represents the underlying struc-
ture of the data, as decision trees do. The results indicate
variability in accuracy across different datasets, with nested
k-DNFs outperforming depth- kdecision trees in some cases,
and vice versa in others. Overall, the results achieved by both
depth-k decision trees and nested k-DNFs are comparable.Test accuracy (%)
Dataset k= 2 k= 3 k= 4
DT DNF DNF DT DNF DNF DT DNF DNF
Balance-scale 93.28 89.04 93.28 93.28 92.46 93.28 93.28 92.10 93.28
Banknote 86.40 88.95 83.35 89.45 88.95 83.35 95.49 90.53 86.24
Car-evaluation 85.78 77.80 73.35 86.65 84.51 89.13 91.68 83.15 92.14
Compas discretized 64.47 64.02 65.71 65.90 65.87 67.18 66.40 66.07 67.12
Indians Diabetes 77.01 78.70 76.16 78.18 79.48 77.48 77.42 79.56 77.52
Iris 98.00 96.00 99.33 98.00 97.53 98.00 98.00 98.60 98.00
Lymph 81.33 76.73 85.33 79.93 79.67 87.13 85.13 82.07 86.07
Monks-1 75.00 75.00 66.67 83.33 77.78 66.67 83.33 78.50 75.22
Monks-2 56.94 60.65 60.26 63.89 63.66 61.13 61.31 65.15 63.49
Monks-3 97.22 97.22 97.22 94.44 97.22 97.22 95.37 97.22 94.59
Tic-tac-toe 68.23 68.76 68.31 72.40 70.05 75.65 81.77 75.27 80.16
Test accuracy (%)
Dataset k= 5 k= 6
DT DNF DNF DT DNF DNF
Balance-scale 92.96 92.05 93.28 92.18 90.58 93.10
Banknote 98.25 90.25 88.52 99.02 90.01 88.52
Car-evaluation 92.83 82.51 91.48 93.64 82.97 91.79
Compas discretized 67.31 66.40 67.31 66.97 66.51 67.70
Indians Diabetes 77.64 79.66 77.23 77.43 79.57 76.97
Iris 98.00 98.00 98.00 98.00 97.27 98.00
Lymph 85.00 81.93 85.93 84.27 80.40 86.27
Monks-1 83.33 82.20 77.41 83,33 91.17 80.52
Monks-2 68.26 67.32 68.33 78.85 67.55 73.63
Monks-3 89.81 89.00 92.46 92.59 87.09 88.19
Tic-tac-toe 90.98 75.52 78.07 92.28 77.55 79.38
Table 2: Test accuracy of depth-k decision trees and nested k-DNFs
Thus, nested k-DNFs emerge as a promising alternative to
decision trees, with these initial results highlighting the po-
tential of this family of models.
We also compared the size of the DT and nested k-DNF
models. Table 3 shows the average number of leaves in the
DTs and the average number of terms in the DNFs across
five datasets splits, with both training algorithms executed 10
times per split, and the best-performing model selected from
these iterations. A nested k-DNF is composed of terms asso-
ciated with a single class, while a DT contains paths leading
to both classes. However, despite this difference, the number
of terms is generally less than half of the number of leaves,
with a significant number of cases exhibiting an even greater
disparity. A similar observation was noted for the number
of terms in DNF. This suggests that the nested k-DNFs are
simpler than the DTs in terms of size.
7 Conclusion and future work
A machine-learning model can be deemed interpretable if
each of its decisions has an explanation that is intelligible by a
human user. We formalized this definition of interpretability
based on abductive or counterfactual explanations of size atmost a small constant k. In the case of binary classifiers over
boolean domains, we showed that this definition is equivalent
to the classifier and its complement both being expressible as
k-DNFs. Depth- kdecision trees are the most well-known ex-
ample of a family of models satisfying this definition. De-
cision trees are widely used either directly or as surrogate
models to provide explanations. This paper investigated the
existence of other families of interpretable models.
We introduced a graph-theoretical sufficient condition for
interpretability in terms of maximum induced matchings of
DNF formulas, before giving a novel concrete family of in-
terpretable models which we call nested k-DNFs. We showed
experimentally that a simple heuristic algorithm produces
nested k-DNFs whose accuracy is comparable with depth- k
decision trees found by CART.
An intriguing open question is whether there exist more
general families of interpretable DNFs that could achieve bet-
ter accuracy than decision trees. In contrast to decision trees
of depth k, the property of a function being expressible as a
nested k-DNF is not invariant under complementation in gen-
eral. In addition, nested k-DNFs cannot contain more than k2
distinct literals. These limitations come from our definitionsDataset k= 2 k= 3 k= 4 k= 5 k= 6
DT DNF DT DNF DT DNF DT DNF DT DNF
Balance-scale 4.0 2.0 8.0 2.8 14.6 3.4 23.2 7.2 36.4 13.6
Banknote 4.0 2.0 8.0 2.0 14.0 2.0 21.2 2.2 27.2 2.6
Car-evaluation 3.0 1.0 4.0 2.6 6.0 3.2 9.8 3.8 16.4 4.4
Compas discretized 4.0 1.8 8.0 2.6 15.6 4.2 29.0 4.6 51.4 6.4
Indians Diabetes 4.0 2.0 8.0 3.2 15.6 4.2 27.4 5.0 43.4 5.8
Iris 3.0 1.6 4.4 2.0 4.4 2.0 4.4 2.6 4.4 3.2
Lymph 4.0 1.8 8.0 2.2 13.2 2.4 16.2 2.2 18.0 3.0
Monks-1 3.0 2.0 5.0 3.0 6.0 3.0 8.0 5.0 11.0 6.0
Monks-2 4.0 2.0 8.0 4.0 15.0 5.6 25.0 6.6 40.0 8.6
Monks-3 4.0 1.0 6.0 1.0 9.0 1.0 11.0 3.6 13.0 9.0
Tic-tac-toe 4.0 1.2 8.0 4.2 14.0 8.4 22.4 10.6 33.8 15.0
Table 3: The number of leaves in the DT and the number of terms in the nested k-DNF
and do not arise from fundamental technical reasons, so we
believe there is ample room for further improvement.
Finally, our observations during the experiments revealed
some variability in the test accuracy of the nested k-DNFs
across different runs. This observation suggests that signif-
icantly better results could be achieved by using more so-
phisticated heuristics. In particular, it would be interesting
to compare optimal nested k-DNFs and optimal depth- kde-
cision trees.
Acknowledgments
This work was funded by the French National Research
Agency (ANR) under grant agreement no. ANR-23-CE25-
0009. We would also like to thank Aur ´elie Hurault for many
insightful comments.
References
[1]Leila Amgoud and Jonathan Ben-Naim. Axiomatic
foundations of explainability. In Luc De Raedt, editor,
IJCAI , pages 636–642. ijcai.org, 2022.
[2]Sanjeev Arora and Boaz Barak. Computational Com-
plexity: A Modern Approach . Cambridge University
Press, USA, 1st edition, 2009.
[3]Gilles Audemard, Steve Bellart, Louenas Bounia,
Fr´ed´eric Koriche, Jean-Marie Lagniez, and Pierre Mar-
quis. On the computational intelligibility of boolean
classifiers. In Meghyn Bienvenu, Gerhard Lakemeyer,
and Esra Erdem, editors, KR, pages 74–86, 2021.
[4]Pablo Barcel ´o, Mika ¨el Monet, Jorge P ´erez, and
Bernardo Subercaseaux. Model interpretability through
the lens of computational complexity. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell,
Maria-Florina Balcan, and Hsuan-Tien Lin, editors,
NeurIPS , 2020.
[5]Claude Berge. Hypergraphs: Combinatorics of finite
sets. 1989.
[6]Guy Blanc, Caleb Koch, Jane Lange, and Li-Yang
Tan. The query complexity of certification. In StefanoLeonardi and Anupam Gupta, editors, STOC ’22: 54th
Annual ACM SIGACT Symposium on Theory of Com-
puting , pages 623–636. ACM, 2022.
[7]Leo Breiman, J. H. Friedman, Richard A. Olshen,
and C. J. Stone. Classification and Regression Trees .
Wadsworth, 1984.
[8]Cl´ement Carbonnel, Martin C. Cooper, and Jo ˜ao
Marques-Silva. Tractable explaining of multivariate de-
cision trees. In Pierre Marquis, Tran Cao Son, and
Gabriele Kern-Isberner, editors, KR, pages 127–135,
2023.
[9]Siddhesh Chaubal and Anna G ´al. Diameter versus cer-
tificate complexity of boolean functions. In Filippo
Bonchi and Simon J. Puglisi, editors, 46th International
Symposium on Mathematical Foundations of Computer
Science, MFCS , volume 202 of LIPIcs , pages 31:1–
31:22. Schloss Dagstuhl - Leibniz-Zentrum f ¨ur Infor-
matik, 2021.
[10]Martin C. Cooper and Leila Amgoud. Abductive ex-
planations of classifiers under constraints: Complexity
and properties. In Kobi Gal, Ann Now ´e, Grzegorz J.
Nalepa, Roy Fairstein, and Roxana Radulescu, editors,
ECAI , volume 372 of Frontiers in Artificial Intelligence
and Applications , pages 469–476. IOS Press, 2023.
[11]Martin C. Cooper and Jo ˜ao Marques-Silva. Tractabil-
ity of explaining classifier decisions. Artif. Intell. , 316,
2023.
[12]Emir Demirovic, Emmanuel Hebrard, and Louis Jean.
Blossom: an anytime algorithm for computing optimal
decision trees. In Andreas Krause, Emma Brunskill,
Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
and Jonathan Scarlett, editors, ICML , volume 202,
pages 7533–7562. PMLR, 2023.
[13]Kerven Durdymyradov and Mikhail Moshkov. Bounds
on depth of decision trees derived from decision rule
systems with discrete attributes. Ann. Math. Artif. In-
tell., 92(3):703–732, 2024.
[14]Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, Mar-
tin C. Cooper, Nicholas Asher, and Jo ˜ao Marques-Silva.Tractable explanations for d-DNNF classifiers. In AAAI ,
pages 5719–5728. AAAI Press, 2022.
[15]Alexey Ignatiev, Yacine Izza, Peter J. Stuckey, and Jo ˜ao
Marques-Silva. Using MaxSAT for efficient explana-
tions of tree ensembles. In AAAI , pages 3776–3785.
AAAI Press, 2022.
[16]Alexey Ignatiev and Jo ˜ao Marques-Silva. SAT-based
rigorous explanations for decision lists. In Chu-Min Li
and Felip Many `a, editors, Theory and Applications of
Satisfiability Testing - SAT , volume 12831 of Lecture
Notes in Computer Science , pages 251–269. Springer,
2021.
[17]Alexey Ignatiev, Nina Narodytska, and Jo ˜ao Marques-
Silva. Abduction-based explanations for machine learn-
ing models. In AAAI , pages 1511–1519. AAAI Press,
2019.
[18]Yacine Izza, Alexey Ignatiev, and Jo ˜ao Marques-Silva.
On tackling explanation redundancy in decision trees. J.
Artif. Intell. Res. , 75:261–321, 2022.
[19]Yacine Izza and Jo ˜ao Marques-Silva. On explaining ran-
dom forests with SAT. In Zhi-Hua Zhou, editor, IJCAI ,
pages 2584–2591. ijcai.org, 2021.
[20]Jo˜ao Marques-Silva. Logic-based explainability: Past,
present & future. CoRR , abs/2406.11873, 2024.
[21]Jo˜ao Marques-Silva, Thomas Gerspacher, Martin C.
Cooper, Alexey Ignatiev, and Nina Narodytska. Ex-
plaining naive Bayes and other linear classifiers with
polynomial time and delay. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina
Balcan, and Hsuan-Tien Lin, editors, NeurIPS , 2020.
[22]Jo˜ao Marques-Silva, Thomas Gerspacher, Martin C.
Cooper, Alexey Ignatiev, and Nina Narodytska. Expla-
nations for monotonic classifiers. In Marina Meila and
Tong Zhang, editors, ICML , volume 139, pages 7469–
7479. PMLR, 2021.
[23]Christoph Molnar, Giuseppe Casalicchio, and Bernd
Bischl. Interpretable machine learning - A brief history,
state-of-the-art and challenges. CoRR , abs/2010.09337,
2020.
[24]Cynthia Rudin. Stop explaining black box machine
learning models for high stakes decisions and use inter-
pretable models instead. Nat. Mach. Intell. , 1(5):206–
215, 2019.
[25]Andy Shih, Arthur Choi, and Adnan Darwiche. A sym-
bolic approach to explaining bayesian network classi-
fiers. In J ´erˆome Lang, editor, IJCAI , pages 5103–5111.
ijcai.org, 2018.