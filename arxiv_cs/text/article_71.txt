arXiv:2505.21092v1  [cs.CL]  27 May 2025BLUCK: A Benchmark Dataset for Bengali Linguistic Understanding and
Cultural Knowledge
Daeen Kabir*, Minhajur Rahman Chowdhury Mahim*, Sheikh Shafayat,
Adnan Sadik, Arian Ahmed, Eunsu Kim, Alice Oh†
KAIST, Republic of Korea
{dk2001, minhaj, sheikh.shafayat, adnansadik235, arian.ahmed, kes0317}@kaist.ac.kr
alice.oh@kaist.edu
Abstract
In this work, we introduce BLUCK , a new
dataset designed to measure the performance of
Large Language Models (LLMs) in Bengali lin-
guistic understanding and cultural knowledge.
Our dataset comprises 2366 multiple-choice
questions (MCQs) carefully curated from com-
piled collections of several college and job level
examinations and spans 23 categories covering
knowledge on Bangladesh’s culture and his-
tory and Bengali linguistics. We benchmarked
BLUCK using 6 proprietary and 3 open-source
LLMs - including GPT-4o, Claude-3.5-Sonnet,
Gemini-1.5-Pro, Llama-3.3-70B-Instruct, and
DeepSeekV3. Our results show that while these
models perform reasonably well overall, they,
however, struggles in some areas of Bengali
phonetics. Although current LLMs’ perfor-
mance on Bengali cultural and linguistic con-
texts is still not comparable to that of main-
stream languages like English, our results in-
dicate Bengali’s status as a mid-resource lan-
guage. Importantly, BLUCK is also the first
MCQ-based evaluation benchmark that is cen-
tered around native Bengali culture, history,
and linguistics.
1 Introduction
Recently, Large Language Models (LLMs) have
demonstrated remarkable success in multilingual
capabilities. In the case of Bengali, OpenAI’s
O1 model achieved an impressive score of 0.873
(OpenAI et al., 2024b) on the MMLU benchmark.
However, most evaluations of Bengali, includ-
ing MMLU, rely on translated English datasets
assessing general knowledge skills or focus ex-
clusively on STEM fields, such as math and sci-
ence (Shafayat et al., 2024). Despite the growing
emphasis on evaluations that capture cultural and
linguistic contexts for LLMs, the performance of
*Equal contribution
†Corresponding authorCategories No. of Questions
HistoryAncient Bengal 99
British Bengal 40
Pakistan Era 106
CultureIndigenous People 31
Arts, Heritage & Media 69
National Issues 15
Constitution 31
Resources 36
Geography 87
Law 284
PhoneticsAlphabet 10
Pronunciation 69
Conjunct Letters 23
Sound & Letters 48
Sound Changes 54
Phonetic Combining Rules 184
Miscellaneous Phonetics 80
SemanticsSynonyms 364
Antonyms 165
One Word Expressions 180
Idioms 198
Proverbs 47
Miscellaneous 146
Total 2366
Table 1: Statistics of BLUCK
models in Bengali-specific cultural knowledge or
reasoning skills remains unexplored.
Given that Bengali is the 7th most spoken lan-
guage in the world, with over 237 million native
speakers, it is crucial to address the lack of high-
quality Bengali-specific evaluation datasets. To this
end, we introduce BLUCK1,
a Benchmark Dataset for Bengali Linguistic Un-
derstanding and Cultural Knowledge. Through a
rigorous curation process—encompassing careful
annotation, multiple rounds of cross-inspection,
and digitization—we have compiled a dataset of
2,366 multiple-choice questions (MCQs) that en-
compass extensive knowledge of the culture, his-
1Dataset link: BLUCKtory, and language of Bangladesh, organized into
23 subcategories. Table 1 presents the overall statis-
tics and categories of BLUCK.
Our evaluation of 9 LLMs using BLUCK offers
valuable insights into the current status of LLMs
in understanding Bengali language and cultural
knowledge. Specifically, GPT-4o and Claude-3.5-
Sonnet achieve the highest scores, around 73% in a
0-shot setting—approximately 7% lower than their
performance on the MMLU benchmark. Overall,
the models tend to perform well in the history cat-
egory but show weaker results in the culture cate-
gory, particularly on national issues. Similarly, in
the phonetics category, their performance is gener-
ally low, with GPT-4o scores of 0.377 in pronun-
ciation and 0.407 in sound changes. The lower
performance in specific categories, such as cul-
ture and phonetics, highlights the current models’
limitations in Bengali-specific knowledge. These
findings underscore the potential for improvement
in these areas, providing valuable insights for the
future development of Bengali language models.
2 Related Works
2.1 Cultural Sensitive Dataset
There has been a growing effort to create culturally
sensitive benchmarks for evaluating LLMs across
different languages and regions. Datasets like CUL-
TURALBENCH (Chiu et al., 2024), CLIcK (Kim
et al., 2024) for Korean cultural knowledge Ro-
CulturaBench (Masala et al., 2024) for Romanian
culture are designed to assess LLMs’ ability to un-
derstand cultural context beyond linguistic fluency.
Similarly, BLEnD (Myung et al., 2024) provides a
multilingual cultural dataset from more 13 different
languages.
Despite these advances, Bengali remains sig-
nificantly underrepresented in cultural-sensitive
datasets. Most cultural evaluation benchmarks
focus on high-resource languages or specific re-
gional cultures, leaving a major gap in Bengali
cultural and linguistic understanding. BLUCK is
introduced to address this gap.
2.2 Bengali Dataset
Several benchmarks have been developed to eval-
uate LLMs in general and multilingual tasks, but
only a few focus on specific Bengali knowledge.
M-MMLU (OpenAI, 2024) and Global-MMLU
(Singh et al., 2024) are some of the few well-known
benchmarks that include Bengali in their multilin-gual evaluation settings. Nevertheless, their Ben-
gali questions are mostly translated from English,
limiting their effectiveness in assessing native-level
linguistic understanding.
For Bengali-specific reasoning tasks, MGSM
(Shi et al., 2022) provided the Bengali translations
of GSM8K (Cobbe et al., 2021), one of the promi-
nent datasets for grade school math problems along
with other languages, although its scope remains
limited. Later, BEnQA (Shafayat et al., 2024) pro-
vided multiple choice questions as official English-
Bengali corpus, sourced from Bangladesh’s na-
tional board exams, focusing primarily on STEM
subjects. Bengali Identity Bias Evaluation Dataset
(BIBED) (Das et al., 2023) aims at identifying
cultural-centric biases, which however, is limited
to gender, religion, and nationality. There is barely
a dataset in Bengali that encapsulates its history,
culture, or linguistic intricacies.
To the best of our knowledge, BLUCK is the
first comprehensive benchmark to include Bengali-
related reasoning and knowledge questions, fo-
cusing on Bengali history, culture, and language.
BLUCK is specifically designed to evaluate LLMs
in native Bengali contexts, complementing existing
multilingual and subject-specific benchmarks.
3 BLUCK: A Benchmark Dataset for
Bengali Linguistic Understanding and
Cultural Knowledge
BLUCK contains immersive knowledge organized
into these four major domains: Bangladesh’s his-
tory, Bangladeshi culture, Bengali phonetics, and
Bengali semantics. A summary of these categories,
along with the corresponding number of questions
is provided in Table 1.
3.1 Data Collection
Data is collected from publicly available printed
copies of previous examination papers from the
following sources: a) Bangladesh Civil Service
(BCS) Examinations, b) university entrance exam-
inations in Bangladesh, c) Bangladesh Bar Coun-
cil Preliminary Examinations, d) bank job exam-
inations, and e) several public job examinations.
These official examinations are selected for their
reliability and authoritative assessment of general
knowledge in Bangladesh. These exams consist
of extensive native knowledge on Bangladesh’s
history, culture, law, language, and various other
academic disciplines. For BLUCK’s creation, weselect only the MCQs and follow a question selec-
tion criterion, based on which we omit these types
of questions: a) fact-based questions loosely repre-
senting Bangladesh’s history, culture, and language
b) questions on contemporary issues in Bangladesh
(to ensure long-term relevance), c) insignificant
date-related or ‘number-based answer option’ ques-
tions (to avoid arbitrary or trivial answers).
3.2 Dataset Curation
(1) Categorization After data collection, we cat-
egorize by utilizing general knowledge and Bengali
language guidebooks that organize questions simi-
lar to the ones in our dataset. This approach ensures
proper categorization for some of the categories in
the culture domain, and allows us to group simi-
lar categories into the four main domains of our
dataset.
(2) Two Round Inspection The preliminary
question selection task is distributed among the
authors. First, two rounds of inspection are con-
ducted; in each round different two authors indi-
vidually checks the question selection based on the
aforementioned criteria, and then cross-checks with
each other. This process ensures that our dataset
contains high-quality questions representing the
history and culture of Bangladesh, and its rich lin-
guistic knowledge.
(3) Digitization After inspection, professional
annotators, proficient in Bengali, digitize the
MCQs for easier access and manipulation of the
data. This is done to minimize the errors when
digitized. To finalize our dataset, we conduct re-
finement: a) cleaning duplicate and inconsistent
entries, b) correcting existing typing errors, and c)
final checking to remove erroneous questions. This
extensive approach ensures reliability and proper
representation of the categories in our dataset.
4 Experiment
4.1 Experimental Setup
In order to evaluate LLMs’ performance on the
history and culture of Bangladesh, and Bengali
phonetics and semantics, we conduct experiments
on the BLUCK dataset using both proprietary and
open-source models. We utilized the following
LLMs:
•Proprietary models : GPT-4o, GPT-4o-mini (OpenAI et al., 2024a)2, Claude-3.5-
Sonnet, Claude-3.5-Haiku3, Gemini-1.5-Pro,
Gemini-1.5-Flash (Team et al., 2024)4
•Open-source models : Llama-3.1-8B-Instruct,
Llama-3.3-70B-Instruct (Grattafiori et al.,
2024), DeepSeekV3 (DeepSeek-AI et al.,
2024)
Since BLUCK consists largely of factual-
knowledge based questions, we conduct evalua-
tion without any chain-of-thought (CoT) reasoning,
using both zero-shot and five-shot settings. As
shown in Figure 1 in Appendix 7, for prompt, we
utilize system and user prompts, explicitly instruct-
ing the model to output only the option ‘letter’
in order to save API computational cost (Petrov
et al., 2023). Following the criteria in KoBBQ (Jin
et al., 2024), we only accept generated responses
based on: a) response with only the alphabet as
answer, b) response mentioning term correspond-
ing to one of the options, iii) response convey the
answer in the form ‘answer:’, or ‘answer is’, etc.
Responses showing signs of hallucination, or pro-
ducing bizarre outputs such as single Bengali letter
as response are omitted.
4.2 Result
Our evaluation results are summarized in Table
2, which highlights the performance scores for all
23 categories of our dataset for the major mod-
els. Table 3 in Appendix 7 shows the same for the
small-sized language models. Our results indicate
that Claude-3.5-Sonnet, GPT-4o, Gemini-1.5-pro,
and DeepSeekV3 demonstrate considerable knowl-
edge of Bangladeshi history and the semantics of
Bengali language. However, all the models strug-
gle with phonetics, especially in areas such as pro-
nunciation and sound changes. Claude-3.5-Sonnet
emerges as the best overall model with consistent
performance across all categories in both settings.
It’s performance in Bengali phonetics, which is
the most difficult category, is 10% better than the
2nd best model in this domain. GPT-4o closely
follows, performing the best in history, culture,
and semantics, while Claude-3.5-Sonnet achieves
best performance in culture and phonetics. The
2We use GPT-4o-2024-08-06 and GPT-4o-mini-2024-07-
18 version using OPENAI API.
3We use Claude-3.5-Sonnet-20241022 and Claude-3.5-
Haiku-20241022 version using Anthropic API.
4We use gemini-1.5-pro-Latest, gemini-1.5-flash-latest us-
ing Gemini API key in Google AI Studio.CategoriesGPT-4o Claude-3.5-Sonnet Gemini-1.5-Pro Llama-3.3-70B DeepSeekV3
0-shot 5-shot 0-shot 5-shot 0-shot 5-shot 0-shot 5-shot 0-shot 5-shot
HistoryAncient Bengal 0.899 0.919 0.879 0.889 0.758 0.758 0.687 0.677 0.859 0.889
British Bengal 0.925 0.975 0.875 0.9 0.675 0.95 0.8 0.85 0.9 0.95
Pakistan Era 0.745 0.783 0.67 0.764 0.481 0.613 0.5 0.509 0.717 0.726
Average 0.837 0.869 0.788 0.837 0.624 0.727 0.624 0.633 0.804 0.829
CultureIndigenous People 0.806 0.839 0.871 0.935 0.516 0.71 0.516 0.742 0.774 0.871
Arts, Heritage & Media 0.739 0.768 0.725 0.696 0.58 0.594 0.58 0.551 0.725 0.768
National Issues 0.467 0.467 0.733 0.8 0.6 0.733 0.2 0.6 0.467 0.6
Constitution 0.806 0.871 0.871 0.935 0.806 0.903 0.677 0.71 0.935 0.968
Resources 0.778 0.778 0.722 0.778 0.5 0.639 0.528 0.583 0.694 0.806
Geography 0.828 0.862 0.759 0.793 0.598 0.724 0.655 0.621 0.701 0.77
Law 0.68 0.715 0.648 0.718 0.613 0.715 0.496 0.588 0.588 0.641
Average 0.725 0.758 0.707 0.758 0.604 0.707 0.537 0.604 0.656 0.718
PhoneticsAlphabet 0.6 0.7 0.6 0.9 0.2 0.9 0.6 0.9 0.6 0.9
Pronunciation 0.377 0.406 0.348 0.507 0.246 0.391 0.217 0.333 0.29 0.348
Conjunct Letters 0.652 0.739 0.826 0.957 0.652 0.826 0.739 0.826 0.696 0.826
Sound & Letters 0.771 0.729 0.708 0.792 0.625 0.771 0.542 0.688 0.688 0.75
Sound Changes 0.407 0.611 0.5 0.667 0.463 0.63 0.352 0.537 0.407 0.574
Phonetic Combining Rules 0.516 0.603 0.663 0.761 0.533 0.609 0.446 0.473 0.609 0.63
Miscellaneous Phonetics 0.638 0.675 0.588 0.7 0.5 0.588 0.463 0.575 0.575 0.675
Average 0.538 0.609 0.596 0.718 0.485 0.609 0.432 0.526 0.545 0.618
SemanticsSynonyms 0.874 0.912 0.893 0.923 0.769 0.835 0.676 0.772 0.852 0.907
Antonyms 0.782 0.891 0.855 0.879 0.733 0.812 0.685 0.739 0.77 0.848
One Word Expressions 0.717 0.811 0.778 0.806 0.589 0.661 0.556 0.6 0.717 0.828
Idioms 0.722 0.808 0.652 0.747 0.606 0.662 0.495 0.505 0.626 0.697
Proverbs 0.787 0.83 0.83 0.894 0.723 0.809 0.638 0.745 0.766 0.787
Miscellaneous 0.733 0.712 0.692 0.719 0.575 0.589 0.486 0.514 0.678 0.719
Average 0.785 0.844 0.795 0.837 0.677 0.738 0.598 0.655 0.750 0.817
Overall Average 0.727 0.780 0.735 0.795 0.617 0.704 0.554 0.615 0.693 0.756
Table 2: BLUCK benchmark comparison by subcategories and major categories across major models in 0-shot and
5-shot settings. The highest accuracy(s) for each category are boldy marked.
smaller models exhibit surprisingly reasonable per-
formance, with Gemini-1.5-Flash and Claude-3.5-
Haiku surpassing even Llama-3.3-70B-Instruct in
5-shot setting. Llama-3.1-8B-Instruct, on the other
hand, lags behind all other smaller models, show-
ing very limited performance overall.
5 Discussions
The benchmark results reveal significant variations
in model performance across different categories
and shot settings. Firstly, it is visible that 5-shot
prompting leads to notable performance improve-
ments (between 5% to 10%) across all models,
which aligns with the findings that large language
models pick up categorical cues from the examples
and reduce the ‘search space’ for MCQ solution
under few-shot settings (Brown et al., 2020).
Secondly, proprietary models like GPT-4o and
Claude-3.5-Sonnet consistently outperform open-
source models for most of the categories, suggest-
ing that the former have a stronger contextual un-
derstanding of Bengali.
In addition, ’Pronunciation’, and ’SoundChanges’ are notable categories in which models
exhibit poor performance. This strongly suggests
that phonetic nuances in Bengali still remain under-
represented in existing LLMs, even with few-shot
prompting.
The findings, overall, reinforce the need for more
robust culture sensitive Bengali resources in LLM
pretraining and evaluation benchmarks to improve
performance in underrepresented Bengali linguistic
and cultural areas.
6 Conclusion
In this work we introduced BLUCK, a linguistic
and culture-sensitive Bengali dataset, locally sourc-
ing from official college and job-level examinations
in Bangladesh. BLUCK provides a diverse set of
2366 multiple-choice questions that fall under 23
subcategories organized across four domains. Our
evaluation using state-of-the-art LLMs showcases
their knowledge in historical and semantics aspects
of Bengali, while exposes their weakness in lin-
guistically nuanced areas. Future research should
expand BLUCK and improve LLMs’ understand-ing of Bengali linguistic and cultural nuances.
Limitations
We acknowledge certain limitations in our work.
Since our dataset consists solely of text-based ques-
tions, we cannot determine whether the models ar-
rived at their answers through reasoning processes
different from those of humans. Moreover, given
the richness of Bengali culture, history, and linguis-
tic diversity, as well as the growing importance of
M-MMLU, Global-MMLU and other large-scale
multilingual benchmarks, our contribution remains
relatively small in comparison. However, we hope
that BLUCK serves as a stepping stone to improve
Bengali culture-sensitive LLM research.
Ethical Considerations
The BLUCK dataset is fully available and has been
manually curated and reviewed to mitigate any
chance of having harmful contents. This dataset
will be publicly accessible and distributed under
the CC BY-SA 4.0 license. Our work has been re-
viewed and received approval from the Institutional
Review Board (IRB) at our institution. All anno-
tators involved in this project were compensated
above the minimum wage and standards. Finally,
AI-assisted tools were used solely for grammar and
language refinement. They were not used for writ-
ing, analysis, or coding in any capacity.
References
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec
Radford, Ilya Sutskever, and Dario Amodei. 2020.
Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems ,
volume 33, pages 1877–1901. Curran Associates,
Inc.
Yu Ying Chiu, Liwei Jiang, Bill Yuchen Lin,
Chan Young Park, Shuyue Stella Li, Sahithya Ravi,
Mehar Bhatia, Maria Antoniak, Yulia Tsvetkov,
Vered Shwartz, and Yejin Choi. 2024. Cultural-
bench: a robust, diverse and challenging benchmark
on measuring the (lack of) cultural knowledge of llms.
Preprint , arXiv:2410.02677.Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,
Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
Nakano, Christopher Hesse, and John Schulman.
2021. Training verifiers to solve math word prob-
lems. Preprint , arXiv:2110.14168.
Dipto Das, Shion Guha, and Bryan Semaan. 2023. To-
ward cultural bias evaluation datasets: The case
of Bengali gender, religious, and national identity.
InProceedings of the First Workshop on Cross-
Cultural Considerations in NLP (C3NLP) , pages 68–
83, Dubrovnik, Croatia. Association for Computa-
tional Linguistics.
DeepSeek-AI, Aixin Liu, Bei Feng, Bing Xue, Bingx-
uan Wang, Bochao Wu, Chengda Lu, Chenggang
Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan,
Damai Dai, Daya Guo, Dejian Yang, Deli Chen,
Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai,
Fuli Luo, Guangbo Hao, Guanting Chen, Guowei
Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng
Wang, Haowei Zhang, Honghui Ding, Huajian Xin,
Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang,
Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jiawei Wang,
Jin Chen, Jingchang Chen, Jingyang Yuan, Junjie
Qiu, Junlong Li, Junxiao Song, Kai Dong, Kai Hu,
Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean
Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao,
Litong Wang, Liyue Zhang, Meng Li, Miaojun Wang,
Mingchuan Zhang, Minghua Zhang, Minghui Tang,
Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang,
Peng Zhang, Qiancheng Wang, Qihao Zhu, Qinyu
Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge,
Ruisong Zhang, Ruizhe Pan, Runji Wang, Runxin
Xu, Ruoyu Zhang, Ruyi Chen, S. S. Li, Shanghao
Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu,
Shengfeng Ye, Shengfeng Ye, Shirong Ma, Shiyu
Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou,
Shuting Pan, T. Wang, Tao Yun, Tian Pei, Tianyu Sun,
W. L. Xiao, Wangding Zeng, Wanjia Zhao, Wei An,
Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu,
Wentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang,
Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin Shen,
Xiaokang Chen, Xiaokang Zhang, Xiaosha Chen,
Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin
Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xingkai Yu,
Xinnan Song, Xinxia Shan, Xinyi Zhou, Xinyu Yang,
Xinyuan Li, Xuecheng Su, Xuheng Lin, Y . K. Li,
Y . Q. Wang, Y . X. Wei, Y . X. Zhu, Yang Zhang, Yan-
hong Xu, Yanhong Xu, Yanping Huang, Yao Li, Yao
Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang, Yi Yu,
Yi Zheng, Yichao Zhang, Yifan Shi, Yiliang Xiong,
Ying He, Ying Tang, Yishi Piao, Yisong Wang, Yix-
uan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo,
Yu Wu, Yuan Ou, Yuchen Zhu, Yuduan Wang, Yue
Gong, Yuheng Zou, Yujia He, Yukun Zha, Yunfan
Xiong, Yunxian Ma, Yuting Yan, Yuxiang Luo, Yuxi-
ang You, Yuxuan Liu, Yuyang Zhou, Z. F. Wu, Z. Z.
Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu,
Zhen Huang, Zhen Zhang, Zhenda Xie, Zhengyan
Zhang, Zhewen Hao, Zhibin Gou, Zhicheng Ma, Zhi-
gang Yan, Zhihong Shao, Zhipeng Xu, Zhiyu Wu,
Zhongyu Zhang, Zhuoshu Li, Zihui Gu, Zijia Zhu,Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Ziyi
Gao, and Zizheng Pan. 2024. Deepseek-v3 technical
report. Preprint , arXiv:2412.19437.
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri,
Abhinav Pandey, Abhishek Kadian, Ahmad Al-
Dahle, Aiesha Letman, Akhil Mathur, Alan Schel-
ten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh
Goyal, Anthony Hartshorn, Aobo Yang, Archi Mi-
tra, Archie Sravankumar, Artem Korenev, Arthur
Hinsvark, Arun Rao, Aston Zhang, Aurelien Ro-
driguez, Austen Gregerson, Ava Spataru, Baptiste
Roziere, Bethany Biron, Binh Tang, Bobbie Chern,
Charlotte Caucheteux, Chaya Nayak, Chloe Bi,
Chris Marra, Chris McConnell, Christian Keller,
Christophe Touret, Chunyang Wu, Corinne Wong,
Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-
lonsius, Daniel Song, Danielle Pintz, Danny Livshits,
Danny Wyatt, David Esiobu, Dhruv Choudhary,
Dhruv Mahajan, Diego Garcia-Olano, Diego Perino,
Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy,
Elina Lobanova, Emily Dinan, Eric Michael Smith,
Filip Radenovic, Francisco Guzmán, Frank Zhang,
Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis An-
derson, Govind Thattai, Graeme Nail, Gregoire Mi-
alon, Guan Pang, Guillem Cucurell, Hailey Nguyen,
Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan
Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Is-
han Misra, Ivan Evtimov, Jack Zhang, Jade Copet,
Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park,
Jay Mahadeokar, Jeet Shah, Jelmer van der Linde,
Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu,
Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang,
Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park,
Joseph Rocca, Joshua Johnstun, Joshua Saxe, Jun-
teng Jia, Kalyan Vasuden Alwala, Karthik Prasad,
Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth
Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer,
Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal
Lakhotia, Lauren Rantala-Yeary, Laurens van der
Maaten, Lawrence Chen, Liang Tan, Liz Jenkins,
Louis Martin, Lovish Madaan, Lubo Malo, Lukas
Blecher, Lukas Landzaat, Luke de Oliveira, Madeline
Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar
Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew
Oldham, Mathieu Rita, Maya Pavlova, Melanie Kam-
badur, Mike Lewis, Min Si, Mitesh Kumar Singh,
Mona Hassan, Naman Goyal, Narjes Torabi, Niko-
lay Bashlykov, Nikolay Bogoychev, Niladri Chatterji,
Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick
Alrassy, Pengchuan Zhang, Pengwei Li, Petar Va-
sic, Peter Weng, Prajjwal Bhargava, Pratik Dubal,
Praveen Krishnan, Punit Singh Koura, Puxin Xu,
Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj
Ganapathy, Ramon Calderer, Ricardo Silveira Cabral,
Robert Stojnic, Roberta Raileanu, Rohan Maheswari,
Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ron-
nie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan
Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sa-
hana Chennabasappa, Sanjay Singh, Sean Bell, Seo-
hyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sha-
ran Narang, Sharath Raparthy, Sheng Shen, Shengye
Wan, Shruti Bhosale, Shun Zhang, Simon Van-
denhende, Soumya Batra, Spencer Whitman, StenSootla, Stephane Collot, Suchin Gururangan, Syd-
ney Borodinsky, Tamar Herman, Tara Fowler, Tarek
Sheasha, Thomas Georgiou, Thomas Scialom, Tobias
Speckbacher, Todor Mihaylov, Tong Xiao, Ujjwal
Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh
Ramanathan, Viktor Kerkez, Vincent Gonguet, Vir-
ginie Do, Vish V ogeti, Vítor Albiero, Vladan Petro-
vic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whit-
ney Meers, Xavier Martinet, Xiaodong Wang, Xi-
aofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xin-
feng Xie, Xuchao Jia, Xuewei Wang, Yaelle Gold-
schlag, Yashesh Gaur, Yasmine Babaei, Yi Wen,
Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao,
Zacharie Delpierre Coudert, Zheng Yan, Zhengxing
Chen, Zoe Papakipos, Aaditya Singh, Aayushi Sri-
vastava, Abha Jain, Adam Kelsey, Adam Shajnfeld,
Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand,
Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei
Baevski, Allie Feinstein, Amanda Kallet, Amit San-
gani, Amos Teo, Anam Yunus, Andrei Lupu, An-
dres Alvarado, Andrew Caples, Andrew Gu, Andrew
Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchan-
dani, Annie Dong, Annie Franco, Anuj Goyal, Apara-
jita Saraf, Arkabandhu Chowdhury, Ashley Gabriel,
Ashwin Bharambe, Assaf Eisenman, Azadeh Yaz-
dan, Beau James, Ben Maurer, Benjamin Leonhardi,
Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi
Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Han-
cock, Bram Wasti, Brandon Spence, Brani Stojkovic,
Brian Gamido, Britt Montalvo, Carl Parker, Carly
Burton, Catalina Mejia, Ce Liu, Changhan Wang,
Changkyu Kim, Chao Zhou, Chester Hu, Ching-
Hsiang Chu, Chris Cai, Chris Tindal, Christoph Fe-
ichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty,
Daniel Kreymer, Daniel Li, David Adkins, David
Xu, Davide Testuggine, Delia David, Devi Parikh,
Diana Liskovich, Didem Foss, Dingkang Wang, Duc
Le, Dustin Holland, Edward Dowling, Eissa Jamil,
Elaine Montgomery, Eleonora Presani, Emily Hahn,
Emily Wood, Eric-Tuan Le, Erik Brinkman, Este-
ban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun,
Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat
Ozgenel, Francesco Caggioni, Frank Kanayet, Frank
Seide, Gabriela Medina Florez, Gabriella Schwarz,
Gada Badeer, Georgia Swee, Gil Halpern, Grant
Herman, Grigory Sizov, Guangyi, Zhang, Guna
Lakshminarayanan, Hakan Inan, Hamid Shojanaz-
eri, Han Zou, Hannah Wang, Hanwen Zha, Haroun
Habeeb, Harrison Rudolph, Helen Suk, Henry As-
pegren, Hunter Goldman, Hongyuan Zhan, Ibrahim
Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis,
Irina-Elena Veliche, Itai Gat, Jake Weissman, James
Geboski, James Kohli, Janice Lam, Japhet Asher,
Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jen-
nifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy
Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe
Cummings, Jon Carvill, Jon Shepard, Jonathan Mc-
Phie, Jonathan Torres, Josh Ginsburg, Junjie Wang,
Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khan-
delwal, Katayoun Zand, Kathy Matosich, Kaushik
Veeraraghavan, Kelly Michelena, Keqian Li, Ki-
ran Jagadeesh, Kun Huang, Kunal Chawla, Kyle
Huang, Lailin Chen, Lakshya Garg, Lavender A,
Leandro Silva, Lee Bell, Lei Zhang, LiangpengGuo, Licheng Yu, Liron Moshkovich, Luca Wehrst-
edt, Madian Khabsa, Manav Avalani, Manish Bhatt,
Martynas Mankus, Matan Hasson, Matthew Lennie,
Matthias Reso, Maxim Groshev, Maxim Naumov,
Maya Lathi, Meghan Keneally, Miao Liu, Michael L.
Seltzer, Michal Valko, Michelle Restrepo, Mihir Pa-
tel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark,
Mike Macey, Mike Wang, Miquel Jubert Hermoso,
Mo Metanat, Mohammad Rastegari, Munish Bansal,
Nandhini Santhanam, Natascha Parks, Natasha
White, Navyata Bawa, Nayan Singhal, Nick Egebo,
Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich
Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz,
Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin
Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pe-
dro Rittner, Philip Bontrager, Pierre Roux, Piotr
Dollar, Polina Zvyagina, Prashant Ratanchandani,
Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel
Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu
Nayani, Rahul Mitra, Rangaprabhu Parthasarathy,
Raymond Li, Rebekkah Hogan, Robin Battey, Rocky
Wang, Russ Howes, Ruty Rinott, Sachin Mehta,
Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara
Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov,
Satadru Pan, Saurabh Mahajan, Saurabh Verma,
Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lind-
say, Shaun Lindsay, Sheng Feng, Shenghao Lin,
Shengxin Cindy Zha, Shishir Patil, Shiva Shankar,
Shuqiang Zhang, Shuqiang Zhang, Sinong Wang,
Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala,
Stephanie Max, Stephen Chen, Steve Kehoe, Steve
Satterfield, Sudarshan Govindaprasad, Sumit Gupta,
Summer Deng, Sungmin Cho, Sunny Virk, Suraj
Subramanian, Sy Choudhury, Sydney Goldman, Tal
Remez, Tamar Glaser, Tamara Best, Thilo Koehler,
Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim
Matthews, Timothy Chou, Tzook Shaked, Varun
V ontimitta, Victoria Ajayi, Victoria Montanez, Vijai
Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad
Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu,
Vladimir Ivanov, Wei Li, Wenchen Wang, Wen-
wen Jiang, Wes Bouaziz, Will Constable, Xiaocheng
Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo
Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia,
Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi,
Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,
Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary
DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang,
Zhiwei Zhao, and Zhiyu Ma. 2024. The llama 3 herd
of models. Preprint , arXiv:2407.21783.
Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Al-
ice Oh, and Hwaran Lee. 2024. Kobbq: Korean
bias benchmark for question answering. Preprint ,
arXiv:2307.16778.
Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo,
James Thorne, and Alice Oh. 2024. CLIcK: A bench-
mark dataset of cultural and linguistic intelligence
in Korean. In Proceedings of the 2024 Joint In-
ternational Conference on Computational Linguis-
tics, Language Resources and Evaluation (LREC-
COLING 2024) , pages 3335–3346, Torino, Italia.
ELRA and ICCL.Mihai Masala, Denis C. Ilie-Ablachim, Alexandru
Dima, Dragos Corlatescu, Miruna Zavelca, Ovio
Olaru, Simina Terian, Andrei Terian, Marius
Leordeanu, Horia Velicu, Marius Popescu, Mi-
hai Dascalu, and Traian Rebedea. 2024. "vor-
be¸ sti române¸ ste?" a recipe to train powerful ro-
manian llms with english instructions. Preprint ,
arXiv:2406.18266.
Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki
Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eu-
nsu Kim, Carla Perez-Almendros, Abinew Ali Ayele,
Victor Gutierrez Basulto, Yazmin Ibanez-Garcia,
Hwaran Lee, Shamsuddeen H Muhammad, Kiwoong
Park, Anar Rzayev, Nina White, Seid Muhie Yi-
mam, Mohammad Taher Pilehvar, Nedjma Ousid-
houm, Jose Camacho-Collados, and Alice Oh. 2024.
Blend: A benchmark for llms on everyday knowledge
in diverse cultures and languages. In Advances in
Neural Information Processing Systems , volume 37,
pages 78104–78146. Curran Associates, Inc.
OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher,
Adam Perelman, Aditya Ramesh, Aidan Clark,
AJ Ostrow, Akila Welihinda, Alan Hayes, Alec
Radford, Aleksander M ˛ adry, Alex Baker-Whitcomb,
Alex Beutel, Alex Borzunov, Alex Carney, Alex
Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex
Renzin, Alex Tachard Passos, Alexander Kirillov,
Alexi Christakis, Alexis Conneau, Ali Kamali, Allan
Jabri, Allison Moyer, Allison Tam, Amadou Crookes,
Amin Tootoochian, Amin Tootoonchian, Ananya
Kumar, Andrea Vallone, Andrej Karpathy, Andrew
Braunstein, Andrew Cann, Andrew Codispoti, An-
drew Galu, Andrew Kondrich, Andrew Tulloch, An-
drey Mishchenko, Angela Baek, Angela Jiang, An-
toine Pelisse, Antonia Woodford, Anuj Gosalia, Arka
Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver,
Barret Zoph, Behrooz Ghorbani, Ben Leimberger,
Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin
Zweig, Beth Hoover, Blake Samic, Bob McGrew,
Bobby Spero, Bogo Giertler, Bowen Cheng, Brad
Lightcap, Brandon Walkin, Brendan Quinn, Brian
Guarraci, Brian Hsu, Bright Kellogg, Brydon East-
man, Camillo Lugaresi, Carroll Wainwright, Cary
Bassin, Cary Hudson, Casey Chu, Chad Nelson,
Chak Li, Chan Jun Shern, Channing Conger, Char-
lotte Barette, Chelsea V oss, Chen Ding, Cheng Lu,
Chong Zhang, Chris Beaumont, Chris Hallacy, Chris
Koch, Christian Gibson, Christina Kim, Christine
Choi, Christine McLeavey, Christopher Hesse, Clau-
dia Fischer, Clemens Winter, Coley Czarnecki, Colin
Jarvis, Colin Wei, Constantin Koumouzelis, Dane
Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy,
David Carr, David Farhi, David Mely, David Robin-
son, David Sasaki, Denny Jin, Dev Valladares, Dim-
itris Tsipras, Doug Li, Duc Phong Nguyen, Duncan
Findlay, Edede Oiwoh, Edmund Wong, Ehsan As-
dar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow,
Eric Kramer, Eric Peterson, Eric Sigler, Eric Wal-
lace, Eugene Brevdo, Evan Mays, Farzad Khorasani,
Felipe Petroski Such, Filippo Raso, Francis Zhang,
Fred von Lohmann, Freddie Sulit, Gabriel Goh,
Gene Oden, Geoff Salmon, Giulio Starace, GregBrockman, Hadi Salman, Haiming Bao, Haitang
Hu, Hannah Wong, Haoyu Wang, Heather Schmidt,
Heather Whitney, Heewoo Jun, Hendrik Kirchner,
Henrique Ponde de Oliveira Pinto, Hongyu Ren,
Huiwen Chang, Hyung Won Chung, Ian Kivlichan,
Ian O’Connell, Ian O’Connell, Ian Osband, Ian Sil-
ber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya
Kostrikov, Ilya Sutskever, Ingmar Kanitscheider,
Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub
Pachocki, James Aung, James Betker, James Crooks,
James Lennon, Jamie Kiros, Jan Leike, Jane Park,
Jason Kwon, Jason Phang, Jason Teplitz, Jason
Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Var-
avva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui
Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang,
Joaquin Quinonero Candela, Joe Beutler, Joe Lan-
ders, Joel Parish, Johannes Heidecke, John Schul-
man, Jonathan Lachman, Jonathan McKay, Jonathan
Uesato, Jonathan Ward, Jong Wook Kim, Joost
Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross,
Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao,
Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai
Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin
Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu,
Kenny Nguyen, Keren Gu-Lemberg, Kevin Button,
Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle
Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lau-
ren Workman, Leher Pathak, Leo Chen, Li Jing, Lia
Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lil-
ian Weng, Lindsay McCallum, Lindsey Held, Long
Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kon-
draciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz,
Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine
Boyd, Madeleine Thompson, Marat Dukhan, Mark
Chen, Mark Gray, Mark Hudnall, Marvin Zhang,
Marwan Aljubeh, Mateusz Litwin, Matthew Zeng,
Max Johnson, Maya Shetty, Mayank Gupta, Meghan
Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao
Zhong, Mia Glaese, Mianna Chen, Michael Jan-
ner, Michael Lampe, Michael Petrov, Michael Wu,
Michele Wang, Michelle Fradin, Michelle Pokrass,
Miguel Castro, Miguel Oom Temudo de Castro,
Mikhail Pavlov, Miles Brundage, Miles Wang, Mi-
nal Khan, Mira Murati, Mo Bavarian, Molly Lin,
Murat Yesildal, Nacho Soto, Natalia Gimelshein, Na-
talie Cone, Natalie Staudacher, Natalie Summers,
Natan LaFontaine, Neil Chowdhury, Nick Ryder,
Nick Stathas, Nick Turley, Nik Tezak, Niko Felix,
Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel
Bundick, Nora Puckett, Ofir Nachum, Ola Okelola,
Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins,
Olivier Godement, Owen Campbell-Moore, Patrick
Chao, Paul McMillan, Pavel Belov, Peng Su, Pe-
ter Bak, Peter Bakkum, Peter Deng, Peter Dolan,
Peter Hoeschele, Peter Welinder, Phil Tillet, Philip
Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming
Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Ra-
jan Troll, Randall Lin, Rapha Gontijo Lopes, Raul
Puri, Reah Miyara, Reimar Leike, Renaud Gaubert,
Reza Zamani, Ricky Wang, Rob Donnelly, Rob
Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchan-
dani, Romain Huet, Rory Carmichael, Rowan Zellers,
Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan
Cheu, Saachi Jain, Sam Altman, Sam Schoenholz,Sam Toizer, Samuel Miserendino, Sandhini Agar-
wal, Sara Culver, Scott Ethersmith, Scott Gray, Sean
Grove, Sean Metzger, Shamez Hermani, Shantanu
Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shi-
rong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay,
Srinivas Narayanan, Steve Coffey, Steve Lee, Stew-
art Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao
Xu, Tarun Gogineni, Taya Christianson, Ted Sanders,
Tejal Patwardhan, Thomas Cunninghman, Thomas
Degry, Thomas Dimson, Thomas Raoux, Thomas
Shadwell, Tianhao Zheng, Todd Underwood, Todor
Markov, Toki Sherbakov, Tom Rubin, Tom Stasi,
Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce
Walters, Tyna Eloundou, Valerie Qi, Veit Moeller,
Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne
Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra,
Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian,
Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen
He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and
Yury Malkov. 2024a. Gpt-4o system card. Preprint ,
arXiv:2410.21276.
OpenAI, :, Aaron Jaech, Adam Kalai, Adam Lerer,
Adam Richardson, Ahmed El-Kishky, Aiden Low,
Alec Helyar, Aleksander Madry, Alex Beutel, Alex
Carney, Alex Iftimie, Alex Karpenko, Alex Tachard
Passos, Alexander Neitz, Alexander Prokofiev,
Alexander Wei, Allison Tam, Ally Bennett, Ananya
Kumar, Andre Saraiva, Andrea Vallone, Andrew Du-
berstein, Andrew Kondrich, Andrey Mishchenko,
Andy Applebaum, Angela Jiang, Ashvin Nair, Bar-
ret Zoph, Behrooz Ghorbani, Ben Rossen, Benjamin
Sokolowsky, Boaz Barak, Bob McGrew, Borys Mi-
naiev, Botao Hao, Bowen Baker, Brandon Houghton,
Brandon McKinzie, Brydon Eastman, Camillo Lu-
garesi, Cary Bassin, Cary Hudson, Chak Ming Li,
Charles de Bourcy, Chelsea V oss, Chen Shen, Chong
Zhang, Chris Koch, Chris Orsinger, Christopher
Hesse, Claudia Fischer, Clive Chan, Dan Roberts,
Daniel Kappler, Daniel Levy, Daniel Selsam, David
Dohan, David Farhi, David Mely, David Robinson,
Dimitris Tsipras, Doug Li, Dragos Oprica, Eben Free-
man, Eddie Zhang, Edmund Wong, Elizabeth Proehl,
Enoch Cheung, Eric Mitchell, Eric Wallace, Erik
Ritter, Evan Mays, Fan Wang, Felipe Petroski Such,
Filippo Raso, Florencia Leoni, Foivos Tsimpourlas,
Francis Song, Fred von Lohmann, Freddie Sulit,
Geoff Salmon, Giambattista Parascandolo, Gildas
Chabot, Grace Zhao, Greg Brockman, Guillaume
Leclerc, Hadi Salman, Haiming Bao, Hao Sheng,
Hart Andrin, Hessam Bagherinezhad, Hongyu Ren,
Hunter Lightman, Hyung Won Chung, Ian Kivlichan,
Ian O’Connell, Ian Osband, Ignasi Clavera Gilaberte,
Ilge Akkaya, Ilya Kostrikov, Ilya Sutskever, Irina
Kofman, Jakub Pachocki, James Lennon, Jason Wei,
Jean Harb, Jerry Twore, Jiacheng Feng, Jiahui Yu,
Jiayi Weng, Jie Tang, Jieqi Yu, Joaquin Quiñonero
Candela, Joe Palermo, Joel Parish, Johannes Hei-
decke, John Hallman, John Rizzo, Jonathan Gordon,
Jonathan Uesato, Jonathan Ward, Joost Huizinga,
Julie Wang, Kai Chen, Kai Xiao, Karan Singhal, Ka-
rina Nguyen, Karl Cobbe, Katy Shi, Kayla Wood,
Kendra Rimbach, Keren Gu-Lemberg, Kevin Liu,
Kevin Lu, Kevin Stone, Kevin Yu, Lama Ahmad,Lauren Yang, Leo Liu, Leon Maksin, Leyton Ho,
Liam Fedus, Lilian Weng, Linden Li, Lindsay Mc-
Callum, Lindsey Held, Lorenz Kuhn, Lukas Kon-
draciuk, Lukasz Kaiser, Luke Metz, Madelaine Boyd,
Maja Trebacz, Manas Joglekar, Mark Chen, Marko
Tintor, Mason Meyer, Matt Jones, Matt Kaufer,
Max Schwarzer, Meghan Shah, Mehmet Yatbaz,
Melody Y . Guan, Mengyuan Xu, Mengyuan Yan,
Mia Glaese, Mianna Chen, Michael Lampe, Michael
Malek, Michele Wang, Michelle Fradin, Mike Mc-
Clay, Mikhail Pavlov, Miles Wang, Mingxuan Wang,
Mira Murati, Mo Bavarian, Mostafa Rohaninejad,
Nat McAleese, Neil Chowdhury, Neil Chowdhury,
Nick Ryder, Nikolas Tezak, Noam Brown, Ofir
Nachum, Oleg Boiko, Oleg Murk, Olivia Watkins,
Patrick Chao, Paul Ashbourne, Pavel Izmailov, Pe-
ter Zhokhov, Rachel Dias, Rahul Arora, Randall
Lin, Rapha Gontijo Lopes, Raz Gaon, Reah Mi-
yara, Reimar Leike, Renny Hwang, Rhythm Garg,
Robin Brown, Roshan James, Rui Shu, Ryan Cheu,
Ryan Greene, Saachi Jain, Sam Altman, Sam Toizer,
Sam Toyer, Samuel Miserendino, Sandhini Agarwal,
Santiago Hernandez, Sasha Baker, Scott McKinney,
Scottie Yan, Shengjia Zhao, Shengli Hu, Shibani
Santurkar, Shraman Ray Chaudhuri, Shuyuan Zhang,
Siyuan Fu, Spencer Papay, Steph Lin, Suchir Balaji,
Suvansh Sanjeev, Szymon Sidor, Tal Broda, Aidan
Clark, Tao Wang, Taylor Gordon, Ted Sanders, Te-
jal Patwardhan, Thibault Sottiaux, Thomas Degry,
Thomas Dimson, Tianhao Zheng, Timur Garipov,
Tom Stasi, Trapit Bansal, Trevor Creech, Troy Peter-
son, Tyna Eloundou, Valerie Qi, Vineet Kosaraju,
Vinnie Monaco, Vitchyr Pong, Vlad Fomenko,
Weiyi Zheng, Wenda Zhou, Wes McCabe, Wojciech
Zaremba, Yann Dubois, Yinghai Lu, Yining Chen,
Young Cha, Yu Bai, Yuchen He, Yuchen Zhang, Yun-
yun Wang, Zheng Shao, and Zhuohan Li. 2024b.
Openai o1 system card. Preprint , arXiv:2412.16720.
OpenAI. 2024. Multilingual massive multitask lan-
guage understanding (mmmlu).
Aleksandar Petrov, Emanuele La Malfa, Philip
H. S. Torr, and Adel Bibi. 2023. Language model to-
kenizers introduce unfairness between languages. In
Advances in Neural Information Processing Systems .
Matthew Renze and Erhan Guven. 2024. The effect of
sampling temperature on problem solving in large
language models. Preprint , arXiv:2402.05201.
Sheikh Shafayat, H Hasan, Minhajur Mahim, Rifki Pu-
tri, James Thorne, and Alice Oh. 2024. BEnQA:
A question answering benchmark for Bengali and
English. In Findings of the Association for Compu-
tational Linguistics: ACL 2024 , pages 1158–1177,
Bangkok, Thailand. Association for Computational
Linguistics.
Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,
Suraj Srivats, Soroush V osoughi, Hyung Won Chung,
Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.
Language models are multilingual chain-of-thought
reasoners. arXiv preprint arXiv:2210.03057 .Shivalika Singh, Angelika Romanou, Clémentine Four-
rier, David I. Adelani, Jian Gang Ngui, Daniel
Vila-Suero, Peerat Limkonchotiwat, Kelly Marchi-
sio, Wei Qi Leong, Yosephine Susanto, Raymond
Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith,
Antoine Bosselut, Alice Oh, Andre F. T. Martins,
Leshem Choshen, Daphne Ippolito, Enzo Ferrante,
Marzieh Fadaee, Beyza Ermis, and Sara Hooker.
2024. Global mmlu: Understanding and addressing
cultural and linguistic biases in multilingual evalua-
tion. Preprint , arXiv:2412.03304.
Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan
Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer,
Damien Vincent, Zhufeng Pan, Shibo Wang, Soroosh
Mariooryad, Yifan Ding, Xinyang Geng, Fred Al-
cober, Roy Frostig, Mark Omernick, Lexi Walker,
Cosmin Paduraru, Christina Sorokin, Andrea Tac-
chetti, Colin Gaffney, Samira Daruki, Olcan Ser-
cinoglu, Zach Gleicher, Juliette Love, Paul V oigt-
laender, Rohan Jain, Gabriela Surita, Kareem Mo-
hamed, Rory Blevins, Junwhan Ahn, Tao Zhu, Korn-
raphop Kawintiranon, Orhan Firat, Yiming Gu, Yu-
jing Zhang, Matthew Rahtz, Manaal Faruqui, Natalie
Clay, Justin Gilmer, JD Co-Reyes, Ivo Penchev, Rui
Zhu, Nobuyuki Morioka, Kevin Hui, Krishna Hari-
dasan, Victor Campos, Mahdis Mahdieh, Mandy Guo,
Samer Hassan, Kevin Kilgour, Arpi Vezer, Heng-
Tze Cheng, Raoul de Liedekerke, Siddharth Goyal,
Paul Barham, DJ Strouse, Seb Noury, Jonas Adler,
Mukund Sundararajan, Sharad Vikram, Dmitry Lep-
ikhin, Michela Paganini, Xavier Garcia, Fan Yang,
Dasha Valter, Maja Trebacz, Kiran V odrahalli, Chu-
layuth Asawaroengchai, Roman Ring, Norbert Kalb,
Livio Baldini Soares, Siddhartha Brahma, David
Steiner, Tianhe Yu, Fabian Mentzer, Antoine He,
Lucas Gonzalez, Bibo Xu, Raphael Lopez Kauf-
man, Laurent El Shafey, Junhyuk Oh, Tom Hennigan,
George van den Driessche, Seth Odoom, Mario Lucic,
Becca Roelofs, Sid Lall, Amit Marathe, Betty Chan,
Santiago Ontanon, Luheng He, Denis Teplyashin,
Jonathan Lai, Phil Crone, Bogdan Damoc, Lewis
Ho, Sebastian Riedel, Karel Lenc, Chih-Kuan Yeh,
Aakanksha Chowdhery, Yang Xu, Mehran Kazemi,
Ehsan Amid, Anastasia Petrushkina, Kevin Swersky,
Ali Khodaei, Gowoon Chen, Chris Larkin, Mario
Pinto, Geng Yan, Adria Puigdomenech Badia, Piyush
Patil, Steven Hansen, Dave Orr, Sebastien M. R.
Arnold, Jordan Grimstad, Andrew Dai, Sholto Dou-
glas, Rishika Sinha, Vikas Yadav, Xi Chen, Elena Gri-
bovskaya, Jacob Austin, Jeffrey Zhao, Kaushal Patel,
Paul Komarek, Sophia Austin, Sebastian Borgeaud,
Linda Friso, Abhimanyu Goyal, Ben Caine, Kris
Cao, Da-Woon Chung, Matthew Lamm, Gabe Barth-
Maron, Thais Kagohara, Kate Olszewska, Mia Chen,
Kaushik Shivakumar, Rishabh Agarwal, Harshal
Godhia, Ravi Rajwar, Javier Snaider, Xerxes Doti-
walla, Yuan Liu, Aditya Barua, Victor Ungureanu,
Yuan Zhang, Bat-Orgil Batsaikhan, Mateo Wirth,
James Qin, Ivo Danihelka, Tulsee Doshi, Martin
Chadwick, Jilin Chen, Sanil Jain, Quoc Le, Ar-
jun Kar, Madhu Gurumurthy, Cheng Li, Ruoxin
Sang, Fangyu Liu, Lampros Lamprou, Rich Munoz,
Nathan Lintz, Harsh Mehta, Heidi Howard, Mal-colm Reynolds, Lora Aroyo, Quan Wang, Lorenzo
Blanco, Albin Cassirer, Jordan Griffith, Dipanjan
Das, Stephan Lee, Jakub Sygnowski, Zach Fisher,
James Besley, Richard Powell, Zafarali Ahmed, Do-
minik Paulus, David Reitter, Zalan Borsos, Rishabh
Joshi, Aedan Pope, Steven Hand, Vittorio Selo, Vi-
han Jain, Nikhil Sethi, Megha Goel, Takaki Makino,
Rhys May, Zhen Yang, Johan Schalkwyk, Christina
Butterfield, Anja Hauth, Alex Goldin, Will Hawkins,
Evan Senter, Sergey Brin, Oliver Woodman, Mar-
vin Ritter, Eric Noland, Minh Giang, Vijay Bolina,
Lisa Lee, Tim Blyth, Ian Mackinnon, Machel Reid,
Obaid Sarvana, David Silver, Alexander Chen, Lily
Wang, Loren Maggiore, Oscar Chang, Nithya At-
taluri, Gregory Thornton, Chung-Cheng Chiu, Os-
kar Bunyan, Nir Levine, Timothy Chung, Evgenii
Eltyshev, Xiance Si, Timothy Lillicrap, Demetra
Brady, Vaibhav Aggarwal, Boxi Wu, Yuanzhong Xu,
Ross McIlroy, Kartikeya Badola, Paramjit Sandhu,
Erica Moreira, Wojciech Stokowiec, Ross Hems-
ley, Dong Li, Alex Tudor, Pranav Shyam, Elahe
Rahimtoroghi, Salem Haykal, Pablo Sprechmann,
Xiang Zhou, Diana Mincu, Yujia Li, Ravi Addanki,
Kalpesh Krishna, Xiao Wu, Alexandre Frechette,
Matan Eyal, Allan Dafoe, Dave Lacey, Jay Whang,
Thi Avrahami, Ye Zhang, Emanuel Taropa, Hanzhao
Lin, Daniel Toyama, Eliza Rutherford, Motoki Sano,
HyunJeong Choe, Alex Tomala, Chalence Safranek-
Shrader, Nora Kassner, Mantas Pajarskas, Matt
Harvey, Sean Sechrist, Meire Fortunato, Christina
Lyu, Gamaleldin Elsayed, Chenkai Kuang, James
Lottes, Eric Chu, Chao Jia, Chih-Wei Chen, Pe-
ter Humphreys, Kate Baumli, Connie Tao, Rajku-
mar Samuel, Cicero Nogueira dos Santos, Anders
Andreassen, Nemanja Raki ´cevi´c, Dominik Grewe,
Aviral Kumar, Stephanie Winkler, Jonathan Caton,
Andrew Brock, Sid Dalmia, Hannah Sheahan, Iain
Barr, Yingjie Miao, Paul Natsev, Jacob Devlin, Fer-
yal Behbahani, Flavien Prost, Yanhua Sun, Artiom
Myaskovsky, Thanumalayan Sankaranarayana Pillai,
Dan Hurt, Angeliki Lazaridou, Xi Xiong, Ce Zheng,
Fabio Pardo, Xiaowei Li, Dan Horgan, Joe Stanton,
Moran Ambar, Fei Xia, Alejandro Lince, Mingqiu
Wang, Basil Mustafa, Albert Webson, Hyo Lee, Ro-
han Anil, Martin Wicke, Timothy Dozat, Abhishek
Sinha, Enrique Piqueras, Elahe Dabir, Shyam Upad-
hyay, Anudhyan Boral, Lisa Anne Hendricks, Corey
Fry, Josip Djolonga, Yi Su, Jake Walker, Jane La-
banowski, Ronny Huang, Vedant Misra, Jeremy
Chen, RJ Skerry-Ryan, Avi Singh, Shruti Rijh-
wani, Dian Yu, Alex Castro-Ros, Beer Changpinyo,
Romina Datta, Sumit Bagri, Arnar Mar Hrafnkels-
son, Marcello Maggioni, Daniel Zheng, Yury Sul-
sky, Shaobo Hou, Tom Le Paine, Antoine Yang,
Jason Riesa, Dominika Rogozinska, Dror Marcus,
Dalia El Badawy, Qiao Zhang, Luyu Wang, Helen
Miller, Jeremy Greer, Lars Lowe Sjos, Azade Nova,
Heiga Zen, Rahma Chaabouni, Mihaela Rosca, Jiepu
Jiang, Charlie Chen, Ruibo Liu, Tara Sainath, Maxim
Krikun, Alex Polozov, Jean-Baptiste Lespiau, Josh
Newlan, Zeyncep Cankara, Soo Kwak, Yunhan Xu,
Phil Chen, Andy Coenen, Clemens Meyer, Katerina
Tsihlas, Ada Ma, Juraj Gottweis, Jinwei Xing, Chen-
jie Gu, Jin Miao, Christian Frank, Zeynep Cankara,Sanjay Ganapathy, Ishita Dasgupta, Steph Hughes-
Fitt, Heng Chen, David Reid, Keran Rong, Hongmin
Fan, Joost van Amersfoort, Vincent Zhuang, Aaron
Cohen, Shixiang Shane Gu, Anhad Mohananey,
Anastasija Ilic, Taylor Tobin, John Wieting, Anna
Bortsova, Phoebe Thacker, Emma Wang, Emily
Caveness, Justin Chiu, Eren Sezener, Alex Kaskasoli,
Steven Baker, Katie Millican, Mohamed Elhawaty,
Kostas Aisopos, Carl Lebsack, Nathan Byrd, Hanjun
Dai, Wenhao Jia, Matthew Wiethoff, Elnaz Davoodi,
Albert Weston, Lakshman Yagati, Arun Ahuja, Isabel
Gao, Golan Pundak, Susan Zhang, Michael Azzam,
Khe Chai Sim, Sergi Caelles, James Keeling, Ab-
hanshu Sharma, Andy Swing, YaGuang Li, Chenxi
Liu, Carrie Grimes Bostock, Yamini Bansal, Zachary
Nado, Ankesh Anand, Josh Lipschultz, Abhijit Kar-
markar, Lev Proleev, Abe Ittycheriah, Soheil Has-
sas Yeganeh, George Polovets, Aleksandra Faust,
Jiao Sun, Alban Rrustemi, Pen Li, Rakesh Shivanna,
Jeremiah Liu, Chris Welty, Federico Lebron, Anirudh
Baddepudi, Sebastian Krause, Emilio Parisotto, Radu
Soricut, Zheng Xu, Dawn Bloxwich, Melvin John-
son, Behnam Neyshabur, Justin Mao-Jones, Ren-
shen Wang, Vinay Ramasesh, Zaheer Abbas, Arthur
Guez, Constant Segal, Duc Dung Nguyen, James
Svensson, Le Hou, Sarah York, Kieran Milan, So-
phie Bridgers, Wiktor Gworek, Marco Tagliasacchi,
James Lee-Thorp, Michael Chang, Alexey Guseynov,
Ale Jakse Hartman, Michael Kwong, Ruizhe Zhao,
Sheleem Kashem, Elizabeth Cole, Antoine Miech,
Richard Tanburn, Mary Phuong, Filip Pavetic, Se-
bastien Cevey, Ramona Comanescu, Richard Ives,
Sherry Yang, Cosmo Du, Bo Li, Zizhao Zhang,
Mariko Iinuma, Clara Huiyi Hu, Aurko Roy, Shaan
Bijwadia, Zhenkai Zhu, Danilo Martins, Rachel
Saputro, Anita Gergely, Steven Zheng, Dawei Jia,
Ioannis Antonoglou, Adam Sadovsky, Shane Gu,
Yingying Bi, Alek Andreev, Sina Samangooei, Mina
Khan, Tomas Kocisky, Angelos Filos, Chintu Ku-
mar, Colton Bishop, Adams Yu, Sarah Hodkin-
son, Sid Mittal, Premal Shah, Alexandre Moufarek,
Yong Cheng, Adam Bloniarz, Jaehoon Lee, Pedram
Pejman, Paul Michel, Stephen Spencer, Vladimir
Feinberg, Xuehan Xiong, Nikolay Savinov, Char-
lotte Smith, Siamak Shakeri, Dustin Tran, Mary
Chesus, Bernd Bohnet, George Tucker, Tamara von
Glehn, Carrie Muir, Yiran Mao, Hideto Kazawa,
Ambrose Slone, Kedar Soparkar, Disha Shrivastava,
James Cobon-Kerr, Michael Sharman, Jay Pavagadhi,
Carlos Araya, Karolis Misiunas, Nimesh Ghelani,
Michael Laskin, David Barker, Qiujia Li, Anton
Briukhov, Neil Houlsby, Mia Glaese, Balaji Laksh-
minarayanan, Nathan Schucher, Yunhao Tang, Eli
Collins, Hyeontaek Lim, Fangxiaoyu Feng, Adria
Recasens, Guangda Lai, Alberto Magni, Nicola De
Cao, Aditya Siddhant, Zoe Ashwood, Jordi Orbay,
Mostafa Dehghani, Jenny Brennan, Yifan He, Kelvin
Xu, Yang Gao, Carl Saroufim, James Molloy, Xinyi
Wu, Seb Arnold, Solomon Chang, Julian Schrit-
twieser, Elena Buchatskaya, Soroush Radpour, Mar-
tin Polacek, Skye Giordano, Ankur Bapna, Simon
Tokumine, Vincent Hellendoorn, Thibault Sottiaux,
Sarah Cogan, Aliaksei Severyn, Mohammad Saleh,
Shantanu Thakoor, Laurent Shefey, Siyuan Qiao,Meenu Gaba, Shuo yiin Chang, Craig Swanson, Biao
Zhang, Benjamin Lee, Paul Kishan Rubenstein, Gan
Song, Tom Kwiatkowski, Anna Koop, Ajay Kan-
nan, David Kao, Parker Schuh, Axel Stjerngren, Gol-
naz Ghiasi, Gena Gibson, Luke Vilnis, Ye Yuan, Fe-
lipe Tiengo Ferreira, Aishwarya Kamath, Ted Kli-
menko, Ken Franko, Kefan Xiao, Indro Bhattacharya,
Miteyan Patel, Rui Wang, Alex Morris, Robin
Strudel, Vivek Sharma, Peter Choy, Sayed Hadi
Hashemi, Jessica Landon, Mara Finkelstein, Priya
Jhakra, Justin Frye, Megan Barnes, Matthew Mauger,
Dennis Daun, Khuslen Baatarsukh, Matthew Tung,
Wael Farhan, Henryk Michalewski, Fabio Viola, Fe-
lix de Chaumont Quitry, Charline Le Lan, Tom Hud-
son, Qingze Wang, Felix Fischer, Ivy Zheng, Elspeth
White, Anca Dragan, Jean baptiste Alayrac, Eric Ni,
Alexander Pritzel, Adam Iwanicki, Michael Isard,
Anna Bulanova, Lukas Zilka, Ethan Dyer, Deven-
dra Sachan, Srivatsan Srinivasan, Hannah Mucken-
hirn, Honglong Cai, Amol Mandhane, Mukarram
Tariq, Jack W. Rae, Gary Wang, Kareem Ayoub,
Nicholas FitzGerald, Yao Zhao, Woohyun Han, Chris
Alberti, Dan Garrette, Kashyap Krishnakumar, Mai
Gimenez, Anselm Levskaya, Daniel Sohn, Josip
Matak, Inaki Iturrate, Michael B. Chang, Jackie Xi-
ang, Yuan Cao, Nishant Ranka, Geoff Brown, Adrian
Hutter, Vahab Mirrokni, Nanxin Chen, Kaisheng
Yao, Zoltan Egyed, Francois Galilee, Tyler Liechty,
Praveen Kallakuri, Evan Palmer, Sanjay Ghemawat,
Jasmine Liu, David Tao, Chloe Thornton, Tim Green,
Mimi Jasarevic, Sharon Lin, Victor Cotruta, Yi-Xuan
Tan, Noah Fiedel, Hongkun Yu, Ed Chi, Alexan-
der Neitz, Jens Heitkaemper, Anu Sinha, Denny
Zhou, Yi Sun, Charbel Kaed, Brice Hulse, Swa-
roop Mishra, Maria Georgaki, Sneha Kudugunta,
Clement Farabet, Izhak Shafran, Daniel Vlasic, An-
ton Tsitsulin, Rajagopal Ananthanarayanan, Alen
Carin, Guolong Su, Pei Sun, Shashank V , Gabriel
Carvajal, Josef Broder, Iulia Comsa, Alena Repina,
William Wong, Warren Weilun Chen, Peter Hawkins,
Egor Filonov, Lucia Loher, Christoph Hirnschall,
Weiyi Wang, Jingchen Ye, Andrea Burns, Hardie
Cate, Diana Gage Wright, Federico Piccinini, Lei
Zhang, Chu-Cheng Lin, Ionel Gog, Yana Kulizh-
skaya, Ashwin Sreevatsa, Shuang Song, Luis C.
Cobo, Anand Iyer, Chetan Tekur, Guillermo Gar-
rido, Zhuyun Xiao, Rupert Kemp, Huaixiu Steven
Zheng, Hui Li, Ananth Agarwal, Christel Ngani,
Kati Goshvadi, Rebeca Santamaria-Fernandez, Woj-
ciech Fica, Xinyun Chen, Chris Gorgolewski, Sean
Sun, Roopal Garg, Xinyu Ye, S. M. Ali Eslami,
Nan Hua, Jon Simon, Pratik Joshi, Yelin Kim, Ian
Tenney, Sahitya Potluri, Lam Nguyen Thiet, Quan
Yuan, Florian Luisier, Alexandra Chronopoulou, Sal-
vatore Scellato, Praveen Srinivasan, Minmin Chen,
Vinod Koverkathu, Valentin Dalibard, Yaming Xu,
Brennan Saeta, Keith Anderson, Thibault Sellam,
Nick Fernando, Fantine Huot, Junehyuk Jung, Mani
Varadarajan, Michael Quinn, Amit Raul, Maigo Le,
Ruslan Habalov, Jon Clark, Komal Jalan, Kalesha
Bullard, Achintya Singhal, Thang Luong, Boyu
Wang, Sujeevan Rajayogam, Julian Eisenschlos,
Johnson Jia, Daniel Finchelstein, Alex Yakubovich,
Daniel Balle, Michael Fink, Sameer Agarwal, JingLi, Dj Dvijotham, Shalini Pal, Kai Kang, Jaclyn
Konzelmann, Jennifer Beattie, Olivier Dousse, Diane
Wu, Remi Crocker, Chen Elkind, Siddhartha Reddy
Jonnalagadda, Jong Lee, Dan Holtmann-Rice, Krys-
tal Kallarackal, Rosanne Liu, Denis Vnukov, Neera
Vats, Luca Invernizzi, Mohsen Jafari, Huanjie Zhou,
Lilly Taylor, Jennifer Prendki, Marcus Wu, Tom
Eccles, Tianqi Liu, Kavya Kopparapu, Francoise
Beaufays, Christof Angermueller, Andreea Marzoca,
Shourya Sarcar, Hilal Dib, Jeff Stanway, Frank Per-
bet, Nejc Trdin, Rachel Sterneck, Andrey Khor-
lin, Dinghua Li, Xihui Wu, Sonam Goenka, David
Madras, Sasha Goldshtein, Willi Gierke, Tong Zhou,
Yaxin Liu, Yannie Liang, Anais White, Yunjie Li,
Shreya Singh, Sanaz Bahargam, Mark Epstein, Su-
joy Basu, Li Lao, Adnan Ozturel, Carl Crous, Alex
Zhai, Han Lu, Zora Tung, Neeraj Gaur, Alanna
Walton, Lucas Dixon, Ming Zhang, Amir Glober-
son, Grant Uy, Andrew Bolt, Olivia Wiles, Milad
Nasr, Ilia Shumailov, Marco Selvi, Francesco Pic-
cinno, Ricardo Aguilar, Sara McCarthy, Misha Khal-
man, Mrinal Shukla, Vlado Galic, John Carpen-
ter, Kevin Villela, Haibin Zhang, Harry Richard-
son, James Martens, Matko Bosnjak, Shreyas Ram-
mohan Belle, Jeff Seibert, Mahmoud Alnahlawi,
Brian McWilliams, Sankalp Singh, Annie Louis,
Wen Ding, Dan Popovici, Lenin Simicich, Laura
Knight, Pulkit Mehta, Nishesh Gupta, Chongyang
Shi, Saaber Fatehi, Jovana Mitrovic, Alex Grills,
Joseph Pagadora, Tsendsuren Munkhdalai, Dessie
Petrova, Danielle Eisenbud, Zhishuai Zhang, Damion
Yates, Bhavishya Mittal, Nilesh Tripuraneni, Yan-
nis Assael, Thomas Brovelli, Prateek Jain, Miha-
jlo Velimirovic, Canfer Akbulut, Jiaqi Mu, Wolf-
gang Macherey, Ravin Kumar, Jun Xu, Haroon
Qureshi, Gheorghe Comanici, Jeremy Wiesner, Zhi-
tao Gong, Anton Ruddock, Matthias Bauer, Nick
Felt, Anirudh GP, Anurag Arnab, Dustin Zelle,
Jonas Rothfuss, Bill Rosgen, Ashish Shenoy, Bryan
Seybold, Xinjian Li, Jayaram Mudigonda, Goker
Erdogan, Jiawei Xia, Jiri Simsa, Andrea Michi,
Yi Yao, Christopher Yew, Steven Kan, Isaac Caswell,
Carey Radebaugh, Andre Elisseeff, Pedro Valen-
zuela, Kay McKinney, Kim Paterson, Albert Cui, Eri
Latorre-Chimoto, Solomon Kim, William Zeng, Ken
Durden, Priya Ponnapalli, Tiberiu Sosea, Christo-
pher A. Choquette-Choo, James Manyika, Brona
Robenek, Harsha Vashisht, Sebastien Pereira, Hoi
Lam, Marko Velic, Denese Owusu-Afriyie, Kather-
ine Lee, Tolga Bolukbasi, Alicia Parrish, Shawn Lu,
Jane Park, Balaji Venkatraman, Alice Talbert, Lam-
bert Rosique, Yuchung Cheng, Andrei Sozanschi,
Adam Paszke, Praveen Kumar, Jessica Austin, Lu Li,
Khalid Salama, Bartek Perz, Wooyeol Kim, Nandita
Dukkipati, Anthony Baryshnikov, Christos Kapla-
nis, XiangHai Sheng, Yuri Chervonyi, Caglar Unlu,
Diego de Las Casas, Harry Askham, Kathryn Tun-
yasuvunakool, Felix Gimeno, Siim Poder, Chester
Kwak, Matt Miecnikowski, Vahab Mirrokni, Alek
Dimitriev, Aaron Parisi, Dangyi Liu, Tomy Tsai,
Toby Shevlane, Christina Kouridi, Drew Garmon,
Adrian Goedeckemeyer, Adam R. Brown, Anitha Vi-
jayakumar, Ali Elqursh, Sadegh Jazayeri, Jin Huang,
Sara Mc Carthy, Jay Hoover, Lucy Kim, SandeepKumar, Wei Chen, Courtney Biles, Garrett Bingham,
Evan Rosen, Lisa Wang, Qijun Tan, David Engel,
Francesco Pongetti, Dario de Cesare, Dongseong
Hwang, Lily Yu, Jennifer Pullman, Srini Narayanan,
Kyle Levin, Siddharth Gopal, Megan Li, Asaf Aha-
roni, Trieu Trinh, Jessica Lo, Norman Casagrande,
Roopali Vij, Loic Matthey, Bramandia Ramadhana,
Austin Matthews, CJ Carey, Matthew Johnson, Kre-
mena Goranova, Rohin Shah, Shereen Ashraf, King-
shuk Dasgupta, Rasmus Larsen, Yicheng Wang, Man-
ish Reddy Vuyyuru, Chong Jiang, Joana Ijazi, Kazuki
Osawa, Celine Smith, Ramya Sree Boppana, Tay-
lan Bilal, Yuma Koizumi, Ying Xu, Yasemin Altun,
Nir Shabat, Ben Bariach, Alex Korchemniy, Kiam
Choo, Olaf Ronneberger, Chimezie Iwuanyanwu,
Shubin Zhao, David Soergel, Cho-Jui Hsieh, Irene
Cai, Shariq Iqbal, Martin Sundermeyer, Zhe Chen,
Elie Bursztein, Chaitanya Malaviya, Fadi Biadsy,
Prakash Shroff, Inderjit Dhillon, Tejasi Latkar, Chris
Dyer, Hannah Forbes, Massimo Nicosia, Vitaly Niko-
laev, Somer Greene, Marin Georgiev, Pidong Wang,
Nina Martin, Hanie Sedghi, John Zhang, Praseem
Banzal, Doug Fritz, Vikram Rao, Xuezhi Wang, Ji-
ageng Zhang, Viorica Patraucean, Dayou Du, Igor
Mordatch, Ivan Jurin, Lewis Liu, Ayush Dubey, Abhi
Mohan, Janek Nowakowski, Vlad-Doru Ion, Nan
Wei, Reiko Tojo, Maria Abi Raad, Drew A. Hud-
son, Vaishakh Keshava, Shubham Agrawal, Kevin
Ramirez, Zhichun Wu, Hoang Nguyen, Ji Liu, Mad-
havi Sewak, Bryce Petrini, DongHyun Choi, Ivan
Philips, Ziyue Wang, Ioana Bica, Ankush Garg,
Jarek Wilkiewicz, Priyanka Agrawal, Xiaowei Li,
Danhao Guo, Emily Xue, Naseer Shaik, Andrew
Leach, Sadh MNM Khan, Julia Wiesinger, Sammy
Jerome, Abhishek Chakladar, Alek Wenjiao Wang,
Tina Ornduff, Folake Abu, Alireza Ghaffarkhah, Mar-
cus Wainwright, Mario Cortes, Frederick Liu, Joshua
Maynez, Andreas Terzis, Pouya Samangouei, Ri-
ham Mansour, Tomasz K˛ epa, François-Xavier Aubet,
Anton Algymr, Dan Banica, Agoston Weisz, An-
dras Orban, Alexandre Senges, Ewa Andrejczuk,
Mark Geller, Niccolo Dal Santo, Valentin Anklin,
Majd Al Merey, Martin Baeuml, Trevor Strohman,
Junwen Bai, Slav Petrov, Yonghui Wu, Demis Has-
sabis, Koray Kavukcuoglu, Jeff Dean, and Oriol
Vinyals. 2024. Gemini 1.5: Unlocking multimodal
understanding across millions of tokens of context.
Preprint , arXiv:2403.05530.7 Appendix
7.1 Evaluation Details
Since our MCQ questions are largely factual-based
and do not require reasoning for most cases, we set
the maximum output token length is set to 1024
for all experiments. This allows use to analyze
responses from models during cases where mod-
els produce verbose responses, primarily in 0-shot
setting, due to lack of guiding examples in 5-shot
setting, despite being instructed in the prompt to
produce only option ID as output. We set the de-
coding temperature to 0.2 to reduce randomness,
however, as shown in (Renze and Guven, 2024),
changing temperature from 0 to 1 do not have a
significant performance change in LLMs.
For 5-shot setting, we randomly pick 5 questions
from each category. Since we perform a meticu-
lous categorization and double-inspection process,
our randomly selected samples are generally good
representations of the category.
Figure 1: Illustration of our prompt.Prompting Strategies
Figure 2: Prompt Structure for 5-shot setting using GPT
model.
7.2 Additional BLUCK ResultsCategoriesGPT-4o-mini Claude-3.5-Haiku Gemini-1.5-Flash Llama-3.1-8B
0-shot 5-shot 0-shot 5-shot 0-shot 5-shot 0-shot 5-shot
HistoryAncient Bengal 0.667 0.737 0.687 0.717 0.646 0.697 0.394 0.404
British Bengal 0.775 0.8 0.7 0.825 0.675 0.8 0.35 0.475
Pakistan Era 0.528 0.566 0.453 0.491 0.377 0.491 0.302 0.396
Average 0.624 0.673 0.588 0.637 0.535 0.624 0.347 0.412
CultureIndigenous People 0.484 0.677 0.452 0.645 0.355 0.581 0.355 0.452
Arts, Heritage & Media 0.478 0.536 0.42 0.478 0.449 0.449 0.29 0.377
National Issues 0.4 0.533 0.267 0.4 0.467 0.667 0.133 0.533
Constitution 0.677 0.774 0.581 0.71 0.645 0.839 0.355 0.387
Resources 0.472 0.722 0.417 0.5 0.472 0.583 0.25 0.389
Geography 0.563 0.598 0.46 0.506 0.471 0.529 0.299 0.333
Law 0.472 0.546 0.496 0.514 0.493 0.577 0.345 0.405
Average 0.497 0.584 0.472 0.523 0.483 0.571 0.320 0.394
PhoneticsAlphabet 0.2 0.8 0.7 0.8 0.7 0.9 0.2 0.7
Pronunciation 0.159 0.275 0.261 0.275 0.203 0.319 0.217 0.29
Conjunct Letters 0.478 0.652 0.783 0.957 0.609 0.783 0.478 0.522
Sound & Letters 0.5 0.625 0.438 0.646 0.625 0.667 0.25 0.292
Sound Changes 0.278 0.333 0.315 0.444 0.278 0.481 0.296 0.352
Phonetic Combining Rules 0.402 0.418 0.435 0.505 0.457 0.478 0.31 0.359
Miscellaneous Phonetics 0.55 0.6 0.525 0.613 0.575 0.575 0.4 0.363
Average 0.387 0.459 0.434 0.526 0.449 0.515 0.310 0.357
SemanticsSynonyms 0.681 0.747 0.761 0.843 0.687 0.775 0.385 0.426
Antonyms 0.642 0.691 0.679 0.77 0.691 0.758 0.412 0.527
One Word Expressions 0.506 0.567 0.589 0.661 0.522 0.606 0.433 0.406
Idioms 0.515 0.5 0.444 0.495 0.48 0.581 0.354 0.333
Proverbs 0.66 0.66 0.638 0.723 0.66 0.787 0.404 0.426
Miscellaneous 0.616 0.589 0.521 0.568 0.514 0.555 0.363 0.39
Average 0.607 0.640 0.626 0.698 0.599 0.681 0.389 0.416
Overall Average 0.540 0.595 0.548 0.617 0.536 0.617 0.353 0.399
Table 3: BLUCK benchmark comparison by subcategories and major categories across smaller models in 0-shot
and 5-shot settings. The highest accuracy(s) for each category are boldy marked.(a) Accuracy for the history domain (0-shot and 5-shot).
(b) Accuracy for the culture domain (0-shot and 5-shot).
Figure 3: Comparison of accuracy across history and culture domains under 0-shot and 5-shot settings.(a) Accuracy for the phonetics domain (0-shot and 5-shot).
(b) Accuracy for the semantics domain (0-shot and 5-shot).
Figure 4: Comparison of accuracy across phonetics and semantics domains under 0-shot and 5-shot settings.