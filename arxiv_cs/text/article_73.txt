arXiv:2505.21087v1  [cs.LO]  27 May 2025Stopping Criteria for Value Iteration on Concurrent
Stochastic Reachability and Safety Games
Marta Grobelna∗, Jan K ˇret´ınsk´y†∗, Maximilian Weininger‡§∗
∗Technical University of Munich
Munich, Germany†Masaryk University
Brno, Czech Republic‡Ruhr-University Bochum
Bochum, Germany§Institute of Science and
Technology Austria
marta.grobelna@tum.de jan.kretinsky@fi.muni.cz maximilian.weininger@rub.de Klosterneuburg, Austria
Abstract —We consider two-player zero-sum concurrent stochas-
tic games (CSGs) played on graphs with reachability and safety
objectives. These include degenerate classes such as Markov
decision processes or turn-based stochastic games, which can be
solved by linear or quadratic programming; however, in practice,
value iteration (VI) outperforms the other approaches and is the
most implemented method. Similarly, for CSGs, this practical
performance makes VI an attractive alternative to the standard
theoretical solution via the existential theory of reals.
VI starts with an under-approximation of the sought values for
each state and iteratively updates them, traditionally terminating
once two consecutive approximations are ϵ-close. However, this
stopping criterion lacks guarantees on the precision of the
approximation, which is the goal of this work. We provide bounded
(a.k.a. interval) VI for CSGs: it complements standard VI with a
converging sequence of over-approximations and terminates once
the over- and under-approximations are ϵ-close.
Index Terms —Formal methods, foundations of probabilistic
systems and games, verification, model checking
I. I NTRODUCTION
Concurrent stochastic games (CSGs, e.g., [14]): We
consider two-player zero-sum games played on a graph. Every
vertex represents a state . Edges are directed, originating from
one state and leading to one or several other states. An
edge is associated with a probability distribution over the
successor states. A play proceeds through the graph as follows:
starting from an initial state, both players simultaneously and
independently choose an action , determining the edge to follow.
Then, the next state is sampled according to the probability
distribution, and the process is repeated in the successor.
We focus on infinite-horizon reachability and safety ob-
jectives [14]. The goal of reachability is to maximize the
probability of reaching a given goal state. In contrast, the
safety objective aims to maximize the probability of staying
within a given set of states. The two objectives are dual,
as instead of maximizing the probability of reaching a set
of target states, one can minimize the probability of staying
within the set of non-target states. Thus, we refer to both types
collectively as CSGs. Popular subclasses of CSGs include
turn-based stochastic games (TSGs), where the players make
This research was funded in part by the German Research Foundation
(DFG) project 427755713 GOPro, the MUNI Award in Science and Humanities
(MUNI/I/1757/2021) of the Grant Agency of Masaryk University, the European
Union’s Horizon 2020 research and innovation programme under the Marie
Sklodowska-Curie grant agreement No 101034413, and the ERC Starting
Grant DEUCE (101077178).decisions in turns, or Markov decision processes (MDPs), which
involve only one player.
The value problem: In CSGs, memoryless (a.k.a. sta-
tionary) strategies suffice for both players, meaning they
yield the same supremum probability as history-dependent
strategies. However, unlike in TSGs, the strategies for CSGs
require randomization , meaning players choose distributions
over actions rather than single actions. Additionally, while the
safety objective player, Player S, can attain optimal strategies
[39], the reachability objective player, Player R, only possesses
ε-optimal strategies for a given ε >0[21]. As a result, the
problem of deciding whether the supremum probability (a.k.a.
the value ) is at least pforp∈[0,1], is thus more subtle than for
the mentioned subclasses. While for MDPs the value problem
is in P, and for TSGs it is known to be in NP∩co-NP , for
CSGs it can be elegantly encoded into the existential theory of
reals (ETR) , which is only known to be decidable in PSPACE
(although not known to be complete for it) [20]. Unfortunately,
algorithms for ETR are practically even worse than the more
general, doubly exponential methods for the first-order theory
of reals [40]. “Finding a practical algorithm remains a very
interesting open problem” [25].
Practical approximation: We focus on algorithms approx-
imating the value with a predefined precision ε >0. Both for
MDPs and TSGs, dynamic programming techniques such as
value iteration (VI) or strategy iteration (SI) are practically more
efficient than mathematical programming (linear or quadratic,
respectively) [27], [30]. Thus, VI algorithms are prevalently
used and implemented in popular tools such as PRISM -GAMES
[33], motivating the focus on VI here.
Problem and our contribution: In VI, the lowest possible
value is initially assigned to each state and then iteratively
improved, computing an under-approximation of the value,
converging to it in the limit. The algorithm (in practical imple-
mentations) terminates once two consecutive approximations
areε-close. However, the result can then be arbitrarily imprecise
[23]. In this work, we introduce bounded value iteration for
CSGs , following its previous success for MDPs [1], [5] or TSGs
[18]. Its main idea is to enhance standard VI by introducing
an over-approximation of the values computed in parallel with
the under-approximation. Once the upper and lower bounds
areε-close, VI terminates, ensuring that the true value is at
most εaway from the obtained approximation. Since the na ¨ıve
formulation of an upper bound does not converge to the valuein general, previous approaches, notably including [7], have
attempted to fix this but have failed. In this paper, we finally
provide a valid solution.
Technical challenge: The fundamental technical difficul-
ties arise from the following. The non-convergence of upper-
bound approximations is primarily due to cyclic components,
so-called end components (ECs) , see [14], [18], [29]. Notably,
non-convergence, for this reason, is an issue present already in
MDPs and TSGs; see [18], [29]. Solutions have been developed
over the past decade for these two subclasses. Indeed, for
MDPs with reachability objective, these end components can
effectively be removed from the graph without altering the
value [6], [24]. Other objectives were considered in [1], [3].
TSGs with reachability objectives already require more careful
analysis, decomposing the end components into sub-parts, so-
called simple ECs [18]. A comprehensive framework for various
quantitative objectives was proposed in [29]. Unfortunately,
the idea of simple ECs is not easily extendable from TSGs to
CSGs due to the absence of optimal strategies.
Summary of our contribution: We provide a stopping
criterion for VI on CSGs, solving an open problem with
erroneous solution attempts in the literature (see the related
work in Subsec. I-Abelow). To this end, we unravel the
recursive hierarchical structure of end components in CSGs
(see Rem. 31) and adapt the bounded VI algorithm.
A. Related Work
Available approaches: The PSPACE -algorithm intro-
duced in [20] for deciding whether the value of a given
game is at least p, forp∈[0,1], allows for a trivial stopping
criterion by iteratively executing this algorithm for a suitable
sequence of (pi)i∈N(intuitively, we choose pisuch that
alternatingly, the value of the game is above and below,
while the distance between two consecutive pi’s monotonically
decreases). However, this criterion is impractical since it uses
the existential theory of reals [20]. The best known complexity
upper bound comes from [22] and states that the problem of
approximating the value of a CSG is in TFNP[NP], i.e. total
function from NP with an oracle for NP. However, the proposed
algorithm is not practical, as it relies on guessing a floating
point representation of the value and optimal strategies for both
players. A recursive bisection algorithm was introduced in [26],
which is also impractical as its time complexity is best-case
doubly exponential in the number of states. For the algorithms
commonly employed for in the non-concurrent case, VI and
SI, [25] provide doubly exponential lower and upper bounds
on the number of iterations that VI requires in the worst-case
for computing an ε-approximation. Their counter-example uses
a CSG where all states have value 1. Thus, the worst-case
complexity of our approach is the same since an additional
over-approximation does not speed up convergence in this
example. Nonetheless, these results are worst-case bounds, i.e.
they hold a priori for all games; earlier termination is possible,
but necessarily requires a stopping criterion, which has so
far been elusive. Finally, in [38], an algorithm is provided
that, unlike all other known algorithms, only has a single -exponential dependency on the number of states. A practical
comparison of our value iteration and [38] is an interesting
future step, as better worst-case complexity of an algorithm
need not translate to better practical performance on typical
instances; for example, in MDPs, worst-case exponential VI
and SI typically outperform the polynomial approach of linear
programming [27].
Previous attempts at stopping criterion: A stopping
criterion for SI and VI on CSGs was first presented in [7], but
later found to contain an irreparable mistake [10]. Specifically,
the algorithm returned over-approximations smaller than the
actual values in certain situations, as detailed in [10]. We
analyze the counter-example from [10] in App. D-A Later,
[19] proposed a stopping criterion for VI, which also contains
a fundamental flaw: it fails to converge for CSGs with ECs. We
analyze the counter-example to this approach in App. D-B.Our
work thus delivers the first stopping criterion in this context.
Further directions of related work: Variants of CSGs
have appeared very early, under the names of Everett, Gilette,
or Shapley game. See [26] for an explanation of all game
types, their relations, and algorithms to solve them. These
games also consider discounted payoff or limit-average payoff,
generalizing the reachability and safety CSGs we consider
here. A generalization of CSGs to ω-regular objectives has
been considered in [8], [15]. An insightful characterization of
optimal strategies in concurrent games with various objectives
can be found in [4].
II. P RELIMINARIES
A. Concurrent Stochastic Games
Probability Distributions: For a countable set X, a
function µ:X→[0,1]is called a distribution over XifP
x∈Xµ(x) = 1 . The support ofµisSupp(µ) :={x|µ(x)>
0}. The set of all distributions over Xis denoted by Dist(X).
Concurrent Stochastic Games: Aconcurrent stochastic
game (CSG) [16] is a tuple G:= (S,A,ΓR,ΓS, δ,s0,T),
where Sis a finite set of states ,A:=A×Bis a finite set
ofactions with A:={a1, . . . , al}andB:={b1, . . . , bm}the
sets of actions available for player RandS, respectively,
ΓR:S→(2A\ ∅)andΓS:S→(2B\ ∅)are two enabled
actions assignments and δ:S×A×B→Dist(S)is a
transition function , where δ 
s,a,b 
s′
gives the probability
of a transition from state sto state s′when player Rchooses
action a∈ΓR(s)and player Saction b∈ΓS(s),s0∈Sis
aninitial state , and T⊆Sis a set of target states . ACSG is
turn-based if for every state sonly one player has a meaningful
choice, i.e. either ΓR(s)orΓS(s)is a singleton; we call such
game a turn-based stochastic game (TSG).
Example 1 (CSGs ).Consider the CSGHide-Run-or-Slip
depicted in Fig. 1 (an adaption of the Hide-or-Run game
in [16], [21], [31]). Circles represent states and black dots depict
a probabilistic transition with uniform distribution. Each edge is
labeled with a pair of actions, the left for player Rand the right
for player S;□is a placeholder for an arbitrary action. We
have S:={shide, shome, swet}, with ΓR(shide):={hide,run}
2shideshome
swet(hide,throw )
(run,throw )(run,wait)(hide,wait)(□,□)
(□,□)
Fig. 1: Example CSG called Hide-Run-or-Slip .
andΓS(s0):={wait,throw},s0is the initial state denoted
by the arrow with no predecessor state, and T:={shome}.
The game has the following intuitive interpretation: Player R
wants to get home without getting wet. Player Shas a single
snowball and can make player Rwet by throwing it at player
R. If player Rruns and player Sthrows the ball, player
Rgets wet. If player Rruns but player Swaits, with a
probability of1
3the player reaches home or the player slips
instead and with a probability of1
3does not move at all or
with a probability of1
3falls on the ground and gets wet. △
Plays: Aplayπof a CSG Gis an infinite sequence of
states s0s1s2···, such that for all i∈Nthere are actions
a∈ΓR(si)andb∈ΓS(si)with δ 
si,a,b 
si+1
>0.
Play(G)is the set of all plays and Plays(G)the set of all
plays s0s1s2···withs0=s.
Strategies: Astrategy for player R(orS) is a function
ρ:S→Dist(A)(orσ:S→Dist(B)) that assigns a
distribution over actions available to player R(orS) to
each state, i.e., for all s∈S,Supp(ρ(s))⊆ΓR(s)(or
Supp(σ(s))⊆ΓS(s)).1We call a Player R(or Player
S) strategy ρpure if all distributions it returns are Dirac
distributions, i.e., at each s∈Swe have a unique action
a∈ΓR(s)(orb∈ΓS(s)) such that ρ(s)(a) = 1 (or
σ(s)(b) = 1 ). Otherwise, the strategy is mixed . For player
R(orS) we denote the set of strategies by R(orS) and a
single strategy by ρ(orσ).
Markov Decision Processes: Given a CSG G, if we fix a
strategy ρ∈ R of player R, the game becomes a S-Markov
Decision Process (MDP, [41]) Gρwith the transition function
δρ 
s,b 
s′:=X
a∈ΓR(s)δ 
s,a,b 
s′
·ρ(s)(a),
for all s∈Sanda∈ΓR(s). The MDP induced by a fixed
strategy σ∈ S is defined analogously.
Markov Chains: Similarly, by fixing a pair of strategies
(ρ, σ)∈ R×S , we obtain a Markov chain Gρ,σwith the same
state space S, the initial state s0, and the transition probabilities
Pgiven by
δρ,σ 
s 
s′:=X
(a,b)∈Aδ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b).
1Since memoryless strategies are sufficient for the objectives considered in
this paper, we do not introduce general history-dependent strategies to avoid
clutter. We refer to [16] for more details.Thus, a pair of strategies ( ρ, σ) induces a unique probability
measure Pρ,σ
s0over plays in the Markov chain as usual, see [2,
Chap. 10.1], where the set of paths starting in s0has measure 1.
Objectives: We partition SintoT, denoting the set of
states player Rwants to reach, and F:=S\T, denoting
the set of states player Swants to confine the game in. We
denote the reachability objective by ♢T:={s0s1s2··· | ∃ i∈
N:si∈T}and the safety objective by □F:={s0s1s2··· |
∀i∈N:si∈F}. The value of the objective ♢T, i.e. VR(s),
and the objective □F, i.e. VS(s), at state sare given by
VR(s):=sup
ρ∈Rinf
σ∈SPρ,σ
s 
♢T
VS(s):=sup
σ∈Sinf
ρ∈RPρ,σ
s 
□F
.
By the determinacy of CSGs and the duality of these
objectives [21], [34], it holds that VR(s) +VS(s) = 1 .
Consequently, the task of approximating VR(s)with a given
precision is equivalent to approximating VS(s). Further, the
objective of minimizing the reachability for Tis equivalent to
the objective of maximizing safety for Ffor the same player.
Consequently, in the following we only focus on maximizing
reachability as both minimization and the safety objectives can
be reduced to it.
Example 2 (Optimal Strategies Need Not Exist) .InCSGs , an
optimal strategy for player Rmight not exist [31], meaning that
at some states, the value is attainable only in the limit. Consider
our running example from Fig. 1, Hide-Run-or-Slip .
Assume for the moment that there is no chance of slipping,
i.e. upon playing runandwait, the target state is reached. To
win, Player Rhas to run eventually. However, Player Scan
utilize a strategy that throws with positive probability at all
points in time. Thus, Player Rcannot win almost surely.
However, Player Rhas the possibility of limit-sure winning
in [16]: By running with vanishingly low probability εin every
round, the probability of winning is 1−ε. This is because
PlayerShas the highest probability εof hitting Player R
by throwing in the first round; throwing in a later round n
only has hitting probability εn. For any ε >0, this strategy of
PlayerRachieves 1−ε. The value, being the supremum over
all strategies, is 1.
This notion of obtaining a value only in the limit is not
restricted to sure winning: By adding the chance of slipping,
the value of the game becomes 0.5. However, by the same
argument as above, Player Rcannot win with probability 0.5,
but only with a probability 0.5−εfor all ε >0. △
Fors∈S,a∈ΓR(s)and b∈ΓS(s), the set of
potential successors ofsis denoted by Post(s,a,b):=
Supp(δ 
s,a,b
). We lift the notation to strategies ρ∈ R
andσ∈ S by
Post(s,ρ, σ) =[
a∈Supp(ρ(s))[
b∈Supp(σ(s))Post(s,a,b).
We denote by WS:={s∈S|VR(s) = 0}thesure
winning region of player S. It can be computed in at most |S|
steps by iteration W0
S:= (S\T)andWk+1
S:={s∈S\T|
3∃b∈ΓS(s) :∀a∈ΓR(s) :Post(s,a,b)⊆Wk
S}for all
k∈N[15]. Consequently, we can assume without loss of
generality that TandWSare both singletons and absorbing.
Example 3 (The Sets T,F, and WS).In Fig. 1, player R
wants to reach T={s1}, while player Saims to stay in
F={s0, s2}. Since s2is absorbing, WS={s2}. △
Matrix Games: At each state of a CSG the players R
andSplay a two-player zero-sum matrix game [35], [43]. In
general, a matrix game is a tuple Z:= (N, A, u )[43] where,
N:={1, . . . , n }is a finite set of players, A:={α1, . . . , α m}
is a finite set of actions available to each player, and u:A→Q
is a utility function. In CSGs , the matrix game played at a
specific state scan be represented by a matrix ZVR(s)∈Ql×m,
where ΓR(s) ={a1, . . . , a l}andΓS(s) ={b1, . . . , b m}. The
entries of the matrix correspond to the utility, i.e., the value
attainable upon choosing a pair of actions (ai,bj)∈A. Thus,
thei-th row and the j-th column is given by ZVR(s)(i, j):=P
s′∈Sδ 
s, ai, bj 
s′
·VR(s′).
Example 4 (Matrix Game) .Consider the CSG in Fig. 1. The
matrix game played at state shideis given by the following
matrix.
ZVR(shide) =throw wait 
01
3·VR(shide) +1
3run
1 VR(shide) hide(1)
PlayerRis the so called row player while Player Sis the
column player . △
In a matrix game, a player’s strategy is a distribution over
the available actions at a specific state. To distinguish between
strategies of a CSG and strategies of a matrix game, we refer
to strategies of a matrix game as local strategies and strategies
of a CSG asglobal strategies. The set of all local strategies
at a state sis denoted by R(s)orS(s)for player RorS,
respectively. The existence of optimal (local) strategies in a
matrix game for both players is guaranteed by Nash’s Theorem
[36], [37]. The payoff that is attainable with an optimal local
strategy is called value that we denote by V(ZVR)for a matrix
game ZVR. It can be calculated using linear programming (e.g.,
[28], see App. A-A).
End Components: A non-empty set of states C⊆S
is called an end component (EC) if (i) there exists a pair
of strategies (ρ, σ)∈ R × S such that for each s∈Cit
holds that Post(s,ρ, σ)⊆C; and (ii) for every pair of states
s, s′∈Cthere is a play s0s1···such that s0=sandsn=s′
for some n, and for all 0≤i < n , it holds si∈Cand
si+1∈Post(si,ρ, σ).
Intuitively, an ECis a set of states where a play can stay
forever under some pair of strategies. In other words, the
players can cooperate to keep the play inside the EC(this
is the usual way to lift the definition of [14] from MDP to
games). Thus, we can compute ECs in a CSG by computing
ECs in the corresponding MDP with both players unified, i.e.,
every pair of actions is interpreted as an action in the MDP .
Efficient algorithms for this exist [2], [12], [46]. An ECCiscalled inclusion maximal (short maximal) if there exists no EC
C′such that C⊊C′.
B. Value Iteration
Value iteration (VI, e.g. [11]) assigns an initial value estimate
to each state and then iteratively updates it. In classical VI,
which approximates the reachability value from below, the
initial estimates are 1 for states in Tand below the actual value
otherwise, e.g. 0. Each iteration backpropagates the estimate by
maximizing the expectation of the value player Rcan ensure
with respect to the previous estimate.
Formally, we capture estimates as valuations, where a
valuation υ:S→[0,1]is a function mapping each state s
to a real number representing the (approximate or true) value
of the state. For two valuations υ,υ′, we write υ≤υ′if
υ(s)≤υ′(s)for every s∈S.
To compute the expected value at a state s, the matrix game
Zυ(s)has to be solved, meaning its value, V(Zυ(s)), has to
be estimated. This computation is, especially in the turn-based
setting, also referred to as Bellman update . Formally,
V(Zυ(s)):=B(υ)(s):=sup
ρ∈R(s)inf
σ∈S(s)B(υ)(s,ρ, σ),
whereB(υ)(s,ρ, σ):=
X
(a,b)∈AX
s′∈Sρ(a)·σ(b)·δ 
s,a,b 
s′
·υ(s′).
Convergent Under-approximation: We recall VI from
below as in [9]: starting from the initial valuation L0, we
perform the Bellman update on every state to obtain a new
valuation. We denote by Lkthe valuation obtained in the k-th
iteration. Formally:
L0(s):=(
1,ifs∈T;
0,else,Lk+1(s):=B(Lk)(s).(2)
Since TandWSare absorbing, for all k∈Nwe have
Lk(s) = 1 for all s∈T, and Lk(s) = 0 for all s∈WS. The
updated valuation, i.e. Lk+1(s), is computed by solving the
corresponding matrix game.
Theorem 5 (VI converges from below [17, Thm. 1]) .VI from
below converges to the value, i.e. lim
k→∞Lk=VR.
Bounded Value Iteration: While Thm. 5 proves that VI
from below converges in the limit, this limit may not be reached
in finitely many steps and can be irrational [17]. Thus, we
do not know when to stop the algorithm to guarantee certain
precision of the approximation, as there is no practical bound
how close any valuation Lkis to the actual value. We merely
have the worst-case bound of [25]: Running for a number of
iterations that is doubly-exponential in the number of states
allows to conclude that the lower bound is ε-close to the value.
To obtain a practical stopping criterion, we use the approach
of Bounded Value Iteration (BVI), shown in Alg. 1. In addition
to the lower bound L, it maintains an upper bound on the value
4Algorithm 1 Bounded value iteration procedure for CSGs.
1:Algorithm BVI(CSG G, threshold ε >0)
2: WS← {s∈S|VR(s) = 0}▷Winning region for S
3: L0,U0initialized by Eq. (1) and (2), respectively
4: MEC←FIND MECs (G)▷Find all MECs in the game
5: k←0
6: repeat
7: fors∈Sdo▷Standard Bellman update of both bound
8: Lk+1(s)←B(Lk)(s)
9: Uk+1(s)←B(Uk)(s)
10: forC∈MEC do
11: Uk+1←DEFLATE (G,Uk+1,C)
12: k←k+ 1
13: until Uk+1−Lk+1≤ε
Uthat is meant to converge to the value from above. Na ¨ıvely,
this upper bound is defined as follows:
U0(s):=(
0,ifs∈WS;
1,else,Uk+1(s):=B(Uk)(s).(3)
Given a precision ε >0, the algorithm terminates once the
under- and the over-approximations are ε-close, i.e., when
both approximations are at most ε-away from the actual value.
However, applying Bellman updates does not suffice for the
over-approximation to converge in the presence of ECs, as the
following example shows:
Example 6 (Non-convergent Over-approximations) .Consider
the CSG Hide-Run-or-Slip in Fig. 1. To compute
Uk+1(shide)according to Eq. (3), in each iteration we solve the
matrix game ZUk(shide)is given by Eq. (1) where the unknown
VRis replaced by Uk, i.e.:
ZUk(shide) =throw wait 01
3·Uk(shide) +1
3run
1 Uk(shide) hide(4)
Table I shows the updates of the lower and upper bounds,
Lk(shide)andUk(shide), respectively. While the lower bound
converges to 0.5, the upper bound stays at 1. This is because
for player R, action hide always “promises” a valuation of 1
in the next step, as the lower row of the matrix yields 1 for
all player Sstrategies. △
III. C ONVERGENT OVER-APPROXIMATION : OVERVIEW
Here, we describe the structure of our solution. Ex. 6 shows
that the na ¨ıve definition of the over-approximation need not
TABLE I: BVI for the game Hide-Run-or-Slip (Fig. 1),
where the over-approximations do not converge.
k 0 1 2 ··· ∞
Lk(shide) 0.0 0.25 0.36 ··· 0.5
Uk(shide) 1.0 1.0 1.0 ··· 1.0converge to the true value. In particular, in the presence of
ECs, Bellman updates do not have a unique fixpoint. Thus,
our goal is to define a function DEFLATE (usage highlighted
in Alg. 1, definition in Alg. 3) that, intuitively, decreases the
“bloated” upper bounds inside each ECto a realistic value
substantiated by a value promised outside of this EC. Formally,
we ensure that Alg. 1 produces a monotonically decreasing
sequence of valuations (i) over-approximating the reachability
value and (ii) converging to it in the limit. This idea has been
successfully applied for TSGs [18], [29].
Remark 7 (Inflating for Safety) .Since over-approximations
need not converge for VR, dually under-approximations need
not converge for VS(as is the case in TSGs , see [29]). Thus,
to directly solve a safety game, one needs an inflating operation
dual to deflating. As described when introducing the objectives,
we take the conceptually easier route of reducing everything
to maximizing reachability objectives.
We proceed in two steps. First, in Sec. IV we prove that
indeed ECs are the source of non-convergence, in particular
what we call Bloated End Components . Intuitively, these are
ECs where at each state both players prefer local strategies
which all successor states belong to the EC. Second, in Sec. V,
we define the DEFLATE algorithm, which essentially ensures
that we focus on player Rstrategies that do not make the game
stuck in a ECbut rather progress towards the target. To this
end, we lift the notion of best exit from TSGs [18, Definition
3] to CSGs.
IV. T HECORE OF THE PROBLEM : CHARACTERIZING
BLOATED ENDCOMPONENTS
A locally optimal strategy of Player Rdoes not coincide with
an optimal global strategy of Player Rbecause the latter must
eventually leave ECs, while the former is under the illusion
that staying is optimal. Thus, in this section, we want to find
properties that local strategies (of both players) must fulfill in
order to leave an ECin a way that is globally optimal. Thus,
since this section mainly concerns local strategies, we use the
word strategy to speak about local strategies and explicitly
make clear when we talk about global ones.
Remark 8.Throughout the technical sections and the appendix,
we always fix a CSG G:= (S,A,ΓR,ΓS, δ,s0,T).
A. Convergence without ECs
As a first step, we prove that ECs are the only source of
non-convergence, and without them, the na ¨ıveBVI using only
Bellman updates converges.
Theorem 9 (Convergence without ECs — Proof in App. C-A).
LetGbe a CSG where all ECs are trivial, i.e. for every EC
Cwe have C⊆WS∪T. Then, the over-approximation using
only Eq. (3)converges, i.e. lim
k→∞Uk=VR.
Proof sketch. This proof is an extension of the proof of [18,
Theorem 1] for turn-based games to the concurrent setting. The
underlying idea is the same, and can be briefly summarized as
follows: We assume towards a contradiction that lim
k→∞Uk=:
5TABLE II: BVI for the CSG in Ex. 10, where the over-
approximations converge.
k 0 1 2 3 ··· ∞
Lk(shide) 0.01
34
90.4815 ··· 0.5
Uk(shide) 1.02
35
90.5185 ··· 0.5
U⋆̸=VR, and find a set Xthat maximizes the difference
between upper bound and value. We show that every pair of
strategies leaving the set Xdecreases the difference U⋆−VR.
However, VRandU⋆are fixpoints of the Bellman update,
from [17, Theorem 1] and Lem. 48, respectively. Consequently,
optimal strategies need to remain in the set. However, in the
absence of ECs, optimal strategies have to leave the set, which
yields a contradiction and proves that U⋆=VR.
The key difference to the proof of [18, Theorem 1] is that we
cannot argue about actions anymore, but have to consider mixed
strategies. This significantly complicates notation. Additionally,
and more importantly, the former proof crucially relied on the
fact that for a state of Player R, we know that its valuation
is at least as large as that of any action, and dually for a
state of Player S, its valuation is at most as large as that
of any action. In the concurrent setting, this is not true. The
optimal strategies need not be maximizing nor minimizing
the valuation and, moreover, they can be maximizing for one
valuation and minimizing for another. Thus, we found a more
general, and in fact simpler, way of proving that “no state in
Xcan depend on the outside” [18, Statement 5] and deriving
the contradiction.
Interestingly, not all ECs cause non-convergence of Eq. (3)
as the following example illustrates.
Example 10 (Unproblematic EC).We modify the CSG
Hide-Run-or-Slip (Fig. 1) such that the matrix game
played at shideisZ′
Uk(shide)below.
Z′
Uk(shide) =throw wait 11
3·Uk(shide) +1
3run
0 Uk(shide) hide
The difference is that Z′
Uk(shide)(run,throw ) = 1 and
Z′
Uk(shide)(hide,throw ) = 0 , switching the values as compared
to the original CSG (see Eq. (4)). Here, both bounds converge
to 0.5 despite the presence of the EC{shide}, as shown in
Table II. △
B. Towards Characterizing Bloated End Components
Intuition: In Ex. 10, the best strategy of Player Rleaves
theECalmost surely against all counter-strategies of Player S,
and hence BVI converges. In contrast, in Ex. 6, the best strategy
of Player Ris one where Player Shas a counter-strategy
that forces the play to stay inside the EC; this causes non-
convergence. Generalizing these ideas, we see that a problem
occurs if Player Rhas a strategy that is locally optimal but
non-leaving, i.e. Player Shas a counter-strategy that keeps
the play inside an EC.Outline: We formalize these ideas in the following
definitions: First, Def. 12 formalizes optimal (local) strategies
using weakly dominant strategies in matrix games, extending
the standard definition (e.g. [35]) to sets of strategies. This
extension is not straightforward, and there are several technical
intricacies that we comment on. Next, Def. 15 captures
leaving and staying strategies. We differentiate strategies that
are leaving (irrespective of the opponent’s strategy), staying
(irrespective of the opponent’s strategy), and non-leaving
(where there exists an opponent’s strategy that leads to staying)
with respect to a given set of states. Based on this, we formally
describe hazardous strategies in Def. 16, which are (locally)
optimal strategies of Player Rthat are non-leaving; additionally,
to be problematic for convergence, they are better than all
leaving strategies. Using these, we can precisely characterize
the Bloated End Components ( BECs , Def. 18) that cause non-
convergence.
Remark 11 (Additional Challenges Compared to TSGs ).The
core problem is the same as in TSGs : Player Ris under the
illusion that staying inside an ECyields a better valuation
than leaving. However, in TSGs , the definitions of optimality
and leaving are straightforward, since every state belongs to a
single player and pure strategies are optimal; the definitions
of hazardous and trapping strategies are not even necessary. In
contrast, the definitions in CSGs are technically involved, as
we have to take into account the interaction of the players and
the possibility of optimal mixed strategies. In particular, [19]
defined a straightforward extension of leaving based only on
actions, not strategies. This is incorrect, as we demonstrate in
App. D-B.
Definition 12 (Dominating Sets of Strategies) .Letυbe a
valuation, s∈Sa state, R1,R2,R′⊆ R(s)andS1,S2,S′⊆
S(s)sets of local strategies. We now define two notions of
domination for sets of strategies, namely weak domination and
being not worse . Both of these depend on the player.
Definition for Player R:We write R2≺υ,S′R1to denote that
R1weakly dominates R2under the set of counter-strategies
S′with respect to υ. Formally, ∃ρ1∈ R1.∀ρ2∈ R2:
(i)infσ∈S′B(υ)(s,ρ2, σ)≤infσ∈S′B(υ)(s,ρ1, σ), and
(ii)∃σ′∈ S′such that B(υ)(s,ρ2, σ′)<B(υ)(s,ρ1, σ′).
If only Condition (i) is satisfied, we write R2⪯υ,S′R1to
denote that the set R1isnot worse thanR2under S′with
respect to υ.
Definition for Player S:Dually, we write S2≺υ,R′S1to
denote that set S1weakly dominates S2underR′with respect
toυ. Formally, ∃σ1∈ S1.∀σ2∈ S2:
(i)supρ∈R′B(υ)(s,ρ, σ2)≥supρ∈R′B(υ)(s,ρ, σ1), and
(ii)∃ρ′∈ R′such that B(υ)(s,ρ′, σ2)>B(υ)(s,ρ′, σ1).
If only Condition (i) is satisfied, we write S2⪯υ,R′S1to
denote that the set S1isnot worse thanS2under R′with
respect to υ.
Example 13 (Dominating Sets of Strategies) .Consider the
matrix game defined in Eq. (4) and the valuation Uk(shide) =
6Uk(shome) = 1 andUk(swet) = 0 . Here, for Player R, the pure
strategy {hide7→1}dominates the pure strategy {run7→1}:
{run7→1}≺ Uk,S(shide){hide7→1}.
This is because when Player Sthrows the ball, hiding yields
1 while running yields 0. Note that this is in fact independent
of the valuation, so it also holds for VR.
LetRunPositive :={(run7→ε,hide7→1−ε)|ε >0}be
the set of all strategies that put positive probability on running.
We have
RunPositive ≺Uk,S(shide){hide7→1}.
Again, this is true even when using VRas valuation. Note
that we have this weak dominance even though the supre-
mum over the set yields the optimum valuation, namely
supρ∈RunPositive infσ∈S(shide)B(Uk)(s,ρ, σ) = 1 . This exem-
plifies the strictness of our notion of dominance. It is crucial
that our notion of domination can distinguish these sets of
strategies: The set RunPostive contains all strategies that leave
theEC. However, none of them is optimal (even though the
supremum over all of them is), which is exactly the reason
why VI chooses the staying strategy {hide7→1}for updating
the valuation, and thus is stuck. △
We remark on several technicalities of Def. 12:
•“Weak” dominance: The term “weak” might be misleading.
We choose to use the word for consistency with [35, Def.
4.12]. There, weak domination concerns Condition (ii),
only requiring that there exists a counter-strategy where the
inequality is strict; strict domination requires Condition (ii)
for all counter-strategies. One might be tempted to use
weak domination to denote what we call “not worse”, i.e.
only require that there is a strategy in the first set that has
optimal valuation at least as good as all in the other set;
or to think it only refers to the numerical comparators, e.g.
≥and>(as is sometimes the case when only comparing
single strategies).
•Set-related challenges: The definition is challenging since
we cannot speak about actions, but have to consider sets of
— possible mixed — strategies. The exact quantification
of the strategies is relevant. Further, it depends not only
on the two sets we are comparing, but also on the counter-
strategies of the opponent. Thus, we provide the definition
explicitly for both players, to avoid confusion that could
arise from just saying that they are analogous.
•All-quantification instead of optima: The definition uses
all-quantification instead of optima. Concretely, weak
dominance for Player Ruses∀ρ2∈ R2instead of
writing supρ2∈R2. The latter definition cannot sufficiently
distinguish sets of strategies, since the supremum of a
set need not be contained in it, as we exemplified in
Ex. 13. This fact is extremely important, as in the proof
of Thm. 21, we pick the maximum from a set of strategies,
and the existence of this maximum is guaranteed only
because of the correct definition of dominance.
•Locally optimal strategies are not worse than any other:
Formally, this claim is that for all locally optimal strategyρ∈ R(s)with respect to υ, we have R(s)⪯υ,S(s){ρ7→
1}(and dually for Player S). This is immediate from
Def. 12, since a locally optimal strategy maximizes
infσ∈S′B(υ)(s,ρ, σ), and thus satisfies Condition (i)
when compared to all other strategies. We will use this
fact throughout the paper.
•Notation: When the valuation is clear from the context,
we omit it for the sake of readability. Further, if we say
that a strategy ρ1weakly dominates another strategy ρ2
with respect to a counter-strategy σ, then we mean that
{ρ2}≺{σ}{ρ1}.
We prove a lemma about the relation of weak domination
and not being worse that is useful and instructive. The proof
in App. C-B works by straightforward unfolding of definitions
and rewriting.
Lemma 14 (Negating Weak Domination — Proof in App. C-B).
Letυbe a valuation, s∈Sa state, R1,R2,R′⊆ R(s)and
S1,S2,S′⊆ S(s)sets of local strategies.
If for some sets of strategies we do nothaveR2≺υ,S′R1,
then we have R1⪯υ,S′R2. Analogously, not S2≺υ,R′S1
implies S1⪯υ,R′S2.
To complete our intuitive understanding of the definition
of domination, we point out a connection to the standard
definition of weak domination: “A rational player does not
use a dominated strategy.” [35, Asm. 4.13] If a strategy is
not dominated, by Lem. 14 it is not worse than any other
strategy. This is exactly what we argued above: Locally optimal
strategies are not worse than any other. We often use this fact
throughout the paper.
Next, we formally define leaving and staying strategies .
Given a set of states, a leaving strategy ensures that the set
of successor states contains states outside the given set of
states for all given counter-strategies of the opponent player.
A strategy is staying if all successor states belong to the given
set of states for all given counter-strategies of the opponent
player. Note that a strategy can be neither leaving nor staying,
if the set is exited for some, but not all counter-strategies of
the opponent.
Definition 15 (Leaving and Staying Strategies) .Consider a
set of states X ⊆ Sand a state s∈ X . LetR′⊆ R(s)
andS′⊆ S(s)be sets of strategies of Player RandS,
respectively. The set of (local) leaving strategies for Player R
with respect to S′, is given by
RL(S′,X, s):={ρ∈ R(s)| ∀σ∈ S′.(s,ρ, σ)leaves X },
and for Player Swith respect to R′by
SL(R′,X, s):={σ∈ S(s)| ∀ρ∈ R′.(s,ρ, σ)leaves X }.
A strategy that is not leaving is called non-leaving . The set of
all non-leaving Player Rstrategies is denoted by RL(S′,X, s)
(orSLfor Player S).
7In contrast, the set of staying strategies at a state s∈ X for
PlayerRwith respect to S′, is given by
RS(S′,X, s):={ρ∈ R(s)| ∀σ∈ S′.(s,ρ, σ)staysIn X },
and for Player Swith respect to R′by
SS(R′,X, s):={σ∈ S(s)| ∀ρ∈ R′.(s,ρ, σ)staysIn X }.
Notation: If we consider leaving (or staying) strategies
with respect to all counter-strategies, then we omit the
set of counter-strategies, i.e. instead of RL(S(s),X, s)(or
RS(S(s),X, s)) we write RL(X, s)(orRS(X, s)). We use
the same shorthand notion for leaving (staying) strategies of
PlayerS.
We often speak about a leaving/staying pair of local strate-
gies, so we provide the following shorthand notations: For a tu-
ple(s,ρ, σ)∈S×R×S , we say that (s,ρ, σ)leaves Xif and
only if Post(s,ρ, σ)∩(S\ X)̸=∅. Analogously, we say that
(s,ρ, σ)staysIn Xif and only if Post(s,ρ, σ)∩(S\ X) =∅
(or, equivalently, Post(s,ρ, σ)⊆ X ).
Intuition of Hazardous Strategies: Using the definitions
of dominance and leaving or staying, we can now classify
strategies of Player Rthat can lead to non-convergence.
Intuitively, a hazardous strategy is one that Player Rchooses,
even though it can be staying for some counter-strategies.
Thus, such a strategy (i) is non-leaving (i.e. there exist counter-
strategies that make it staying), and (ii) it is not worse than
any other strategy so that Player Rmay choose it. Moreover,
to be problematic for convergence, (iii) the strategy weakly
dominates all leaving strategies, i.e. leaving strategies are not
chosen for the update.
Definition 16 (Hazardous Strategy) .LetX ⊆ S\(T∪WS)be
as set of states, υa valuation, and s∈ X. A strategy ρ∈ R(s)
is called hazardous with respect to υif it satisfies:
(i)ρ∈ RL(X, s),
(ii)R(s)\ {ρ}⪯S(s){ρ}, and
(iii)RL(X, s)≺S(s){ρ}.
Hazard υ(X, s)denotes the set of all hazardous strategies at
state swith respect to a set of states Xand a valuation υ.
We mention a corner case: In a state where Player R
possesses no leaving strategies, all optimal strategies are
hazardous (note in particular that Condition (iii) is trivially
satisfied, since the dominated set of strategies RL(X, s)is
empty).
Example 17 (Hazardous strategy) .Consider again the matrix
game defined in Eq. (4) and the initial valuation U0(shide) =
U0(shome) = 1 andU0(swet) = 0 . The strategy ρ′:={hide7→
1}is hazardous because: (i) It is non-leaving. (ii) It is an
optimal strategy, i.e. it is not worse than any other strategy.
(iii) It weakly dominates the set of all leaving strategies, see
Ex. 13. △
Definition 18 (Bloated End Component ( BEC )).AnEC
X ⊆ S\(T∪WS)is called bloated end component (BEC)
with respect to a valuation υif for all s∈ X it holds that
Hazard υ(X, s)̸=∅.Example 19 (Bloated End Component) .Consider the CSG
Hide-Run-or-Slip from Fig. 1. As discussed in Ex. 17,
there exists a hazardous strategy in state shide. Moreover, {shide}
is an EC, since under the pair of strategies that plays hide and
wait, the play stays in it. Consequently, {shide}is aBEC and
therefore VI does not converge in this state, see Ex. 6. △
We provide a lemma that captures the intuition of what it
means to (not) be a BEC , and that is also useful in several
proofs:
Lemma 20 (Negating Bloated — Proof in App. C-B ).If an
ECX ⊆ S\(T∪WS)is not bloated for a valuation υ, then
there exists a state s∈ X that has a locally optimal strategy
that is leaving, formally ∃ρ∈ RL(X, s).R(s)⪯υ,S(s){ρ}.
C. Convergence in the Absence of BECs
Now we can prove that BECs indeed are the reason that VI
does not converge for over-approximations.
Theorem 21 (Non-convergence implies BECs — Proof in
App. C-B ).LetU⋆:= lim k→∞Ukbe the limit of the na ¨ıve
upper bound iteration (Eq. (3)) on the CSG G. If VI from above
does not converge to the value in the limit, i.e. U⋆>VR, then
theCSG Gcontains a BEC inS\(T∪WS)with respect to
U⋆.
Proof sketch. This proof builds on the proof of Thm. 9. There,
we constructed a set Xmaximizing the difference between U⋆
andVRand showed that if there is a pair of optimal strategies
leaving X, then we can derive a contradiction: The upper bound
decreases, which contradicts the fact that it is a fixpoint. In the
context of the other proof, that allowed us to show that without
ECs, VI converges, because without ECs it is impossible to
have a set of states where all optimal strategies stay in that
set.
In the presence of ECs, states can indeed have a positive
difference between U⋆andVR, see e.g. Ex. 6. Our goal is
to prove that at least one of these ECs is bloated. Thus, we
assume for contradiction that no ECis bloated under U⋆. Then,
by Lem. 20, there is an optimal leaving strategy for Player
R. Using that, we can repeat the argument from Thm. 9,
showing that in this case U⋆would decrease. Again, this is
a contradiction because it is a fixpoint of applying Bellman
updates (Lem. 48). Thus, the initial assumption that no EC
is bloated is false, and we can conclude that there exists a
BEC.
Remark 22 (Relation to [18]) .Def. 18 of BEC is more general
than the definition of BEC for TSGs in [18, Definition 4]. The
differences are that in [18], an ECis only called bloated if
it is bloated with respect to VR, whereas we extended that
definition to speak about a concrete valuation, similar to [29,
Def. 3]. Further, the definition for TSGs speaks about the best
exit value, which is the optimum among the available actions;
in contrast, in CSGs the definition of best exit is technically
involved and dependent on hazardous strategies, see Def. 27.
Thus, our definition of BEC does not analyze the exit values,
but instead uses a fundamental analysis of the strategies.
8Key Contribution: The key novelty of this section is
the correct definition of BEC that captures when VI from
above does not converge. We highlight that this definition
contains many technical intricacies: Lifting the notion of an
exiting action [18, Section 2.2] from TSGs toCSGs requires
considering sets of local strategies that leave against all
opponent-strategies (Def. 15), and considering the additional
complication that strategies can be neither leaving nor staying.
Further, the exact definition of dominance is very important,
as the supremum over all leaving strategies can be a staying
strategy, see Ex. 13 and the related discussion in the item
“All-quantification instead of optima” after the example.
V. R ESOLVING BLOATED ENDCOMPONENTS
A. Solution in TSGs
We have identified BECs as the cause of non-convergence.
Our method for fixing the over-approximation is again based on
the ideas for TSGs . We explain the intuition of their solution:
Firstly, staying actions yield the valuation that is bloated; thus,
we need an additional update of the over-approximation that
depends only on leaving actions. The valuation to which we
reduce the over-approximation is the best exit from the EC,
which in TSGs simply is the leaving action attaining the highest
value over all states of Player R[18, Definition 3]. Secondly,
not all states in an ECneed to have the same value, since Player
Scan prevent Player Rfrom reaching the state that attains
the best exit valuation. Hence, an ECis decomposed into parts
that share the same value, called simple ECs (SECs) in [18,
Definition 5]. Repeatedly finding these SECs and deflating their
valuation by setting it to the best exit from the SEC suffices
for convergence in TSGs.
When generalizing these ideas to CSGs , we encounter the
following problems: Firstly, the definition of best exit is more
involved, since staying and leaving depends not only on actions,
but on strategies. Additionally, the supremum over all leaving
strategies can be a non-leaving strategy, see Ex. 13. (This is
also the reason why globally optimal strategies need not exist
inCSGs , see Ex. 2). This was the fundamental reason why
the solution proposed in [19] did not work, as it was based on
actions. Secondly, we need to decompose the ECinto parts.
For this, we use a recursive approach, removing states that
have been successfully deflated and checking whether there
are further problematic states in the remainder of the EC.
Outline: In Sec. V-B, we develop a strategy-based
definition of best exit (Def. 28), which relies on identifying
thetrapping strategies (Def. 24) that Player Suses to keep
the play inside the BEC ; and the deflating strategies (Def. 25),
the best response of Player R, namely the leaving strategies
that should be played with arbitrarily small probability ε. In
Sec. V-C, we provide the FIND MBEC algorithm that finds
all maximal BECs that are present in a given MEC . A maximal
BEC is a set of states X ⊆ S\(T∪WS)that is a BEC and
there exists no another set of states X′⊆S\(T∪WS)that
is a BEC andX⊊X′. Finally, Sec. V-D provides the full
deflating procedure for CSGs in Alg. 3. In particular, it usesa recursive call to decompose a given MEC and deflate all
relevant parts of it.
B. Defining the Best Exit
The key problem of a BEC is that in all states, all leaving
strategies of Player Rare dominated by hazardous strategies.
PlayerScan play a trapping strategy and thereby make the
Bellman update self-dependent. If the current valuation is
too high, this prevents convergence. Intuitively, the “pressure”
inside the BEC is too high, and we want to “deflate” it, by
adjusting it to the pressure, i.e. valuation, outside of the BEC .
Since non-trivial BECs neither contain target states nor belong
to the winning region of Player S, there has to exist a state in
aBEC where the supremum over leaving and staying strategies
is equal. To “equalize the pressure” between the BEC and the
rest of the states, we need to estimate the pressure outside
theBEC . To do so, we estimate the valuation attainable upon
leaving the BEC at every state of the BEC , called exit value of
the state. The best exit value is the maximum of all exit values.
Reducing the upper bounds of the states inside the BEC to
the best exit value, in case the best exit is smaller than the
current valuation, “decreases the pressure”. However, since the
valuations of the exiting strategies can depend on the valuations
of the states that belong to the BEC , this procedure may still
only converge in the limit, already in TSGs . We provide an
illustrative example:
Example 23 (Deflating BECs ).Consider again the CSG
Hide-Run-or-Slip (Fig. 1). Under the initial valuation
(see Eq. (3)) the matrix game played at shideis given by
ZU0(shide) =throw wait 
02
3run
1 1 hide
Due to the hazardous strategy {hide7→1}, the Bellman update
cannot improve the initial upper bound of shide, but remains
at 1. However, by the same argument as in Ex. 2, Player R
can use the strategy of running with a probability ε >0that
is arbitrarily small. This yields a value arbitrarily close to2
3.
Consequently, we can deflate, i.e. decrease the upper bound
ofshideto2
3which is the valuation attainable upon leaving
theBEC atshide. After the Bellman update, the matrix game
played at shideis given by
ZU1(shide) =throw wait 05
9run
12
3hide
The strategy {hide7→1}is still hazardous, but we can deflate
the upper bound of shideto5
9. By repeating these steps, the
upper bound converges to1
2in the limit, and only in the limit,
similar to the lower bound in Table I. △
How can we find the best exit value in general?: In
Ex. 23, we used an argument about playing a leaving action
with vanishingly small probability in order to figure out which
entry in the matrix we choose for deflating. We provide an
9alternative intuition: Player Rplays a hazardous strategy most
of the time. The best response of Player Sis to play a weakly
dominant strategy that stays in the EC, trapping the play. Thus,
we can ignore the other strategies of Player Sand consider
only the columns of the matrix that correspond to trapping
strategies . Now, we need to select a leaving strategy of Player
Rwhich then is played with vanishingly low probability. Thus,
we restrict the matrix further to use only rows corresponding to
leaving strategies. In the example, we end up with the top right
entry. We now formalize how we can construct this sub-matrix,
called the exiting sub-game.
Definition 24 (Trapping Strategy) .LetX ⊆ S\(T∪WS)be
as set of states, υa valuation, and s∈ X. A strategy σ∈ S(s)
is called trapping strategy if two conditions are satisfied:
(i)σ∈arg minσ′∈S(s)maxρ∈R(s)B(υ)(s,ρ, σ′), and
(ii)∀ρ∈Hazard υ(X, s) : (s,ρ, σ)staysIn X.
Trapυ(X, s)denotes the set of all trapping strategies at state
swith respect to a set of states Xand a valuation υ.
Definition 25 (Deflating Strategies) .LetX ⊆ S\(T∪WS)
be a set of states. A Player Rstrategy ρ∈ R(s)is called
deflating if two conditions are satisfied:
(i)∃σ∈Trapυ(X, s)such that (s,ρ, σ)leaves X, and
(ii)Supp(ρ)∩S
ρ′∈Hazard υ(X,s)Supp(ρ′) =∅.
Deflυ(X, s)denotes the set of all deflating strategies at state
swith respect to a set of states Xand a valuation υ.
Definition 26 (Exiting sub-game) .LetX ⊆ S\(T∪WS)
be a set of states, s∈ X a state, and υa valuation.
Further, let Zυ(s)be the matrix game played at state s∈ X.
IfTrapυ(X, s)̸=∅then, the exiting sub-game played
at state s, denoted by Zexit
υ(s), is the matrix game where
Player Rhas the actions inS
ρ∈Deflυ(X,s)Supp(ρ)and
PlayerShas the actions inS
σ∈Trapυ(X,s)Supp(σ). The
value of the exiting sub-game is given by V(Zexit
υ(s)):=
max(0,supρ∈Deflυ(X,s)infσ∈Trapυ(X,s)B(υ)(s,ρ, σ)).
We explain how this exiting sub-game and its value are well-
defined: The set of deflating strategies can be empty, namely in
a state which has no leaving strategies. In this case, the value
of the exiting sub-game is the supremum over an empty set,
i.e. the smallest possible value, commonly minus infinity and
0 in our case. We highlight this possibility by explicitly taking
the maximum of 0 and the value of the exiting subgame when
we compute it. If the set of deflating strategies is non-empty,
then the set of trapping strategies necessarily is non-empty, too,
since the Item (i) of Def. 25 requires existence of a trapping
strategy.
Definition 27 (Exit value) .LetX ⊆ S\(T∪WS)be a set
of states, s∈ X a state, and υan over-approximation. Then,theexit value fromXattainable at state sis given by
exitValυ(X, s):=


sup
ρ∈R(s)inf
σ∈S(s)B(υ)(s,ρ, σ),ifHazard υ(X, s) =∅
∨Trapυ(X, s) =∅;
V(Zexit
υ(s)), else.
ForBECs that consists of more than one state, the exit
values attainable at different states within the BEC may differ.
Consequently, to ensure that deflating does not reduce the
value of any of the states in the BEC below its actual value
we estimate the exit values at each state of the BEC , and
finally select the maximal exit value, i.e. the best exit value,
for deflating.
Definition 28 (Best Exit) .LetX ⊆ (S\(T∪WS))be a set
of states and υa valuation. The best exit value with respect
to a set Xand a valuation υis given by
bestExitValυ(X) := max
s∈XexitValυ(X, s).
The best exit , denoted by bestExitυ(X), is a state obtaining
bestExitValυ(X), and the set of all best exits is denoted by
bestExitsυ(X).
Lemma 29 (bestExitVal is sound) .LetX ⊆ S\(T∪WS)be
anEC, and U∈[0,1]|S|be a valuation with U≥VR. Then,
for all states s∈ X, we have bestExitValU(X)≥VR(s).
Proof. This proof relies on the technical Lem. 49 stating that
(i)X′:={s∈ X | VR(s)≤exitValVR(X, s)} ̸=∅, and
(ii)maxs∈X′VR(s)≥maxs∈X \X′VR(s).
LetX′⊆ X be a set of states satisfying Condition (i). Further,
choose e∈arg maxt∈X′VR(t)as one of the exits from X′.
By Item (ii) of Lem. 49 , we have that for all s∈ X:VR(s)≤
VR(e). It remains to show bestExitValU(X)≥VR(e). We
conclude as follows:
VR(e)≤exitValVR(X, e) (Since e∈ X′)
=bestExitValVR(X) (By the choice of e)
≤exitValU(X, e) (By Lem. 50)
≤bestExitValU(X). (By Def. 28)
We want to highlight that the statement of Lem. 50 is non-
trivial and the proof is technically involved.
C. Finding Maximal BECs
Since a BEC might contain other BECs , we want to find
maximal BECs . ABECXis maximal if there exists no BEC
X′such that X⊊X′. The existence of maximal BECs is
proven in App. C-C.
Given a CSG G, aMEC Cand the current upper bound
estimate U, Alg. 2 finds all maximal BECs within Cas follows:
The set Bcontains all states for which hazardous strategies
exist with respect to the set C. In case Bis non-empty, it might
consist of multiple disjoint MECs . Therefore, the algorithm
calls the FIND MECs procedure on Band returns all MECs
10Algorithm 2 Algorithm for finding maximal BECs.
1:Algorithm FIND MBECs (CSG G,MEC C, upper bound
estimate U)
2: B:={s∈C|Hazard U(C, s)̸=∅}
3: ifB̸=∅then ▷Ccontains BECs
4: return FIND MECs (G, B)
5: else
6: return ∅ ▷There exists no BEC in C
Algorithm 3 Algorithm for deflating BECs.
1:Algorithm DEFLATE (CSG G, upper bound estimate U,
MEC C)
2: forX ∈ FIND MBECs (G,C,U)do
3: u←bestExitValU(X)
4: fors∈ Xdo
5: U(s)←min(U(s),u)
6: forE ∈FIND MECs (X \bestExitsU(X))do
7: U←DEFLATE (G,U,E)
▷Recursively deflate sub-BECs
8: return U
that are in B. Otherwise, Bis empty, which means that C
does not contain any BECs, and the empty set is returned.
Lemma 30 (FIND MBECs is correct— Proof in App. C-C).
For a CSG, aMEC Cand a valid upper bound U, it holds
thatX ∈ FIND MBECs (G,C,U)if and only if Xis aBEC
inCand there exists no T⊆Cthat is a BEC and X⊊T.
Proof sketch. To prove the “ ⇒” direction, we assume towards
a contradiction that Xis a maximal ECbut not a BEC .
Then, all Player Rstrategies must violate at least one of
the conditions posed by Def. 16. We consider each condition
separately and derive a contradiction to the assumption that
X ∈ FIND MBECs (G,C,U).
To prove the opposite direction, i.e., “ ⇐”, we assume
towards a contradiction that Xis a maximal BEC but
X/∈FIND MBECs (G,C,U). Here, we again make a case
distinction. In the first case, some state s∈ X was removed
by the algorithm because at least one player has an optimal
strategy that can leave X. However, this is a contradiction to
the assumption that Xis aBEC . In the second case, we assume
towards a contradiction that X/∈FIND MECs (G, B)(where
Bis defined as in FIND MBECs ). This is then a contradiction
to the assumption that Xis a maximal EC.
D. Convergent Bounded Value Iteration for CSGs and the
Recursive Structure of ECs
Finally, Alg. 3 describes our main goal: the deflating
procedure for BECs , to be plugged into Alg. 1. The algorithm
takes a CSG G, the current upper bound estimate U, and a MEC
Cas input. First, the algorithm searches for all maximal BECs
that might be contained in the MEC . The current upper bound
estimate is returned if no BEC exists in the MEC . Otherwise,
at least one BEC exists and must be deflated. If multiple BECsare found, each maximal BEC is deflated separately (see the
for-loop in Line 2).
To deflate a maximal BECX, first, the best exit value
bestExitValU(X)is estimated. Next, each state of the BEC is
considered and if the best exit value is smaller than the current
upper bound estimate at that state, then it is reduced to the
best exit value (as nothing better can be reached).
However, notice that within a BECXthere might exist
another BECX′. From X′, Player Rmight not be able to
get to the best exit of X(the globally best one in X) but
only to a worse one, locally optimal for X′. Hence, for such a
sub-BECX′, more aggressive deflating to bestExitValU(X′)
is due. In other words, after deflating to bestExitValU(X)we
have handled all states where Rcan ensure reaching this best
exit, and we can ignore these states for the moment; we can
also ignore this best exit and have a fresh look at which states
arenow in a BEC and can reach the second best exit, i.e., the
best exit in this remainder. Subsequently, we continue with the
third best option etc.
Consequently, on Lines 6-7, DEFLATE is called recursively
on all MECs that are contained in X \bestExitsU(X), i.e. after
removing all best exits. The procedure is repeated independently
for each maximal BEC contained in C. A full example showing
howDEFLATE works on a more complex BEC is included in
App. B-B.
Remark 31 (Structure of ECs) .This elucidates the hierarchical
structure of ECs and their corresponding best exits. The
recursive call of DEFLATE exposes the partial order over
theECs, their sub- ECs, and “internally” transient states (those
not within sub-ECs after removing the best exit since they
are bound to the just removed exit or another sub- ECthat is
aBEC with a lower value). This hierarchy captures (i) the
ordering of exits by their values and (ii) the “independence”
of exits with possibly different values when visiting one from
another cannot be ensured.
Our goal for the remainder of this work is to show that Alg. 1
with deflation is correct and converges, i.e. that complementing
Bellman updates Bwith deflation results in a sequence of
upper bounds Uthat converges to the value from above. For
the sake of simplicity, we denote by D: [0,1]|S|→[0,1]|S|
the operator that performs DEFLATE on a given valuation
for all MECs in the CSG (which is reasonable since Alg. 1
performs DEFLATE on all MECs (in an arbitrary but fixed
ordering)).
Remark 32 (Valid upper bonds) .In the following, whenever
we quantify an upper bound (a.k.a. over-approximation), we
require it to be valid ; meaning that it was obtained by iteratively
applying deflating and the Bellman update on the initial over-
approximation U0from Eq. (3), i.e. U= (D◦B)k(U0)
for some k∈N. The reason for this restriction is that for
convergence, we require Dto be order-preserving. While D
is order-preserving on valid upper bounds (see Lem. 36), in
general it is not monotonic for arbitrary U∈R|S|. We illustrate
this in App. B-C.
11Definition 33 (Valid upper bound) .An upper bound U∈
[0,1]|S|is called valid if there exists k∈Nsuch that U=
(D◦B)k(U0), or if U=VR.
We proceed as follows: After proving fundamental properties
of both operators BandDin Lem. 34, we use these properties
to show that valid upper bounds are indeed upper bounds, i.e.
they are always greater or equal than the value in Lem. 35. With
correctness established, Lem. 36 shows that on valid upper
bounds, (D◦B)is order-preserving, which is a necessary
ingredient for convergence. Finally, Thm. 37 concludes by
proving soundness and completeness of the full algorithm.
Lemma 34 (Properties of DandB).Letυ∈[0,1]|S|be a
valuation. If υ≥VR, then:
(i)B(υ)≤υandD(υ)≤υ.
(ii)B(υ)≥VRandD(υ)≥VR.
Proof. For the Bellman operator, both properties follow from
the fact that the value VRis the least fixpoint of the Bellman
operator, see [17, Thm. 1]. Thus, given a valuation greater than
the value, it cannot increase, and it cannot become smaller
than the value.
ForD, observe that deflation only updates the valuation
in Line 5 of Alg. 3 when setting the valuation of a state to
min(U(s),bestExitValU(X))for some EC X. Item (i) holds
because of taking the minimum with the current valuation, so it
can only decrease. For Item (ii), we show in Lem. 29 that for
every ECXand state s∈ X, we have that bestExitValU(X)≥
VR(s). This proves our goal, as the only update of DEFLATE
keeps the valuation greater than VRin every state. The proof
of Lem. 29 is technically involved, having to unfold many
definitions in order to show the following intuitive fact: No
state can have a larger value than that of some exit from the
EC it is contained in.
Lemma 35 (Soundness of valid upper bounds) .For all k∈N
it holds that (D◦B)k(U0)≥VR.
Proof. We proceed by induction over k.
Base case: k= 0, thus (D◦B)0(U0) =U0≥VR.
Induction hypothesis: For all k≥0, we assume that
(D◦B)k(U0)≥VR.
Induction step: To show: (D◦B)k+1(U0)≥VR. We know
by induction hypothesis that (D◦B)k(U0)≥VR.
Applying Item (ii) of Lem. 34 for B, we obtain
B((D◦B)k(U0))≥VR. From this and Item (ii) for
D, we get D(B((D◦B)k(U0)))≥VR, proving our
goal.
Lemma 36 ((D◦B)is order-preserving on valid upper bounds) .
Letυ1,υ2be valid upper bounds with υ1≥υ2. It holds that
(D◦B)(υ1)≥(D◦B)(υ2).
Proof. We know by Lem. 35 that all valid upper bounds are
greater or equal to the value (including the value itself). Thus,
ifυ2=VR, the statement holds, and if υ1=VR, thenυ2=VRas well, since υ1≥υ2. It remains to consider the
case that both valuations come from repeated application of
the deflation and Bellman operators, i.e. υ1= (D◦B)i(U0)
andυ2= (D◦B)j(U0). We assume υ1̸=υ2, since otherwise
the claim trivially holds.
By Item (i) of Lem. 34, every application of the operators can
only decrease the resulting value; the item remains applicable,
since the resulting valuations are always greater than or equal
to the value. Consequently, i < j , asυ1>υ2. Using this and
applying Item (i) of Lem. 34 again, we conclude by stating
(D◦B)(υ1) =(D◦B)i+1(U0)≥
(D◦B)j+1(U0) = (D◦B)(υ2).
Theorem 37 (Soundness and completeness - Proof in
App. C-D).ForCSGs Alg. 1, using Alg. 3 as DEFLATE , pro-
duces monotonic sequences Lunder- and Uover-approximating
VR, and terminates for every ε >0.
Proof sketch. Soundness and convergence of lower bounds is
classical [17, Thm. 1], and our algorithm does not modify
the computation of under-approximations. The soundness of
the upper bounds is immediate from Lem. 35, since all upper
bounds computed by the algorithm are valid, and thus greater
or equal than the value. Proving the convergence of the upper
bounds is the main challenge. First, in Lem. 52 we provide the
technical proof that the upper bound in Alg. 1 indeed converges
to a fixpoint, using that the operators are order-preserving
(Lem. 36) and arguments from lattice theory. Then, we use the
same idea we have utilized in the proofs of Thms. 9 and 21:
We assume for contradiction that there exists a state where the
difference between the fixpoint of the upper bound in Alg. 1
and the true value is strictly greater than zero. The states with
the largest such difference contain a BEC (by Thm. 21), that
eventually will be found and deflated; since deflation depends
on the outside of the BEC , this decreases the upper bound.
This causes a contradiction to the fact that the upper bound
has converged to a fixpoint. Consequently, there can be no
state with a positive difference, and the upper bounds converge,
too.
VI. C ONCLUSION AND FUTURE WORK
We have introduced a convergent over-approximation for
concurrent stochastic games with reachability and safety
objectives, thus giving value iteration the first sound stopping
criterion and turning it into an anytime algorithm. Since the
games are concurrent and ( ε-)optimal strategies may need to
be randomized, we could not use the technique of simple
end components of [18]. Instead, we identify bloated end
components where the play can get stuck forever and recursively
deflate the over-approximations of these states to the best
possible value attainable upon leaving the end component.
We leave an efficient implementation for future work as an
extension of the standard model checker PRISM -GAMES [32].
12REFERENCES
[1]Pranav Ashok, Krishnendu Chatterjee, Przemysław Daca, Jan K ˇret´ınsk´y,
and Tobias Meggendorfer. Value Iteration for Long-Run Average Reward
in Markov Decision Processes. In Rupak Majumdar and Viktor Kun ˇcak,
editors, Computer Aided Verification . Springer International Publishing,
2017.
[2]Christel Baier and Joost-Pieter Katoen. Principles of Model Checking .
MIT Press, April 2008.
[3]Christel Baier, Joachim Klein, Linda Leuschner, David Parker, and Sascha
Wunderlich. Ensuring the Reliability of Your Model Checker: Interval
Iteration for Markov Decision Processes. In Rupak Majumdar and Viktor
Kunˇcak, editors, Computer Aided Verification . Springer International
Publishing, 2017.
[4]Benjamin Bordais, Patricia Bouyer, and St ´ephane Le Roux. Subgame
Optimal Strategies in Finite Concurrent Games with Prefix-Independent
Objectives. In Orna Kupferman and Pawel Sobocinski, editors, Founda-
tions of Software Science and Computation Structures , pages 541–560.
Springer Nature Switzerland, 2023.
[5]Tom´aˇs Br´azdil, Krishnendu Chatterjee, Martin Chmelik, V ojt ˇech Forejt,
Jan K ˇret´ınsk`y, Marta Kwiatkowska, David Parker, and Mateusz Ujma.
Verification of Markov decision processes using learning algorithms. In
International Symposium on Automated Technology for Verification and
Analysis , pages 98–114. Springer, 2014.
[6]Tom´aˇs Br´azdil, Krishnendu Chatterjee, Martin Chmel ´ık, V ojt ˇech Forejt,
Jan K ˇret´ınsk´y, Marta Kwiatkowska, David Parker, and Mateusz Ujma.
Verification of Markov Decision Processes Using Learning Algorithms. In
Franck Cassez and Jean-Fran c ¸ois Raskin, editors, Automated Technology
for Verification and Analysis . Springer International Publishing, 2014.
[7]Krishnendu Chatterjee, Luca de Alfaro, and Thomas A Henzinger.
Termination criteria for solving concurrent safety and reachability games.
InProceedings of the twentieth annual ACM-SIAM symposium on
Discrete algorithms , pages 197–206. SIAM, 2009.
[8]Krishnendu Chatterjee, Luca De Alfaro, and Thomas A. Henzinger.
Qualitative concurrent parity games. ACM Transactions on Computational
Logic , July 2011.
[9]Krishnendu Chatterjee, Luca de Alfaro, and Thomas A Henzinger.
Strategy improvement for concurrent reachability and safety games.
arXiv preprint arXiv:1201.2834 , 2012.
[10] Krishnendu Chatterjee, Luca de Alfaro, and Thomas A Henzinger. Strat-
egy improvement for concurrent reachability and turn-based stochastic
safety games. Journal of computer and system sciences , 79(5):640–657,
2013.
[11] Krishnendu Chatterjee and Thomas A. Henzinger. Value Iteration. In
Orna Grumberg and Helmut Veith, editors, 25 Years of Model Checking:
History, Achievements, Perspectives . Springer, 2008.
[12] Costas Courcoubetis and Mihalis Yannakakis. The complexity of
probabilistic verification. Journal of the ACM , 42, July 1995.
[13] B. A. Davey and H. A. Priestley. Introduction to Lattices and Order .
Cambridge University Press, Cambridge, 2 edition, 2002.
[14] Luca De Alfaro. How to specify and verify the long-run average
behaviour of probabilistic systems. In Proceedings. Thirteenth Annual
IEEE Symposium on Logic in Computer Science (Cat. No. 98CB36226) ,
pages 454–465. IEEE, 1998.
[15] Luca de Alfaro and Thomas A Henzinger. Concurrent omega-regular
games. In Logic in Computer Science, 2000. Proceedings. 15th Annual
IEEE Symposium on , pages 141–154. IEEE, 2000.
[16] Luca De Alfaro, Thomas A Henzinger, and Orna Kupferman. Concurrent
reachability games. Theoretical Computer Science , 2007.
[17] Luca de Alfaro and Rupak Majumdar. Quantitative solution of omega-
regular games. Journal of Computer and System Sciences , 68(2):374 –
397, 2004. Special Issue on STOC 2001.
[18] Julia Eisentraut, Edon Kelmendi, Jan K ˇret´ınsk´y, and Maximilian
Weininger. Value iteration for simple stochastic games: Stopping criterion
and learning algorithm. Information and Computation , 2022.
[19] Julia Eisentraut, Jan K ˇret´ınsk´y, and Alexej Rotar. Stopping Criteria
for Value and Strategy Iteration on Concurrent Stochastic Reachability
Games, 2019.
[20] Kousha Etessami and Mihalis Yannakakis. Recursive Concurrent
Stochastic Games. Logical Methods in Computer Science , V olume 4,
Issue 4, November 2008.
[21] H. Everett. RECURSIVE GAMES , pages 47–78. Princeton University
Press, 1957.[22] Søren Kristoffer Stiil Frederiksen and Peter Bro Miltersen. Approximating
the Value of a Concurrent Reachability Game in the Polynomial Time
Hierarchy. Springer, 2013.
[23] Serge Haddad and Benjamin Monmege. Interval iteration algorithm for
mdps and imdps. Theoretical Computer Science , 735:111–131, 2018.
[24] Serge Haddad and Benjamin Monmege. Interval Iteration Algorithm for
MDPs and IMDPs. Theoretical Computer Science , 2018.
[25] Kristoffer Arnsfelt Hansen, Rasmus Ibsen-Jensen, and Peter Bro Mil-
tersen. The Complexity of Solving Reachability Games Using Value and
Strategy Iteration. Theory of Computing Systems , 55, 2014.
[26] Kristoffer Arnsfelt Hansen, Michal Koucky, Niels Lauritzen, Peter Bro
Miltersen, and Elias P. Tsigaridas. Exact algorithms for solving stochastic
games: Extended abstract. In Proceedings of the Forty-Third Annual
ACM Symposium on Theory of Computing , 2011.
[27] Arnd Hartmanns, Sebastian Junges, Tim Quatmann, and Maximilian
Weininger. A Practitioner’s Guide to MDP Model Checking Algorithms.
In Sriram Sankaranarayanan and Natasha Sharygina, editors, Tools and
Algorithms for the Construction and Analysis of Systems . Springer Nature
Switzerland, 2023.
[28] Frederick S. Hillier and Gerald J. Lieberman. Introduction to Operations
Research . McGraw-Hill Higher Education, 2010.
[29] Jan K ˇret´ınsk´y, Tobias Meggendorfer, and Maximilian Weininger. Stop-
ping Criteria for Value Iteration on Stochastic Games with Quantitative
Objectives. In 2023 38th Annual ACM/IEEE Symposium on Logic in
Computer Science (LICS) , 2023.
[30] Jan K ˇret´ınsk´y, Emanuel Ramneantu, Alexander Slivinskiy, and Maximil-
ian Weininger. Comparison of algorithms for simple stochastic games.
Information and Computation , 2022.
[31] P. R. Kumar and T. H. Shiau. Existence of Value and Randomized
Strategies in Zero-Sum Discrete-Time Stochastic Dynamic Games. SIAM
Journal on Control and Optimization , 19, 1981.
[32] Marta Kwiatkowska, Gethin Norman, David Parker, and Gabriel Santos.
Automated verification of concurrent stochastic games. In International
Conference on Quantitative Evaluation of Systems , pages 223–239.
Springer, 2018.
[33] Marta Kwiatkowska, Gethin Norman, David Parker, and Gabriel Santos.
PRISM-games 3.0: Stochastic Game Verification with Concurrency,
Equilibria and Time. In Shuvendu K. Lahiri and Chao Wang, editors,
Computer Aided Verification . Springer International Publishing, 2020.
[34] Donald A. Martin. The determinacy of blackwell games. 1998.
[35] Michael Maschler, Shmuel Zamir, and Eilon Solan. Game Theory .
Cambridge University Press, June 2020.
[36] John Nash. Non-Cooperative Games. 1951.
[37] John F. Nash. Equilibrium points in n-person games. 1950.
[38] Miquel Oliu-Barton. New algorithms for solving zero-sum stochastic
games. Mathematics of Operations Research , 46(1):255–267, 2021.
[39] T. Parthasarathy. Discounted, positive, and noncooperative stochastic
games. International Journal of Game Theory , 2(1):25–37, Dec 1973.
[40] Grant Olney Passmore and Paul B. Jackson. Combined decision
techniques for the existential theory of the reals. In Calculemus/MKM ,
volume 5625 of Lecture Notes in Computer Science , pages 122–137.
Springer, 2009.
[41] Martin L. Puterman. Markov Decision Processes: Discrete Stochastic
Dynamic Programming . Wiley, 2009.
[42] T. E. S. Raghavan and J. A. Filar. Algorithms for stochastic games —
A survey. Zeitschrift f ¨ur Operations Research , 35(6):437–472, 1991.
[43] G. H. R. Santos. Automatic Verification and Strategy Synthesis for Zero-
Sum and Equilibria Properties of Concurrent Stochastic Games . PhD
thesis, University of Oxford, 2020.
[44] Dana Scott. Outline of a Mathematical Theory of Computation.
Kiberneticheskij Sbornik. Novaya Seriya , 14, January 1977.
[45] Stephen Simons. Minimax Theorems and Their Proofs. In Ding-Zhu
Du and Panos M. Pardalos, editors, Minimax and Applications . Springer
US, 1995.
[46] Anton Wijs, Joost-Pieter Katoen, and Dragan Bo ˇsnaˇcki. Efficient GPU
algorithms for parallel decomposition of graphs into strongly connected
and maximal end components. Formal Methods in System Design , 48(3),
June 2016.
13APPENDIX A
FURTHER DEFINITIONS AND CONCEPTS
A. Solving Matrix Games
Given a valuation υfor all states in a two-player CSG , we can update the valuation at state sby solving a Linear Program
(LP). Let the matrix game played at sbe given by a matrix Z∈Ql×m, where l(resp. m) is the number of actions available
to player R(resp.S) at the state s. Then the LP that yields the value is the following [43]: Maximize υ(s)subject to the
constraints:
υ(s)≤x1·z1j+···+xl·zljfor1≤j≤m
xi≥0for1≤i≤l
1 =x1+···+xl
where zij=Z(s)(i, j) =P
s′∈Sδ 
s, ai, bj 
s′
·υ(s′), and xiis the probability that player Rwill take action i. Thus, by
solving the LP, we not only obtain the value but also the optimal local strategy for player R.
B. Domination of Strategies
Definition 38 (Domination [35, Def. 4.12]) .Given a valuation υ, a state s∈Sand sets of strategies R(s)andS(s)available
at the state sfor player RandS, respectively.
-A strategy ρ∈ R(s)isweakly dominated if there exists another strategy ρ′∈ R(s)satisfying the following two conditions:
•infσ∈S(s)B(υ)(s,ρ, σ)≤infσ∈S(s)B(υ)(s,ρ′, σ), and
•∃σ′∈ S(s)such that B(υ)(s,ρ, σ′)<B(υ)(s,ρ′, σ′).
-Dually, a strategy σ∈ S(s)isweakly dominated if there exists another strategy σ′∈ S(s)satisfying the following two
conditions:
•supρ∈R(s)B(υ)(s,ρ, σ)≥supρ∈R(s)B(υ)(s,ρ, σ′), and
•∃ρ′∈ R(s)such that B(υ)(s,ρ′, σ)>B(υ)(s,ρ′, σ′).
APPENDIX B
FURTHER EXAMPLES
A. Best Exit
Example 39 (Deflating BECs) .Consider the CSG depicted in Fig. 2. At each state both players can choose among two actions.
The game contains one MEC C:={s0, s1, s2}. The matrix games played at the states s0, s1ands2with respect to an upper
bound Uk(where k∈N) are defined as follows.
ZUk(s0) :=d1 d2 Uk(s0) Uk(s1) a1
1
2(Uk(s0) +α) α a2, ZUk(s1) =e1 e2 γ β b1
1
2(Uk(s1) +Uk(s2))Uk(s0) b2,
ZUk(s2) =f1 f2 Uk(s0)1
2(Uk(s0) +Uk(s1)) c1
γ Uk(s2) c2.
s0s1
s2α
γβ
a1d2
a1d1
a2d2
a2d1b1e2
b2e2b2e1
b1e1
c2f1c1f1c1f2c2f2
Fig. 2: CSG with non-single-state EC. The clouds represent the irrelevant parts of the game.
14TABLE III: Full example of the BVI algorithm for the CSG depicted in Fig. 2. iis the i-th iteration of the main loop in Alg. 1,
jis the j-th iteration of the while-loop in Alg. 3. Cis the set of states among which a BECXis searched. The best exit from
Xis underlined.
i j Ui(s0)Ui(s1)Ui(s2) C X
00 1 1 1 {s0, s1, s2} { s0, s1, s2}
1 0.9 0.9 0.9 {s0, s1} { s0, s1}
2 0.7 0.7 0.9 {s0} { s0}
3 0.2 0.7 0.9 ∅ ∅
10 0.2 0.7 0.9 {s0, s1, s2} { s0}
1 0.2 0.7 0.9 {s1, s2} { s2}
2 0.2 0.7 0.45 {s1} ∅
s0 s1 α
γβa1d2a1d1
a2d2
a2d1b1e2
b2e2b1e1
Fig. 3: The resulting game after removing s2from the game illustrated in Fig. 2.
Fork= 0,α= 0.2, β= 0.7andγ= 0.9and the initialization U0(s0) =U0(s1) =U0(s2) = 1 , the matrix games look as
follows.
ZU0(s0) =d1 d2 1 1 a1
0.6 0 .2 a2, ZU0(s1) =e1 e2 0.9 0 .7 b1
1 1 b2, ZU0(s2) =f1 f2 1 1 c1
0.9 1 c2.
Since at each state, there exist hazardous and trapping strategies, the three states form a BEC . To estimate the values of each
exit, we need to solve three sub-matrix games played at each state of the BEC . The best exit value is then the maximum of the
three solutions. The three linear programs that solve the three sub-matrix games are the following.
max U1(s0)s.t.
U1(s0)≤0.6·x2
U1(s0)≤0.2·x2
x2= 1max U1(s1)s.t.
U1(s1)≤0.9·x1
U1(s1)≤0.7·x1
x1= 1max U1(s2)s.t.
U1(s2)≤0.9·x2
U1(s2)≤1·x2
x2= 1
Here x1andx2are the probabilities that player Rchooses the first or second action available at the corresponding state.
The best exit is max{0.2,0.7,0.9}= 0.9, therefore, the upper bound of all the three states can be safely reduced to 0.9. △
B. BVI Algorithm - Full Example
Example 40 (BVI) .Consider the CSG depicted in Fig. 2, where α= 0.2, β= 0.7, and γ= 0.9. The matrix games played at
each state are given by the matrices ZUk(s0),ZUk(s1)andZUk(s2)defined as in Example 39. We choose ε= 0.001.
Table III summarizes the steps of the algorithm. Initially it holds that U0(s0) =U0(s1) =U0(s2) = 1 . Since C={s0, s1, s2}
is aMEC , it will be found by Alg. 3. Within CtheBECX=Cis found. The best exit value from Xis calculated, i.e. the
three sub-matrix games are solved and the best exit value needed for deflating the BEC is the maximum among the solutions:
Zexit
U0(s0) :=d1 d2
( )0.6 0 .2 a2Zexit
U0(s1) =e1 e2
( )0.9 0 .7 b1 Zexit
U0(s2) =f1 f2
( )0.9 1 c1
The best exit from Xiss2(underlined in Table III) and the value of the exit is 0.9, so in the next step the over-approximations
of those three states are deflated to 0.9 and the best exit is removed from C. The resulting sub-game is depicted in Fig. 3. For
15s0 αa1d1
a2d2
a2d1
Fig. 4: The resulting game after removing s1from the game illustrated in Fig. 3.
s0 s1βα
γ(□,□)(a,d)(b,e)
(c,e)
(a,e)(b,d)(c,d)
Fig. 5: Monotonicity is not guaranteed in general.
U′(s0) =U′(s1) =U′(s2) = 0 .9, the set C={s0, s1}contains the BECX=C. Now, two linear programs need to be solved
to solve the two sub-matrix games.
Zexit
U′(s0) :=d1 d2
( )0.55 0 .2 a2Zexit
U′(s1) =e1 e2
( )0.9 0 .7 b1
The best exit is s1and the value of the exit 0.7, thus, the upper bounds of s0ands1are reduced to 0.7 and s1is removed
from C. The resulting sub-game is depicted in Fig. 4. Finally, for U′′(s0) = 0 .7, the set C={s0}contains the BECX={s0}.
To deflate it, we need to solve the following exiting sub-game.
Zexit
U′′(s0) :=d1 d2
( )0.45 0 .2 a2
The best exit value is 0.2. The upper upper bound of s0is reduced to 0.2. After removing the state from Xwe obtain an empty
set and the algorithm can proceed with the next MEC . Since, we assumed that the game has only one MEC , the deflating
phase is finished. As U0−L0> εholds, the next iteration of the algorithm is executed.
The Bellman update returns the same valuation for all states, i.e. U1(s0) = 0 .2,U1(s1) = 0 .7andU1(s2) = 0 .9. Alg. 3
again finds the MEC C={s0, s1, s2}that contains the two BECs X1:={s0}andX1:={s2}. First X={s0}is deflated to
0.2 and next the BEC X={s2}is deflated. For this, the following exiting sub-game needs to be solved.
Zexit
U1(s0) :=d2
( )0.45 a2
Notice, that here we only consider staying strategies for player S, which is why we only consider action d2. Therefore, the
best exit value is 0.45. After removing s0ands2fromX, no further BECs are contained in the MEC . Since now U1−L1< ε
holds, the BVI algorithm terminates. △
C. Non-Monotonicity of Deflation
Example 41.Consider the CSG depicted in Fig. 5. The variables α, β andγare placeholders indicating that upon leaving
a certain valuation is obtained. The set of states {s0, s1}is an EC. We consider two valuations, UandU′, such that
U≥U′and the EC{s0, s1}is for both upper bounds a BEC . The upper bound Uassigns the following valuations:
U(s0) = 0 .6,U(s1) = 0 .6,U(α) = 0 .8,U(β) = 0 .5,U(γ= 0.55). The upper bound U′assigns the following valuations:
U′(s0) = 0 .6,U′(s1) = 0 .45,U′(α) = 0 .5,U′(β) = 0 .5,U′(γ= 0.55). Then, the matrix games played for the two valuations at
state s1are given by the following matrices:
ZU(s1):=d e !0.8 0 .6 a
0.6 0 .5 b
0.55 0 .5 c,ZU′(s1):=d e !0.5 0 .45 a
0.6 0 .5 b
0.55 0 .5 c
16Then, under Uthe strategy {a7→1}is a hazardous strategy and the exit value is 0.5. In contrast for U′the strategy {b7→1}is
hazardous and the exit value is 0.55. Thus, for arbitrary U,U′∈R|S|it might happen that exitValU(X)<exitValU′(X)for
some BECXalthough U≥U′holds. Intuitively, this arises because the sub-EC which forms a BEC changes when the relative
ordering of exits is modified. However, the problem cannot occur when considering valid over-approximations (see Lem. 36)
because then the upper bounds decrease in a well-behaved way when the relative ordering of the exits changes. △
APPENDIX C
PROOFS FOR SEC. III
Throughout the whole Appendix C, when proving convergence of BVI, we utilize definitions and theorems from lattice and
fixpoint theory. Thus, we first briefly recall some necessary definitions (adjusting notation to our work) and theorems.
Definition 42 (Ordered set [13, Chapter 1.2]) .A set Pequipped with a relation ⪯:P×Pis called an ordered set if and only
if⪯is reflexive, antisymmetric and transitive.
Definition 43 (Directed set [13, Chapter 7.7]) .LetPbe an ordered set. A non-empty set D⊆Pisdirected if and only if for
every pair of elements x, y∈Dthere exists z∈Dthat is a lower bound for both, formally z⪯xandz⪯y.
Definition 44 (Complete partial order (CPO) [13, Chapter 8.1]) .An ordered set Pis a complete partially ordered set (CPO) if
and only if
(i)Phas a top element, ⊤:=infP∅, and
(ii) for every directed set D⊆P, we have infPDexists.
Definition 45 (Continuity [13, Chapter 8.6]) .LetPandQbe two CPOs. A mapping φ:P→Qiscontinuous if
(i) for every directed set D⊆P, the subset φ(D)ofQis also directed, and
(ii) it holds that φ(infD) =infφ(D) := inf{φ(x)|x∈D}.
Definition 46 (Order-preserving [13, Chapter 1.34]) .LetPandQbe ordered sets. A map φ:P→Qisorder-preserving
(also called monotone) if x⪯yinPimplies φ(x)⪯φ(y)inQ.
Theorem 47 (Fixpoint Theorem [13, Chapter 8.15]) .LetPbe a complete partial order, let Fbe an order-preserving and
continuous self-map on Pand define α:=supn≥0Fn(⊤). Then αis the greatest fixpoint of F, i.e. the largest element of P
satisfying F(α) =α.
We remark that we inverted the definitions and the theorem: This is because we are interested in a greatest fixpoint, whereas
the textbook [13] only speaks about least fixpoints. Inverting the comparator and replacing infwith supyields the original
definitions. With only these changes, the proof of [13, Chapter 8.15] yields our modified claim Thm. 47.
A. Convergence in the absence of end components
We start by proving a technical lemma that is also useful for several future proofs: The over-approximation computed using
only Bellman updates converges to a fixpoint.
Lemma 48 (Upper bound converges to a fixpoint) .Let(Uk)k∈N0be the sequence of upper bounds computed by applying
Eq.(3)on a CSG G. Let U⋆:= lim
k→∞Ukbe the limit of the sequence. This limit is a fixpoint of the Bellman update, i.e. for all
s∈S,B(U∗)(s) =U⋆(s).
Proof. This lemma is a consequence of the fixpoint theorem we just recalled. Thus, we proceed as follows: We explain that the
domain of Bis a CPO and prove that Bis order-preserving and continuous. Then, Thm. 47 yields that U⋆is a (namely the
greatest) fixpoint.
Complete partial order . The domain of Bare valuations, i.e. vectors [0,1]|S|mapping every state to a number. Thus, we define
the set Pto be the set of all valuations. We use the standard point-wise comparisons as relation, i.e. υ1⪯υ2if and only if for
all states s∈Swe have υ1(s)≤υ2(s). Thus, the top element ⊤is the function that maps all states to 1. For every directed
setD, a greatest lower bound ℓ=infPDexists: Set ℓ(s) =infd∈Dd(s)for all s∈S. It is a lower bound, as by point-wise
comparison, it is smaller than all valuations in D; it is the greatest lower bound, since picking a larger number for any state
would not be a lower bound any more. Thus, the set consisting of valuations [0,1]|S|with this relation is a CPO.
Order-preserving . Recall that the Bellman operator on a state is defined as follows: B(υ)(s):=sup
ρ∈R(s)inf
σ∈S(s)B(υ)(s,ρ, σ),
where
B(υ)(s,ρ, σ):=X
(a,b)∈AX
s′∈Sυ(s′)·δ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b).
17Eq. (3) lifts it to valuations by applying it state-wise. Hence, for every state, we apply an operation consisting of multiplications
and summations, which are order-preserving. Thus, overall, the Bellman operator is order-preserving.
Continuous . We just showed that the Bellman operator on valuations is an order-preserving self-map on the set Pof valuations.
Then, [13, Lemma 8.7 (i)] yields that for every directed subset D⊆P, we have that B(D):={B(d)|d∈D}is a directed
subset, which is Condition (i) of Def. 45. It remains to show Condition (ii): infd∈DB(d) =B(infD). Since the comparisons
by the relation ⪯are performed point-wise, we have to prove that for all states s∈S, we have infd∈DB(d)(s) =B(infD)(s).
Thus, fix an arbitrary state s∈S, and conclude using the following chain of equations.
B(infD)(s)
=sup
ρ∈R(s)inf
σ∈S(s)X
(a,b)∈AX
s′∈S
inf
d∈Dd(s′)
·δ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b) (Unfolding definition of Bellman operator)
=sup
ρ∈R(s)inf
σ∈S(s)inf
d∈DX
(a,b)∈AX
s′∈S
d(s′)·δ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b) (Claim 1)
=inf
d∈Dsup
ρ∈R(s)inf
σ∈S(s)X
(a,b)∈AX
s′∈S
d(s′)·δ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b) (Claim 2)
=inf
d∈DB(d)(s). (Collapsing the Bellman operator definition)
Claim 1: This step moves the infd∈Dout of the summation, which is correct, since addition is a continuous operation. ▲
Claim 2: This step moves infd∈Dto the front, first utilizing that infima can be switched. Then, to switch infd∈Dandsupρ∈R(s),
we make use of the Minimax Theorem [45] which states that for a function f:X×Y→Rthat is concave-convex, it holds
thatsupx∈Xinfy∈Yf(x, y) =infy∈Ysupx∈Xf(x, y).fis concave-convex if fis concave for a fixed y∈Yand convex for a
fixed x∈X. This holds, in particular, for bilinear functions, i.e. functions that are linear in both arguments. The function
considered at this point is the following:
f(ρ, d) = inf
σ∈S(s)X
(a,b)∈AX
s′∈S
d(s′)·δ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b).
This function is indeed bilinear, since addition and multiplication are linear functions. ▲
Overall, we have shown that the sequence (Uk)k∈N0is the result of applying an order-preserving, continuous function to the
top-element of a complete partial order, and thus it converges to a (the greatest) fixpoint.
Theorem 9 (Convergence without ECs — Proof in App. C-A).LetGbe a CSG where all ECs are trivial, i.e. for every ECC
we have C⊆WS∪T. Then, the over-approximation using only Eq. (3)converges, i.e. lim
k→∞Uk=VR.
Proof. This proof is an extension of the proof of [18, Theorem 1] for turn-based games to the concurrent setting. The underlying
idea is the same, and can be briefly summarized as follows: We assume towards a contradiction that U⋆̸=VR, and find a set X
that maximizes the difference between upper bound and value. Every pair of strategies leaving the set decreases the difference.
However, VRandU⋆are fixpoints of the Bellman updates, from [17, Theorem 1] and Lem. 48, respectively. Consequently,
optimal strategies need to remain in the set. However, in the absence of ECs, optimal strategies have to leave the set, which
yields a contradiction and proves that U⋆=VR.
Main challenge . The key difference to the proof of [18, Theorem 1] is that we cannot argue about actions anymore, but have
to consider mixed strategies. This significantly complicates notation. Additionally, and more importantly, the former proof
crucially relied on the fact that for a state of player R, we know that its valuation is at least as large as that of any action, and
dually for a state of player S, its valuation is at most as large as that of any action. In the concurrent setting, this is not true.
The optimal strategies need not be maximizing nor minimizing the valuation and, moreover, they can be maximizing for one
valuation and minimizing for another. Thus, we found a more general, and in fact simpler, way of proving that “no state in
Xcan depend on the outside” [18, Statement 5] and deriving the contradiction. The crucial insight is that we can fix locally
optimal strategies and then apply Claim 4.
18Notation for Bellman operator . Before we begin the formal proof, we establish a condensed notation for a number of terms in
the Bellman operator:
rest(s, a, b, s′,ρ, σ) =δ 
s,a,b 
s′
·ρ(s)(a)·σ(s)(b).
Thus, the Bellman operator for some valuation υand pair of strategies (ρ, σ)simplifies to
B(υ)(s,ρ, σ) =X
(a,b)∈AX
s′∈Sυ(s′)·rest(s, a, b, s′,ρ, σ).
The set Xwith maximum difference . We define the difference of a state s∈Sas∆(s):=U⋆(s)−VR(s). Recall that VRis
the least fixpoint and U⋆the greatest fixpoint of B. Hence, we know that ∆(s)≥0for all states. Further, since we assume for
contradiction that U⋆̸=VR, there exist states with ∆(s)>0. Thus, we can find a non-empty set of states with maximum
difference: X:={s∈S|∆(s) =maxs∈S∆(s)}.
A leaving pair of strategies decreases the difference . Let s∈ X be a state in X. Let (ρ, σ)∈(R(s)× S(s))be a pair of
strategies such that (s,ρ, σ)leaves X. Then, following this pair of strategies for one step decreases the difference, formally
B(∆)(s,ρ, σ)<∆(s). (5)
We prove this using the following chain of equations:
B(∆)(s,ρ, σ)
=X
(a,b)∈AX
s′∈S∆(s′)·rest(s, a, b, s′,ρ, σ) (Definition of Bellman operator)
<∆(s). (Claim 3)
Claim 3: By assumption, we have that there exists a t∈S\ X such that this tis reached with positive probability
under the exiting strategies, i.e.P
(a,b)∈Arest(s, a, b, t, ρ, σ)>0. For this toutside of X, we have ∆(t)<∆(s), since
Xis defined as the set of all states with maximum difference. Further, no state can have a difference larger than ∆(s).
Furthermore, the remaining terms in the sum that the differences ∆(s′)are multiplied with are a probability distribution,
formallyP
(a,b)∈AP
s′∈Srest(s, a, b, s′,ρ, σ) = 1 . Thus, if all differences were equal to ∆(s), the sum would yield ∆(s). As
one of the summands is smaller than ∆(s)and all others are at most ∆(s), we get that the sum has to be smaller than ∆(s).▲
Without non-trivial ECs,Xmust be left . We have that X ∩(WS∪T) =∅: The difference is 0 for target states because both
the value and the upper bound are equal to 1; and the difference is 0 for the sure winning region of player S, since the upper
bound and value are equal to 0 (see Eq. (3)). Thus, since by assumption there are no ECs inS\(WS∪T), the set Xcannot
contain an EC. Consequently, there exists a state s∈ X such that for all pairs of available actions (a, b)∈ΓR(s)×ΓS(s), we
have Supp(δ 
s, a, b
)∩(S\ X)̸=∅, i.e. there is a successor state outside of X. This is the case because, if all states had a
pair of actions that stays in X, then there exists a pair of strategies that keeps the play inside a subset of X, which would then
form an EC. For a formal proof, we refer to [18, Lemma 2]. Note that, while their proof is for turn-based games, the definition
ofECis a graph theoretic notion where, intuitively, the players “work together” (formally, it is only about the existence of an
edge in the underlying hypergraph), and thus the proof is applicable to CSGs , too. In the following, we let sdenote such a
state where all strategies are leaving.
Notation for locally optimal strategies . For any state and valuation, locally optimal strategies for both players exist. We establish
a shorthand for the locally optimal strategies in state s(the one obtained in the previous step) with respect to U⋆andVR. For
playerRand valuation U⋆, we denote a locally optimal strategy by
ρU∈arg max
ρ∈R(s)inf
σ∈S(s)B(U⋆)(s,ρ, σ).
Similarly we denote a locally optimal strategy of player Swith respect to U⋆by
σU∈arg min
σ∈S(s)sup
ρ∈R(s)B(U⋆)(s,ρ, σ).
Analogously, we define locally optimal strategies with respect to VR, namely ρVandσV, obtained by replacing U⋆with VR
in the above definition.
19Deriving the contradiction . Recall that s∈ X is a state where all available pairs of actions, and thus all strategies, leave X. We
derive the contradiction ∆(s)<∆(s).
∆(s) =U⋆(s)−VR(s) (Definition of ∆)
=B(U⋆)(s)−VR(s) (U⋆is fixpoint by Lem. 48)
=B(U⋆)(s,ρU, σU)−VR(s) ((ρU, σU)locally optimal w.r.t. U⋆)
≤B(U⋆)(s,ρU, σV)−VR(s) (Claim 4)
=B(U⋆)(s,ρU, σV)−B(VR)(s) (VRis fixpoint [17, Theorem 1])
=B(U⋆)(s,ρU, σV)−B(VR)(s,ρV, σV) ((ρV, σV)locally optimal w.r.t. VR)
≤B(U⋆)(s,ρU, σV)−B(VR)(s,ρU, σV) (Claim 4)
=B(∆)(s,ρU, σV) (Claim 5)
<∆(s). ((s,ρU, σV)leaves Xand Eq. (5))
Claim 4: This argument is used in two steps in the above chain of equations. For the first usage, observe that σVcan be at
most as good as the optimal σU. More formally, recall σUwas chosen as the arg min of the Bellman operator with respect to
U⋆. Thus, B(U⋆)(s,ρU, σU)≤B(U⋆)(s,ρU, σV).
For the second usage, by the analogous argument we have B(VR)(s,ρV, σV)≥B(VR)(s,ρU, σV). Since this term is the
subtrahend of the subtraction, the overall expression can only become greater. ▲
Claim 5: This step follows from expanding the definition of the Bellman operator, rearranging the sums and collapsing the
definition of Bellman operator. Formally, for all states q and strategy pairs (ρ, σ)it holds that
B(∆)(q,ρ, σ) =X
(a,b)∈AX
s′∈S∆(s′)·rest(s, a, b, s′,ρ, σ) (Definition of Bellman operator)
=X
(a,b)∈AX
s′∈S 
U⋆(s′)−VR(s′)
·rest(s, a, b, s′,ρ, σ) (Definition of ∆)
=
X
(a,b)∈AX
s′∈SU⋆(s′)·rest(s, a, b, s′,ρ, σ)
−

X
(a,b)∈AX
s′∈SVR(s′)·rest(s, a, b, s′,ρ, σ)
 (Splitting the sum)
=B(U⋆)(q,ρ, σ)−B(VR)(q,ρ, σ). (Definition of Bellman operator)
▲
Summary . Starting from the assumption that U⋆̸=VR, we derived that there exists a set of states Xwhere the difference ∆
between upper bound and value is maximized. Further, a pair of strategies leaving Xdecreases this difference. However, since
there are no ECs in X, there has to be a state where the optimal strategies for VRandU⋆leave, which allows us to derive a
contradiction. Thus, the initial assumption is false, and we have U⋆=VR.
B. Convergence without Bloated End Components
Lemma 14 (Negating Weak Domination — Proof in App. C-B).Letυbe a valuation, s∈Sa state, R1,R2,R′⊆ R(s)and
S1,S2,S′⊆ S(s)sets of local strategies.
If for some sets of strategies we do nothaveR2≺υ,S′R1, then we have R1⪯υ,S′R2. Analogously, not S2≺υ,R′S1implies
S1⪯υ,R′S2.
Proof. We only provide the proof for Player R, as the other one is analogous by exchanging the names of the strategy sets,
replacing RwithSand vice versa.
We assume that we do not have R2≺υ,S′R1under the set of counter-strategies S′with respect to υ. Writing out the
definition, this means that ∀ρ1∈ R1.∃ρ2∈ R2:
(i)infσ∈S′B(υ)(s,ρ2, σ)>infσ∈S′B(υ)(s,ρ1, σ), or
(ii)∀σ′∈ S′we have B(υ)(s,ρ2, σ′)≥B(υ)(s,ρ1, σ′).
Our goal is to have that R1⪯υ,S′R2, formally: Formally, ∃ρ2∈ R2.∀ρ1∈ R1:
inf
σ∈S′B(υ)(s,ρ1, σ)≤inf
σ∈S′B(υ)(s,ρ2, σ).
20Both conditions of negated weak dominance imply our goal. The only remaining problem is the order of quantifiers. However,
the choice of ρ2does not depend on ρ1, and we can always pick ρ2as the strategy that maximizes infσ∈S′B(υ)(s,ρ2, σ).
Thus, we can exchange the order of quantifiers and prove our goal.
Lemma 20 (Negating Bloated — Proof in App. C-B ).If an ECX ⊆ S\(T∪WS)is not bloated for a valuation υ, then
there exists a state s∈ X that has a locally optimal strategy that is leaving, formally ∃ρ∈ RL(X, s).R(s)⪯υ,S(s){ρ}.
Proof. Since X′is not a BEC , we know that at some s∈ X′it holds that Hazard U⋆(X′, s) =∅. Fix sto be such a state.
Every strategy ρ′∈ R(S)must violate at least one of the 3 conditions of Def. 16. We write out the negations:
(i)ρ′/∈ RL(X′, s), i.e. the strategy is leaving ρ′∈ RL(X′, s).
(ii)We do not have R(s)\{ρ′}⪯S(s){ρ′}. By contraposition of Lem. 14, this implies {ρ′}≺S(s)R(s)\{ρ′}, so the strategy
is sub-optimal.
(iii) We do not have RL(X′, s)≺S(s){ρ′}. By Lem. 14, this implies {ρ′}⪯S(s)RL(X′, s), i.e. there are leaving strategies
that are not worse than ρ′. Note that in particular, this implies that RL(X′, s)is non-empty.
Our assumption gives us the disjunction over the three violated conditions. We proceed by a case distinction, always assuming
that all strategies violate a certain condition, which allows us to prove our goal, or, if there exists a strategy satisfying the
condition, we continue with the next one.
Case “Not (i)”: If all strategies violate Condition (i), that means all strategies are leaving, i.e. RL(s) =R(s). Thus, since
there always exist optimal strategies, by picking an optimal ρ∈ R we naturally have R(s)⪯υ,S(s){ρ}.
Case “(i), but not (iii)”: We assume there exists strategies that satisfies Condition (i), but all strategies that satisfy it violate
Condition (iii). This means that for every non-leaving strategy, the set of leaving strategies is not worse than it. As this holds for
all non-leaving strategies, we have RL(s)⪯υ,S(s)RL(s). Using that R(s) =RL(s)∪ RL(s)and the set of leaving strategies
trivially is not worse than itself, we obtain: R(s)⪯υ,S(s)RL(s). Moreover, the set of leaving strategies is non-empty, since
the definition of not worse requires that there exists a strategy in the right-hand set. This proves our goal.
Case “(i) and (iii), but not (ii)”: We assume there exists strategies that satisfy Condition (i) and (iii), but all these strategies
violate Condition (ii). This case cannot happen, and below we derive a contradiction. This completes our cast distinction, since
every strategy has to violate at least one of the three conditions.
Our assumption is that there exists a non-leaving strategy that weakly dominates the set of all leaving strategies. However,
every such strategy is suboptimal, as by violating Condition (ii) it is weakly dominated by all other strategies. This is a
contradiction, because then there are no optimal strategies. More formally, if we assume the optimal strategy is leaving, this is a
contradiction, because there exists a non-leaving strategy dominating the set of leaving strategies. And if we assume the optimal
strategy is non-leaving, this is a contradiction, because every non-leaving strategy is weakly dominated by the set of others.
Theorem 21 (Non-convergence implies BECs — Proof in App. C-B ).LetU⋆:= lim k→∞Ukbe the limit of the na ¨ıve upper
bound iteration (Eq. (3)) on the CSG G. If VI from above does not converge to the value in the limit, i.e. U⋆>VR, then the
CSG Gcontains a BEC in S\(T∪WS)with respect to U⋆.
Proof. Intuition and outline: . This proof builds on the proof of Thm. 9. There, we constructed a set Xmaximizing the
difference between U⋆andVRand showed that if there is a pair of optimal strategies leaving leaving X, then we can derive a
contradiction: The upper bound decreases, which contradicts the fact that it is a fixpoint. In the context of the other proof, that
allowed us to show that without ECs, VI converges, because without ECs it is impossible to have a set of states where all
optimal strategies stay in that set.
In the presence of ECs, states can indeed have a positive difference between U⋆andVR, see e.g. Ex. 6. Our goal is to prove
that at least one of these ECs is bloated. Thus, we assume for contradiction that no ECis bloated under U⋆. Thus, by Lem. 20,
there is an optimal leaving strategy for player R. Using that, we can repeat the argument from Thm. 9, showing that in this
case U⋆would decrease. Again, this is a contradiction because it is a fixpoint of applying Bellman updates (Lem. 48). Thus,
the initial assumption that no EC is bloated is false, and we can conclude that there exists a BEC.
Establishing the Context . As in the proof of Thm. 9, let X:={s∈S|∆(s) =maxs∈S∆(s)}be the set of states with
maximum difference. We denote the maximum by ∆max, and our assumption yields that ∆max>0. Note that this implies
thatX ∩(WS∪T) =∅, since for those states, their value is set correctly by initialization, and their difference is 0. Thus, if
we find a BEC that is a subset of X, it also satisfies the additional condition of being non-trivial, i.e. not in (WS∪T). The
contraposition of Thm. 9 yields that there has to be an EC in X.
Bottom MECs . To derive the contradiction, in the following, we consider ECs with a particular property, namely ECs that are
bottom in X. AMECX′is bottom in Xif the successors of a pair of strategies that leaves the MEC reaches states outside of
Xwith positive probability. Intuitively, a bottom MEC inXis aMEC , such that after leaving it, none of the successors is
21part of another MEC inX. One can compute such ECs using the MEC decomposition of X, ordering them topologically and
picking one at the end of a chain.
LetX′be a bottom MEC withX′⊆ X .X′exists because by assumption Xcontains at least one EC, thus, there also has
to exist an EC that is bottom in X.
Optimal Leaving Strategies in Non- BECs . We use the assumption for contradiction to say that X′is not bloated with respect
toU⋆. Then, using Lem. 20, we know that there exists a state s∈ X′where an optimal strategy ρUexists that is leaving X′.
Moreover, since X′is a bottom MEC in X, we also have that it is leaving with respect to X.
Deriving the Contradiction . Using these facts, we can exactly repeat the argument used in the proof of Thm. 9 under the
paragraph-heading “Deriving the Contradiction”. Recall we denote locally optimal strategies with respect to U⋆byρU, σU(and
we just proved ρUis leaving for all counter-strategies), and analogously locally optimal strategies with respect to VR, byρV
andσV. We highlight that Eq. (5), Claim 4 and Claim 5 from Thm. 9 are applicable in the context of this proof, too.
∆(s) =U⋆(s)−VR(s) (Definition of ∆)
=B(U⋆)(s)−VR(s) (U⋆is fixpoint by Lem. 48)
=B(U⋆)(s,ρU, σU)−VR(s) ((ρU, σU)locally optimal w.r.t. U⋆)
≤B(U⋆)(s,ρU, σV)−VR(s) (Claim 4 in Thm. 9)
=B(U⋆)(s,ρU, σV)−B(VR)(s) (VRis fixpoint [17, Theorem 1])
=B(U⋆)(s,ρU, σV)−B(VR)(s,ρV, σV) ((ρV, σV)locally optimal w.r.t. VR)
≤B(U⋆)(s,ρU, σV)−B(VR)(s,ρU, σV) (Claim 4 in Thm. 9)
=B(∆)(s,ρU, σV) (Claim 5 in Thm. 9)
<∆(s). ((s,ρU, σV)leaves Xand Eq. (5))
Now that we have derived a contradiction, our initial assumption that all ECs are not BECs is wrong, so we know there
exists a BEC X′⊆(S\(T∪WS))with respect to U⋆. This concludes the proof.
As a side note, we remark that it is indeed possible that there is only one BEC that causes many states, even some not in an
EC, to have a positive difference ∆. For an example, we refer to [18, Fig. 4].
C. Soundness of DEFLATE
The following is a technical lemma that is needed to show the soundness of deflation. Intuitively, it says that the value of all
states in an EC needs to depend on some exit. Note that there can be states whose value is higher than their own exit value,
namely if they can reach a better exit. However, this cannot be the case for all states, but there must be some whose value is
less than or equal to their exit value (first condition), and in fact no state can have a higher value than these states that actually
depend on exiting (second condition). The proof is very technical, as essentially it requires unfolding all the definitions, and
thereby also unfolding all the included case distinctions.
Lemma 49 (No state has a larger value than that of an exit from its EC) .LetX ⊆ S\(T∪WS)be an EC. Then, it holds that
(i)X′:={s∈ X | VR(s)≤exitValVR(X, s)} ̸=∅, and
(ii)maxs∈X′VR(s)≥maxs∈X \X′VR(s).
Proof. We prove the lemma by contradiction, i.e. we assume that one of the two conditions posed by the lemma is violated.
We make the following case distinction:
(i)X′=∅; and
(ii)X′̸=∅butmaxs∈X′VR(s)<maxs∈X \X′VR(s).
Case (i)
In this case it holds that X′=∅, i.e. for all s∈ X we have VR(s)>exitValVR(X, s). Recall that in Def. 27 if
Hazard VR(X, s) =∅is true at some state s∈ X, then the exit value is given by the value of the matrix game played at
that state.
Consequently, as for all s∈ X it holds that VR(s)>exitValVR(X, s), at all states s∈ X it must hold
that Hazard VR(X, s)̸=∅because otherwise we would obtain the following contradiction: exitValVR(X, s) =
supρ∈R(s)infσ∈S(s)B(VR)(s,ρ, σ) =VR(s).
22We proceed with the assumption that for all s∈ X it holds that Hazard VR(X, s)̸=∅. Since by the case assumption at all
s∈ X it holds that VR(s)>exitValVR(X, s)the true values of the states in Xare attainable with the hazardous and
trapping strategies. More formally, at each s∈ X the following chain of equations holds.
VR(s) = sup
ρ∈R(s)inf
σ∈S(s)B(VR)(s,ρ, σ) (VRis a fixpoint & case assumption: VR(s)>exitValVR(X, s))
= sup
ρ∈Hazard VR(X,s)inf
σ∈S(s)B(VR)(s,ρ, σ) (Hazard VR(X, s)are optimal by Def. 16)
= sup
ρ∈Hazard VR(X,s)inf
σ∈TrapVR(X,s))B(VR)(s,ρ, σ). (TrapVR(X, s)are optimal by Def. 24)
Thus, for player Rstaying in Xis optimal and since no target state is contained in Xit has to hold that VR(s) = 0 for
alls∈ X. However, this is a contradiction to the assumption that VR(s)>exitValVR(X, s)since exitValVR(X, s)≥0
(as all possible valuations are non-negative and the value of the exiting sub-game is given by the maximum between 0 and
exitVal - see Def. 27 and Def. 26). Thus, the case assumption that X′=∅must be false.
Case (ii)
In this case it holds that X′̸=∅butmaxs∈X′VR(s)<maxs∈X \X′VR(s)is true.
We make the following case distinction: (ii.a) X \ X′is a BEC; and (ii.b) X \ X′is not a BEC.
Case (ii.a)
In this case X \ X′is a BEC, i.e. for all s∈ X \ X′it holds that Hazard VR(X \ X′, s)̸=∅.
In case it holds that VR(s)>exitValVR(X \ X′, s)for all s∈ X \ X′, then since no target state is contained in Xand
therefore neither in X \ X′, it must be true that VR(s) = 0 for all s∈ X \ X′. However, similarly as in Case (i), this is
a contradiction to the assumption that VR(s)>exitValVR(X \ X′, s)for all s∈ X \ X′asexitValVR(X \ X′, s)≥0
for all s∈ X \ X′.
Consequently, there has to exist s∈ X \ X′such that VR(s)≤exitValVR(X \ X′, s).
We make another case distinction:
(ii.a.1) for all s′∈arg maxs∈X \X′VR(s)it holds that VR(s′)>exitValVR(X \ X′, s′); and
(ii.a.2) there exists s′∈arg maxs∈X \X′VR(s)such that VR(s′)≤exitValVR(X \ X′, s′).
Case (ii.a.1) In this case for all s′∈arg maxs∈X \X′VR(s)it holds that VR(s′)>exitValVR(X \ X′, s′).
Thus, at each s′∈arg maxs∈X \X′VR(s′)the following chain of equations holds.
VR(s′) = sup
ρ∈R(s′)inf
σ∈S(s′)B(VR)(s′,ρ, σ) (VRis a fixpoint)
= sup
ρ∈Hazard VR(X \X′,s′)inf
σ∈S(s′)B(VR)(s′,ρ, σ) (Hazard VR(X \ X′, s′)are optimal by Def. 16)
= sup
ρ∈Hazard VR(X \X′,s′)inf
σ∈TrapVR(X \X′,s′))B(VR)(s′,ρ, σ).(TrapVR(X \ X′, s′)are optimal by Def. 24)
Thus, at all states that attain the highest value among X \ X′, it is optimal for both players to choose strategies
that together are staying in X \ X′. However, since Xand thus X \ X′does not belong to the winning region
of Player S, leaving X \ X′has to be possible. Thus, from each state s′∈arg maxs∈X \X′VR(s), Player R
has to possess an optimal strategy that leads to a state s′′∈ X \ X′where leaving X \ X′is possible, i.e.
where VR(s′′)≤exitValVR(X \ X′, s′′)holds. Thus, at state s′′the highest value also has to be attainable.
However, this is a contradiction to the case assumption that for all s′∈arg maxs∈X \X′VR(s)it holds that
VR(s′)>exitValVR(X \ X′, s′).
Case (ii.a.2) In this case there exists s′∈arg maxs∈X \X′VR(s)such that VR(s′)≤exitValVR(X \ X′, s′). Let
X′′:={s′∈arg maxs∈X \X′VR(s)|VR(s′)≤exitValVR(X \ X′, s′)}. Then, we need to make another case
distinction:
(ii.a.2.1) for all s∈ X′′it holds that Defl VR(X \ X′, s) =∅; and
(ii.1.2.2) there exists s′∈ X′′such that Defl VR(X \ X′, s′)̸=∅.
Case (ii.a.2.1) In this case for all s∈ X′′it holds that Defl VR(X \X′, s) =∅. Then, since Xand so X \X′do not
belong to the winning region of Player S, there has to exist a state s′′∈ X \X′such that Defl VR(X \X′, s′′)̸=∅
23so leaving X \ X′is possible. However, then s′′∈arg maxs∈X \X′VR(s)must hold which is a contradiction to
the case assumption.
Case (ii.a.2.2) In this case there exists s′∈ X′′such that Defl VR(X \ X′, s′)̸=∅. Then, the following chain of
equations holds.
VR(s′)≤exitValVR(X \ X′, s′) (Case assumption)
=max(0,V(ˆZ(s′))) (By case assumption (ii.a): Hazard VR(X \ X′, s′)̸=∅)
=V(ˆZ(s′)) (Case assumption: Defl VR(X \ X′, s′)̸=∅)
= sup
ρ∈Defl VR(X \X′,s′)inf
σ∈TrapVR(X \X′,s′)B(VR)(s′,ρ, σ)
(Value of the exiting sub-game — see Def. 27)
= sup
ρ∈Defl VR(X \X′,s′)inf
σ∈TrapVR(X \X′,s′)X
(a,b)∈AX
s∈X \X′VR(s)|{z}
≤maxs′′∈X \X ′VR(s′′)·δ 
s′,a,b 
s
·ρ(a)·σ(b)
+X
s∈X′VR(s)|{z}
<maxs′′∈X \X ′VR(s′′)·δ 
s′,a,b 
s
·ρ(a)·σ(b)
(Def. of Band case assumption (ii): maxs∈X′VR(s)<maxs∈X \X′VR(s))
<max
s∈X \X′VR(s). (Everything sums up to 1)
Thus, from the case assumption we derived a contradiction because s′∈arg maxs∈X \X′VR(s).
Case (ii.b)
In this case X \ X′is not a BEC, i.e. there exists s′∈ X \ X′such that Hazard VR(X \ X′, s′) =∅.
We need another case distinction:
(ii.b.1) for all s′∈arg maxs∈X \X′VR(s)it holds that Hazard VR(X \ X′, s′)̸=∅; and
(ii.b.2) there exists s′∈arg maxs∈X \X′VR(s)such that Hazard VR(X \ X′, s′) =∅.
Case (ii.b.1) In this case we have that for all s′∈arg maxs∈X \X′VR(s)it holds that Hazard VR(X \ X′, s′)̸=∅.
Then, at each s′∈arg maxs∈X \X′VR(s)the following chain of equations holds.
VR(s′) = sup
ρ∈Hazard VR(X \X′,s′)inf
σ∈S(s′)B(VR)(s′,ρ, σ) (Hazard VR(X \ X′, s′)are optimal by Def. 16)
= sup
ρ∈Hazard VR(X \X′,s′)inf
σ∈TrapVR(X \X′,s′))B(VR)(s′,ρ, σ).(TrapVR(X \ X′, s′)are optimal by Def. 24)
Thus, at all states that attain the highest value among X \ X′it is optimal for both players to choose strategies
that together are staying in X \ X′. However, since Xand thus X \ X′does not belong to the winning region
of Player S, leaving X \ X′has to be possible. Thus, from each state s′∈arg maxs∈X \X′VR(s), Player R
has to possess an optimal strategy that leads to a state s′′∈ X \ X′where leaving X \ X′is possible, i.e. where
Hazard VR(X \ X′, s′′) =∅holds. Thus, at state s′′the highest value also has to be attainable. However, this is a
contradiction to the case assumption that for all s′∈arg maxs∈X \X′VR(s)it holds that Hazard VR(X \ X′, s′)̸=∅.
Case (ii.b.2) In this case there exists s′∈arg maxs∈X \X′VR(s)such that Hazard VR(X \ X′, s′) =∅.
Then, the following chain of equations holds.
VR(s′) = sup
ρ∈R(s′)inf
σ∈S(s′)B(VR)(s′,ρ, σ) (VRis a fixpoint)
=sup
ρ∈R(s′)inf
σ∈S(s′)X
(a,b)∈AX
s∈X \X′VR(s)|{z}
≤maxs′′∈X \X ′VR(s′′)·δ 
s′,a,b 
s
·ρ(a)·σ(b)
+X
s∈X′VR(s)|{z}
<maxs′′∈X \X ′VR(s′′)·δ 
s′,a,b 
s
·ρ(a)·σ(b)
<max
s′′∈X \X′VR(s′′).
(Everything sums up to 1 and Hazard VR(X \ X′, s′) =∅thus at least one successor state is in X′)
24Thus, from the case assumption we derived a contradiction because s′∈arg maxs∈X \X′VR(s).
Thus, every case leads to a contradiction which concludes the proof.
Using Lem. 49, we now prove that for every EC X, we have that for an upper bound U, the best exit value of the EC is
also an upper bound. This is the crucial ingredient for showing that deflation cannot decrease a valuation below the value VR.
Lemma 50. LetX ⊆ S\(T∪WS)be an EC, and U∈[0,1]|S|be a valuation with U≥VR. Then, for all states
s∈bestExitsVR(X), we have exitValU(X, s)≥exitValVR(X, s).
Proof. Lets∈bestExitsVR(X). Our goal is to show that exitValU(X, s)≥exitValVR(X, s). Since for the estimation of
exitValU(X, s)andexitValVR(X, s)the sets of strategies, TrapVR(X, s),TrapU(X, s),Hazard VR(X, s), and Hazard U(X, s),
which can be empty or non-empty, we have to consider all possible combinations. We consider the following four main cases,
which we further analyse with respect to the Usets of strategies where necessary:
Case (I) TrapVR(X, s) =∅andHazard VR(X, s) =∅,
Case (II) TrapVR(X, s) =∅andHazard VR(X, s)̸=∅,
Case (III) TrapVR(X, s)̸=∅andHazard VR(X, s) =∅, and
Case (IV) TrapVR(X, s)̸=∅andHazard VR(X, s)̸=∅.
For each case we show that either the case is impossible or that the statement of the lemma holds.
Case (I) TrapVR(X, s) =∅andHazard VR(X, s) =∅.
Letσ′∈ S(s)be an optimal Player Sstrategy under VR. Since TrapVR(X, s) =∅, at least on of the two conditions
posed by the definition of trapping strategies (Def. 24) must be violated under VR.
Since Hazard VR(X, s) =∅, Condition (ii) of Def. 24, i.e. ∀ρ∈Hazard VR(X, s) : (s,ρ, σ′)staysIn X, is trivially satisfied.
Consequently, Condition (i) must be violated, i.e., it must hold that there exists no optimal strategy for Player Swhich
cannot be true. Thus, this case is impossible.
Case (II) TrapVR(X, s) =∅andHazard VR(X, s)̸=∅.
At state sit holds that VR(s) =exitValVR(X, s), since we chose it to be in bestExitsVR. Thus, it is in the set X′
constructed in Condition (i) of Lem. 49. By Condition (ii) of Lem. 49, we know that all states in X \ { s}attain a value
that is either smaller or equal VR(s). Using this and the fact that no staying strategy can be optimal (as the set of trapping
strategies is empty), we can derive a contradiction as follows.
VR(s) = sup
ρ∈R(s)inf
σ∈S(s)B(VR)(s,ρ, σ) (VRis fixpoint of B)
= sup
ρ∈Hazard VR(X,s)inf
σ∈S(s)B(VR)(s,ρ, σ) (Hazard VR(X, s)are optimal under VR)
< sup
ρ∈Hazard VR(X,s)inf
σ∈SL(Hazard VR(X,s),X,s)B(VR)(s,ρ, σ) (TrapVR(X, s) =∅)
= sup
ρ∈Hazard VR(X,s)inf
σ∈SL(Hazard VR(X,s),X,s)X
(a,b)∈AX
s′∈XVR(s′)|{z}
≤VR(s)·δ 
s,ρ, σ
·ρ(a)·σ(b)
(Unfolding definition of B)
≤ sup
ρ∈Hazard VR(X,s)inf
σ∈SL(Hazard VR(X,s),X,s)X
(a,b)∈AX
s′∈XVR(s)·δ 
s,ρ, σ
·ρ(a)·σ(b)
(VR(s′)≤VR(s)by (ii) in Lem. 49)
=B(VR)(s)
=VR(s). (VRis fixpoint of B)
Thus, overall VR(s)<VR(s), a contradiction.
Case (III): TrapVR(X, s)̸=∅andHazard VR(X, s) =∅.
Since Hazard VR(X, s) =∅we know by Lem. 14 that there exists ρL∈ RL(X, s).R(s)⪯VR,S(s){ρL}. Our goal is to
show that there exists ρ∗∈ R(s)that is optimal under VRandρ∗∈Defl U(X, s). We distinguish two cases:
•(III.a): For all ρM∈ R L(X, s)that are optimal under VR, there exists ρH∈Hazard U(X, s)such that Supp(ρM)∩
Supp(ρH)̸=∅, and
•(III.b): There exists ρL∈ RL(X, s)that is optimal under VRand it holds that Supp(ρL)∩S
ρ∈Hazard U(X,s)Supp(ρ) =∅.
Case (III.a) In this case, for all ρM∈ RL(X, s)that are optimal under VR, there exists ρH∈Hazard U(X, s)such that
Supp(ρM)∩Supp(ρH)̸=∅.
25LetρH∈Hazard U(X, s)andρL∈ {ρ∈ RL(X, s)|Supp(ρH)∩Supp(ρL) =∅}such that Supp(ρM)∩Supp(ρH)̸=∅
andSupp(ρM)∩Supp(ρL)̸=∅. In other words, only a strategy that mixes the supports of ρHandρLis optimal under
VR.
Due to the case assumption that only strategies that mix with some hazardous strategy are optimal, ρLmust be
sub-optimal under VR, as well as ρH.
More formally, for all ρM∈ RL(X, s), such that R(s)\ {ρM}⪯{ρM}it holds that
∃ρH∈Hazard U(X, s).∃ρL∈ {ρ∈ RL(X, s)|Supp(ρH)∩Supp(ρL) =∅}:
sup
Dist(Supp(ρM)\Supp(ρL))inf
σ∈S(s)B(VR)(s,ρ, σ)≺{ρM}
and
sup
Dist(Supp(ρM)\Supp(ρH))inf
σ∈S(s)B(VR)(s,ρ, σ)≺{ρM}.
LetS∗⊆ S(s)be the optimal Player Sstrategies with respect to ρM. Since ρMhas to mix ρLandρH, the following
must be true:
•there exists σ1∈ S∗such that
B(VR)(s,ρL, σ1)<B(VR)(s,ρH, σ1),and (6)
•there exists σ2∈ S∗such that
B(VR)(s,ρL, σ2)>B(VR)(s,ρH, σ2), (7)
because otherwise either ρLorρHwould be optimal under VRthus mixing would not be necessary. Further, Player S
also needs to mix the two strategies σ1andσ2to ensure optimality.
Our goal now is to show that s /∈bestExitsVR(X)which would lead to a contradiction to the assumption that
s∈bestExitsVR(X), showing that at a best exit there cannot exist only optimal strategies that fulfill the same properties
asρM.
To estimate the best exit value from Xunder VR, all exists from s∈ X have to be estimated. By Eq. (6) and Eq. (7),
we know that the part of the ECXthat is reachable with (ρH, σ1), sayX′⊂ X , attains a higher value than the one
that is reachable with (ρH, σ2).
More formally, the following holds. Let ˆs∈maxs′∈X′exitValVR(X, s′). Atˆsleaving X′has to be possible, because
otherwise, the values at all s′∈ X′would be equal to 0, which would be a contradiction to the assumption that
X ⊆ S\(T∪WS). Then, the following chain of equations holds.
exitValVR(X, s) = sup
ρ∈R(s)inf
σ∈S(s)B(VR)(s,ρ, σ) (Hazard VR(X, s) =∅)
=inf
σ∈S(s)B(VR)(s,ρM, σ) (ρMis optimal under VR)
=B(VR)(s,ρM, σ1) (σ1is optimal under VRwith respect to ρM)
<B(VR)(s,ρH, σ1). (Under σ1,ρHis optimal by Eq. (6))
=X
(a,b)∈AX
s′∈Post(s,ρH,σ1)VR(s′)·δ 
s,a,b
·ρ(a)·σ(b) (Unfolding the def. of B)
≤X
(a,b)∈AX
s′∈Post(s,ρH,σ1)exitValVR(X,ˆs)·δ 
s,a,b
·ρ(a)·σ(b)
(Atˆsone can attain the highest value upon leaving)
=exitValVR(X,ˆs). (Everything sums-up to 1)
Thus, under VR, the state ˆsattains a higher value upon leaving than s. This is a contradiction to the assumption that
s∈bestExitsVR(X).
Case (III.b) There exists ρL∈ R L(X, s)that is optimal under VRand it holds that Supp(ρL)∩S
ρ∈Hazard U(X,s)Supp(ρ) =∅.
Then, ρLmust belong to the set of deflating strategies under U, as it satisfies both conditions posed by the definition of
deflating strategies (see Def. 25).
26IfTrapU(X, s)̸=∅, then the following chain of equations holds.
exitValVR(X, s) = sup
ρ∈R(s)inf
σ∈σ(s)B(VR)(s,ρ, σ) (By Def. 16 in case Hazard VR(X, s) =∅)
=inf
σ∈σ(s)B(VR)(s,ρL, σ) (ρLis optimal)
≤ sup
ρ∈Defl U(X,s)inf
σ∈σ(s)B(VR)(s,ρ, σ) ({ρL} ⊆Defl U(X, s)})
≤ sup
ρ∈Defl U(X,s)inf
σ∈TrapU(X,s)B(VR)(s,ρ, σ) (TrapU(X, s)⊆ S(s))
≤ sup
ρ∈Defl U(X,s)inf
σ∈TrapU(X,s)B(U)(s,ρ, σ) (Bis order-preserving)
=exitValU(X, s).
IfTrapU(X, s) =∅, then the following chain of equations holds.
exitValVR(X, s) = sup
ρ∈R(s)inf
σ∈σ(s)B(VR)(s,ρ, σ) (By Def. 16 in case Hazard VR(X, s) =∅)
=inf
σ∈σ(s)B(VR)(s,ρL, σ) (ρLis optimal)
≤ sup
ρ∈Defl U(X,s)inf
σ∈S(s)B(U)(s,ρ, σ) (Bis order-preserving)
=exitValU(X, s).
Case (IV): TrapVR(X, s)̸=∅andHazard VR(X, s)̸=∅.
It holds that s∈bestExitsVR(X), thus there exist no other state at which leaving Xattains better value. By Lem. 49 we
have that for all s′∈bestExitsVR(X)it holds that VR(s′)≤exitValVR(X, s′). Consequently, at a best exit it can only
hold that VR(s′) =exitValVR(X, s′)because otherwise VRwould not be a fixpoint of B. Therefore, the following chain
of equations holds at s.
VR(s) = sup
ρ∈R(s)inf
σ∈S(s)B(VR)(s,ρ, σ) (VRis fixpoint of B)
= sup
ρ∈Hazard VR(s)inf
σ∈S(s)B(VR)(s,ρ, σ) (Hazard VR(X, s)are optimal under VR)
= sup
ρ∈Hazard VR(s)inf
σ∈TrapVR(s)B(VR)(s,ρ, σ) (TrapVR(X, s)are optimal under VR)
=exitValVR(X, s).
(By Lem. 49 we have VR(s)≤exitValVR(X, s)however only ”=” can hold as VRis fixpoint of B)
Thus, this case is equal to Case (III) where exitValVR(X, s) =supρ∈R(s)infσ∈S(s)B(VR)(s,ρ, σ), as the assumptions
of cases (I) and (II) are not possible.
Lemma 51 (Existence of maximal BECs ).Given a CSG Gand let C⊆(S\(T∪WS))be an ECand let Ube a valuation.
If under Uthere exists a BECX ⊆ C, then there also exists a maximal BEC, i.e. there exists a BECXmax⊆Csuch that
X ⊆ Xmaxand it holds that Xmax∪ {s}is not a BEC for all s∈C\ Xmax.
Proof. To prove the lemma it suffices to show the following: Let X1⊆(S\(T∪WS))andX2⊆(S\(T∪WS))be two
ECs that are BEC under UwithX1∩ X2̸=∅. Then, it holds that X1∪ X2is also a BEC.
Lets∩∈(X1∩ X2),s′∈ X1, and s′′∈ X2. Since X1is aBEC at each state s∈ X1it holds that Hazard U(X1, s)̸=∅and
TrapU(X1, s)̸=∅for all s∈ X1. Further, since X1is an EC, there exists a play s0s1. . .such that s0=s′andsn=s∩for
some nand for all 0≤i < n it holds that si+1∈Post(si,ρ∗
i, σ∗
i)where ρ∗
i∈Hazard U(X1, si)andσ∗
i∈TrapU(X1, si).
Similarly, since X2is also a BEC , there exists a play s′
0s′
1. . .such that s′
0=s∩ands′
m=s′′for some mand for all
0≤j < m it holds that s′
j+1∈Post(s′
j,ρ′
j, σ∗
j)where ρ′
j∈Hazard U(X2, s′
j)andσ′
j∈TrapU(X2, s′
j).
Therefore, for any s, s′∈ X1∪X2there exist a play s′′
0s′′
1. . .such that s′′
0=s′ands′′
l=s′′for some land for all 0≤k < l
it holds that s′′
k+1∈Post(s′′
k,ρ′′
k, σ′′
k)where ρ′′
k∈Hazard U(X1∪ X2, s′′
k)andσ′′
k∈TrapU(X1∪ X2, s′′
k).
Thus, we have shown that the X1∪ X2is an EC. Further, since at all states of X1andX2the two conditions (i) and (ii) of
Def. 18 are fulfilled, also X1∪ X2fulfills them. Consequently, X1∪ X2is aBEC which in turn proves that there exist maximal
BECs.
27Now that we have proven that maximal BECs indeed exist, the next step is to prove the correctness of FIND MBECs , i.e.
the algorithm that can find maximal BECs.
Lemma 30 (FIND MBECs is correct— Proof in App. C-C).For a CSG , aMEC Cand a valid upper bound U, it holds that
X ∈ FIND MBECs (G,C,U)if and only if Xis a BEC in Cand there exists no T⊆Cthat is a BEC and X⊊T.
Proof. We prove the lemma in two steps. First, if for a set of states X ⊆ Cit holds that X ∈ FIND MBECs (G,C,U)thenX
is a maximal BEC . Second, we will show that if a set of states X ⊆ Cis a maximal BEC , thenX ∈ FIND MBECs (G,C,U).
1. Direction “ ⇒”LetX ∈ FIND MBEC (G,C,U). We need to show that Xis a maximal BEC.
LetB:={s∈C|Hazard U(C, s)̸=∅}. Since, X ∈ FIND MBEC (G,C,U), then X ⊆ B.
We show that Xis a maximal BEC in Cvia a contradiction. In particular, we consider the following two cases:
Case (i) Xis maximal EC in Bbut not BEC in C; and
Case (ii) Xis a BEC in Cbut not a maximal one.
Case (i) Assume towards a contradiction that Xis a maximal ECinBbut not BEC inC. Then there exists s′∈ X such
thatHazard U(X, s′) =∅. Thus, every strategy ρ′∈ R(s′)must violate at least one of the three conditions of Def. 16.
We write out the negatations.
(a)ρ′/∈ RL(X, s′), i.e. the strategy is leaving ρ′∈ RL(X, s).
(b)We do not have R(s′)\ {ρ′}⪯S(s′){ρ′}. By contraposition of Lem. 14, this implies {ρ′}≺S(s′)R(s′)\ {ρ′}, so
the strategy is sub-optimal.
(c)We do not have RL(X, s′)≺S(s′){ρ′}. By Lem. 14, this implies {ρ′}⪯S(s′)RL(X, s′), i.e. there are leaving
strategies that are not worse than ρ′. Note that in particular, this implies that RL(X, s′)is non-empty.
Our assumption gives us the disjunction over the three violated conditions. We proceed by a case distinction, always
assuming that each strategy violates a certain condition, which allows us to prove our goal, or, if there exists a strategy
satisfying the condition, we continue with the next one.
Case “all strategies satisfy (a)” . In this case all strategies of player Rmust be leaving X, i.e.RL(X, s′) =R(s′).
Thus, for all σ∈ S(s′)and for all ρ∈ R(s′)there exists ˆs∈Post(s′,ρ, σ)such that ˆs /∈ X. Consequently, Xis not
an EC which is a contradiction to the assumption that Xis an EC.
Case “some strategy violates (a) and satisfies (c)” . We assume that ρ′violates (a) but satisfies condition (c). Thus,
PlayerRhas a non-leaving strategy, however it is not optimal. In other words, Player Rhas an optimal strategy
ρ∈ R(s′)such that there exists ˆs∈Post(s′,ρ, σ)such that ˆs /∈ X . Then, using the same argumentation as in the
previous case we can derive a contradiction to the assumption that X ∈ FIND MBEC (G,C,U).
Case “some strategy violate (a) and (c) but satisfy (b)” . We assume that ρ′violates (a) and (c) but satisfies condition
(b). Then, strategy ρ′is dominated by another non-leaving strategy, say ρ∈ RL(X, s′), that in turn has to violate one
of the two previous cases.
Case (ii) Now, assume towards a contradiction that Xis aBEC inCbut not maximal. Since X ∈ FIND MECs (G, B),
Xis maximal BEC inB. Then, due to the case assumption that Xis not maximal, there has to exist a set of states
ˆS⊆C\Bsuch that X ∪ˆS]is a maximal BEC inC. However, since for all s∈C\Bit holds that Hazard U(C, s) =∅,
X ∪ˆScannot be an EC in Cwhich is a contradiction to the assumption that Xis an EC in C.
Overall, we have shown that Xis a BEC and it is also maximal.
2. Direction “ ⇐”LetX ⊆ Cbe a maximal BEC in the MEC C. We need to show that X ∈ FIND MBEC (G,C,U)holds. Let
B:={s∈C|Hazard U(C, s)̸=∅}. Assume towards a contradiction X/∈FIND MBEC (G,C,U). Then, we distinguish
two cases: (i) ∃s∈ X such that s /∈B, or (ii) X/∈FIND MECs (G, B).
Case (i) If∃s∈ X such that s /∈B, then it holds that Hazard U(C, s) =∅, by Claim 6 we also have Hazard U(X, s) =∅
asX ⊆ C, a contradiction.
Case (ii) In this case it holds that ∀s∈ X we have that s∈B, however, X/∈FIND MECs (G, B). We can again
distinguish three cases: (ii.a) ∃X′∈FIND MECs (G, B)such that X′⊊X; and (ii.b) ∃X′∈FIND MECs (G, B)such
thatX′∩ X ̸=∅and∃s′∈ X′\ X; and (ii.c) ∀X′∈FIND MECS (G, B)it holds that X′∩ X=∅.
Case (ii.a) Then there is a state in X \B, a contradiction to the assumption that Xis a maximal BEC in B.
Case (ii.b) By “1. Direction “ ⇒” ” we know that X′is a maximal BEC . In particular, for all s∈ X′it holds that
Hazard U(X′, s)̸=∅. Then, by Lem. 51 we also know that since X′∩ X ̸=∅, then X′∪ X has to be also a BEC .
However, this is a contradiction to the assumption that Xis already a maximal BEC.
Case (ii.c) Since∀X′∈FIND MECS (G, B)it holds that X′∩ X=∅then, for all s′∈ X we have s′/∈B. However,
Xis a maximal BEC in the MEC Cby assumption and therefore for all s∈ X it holds that Hazard U(X, s)̸=∅.
Further, since X ⊆ C, by Claim 6 it also holds that Hazard U(C, s)̸=∅. Consequently, such s′cannot exist and
therefore such X′cannot exist, a contradiction.
28Claim 6: For any ECs C⊆C′we have Hazard U(C, s)⊆Hazard U(C′, s).
Proof. Letρ∈ RL(C, s). We need to show that ρ∈ RL(C′, s)holds.
First, strategy ρsatisfies condition (i) of Def. 16, thus, there has to exist ˆσ∈ S(s), such that (s,ρ,ˆσ)staysIn C holds.
Since C⊆C′, it also has to be true that (s,ρ,ˆσ)staysIn C′. Consequently, ρsatisfies condition (i) of Def. 16 with
respect to C′.
Second, ρsatisfies condition (ii) of Def. 16, i.e. ρis optimal. As the optimality of a strategy is independent of any set
of states this condition is satisfied anyway.
Third, ρsatisfies condition (iii) of Def. 16, i.e. all strategies that are leaving with respect to Care sub-optimal. More
precisely, it holds that RL(C, s)≺S(s){ρ}. Since C⊆C′holds, there cannot exist an optimal strategy that is leaving
with respect to the greater set of states C′. Consequently, ρalso satisfies condition (iii) of Def. 16 with respect to C′.
Consequently, ρsatisfies all conditions posed by Def. 16 with respect to C′, so it holds that Hazard U(C, s)⊆
Hazard U(C′, s).
▲
D. Soundness and Completeness
In order to prove the soundness and completeness of Alg. 1 with Alg. 3 as DEFLATE routine, we need to prove that the
sequence of lower and upper bounds converges to a fixpoint. Therefore, before we can prove Thm. 37, first we need to prove
the following lemma.
Lemma 52 (Alg. 1 converges to a fixpoint) .The BVI algorithm (Alg. 1) converges to a fixpoint, i.e. limk→∞(Bk(L0),(D◦
B)k 
U0
) = (B(limk→∞Bk(L0)),(D◦B)(lim k→∞(D◦B)k 
U0
)).
Proof. We consider the domain V:= [0,1]|S|×[0,1]|S|, i.e. every element consists of two vectors of real numbers, the under-
and over-approximation. The bottom element of the domain, denoted by ⊥, is(⃗0,⃗1), where for a∈[0,1],⃗ adenotes the function
that assigns ato all states. We further restrict the domain to exclude elements of the domain that are trivially irrelevant for the
computation. In particular, we exclude all tuples (L,U)where L(s)<1for a target state s∈TorU(s)>0for a state with
no path to the target state s∈WS. Then the bottom element is ⊥= (L0,U0), i.e. the vector that we have before the first
iteration of the main loop of Alg. 1. Concretely, L0(s)is 1 for all s∈T, i.e. target states, and 0 everywhere else, and U0(s)is
0 for all s∈WS, i.e. states where Scan surely win, and 1 everywhere else.
We define a comparator ⊑onV, to compare two elements of the domain. We write (Lk,Uk)⊑(Lk+1,Uk+1)if and only
if both Lk≤Lk+1andUk≥Uk+1hold with component-wise comparison. Intuitively, (Lk,Uk)⊑(Lk+1,Uk+1)holds if
(Lk+1,Uk+1)is a more precise approximation than (Lk,Uk). The comparator ⊑induces a complete partial order over the
domain, since we have a bottom element and every direct subset has a supremum; the latter claim holds, because ⊑reduces
to component-wise comparison between real numbers from [0,1], where suprema exist. For more details on the definition of
directed set and complete partial orders, we refer to [13].
Alg. 1 first applies the Bellman operator on the over- and under-approximation and subsequently applies the deflate operator
on the over-approximation (i.e. upper bound). Thus, the operator that mimics the behavior of the algorithm is the following
BVI(Lk,Uk) = (B(Lk),(D◦B)(Uk)).
From Thm. 5 we know that the under-approximation converges. Also, by Lem. 36 we know that (D◦B)is order-preserving.
Thus, for the final argument it remains to show that Dis (Scott-)continuous.
A map is (Scott-)continuous if, for every directed set Din(proj2(V),⊑), the subset D(D)of(proj2(V),⊑)is directed, and
D(supD) =supD(D). Let s∈S, ifs, under supD, does not belong to any BEC, then D(supD)(s) =supD(s). Thus, we
proceed with the assumption that under the valuation supD,s∈ X ⊆ Ssuch that Xis a BEC.
LetHazard supD(X, s)be the set of hazardous strategies and let TrapsupD(X, s)be the set of suitable counter strategies for
playerS(see Def. 18). Since Xis aBEC both sets are non-empty, i.e. Hazard supD(X, s)̸=∅andTrapsupD(X, s)̸=∅, at
alls∈ X. Thus, D(supD)(s) =min(supD(s),bestExitValsupD(X)). For the sake of readability let R′(s):=Dist(ΓR(s)\S
ρ′′∈Hazard supD(X,s)Supp(ρ′))andS′(s):=TrapsupD(X, s)for some s∈ X. Then, the following chain of equations holds
fors∈ X.
D(supD)(s) =min(supD(s),bestExitValsupD(X))
=min(supD(s),max
s′∈XexitValsupD(X, s′)) (By Def. 28 (best exit value))
=min(supD(s),max
s′∈Xsup
ρ∈R′(s′)inf
σ∈S′(s′)B(sup
d∈Dd)(s′,ρ, σ)) (By Def. 27 (exit value))
=min(supD(s),max
s′∈Xsup
d∈Dsup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ))(Bellman operator is Scott-continuous (see proof Theorem 9) )
29=min(supD(s),sup
d∈Dmax
s′∈Xsup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ)) (By Claim 7)
=min(supD(s),sup
d∈Dmax
s′∈XexitVald(X, s′)) (By Def. 27 (exit value)))
=min(supD(s),sup
d∈DbestExitVald(X)) (By Def. 28 (best exit value))
=sup
d∈Dmin(d(s),bestExitVald(X)) (min is Scott-continuous)
=sup
d∈DD(d)(s). (SinceDis the deflate operator)
Claim 7: Since maxs′∈XexitValsupD(X, s′)exists the following chain of equations holds:
sup
d∈Dmax
s′∈Xsup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ)
=sup
d∈Dsup
s′∈Xsup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ) (As the maximum exists)
= sup
(d,s′)∈(D×X)sup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ)
=sup
s′∈Xsup
d∈Dsup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ)
=max
s′∈Xsup
d∈Dsup
ρ∈R′(s′)inf
σ∈S′(s′)B(d)(s′,ρ, σ).
▲
We have shown that BandDare both Scott-continuous, thus the composition (D◦B), i.e. the sequential application of the
operators BandD, is also continuous [44]. Further, Vis a complete partial order. Therefore, Kleen’s Fixpoint Theorem [13,
Theorem 8.15] is applicable.
Then, we know that
lim
k→∞(D◦B)k(⊥) =sup
k≥0(D◦B)k(⊥) (⋆)
holds and by the theorem we know that the fixpoint exists and is given by supk≥0(D◦B)k(⊥). Now, we can finally conclude:
(D◦B)( lim
k→∞(D◦B)k(⊥))(⋆)= (D◦B)(sup
k≥0(D◦B)k(⊥))
=sup
k≥0(D◦B)((D◦B)k(⊥)) (since (D◦B)is continuous)
=sup
k≥1(D◦B)k(⊥)
=sup
k≥0(D◦B)k(⊥) (since ⊥ ⊑(D◦B)k(⊥)for all k)
= lim
k→∞(D◦B)k(⊥). by (⋆)
Now we can prove the following theorem.
Theorem 37 (Soundness and completeness - Proof in App. C-D).ForCSGs Alg. 1, using Alg. 3 as DEFLATE , produces
monotonic sequences Lunder- and Uover-approximating VR, and terminates for every ε >0.
Proof. We denote by LkandUkthe lower and upper bound function after the k-th call of DEFLATE .LkandUkare monotonic
under-, respectively, over-approximation of VRbecause they are updated via Bellman updates respectively (D◦B)-updates,
which are order-preserving as shown in Lem. 36 and soundness as shown in Lem. 35 .
Since DEFLATE iterates over finite sets, the computations take a finite time. Thus, it remains to prove that the main loop of
Alg. 1 terminates, i.e., for all ε >0, there exists an n∈Nsuch that for all s∈S Un(s)−Ln(s)≤ε. It suffices to show that
limk→∞Uk−VR= 0, because limk→∞Lk=VR(from e.g. [42]).
In the following let U⋆:= lim k→∞Uk, that exists by Lemma 48, and ∆(s) := U⋆(s)−VR(s). Assume towards a
contradiction that the algorithm does not converge, i.e., there exists a state s∈Swith∆(s)>0.
The proof is structured as follows.
•From ∆>0we derive that there has to exist a BEC.
30•The states with the maximal ∆contain BECs.
•Alg. 1 will find a BEC contained in X ⊆ Sand deflate it.
•Deflating will decrease the upper bound of that states contained in the BEC , which is a contradiction because by Lemma
52,U⋆is a fixpoint.
Let∆max:=maxs∈S∆(s)and let C:={s∈S|∆(s) = ∆max}. IfCdoes not contain any BECs , then the contraposition
of Thm. 9 proves our goal. Thus, we continue with the assumption that Ccontains BECs.
LetX ⊆ Cbe a BEC contained in Cthat Alg. 3 will eventually find and deflate. We now consider bottom BECs . ABECX′
is called bottom in Xif none of the successors of a strategy that leaves the BECX′, is part of another BEC inX. A bottom
BEC can be computed by first finding the bottom MEC within X, then identifying all BECs , ordering them topologically and
finally picking the one at the end of a chain.
LetX′⊆ X be a bottom BEC that Algorithm 3 eventually finds. In order to deflate the BEC , we need to estimate the exit
value for each s∈ X′, as defined in Definition 27. Further, Hazard U⋆(X′, s)is the set of hazardous strategies and since X′is
a BEC, at all states s∈ X′, we have Hazard U⋆(X′, s)̸=∅.
We distinguish two cases: (i) TrapU⋆(X′, s)̸=∅; and (ii) TrapU⋆(X′, s) =∅.
Case (i) In this case we have that TrapU⋆(X′, s)̸=∅. Then Defl U⋆(X′, s)̸=∅(and because X ∩WS=∅), thus the following
chain of equations holds.
U⋆(s) = sup
ρ∈R(s)inf
σ∈S(s)B(U⋆)(s,ρ, σ) (U⋆is a fixpoint)
= sup
ρ∈Hazard U⋆(X′,s)inf
σ∈TrapU⋆(X′,s)B(U⋆)(s,ρ, σ) (Hazard U⋆(X′, s)̸=∅)
> sup
ρ∈Defl U⋆(X′,s)inf
σ∈TrapU⋆(X′,s)B(U⋆)(s,ρ, σ) (Defl U⋆(X′, s)are sub-optimal with respect to U⋆)
=exitValU⋆(X′, s) (By Def. 27)
Since bestExitValU⋆(X′) =maxs′∈X′exitValU⋆(X′, s′), and above we have shown that for all s∈ X′it holds U⋆(s)>
exitValU⋆(X′, s), we obtain a contradiction to the assumption that U⋆is a fixpoint.
Case (ii) In this case it holds that TrapU⋆(X′, s) =∅. Thus, Player Sprefers strategies that are leaving with respect to
Hazard U⋆(X, s). Since X′is a bottom BEC at least one successor state does not belong to X. Therefore, the following
chain of equations holds.
∆max−VR(s) =U⋆(s) (By definition of ∆max)
=sup
ρ∈R(s)inf
σ∈S(s)B(U⋆)(s,ρ, σ) (U⋆is a fixpoint)
=sup
ρ∈R(s)inf
σ∈S(s)X
(a,b)∈AX
s′∈SU⋆(s′)·δ 
s,a,b 
s′
·ρ(a)·σ(b) (Unfolding definition of B)
=sup
ρ∈R(s)inf
σ∈S(s)X
(a,b)∈AX
s′∈XU⋆(s′)·δ 
s,a,b 
s′
·ρ(a)·σ(b)
+X
s′′∈S\XU⋆(s′′)·δ 
s,a,b 
s′′
·ρ(a)·σ(b) (Player Sprefers leaving X′and thus X)
=sup
ρ∈R(s)inf
σ∈S(s)X
(a,b)∈AX
s′∈XU⋆(s′)·δ 
s,a,b 
s′
·ρ(a)·σ(b)
+X
s′′∈S\X
∆(s′′)|{z}
<∆max+VR(s′′)
·δ 
s,a,b 
s′′
·ρ(a)·σ(b) (By def. of ∆)
<sup
ρ∈R(s)inf
σ∈S(s)X
(a,b)∈AX
s′∈XU⋆(s′)·δ 
s,a,b 
s′
·ρ(a)·σ(b)
+X
s′′∈S\X 
∆max+VR(s′′)
·δ 
s,a,b 
s′′
·ρ(a)·σ(b)
(∆for states outside Xis strictly smaller than ∆max)
=U⋆(s) (Everything sums-up to 1)
Thus, we get a contradiction to the assumption that U⋆is a fixpoint.
31s4 s3 s5 s0
s1s2
(□,c)
(b,□)(□,d) (a,□) (a,d)
(a,c),(b,d)(b,c)(□,□)
(□,□)(□,□)
Fig. 6: Counter example for the BVI provided in [7].
APPENDIX D
COUNTER -EXAMPLES FOR PREVIOUS WORKS
A. Mistake in [9]
In [9], the authors provide an exemplary CSG that illustrates the incorrectness of the BVI algorithm presented in [7]. Fig. 6
shows the CSG . The value attainable at s5is 0.6 while the value attainable at s0is2−√
2. In our algorithm (Alg. 3) the set
{s4, s3}does not constitute a BEC for the valuation U(s4) =U(s3) =U(s1) = 1 andU(s2) = 0 . Consequently, the while-loop
of Alg. 3, which is responsible for deflating, is not executed and thus the values of all states are updated only using the Bellman
operator. This yields the correct values by Thm. 9. Therefore, Alg. 3 correctly sets the valuation of states s4ands3to the
value 0.6. In contrast, the algorithm presented in [7] correctly sets the value of state s4to 0.6 but reduces the value of state s3
to2−√
2<0.6.
B. Mistake in [19]
There was an attempt to fix the above problem in [19], however, also this solution is incorrect. An exemplary CSG that
illustrates the incorrectness of the approach presented in [19] is the CSG illustrated in Fig. 2. While our approach correctly
deflates the value of state s0, the best exit as defined in [19] is falsely 1, i.e. the value of s0is never reduced.
32