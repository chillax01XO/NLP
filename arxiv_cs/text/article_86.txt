arXiv:2505.21026v1  [eess.SY]  27 May 2025IEEE TRANSACTIONS ON CYBERNETICS, APRIL 2025 1
Multi-Mode Process Control Using Multi-Task
Inverse Reinforcement Learning
Runze Lin , Junghui Chen , Biao Huang ,Fellow, IEEE , Lei Xie , Hongye Su ,Senior Member, IEEE
Abstract —In the era of Industry 4.0 and smart manufacturing,
process systems engineering must adapt to digital transformation.
While reinforcement learning offers a model-free approach to
process control, its applications are limited by the dependence
on accurate digital twins and well-designed reward functions.
To address these limitations, this paper introduces a novel
framework that integrates inverse reinforcement learning (IRL)
with multi-task learning for data-driven, multi-mode control
design. Using historical closed-loop data as expert demonstra-
tions, IRL extracts optimal reward functions and control policies.
A latent-context variable is incorporated to distinguish modes,
enabling the training of mode-specific controllers. Case studies
on a continuous stirred tank reactor and a fed-batch bioreactor
validate the effectiveness of this framework in handling multi-
mode data and training adaptable controllers.
Index Terms —multi-mode process, data-driven controller de-
sign, deep reinforcement learning, inverse reinforcement learning,
multi-task learning.
I. I NTRODUCTION
IN the era of Industry 4.0 and smart manufacturing, intel-
ligent process control systems have become increasingly
pivotal. The integration of AI into science and engineering
has ushered in a new paradigm, with Deep Reinforcement
Learning (DRL) offering transformative potential for modern
process industries. In recent years, there has been growing
interest in applying DRL to process control, leading to sig-
nificant advancements in both continuous and batch process
applications [1], [2]. A key development in this area is
the incorporation of transfer learning into DRL frameworks,
enhancing the safety, robustness, and practical deployment of
process control systems [3]–[8].
Current research has yet to fully address key challenges
in applying DRL to process control, such as the high costs of
trial-and-error learning, low sample efficiency, and exploration
instability. Industrial settings have amassed vast amounts of
historical closed-loop data from PLC or DCS-control opera-
tions. However, traditional control strategies like MPC, as well
as advanced algorithms such as DRL, have not fully harnessed
This work was supported in part by the National Key R&D Program
of China under Grant 2022YFB3305903, and the National Science and
Technology Council, Taiwan under Grant NSTC 113-2221-E-033-003.
R. Lin, L. Xie, and H. Su are with the State Key Laboratory of
Industrial Control Technology, Institute of Cyber-Systems and Control,
Zhejiang University, Hangzhou 310027, China (e-mail: rzlin@zju.edu.cn;
leix@iipc.zju.edu.cn; hysu@iipc.zju.edu.cn).
J. Chen is with the Department of Chemical Engineering, Chung-
Yuan Christian University, Taoyuan 32023, Taiwan, R.O.C. (e-mail: ja-
son@wavenet.cycu.edu.tw).
B. Huang is with the Department of Chemical and Materials Engi-
neering, University of Alberta, Edmonton, AB T6G 2G6, Canada (e-mail:
biao.huang@ualberta.ca).the valuable insights embedded in this industrial big data.
Extracting control patterns from real-world closed-loop data
could serve as a robust foundation for DRL transfer learning
[9], [10].
Learning from demonstrations involves various techniques
for training Reinforcement Learning (RL) controllers using
expert demonstrations [11], with imitation learning being
a widely used approach. In industrial production, abundant
historical closed-loop data can be utilized to derive controller
priors through imitation learning [12]. These RL controllers
can then be fine-tuned via transfer learning in real-world
processes, improving the safety of DRL training. A basic
method, behavior cloning, fits “state xt-action ut” pairs from
expert trajectories. In contrast, Inverse RL (IRL), particularly
Adversarial IRL (AIRL) [13], offers a more sophisticated
approach by framing learning from demonstrations as a proba-
bilistic inference problem [14]. Nevertheless, conventional IRL
methods struggle to address the multi-mode nature of process
control, where varying operating modes lead to distinct data
distributions, complicating their application in multi-mode
controller design.
Operating processes across different modes is far from ideal,
yet it remains a common and realistic challenge that poses
significant difficulties for DRL. PSE researchers are well-
acquainted with multi-mode processes, particularly in fields
such as data-driven process monitoring, soft sensing, and fault
diagnosis, where multi-mode modeling is frequently required
[15]. This necessitates the development of models tailored
to distinct data distributions. Common approaches include
Kernel Principal Component Analysis, Kernel Partial Least
Squares, Gaussian Mixture Models, Variational Autoencoders,
and InfoGAN [16]. These methods typically utilize latent
variables to capture the variability across operating modes and
data distributions in the modeling process.
However, while these methods have advanced modeling
capabilities, they fall short in addressing the complexities
of optimizing and controlling multi-mode systems. Robust
control, though capable of managing variability, is often overly
conservative. Approaches like controller fusion, weighting or
gain scheduling show promise but require manual design for
each mode and tailored integration strategies. These methods
also struggle with generalization, particularly when confronted
with extrapolated conditions or unforeseen modes. Addition-
ally, MPC relies on predefined models to capture process dy-
namics, but in multi-mode scenarios, model-plant mismatches
frequently arise, leading to suboptimal control performance.
Although different modes involve distinct characteristics,
many share common features since only certain operatingThis work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.2 IEEE TRANSACTIONS ON CYBERNETICS, APRIL 2025
conditions or materials vary. This observation leads to an
important question: Can concepts from multi-mode modeling
be leveraged to design a multi-mode controller using latent
variables? Given that IRL offers a robust probabilistic infer-
ence framework for offline, data-driven control design [9],
this paper proposes augmenting IRL with latent variables
and multi-task learning. The approach enables the learning of
multi-mode controller priors from historical data across differ-
ent modes, creating a universal controller architecture within
the IRL framework. This facilitates multi-mode adaptability,
as multi-task learning integrates prior knowledge from various
modes [17]. Therefore, the multi-mode controller can quickly
adapt to new, unseen scenarios.
This paper introduces a novel framework for developing
a multi-mode process controller using multi-task IRL. The
proposed method incorporates adversarial reward learning,
variational inference, and MaxEnt IRL to train a purely data-
driven controller capable that can adapt to various operating
modes. To the best of our knowledge, this is the first study
to apply multi-task IRL for designing multi-mode process
controllers in a data-driven context. This approach offers
several advantages: 1) reducing the safety risks associated
with DRL’s direct interaction with the environment, 2) fully
utilizing the latent controller features embedded in historical
industrial data, and 3) addressing the complexities of multi-
mode control design through multi-task learning.
The remainder of this paper is structured as follows: Section
II reviews the preliminaries, including RL, IRL, and AIRL.
Section III outlines the motivation and problem statement. In
Section IV, the proposed methodology is presented. It details
the multi-task IRL approach for addressing the multi-mode
process control problem. Section V covers the experimental
setup and result analysis. Finally, Section VI concludes by
summarizing the key contributions of this work.
II. P RELIMINARIES
A. Markov decision process (MDP) and RL
The MDP in RL is defined by a tuple (X,U, pX, η, r, γ ), where
XandUrepresent the state and action spaces respectively,
pX:X×U×X → [0,1]denotes the state transition probability
of the next state xt+1∈ X given the current state xt∈ X and
action ut∈ U,η:X → P (X)is the initial state distribution,
r:X × U → Rspecifies the reward function rt∆=r(xt, ut),
andγ∈(0,1]is the discount factor. Let πdenote the control
policy that selects the optimal action ut∼π(ut|xt)based on
the current state xt, and the state-action marginal distribution
induced by the policy is represented as ρπ(xt, ut).
The objective of RL is to learn a control policy that max-
imizes long-term (discounted) rewards through interactions
with the environment Ein a trial-and-error manner. Formally,
the standard RL problem is defined as follows:
π∗= arg max
πTX
t=1E(xt,ut)∼ρπ[r(xt, ut)] (1)B. Maximum entropy RL
The maximum entropy (MaxEnt) RL objective is defined as:
π∗= arg max
πTX
t=1E(xt,ut)∼ρπ[r(xt, ut) +H(π(ut|xt))](2)
where H(π) =Eπ[−logπ(u|x)]is the entropy-regularization
term for the control policy. Unlike traditional RL, which
focuses solely on maximizing the expected sum of rewards,
MaxEnt RL aims to develop a policy that maximizes the
likelihood of exploring favorable conditions to achieve the
primary objective outlined in Eq. (1). This approach allows
the learned policy to account for sub-optimal or stochastic
behaviors that conventional RL may overlook, thereby pro-
viding valuable insights for inverse RL and enhancing transfer
learning through exploration.
In addition to Eq. (2), there is another explanation
and derivation of MaxEnt RL. Define the trajectory τ∆=
{x1:T, u1:T}as a sequence of state-action pairs generated by
a specific control policy π(ut|xt). The trajectory distribution
under this policy πcan be expressed as follows:
pπ(τ) =η(x1)TY
t=1p(xt+1|xt, ut)π(ut|xt) (3)
In the context of probabilistic graphical model, MaxEnt RL
reframes the RL control problem as an inference problem
that can be addressed using the probability theory. Assuming
the existence of a ground-truth reward function r(xt, ut), the
objective of MaxEnt RL is to learn a policy from the following
optimal trajectory distribution:
p(τ)∆=1
Zη(x1)TQ
t=1p(xt+1|xt, ut) exp( r(xt, ut))
∝
η(x1)TQ
t=1p(xt+1|xt, ut)
expTP
t=1r(xt, ut) (4)
where the partition function Z =R
η(x1)Q
tp(xt+1|xt, ut) exp( r(xt, ut))dτ acts as a
normalizer enforcing p(τ)∈[0,1]andR
p(τ)dτ= 1 .
Now let us consider the KL divergence between pπ(τ)and
p(τ)as follows:
−DKL(pπ(τ)||p(τ)) =Eτ∼pπ(τ)[logp(τ)−logpπ(τ)]
=E(xt,ut)∼pπ(xt,ut)[logη(x1)
+TX
t=1(logp(xt+1|xt, ut) +r(xt, ut))−logZ
−logη(x1)−TX
t=1(logp(xt+1|xt, ut)−logπ(ut|xt))#
=Eπ" TX
t=1r(xt, ut)−logπ(ut|xt)!
−logZ#
∝TX
t=1E(xt,ut)∼pπ(xt,ut)[r(xt, ut)−logπ(ut|xt)]
=TX
t=1E(xt,ut)∼ρπ[r(xt, ut) +H(π(ut|xt))].(5)LIN et al. : MULTI-MODE PROCESS CONTROL USING MULTI-TASK INVERSE REINFORCEMENT LEARNING 3
Therefore, the objective of MaxEnt RL is to maximize the
entropy-regularized long-term (discounted) rewards as shown
in Eq. (2). From another perspective, this is equivalent to
minimizing the KL divergence between 1) the target (optimal)
trajectory distribution p(τ)induced by r(xt, ut), and 2) the
trajectory distribution pπ(τ)generated by the MaxEnt RL
policy πthat is to be learned. The relationship presented in
Eq. (5) can be formally expressed as follows:
arg min
πDKL(pπ(τ)||p(τ))
= arg max
πTX
t=1E(xt,ut)∼ρπ[r(xt, ut) +H(π(ut|xt))].(6)
C. Maximum entropy inverse RL
Inverse RL aims to infer the intent (reward function) of
an expert by observing its behaviors, specifically through
optimal expert demonstrations (trajectories). MaxEnt IRL is a
classical IRL method within the above MaxEnt RL framework
[18], which simultaneously learns the reward function and
the policy based on the trajectories pπE(τ)generated by a
particular expert policy πE. Formally, the objective is to infer
a reward function rθ(xt, ut)parametrized by θ, using the
optimal trajectory distribution pθ(τ), similar to Eq. (4):
pθ(τ)∆=p(τ|θ) =1
Zθη(x1)TQ
t=1p(xt+1|xt, ut) exp( rθ(xt, ut))
∝
η(x1)TQ
t=1p(xt+1|xt, ut)
expTP
t=1rθ(xt, ut)
(7)
where the partition function is defined as:
Zθ∆=Z
η(x1)Y
tp(xt+1|xt, ut) exp( rθ(xt, ut))dτ (8)
Then the objective of MaxEnt IRL is to address the maximum
likelihood estimation (MLE) problem as follows:
arg min
θDKL(pπE(τ)||pθ(τ)) = arg max
θEpπE(τ)[logpθ(τ)]
=Eτ∼πE"TX
t=1rθ(xt, ut)#
−logZθ
(9)
D. Adversarial inverse RL (AIRL)
However, the term Zθin the MLE problem in Eq. (9)
can become computationally intractable when the state-action
spaces are large or even continuous. Additionally, MaxEnt IRL
requires explicit knowledge of the environment’s dynamics,
which is often infeasible. To address these issues, adversarial
IRL (AIRL) [13] is proposed to cast optimization of Eq. (9)
as a GAN problem. This approach also extends conventional
methods that learn from entire trajectories into just learning
over single state-action pairs. The discriminator Dθin the
AIRL algorithm is chosen as a particular form:
Dθ(x, u) =exp{rθ(x, u)}
exp{rθ(x, u)}+πω(u|x)(10)
where rθ(x, u)is the learned reward function and πω(u|x)
is the corresponding policy induced by the reward rθ. In Eq.(10), πω(u|x)is precomputed as a filled-in value for Dθ. The
discriminator aims to distinguish between the samples from
the expert demonstrations and those generated by the current
policy πω. In the AIRL algorithm, the policy πωis trained
to maximize Eρπω[logDθ(x, u)−log(1−Dθ(x, u))], which
is equivalent to maximizing the objective of an MaxEnt RL
policy as follows:
Eπω"TX
t=1log(Dθ(xt, ut))−log(1−Dθ(xt, ut))#
=Eπω"TX
t=1rθ(xt, ut)−logπω(ut|xt)# (11)
Therefore, the objective of the discriminator in the GAN-
inspired IRL framework aligns precisely with learning the
reward function. Simultaneously, the policy (generator) is
adjusted to make it increasingly difficult for the discriminator
to distinguish between expert demonstrations and samples
generated by the policy. It can be demonstrated that, when
trained to optimality, the learned reward function rθ(x, u)can
recover the ground-truth reward up to a constant, provided that
the true reward is solely a function of state [13].
III. P ROBLEM STATEMENT
A. Multi-mode process control problem
The general state-space representation of a control system can
be described as follows:
xt+1∼p(xt+1|xt, ut) (12)
ut∼p(ut|xt, ω)∆=πω(ut|xt) (13)
where p(ut|xt, ω)is the conditional distribution of actions
explicitly denoted as a ω-parameterized policy πω(ut|xt)to
emphasize the role of the control policy.
Utilizing the system dynamics model and controller de-
scribed above, the evolution trajectory of the MDP unfolds
from the initial state as follows:
p(τ) =p(x1, ut, . . . , x T, uT|ω)
=η(x1)TY
t=1p(xt+1|xt, ut)πω(ut|xt).(14)
However, in process control scenarios, many controlled
processes naturally exhibit multi-mode behaviors. These pro-
cesses often operate under a variety of modes or working
conditions, which may include different optimization and
control setpoints, varying feed compositions (recipes), distinct
system parameters, and even different equipment scales. Such
variability leads to different dynamic models for each operat-
ing mode. In this paper, the term “multi-mode processes” is
used broadly to describe processes characterized by multiple
distinct models from their corresponding operating scenarios.
Formally, each mode within a multi-mode control system
possesses unique characteristics, reflecting inherent differences
in their dynamics p(xt+1|xt, ut). Nevertheless, these operating
modes also share certain common features, which adhere to
an underlying probability distribution p(·|·). Unlike the gen-
eral control systems described in Eqs. (12)-(13), the optimal4 IEEE TRANSACTIONS ON CYBERNETICS, APRIL 2025
Fig. 1. Multi-task inverse reinforcement learning framework for designing
multi-mode process control systems.
controllers for each mode in multi-mode processes are not
necessarily consistent. This inconsistency necessitates the de-
velopment of rational and suitable mathematical formulations
to accurately describe the control problem for multi-mode
processes.
Given a process system with M∈N+operating modes,
each associated with a different optimal or near-optimal con-
troller πE∆={π1
E, π2
E,···, πM
E}, where the optimal controller
for mode mis denoted as πm
E, the multi-mode controller πE
will generate Mdifferent but structurally similar trajectory
distributions under the dynamics specific to each mode. Fig.
1 presents the overall concept of the proposed framework.
In multi-mode control systems, multiple expert controllers
correspond to different operating modes, leading to distinct
distributions of expert trajectories. While the trajectory of
each mode captures its unique dynamic behavior, the overall
closed-loop process control system exhibits inherent simi-
larities across modes due to shared system architecture and
underlying trajectory distributions.
Therefore, the multi-mode process control problem is to
learn a universal controller πω(ut|xt)that can adapt to various
control objectives across different operating modes. This paper
aims to leverage historical industrial closed-loop big data
and multi-task IRL to learn controller priors from mode-
specific trajectory distributions, enabling efficient few-shot
adaptation during implementation. In other words, πω∆=
{π1
ω, π2
ω,···, πM
ω}should be capable of providing optimal
or near-optimal control policies for all pm(xt+1|xt, ut), i.e.,
πω.=πEshould be approximately satisfied for each mode.
B. Context-conditional multi-task controller learning problem
With the vast amount of historical closed-loop operational data
available in industrial production, leveraging this multi-mode
data for offline controller design presents both significant
challenges and exciting opportunities. Developing an initial
controller from such data could greatly enhance the practi-
cality and safety of DRL applications. This paper proposes
a framework for designing a multi-mode process controller
based on IRL. The objective is to develop a fully closed-loop-
data-driven controller that can effectively adapt to different
operating modes.
As analyzed in Section III-A, assuming the controlled
system operates under Mdistinct modes, corresponding to M
distinct optimal or near-optimal controllers, the operation of
this multi-mode process control system will generate a largevolume of historical closed-loop data. Over time, this data
will encapsulate Mdifferent trajectory distributions, which
are implicitly and structurally embedded within the database.
The next step is to formally define the problem: how can a
multi-task IRL approach be employed to effectively solve the
multi-mode process control challenge?
This work introduces the concept of contextual policy to re-
frame the multi-task RL (IRL) for multi-mode process control
as the problem of solving a context-conditional MDP using a
latent context model. In a multi-mode process control system
with structurally similar dynamic models and controllers, the
MDP is augmented by a latent context variable, capturing
dependencies across operating modes. This approach enables
the development of a universally structured controller that
can adapt to all modes while accounting for the unique
characteristics of each individual mode.
Letπ(ut|xt, z)represent the controller for each operating
mode. Unlike conventional RL and IRL, where the optimal
control policy is defined solely based on the current state xt,
this framework incorporates a mode-specific representation.
The latent context variable zis introduced as an additional
explicit dependency in the conditional policy π(·|·, z)for each
mode. By incorporating probabilistic latent variable model-
ing, the generative model for the expert trajectory τz
E∆=
{x1:T, u1:T}zunder the z-dependent operating mode can be
expressed as:
x1∼η(x1), z∼p(z), ut∼π(ut|xt, z), xt+1∼p(xt+1|xt, ut)
(15)
The joint distribution of the latent variable zand the trajectory
τz
Ecan be expressed as:
pπE(z, τ) =p(z)pπE(τ|z) (16)
The marginal distribution of the overall historical dataset,
which consists of data from different modes, can be repre-
sented as:
pπE(τ) =Z
ZpπE(z, τ)dz=Z
Zp(z)pπE(τ|z)dz (17)
where p(z)represents the prior distribution of the latent
context variable (i.e., the mode-indicating variable), η(x1)and
p(xt+1|xt, ut)respectively denote the probability distributions
for the initial state and state transition, and pπE(τ|z)represents
the context-conditional trajectory distribution.
The objective is to learn the control policies πω∆=
{π1
ω, π2
ω,···, πM
ω}for each mode, along with the correspond-
ing optimality or rationality descriptions (which, in the context
of DRL, represent the reward functions) based on the overall
marginal distribution pπE(τ). However, the probabilistic latent
variable formulation above abstracts a complex multi-mode
process control problem. In real-world industrial processes,
the necessary assumptions may not hold. Specifically, there is
often limited or no knowledge of the prior distribution p(z)
representing the mode-indicating variable zin Eq. (15) and Eq.
(17), and the conditional trajectory distribution pπE(τ|z)for
different modes may be unknown. In other words, in practice,
only the marginal distribution pπE(τ)of the entire set of expert
trajectories is observable, which makes learning from multi-
mode demonstrations particularly challenging.LIN et al. : MULTI-MODE PROCESS CONTROL USING MULTI-TASK INVERSE REINFORCEMENT LEARNING 5
IV. M ETHODOLOGY
A. From single-task to multi-task MDP
To apply a multi-task IRL approach for learning controllers
from historical closed-loop operating data of multi-mode pro-
cesses, the conventional single-mode MDP definition must
be extended. Specifically, the original MDP is modified and
augmented by introducing a conditional term based on z∈ Z,
where Zrepresents the value space of the latent context
variable z. Consequently, each MDP component—except for
the state transition probability determined by system dynam-
ics—will now include an additional dependency on z. This
enables the MDP to effectively capture the varying character-
istics of different operating modes within the process.
Based on the latent context model, the context-conditional
policy is now defined as π:X × Z → P (U), and the
corresponding reward function is modified to r:X ×U×Z →
R. This generalized MDP formulation enables the controller
design for each mode to be conditioned on the latent context
z. In other words, with different predetermined values of z, the
multi-task IRL agent can be trained in a mode-specific manner
for various control tasks that share common structures or
feature spaces, allowing for efficient adaptation across multiple
operating modes.
Building upon the MaxEnt RL framework in Eq. (2), in
multi-mode scenarios, the optimal context-conditional policy
can be calculated as:
πz
E←π∗= arg max
πEz∼p(z),(x1:T,u1:T)∼pπ(·|z)
"TX
t=1r(xt, ut, z)−logπ(ut|xt, z)#(18)
where r(xt, ut, z)is the mode-specific reward function,
and−logπ(ut|xt, z)is the entropy-regularization term for
the contextual policies. Analogous to Eq. (3), the context-
conditional distribution for the z-th expert trajectory pπE(τ|z)
in Eq. (17) can be formulated as follows:
τz
E∼pπE(τ|z) =pπE(x1:T, u1:T|z)
=η(x1)TY
t=1p(xt+1|xt, ut)πE(ut|xt, z)(19)
In summary, the multi-mode process control problem in
this paper is reframed as a context-conditioned multi-task IRL
training approach that effectively learns from historical multi-
mode closed-loop big data. This approach utilizes a set of
multi-task demonstrations τEi.i.d. sampled from the marginal
distribution pπE(τ)defined by Eq. (17) and Eq. (19), i.e.,
τE∼pπE(τ) =R
Zp(z)pπE(τ|z)dz
=R
Zp(z)η(x1)TQ
t=1p(xt+1|xt, ut)πE(ut|xt, z)dz(20)
The context-conditioned multi-task IRL approach aims to
uncover historical multi-mode patterns and subsequently learn
a multi-mode controller prior in a purely data-driven man-
ner. This methodology enables the identification of distinct
operating modes and their corresponding control strategies,
supporting the development of a robust controller capable of
adapting to varying conditions using only historical data.B. Latent context inference model for multi-task learning
Since both the reward function and the control policy are
conditioned on zin the generalized MDP, estimating this
latent variable is essential for multi-task IRL-based controller
learning. To achieve this, a probabilistic inference model
q(z|τ)should be introduced to approximate the true posterior
distribution p(z|τ=τz
E), which is generally inaccessible. By
denoting the inference model as a variational approximation
for calculating z, the context-conditional reward function
rIRL(x, u, z )(which needs to be learned) can be determined
using the inferred z.
Specifically, when given a set of demonstrations sampled
from the mode prior and the conditional distribution z∼
p(z), τz
E∼pπE(τ|z), the inference model can be employed
to estimate the latent context variable ˆz∼q(z|τz
E). Once the
mode-indicating variable ˆzis inferred, it can be substituted into
the learned reward rIRL(x, u, ˆz). The DRL agent, guided by
this context-conditional reward, should then generate policies
that closely resemble those driven by the true underlying
reward r(x, u, z ). Successfully training the multi-task IRL
agent with latent dependencies equips the IRL-based process
controller to effectively manage scenarios characterized by
multi-mode behaviors.
C. Multi-task IRL using context-conditional probabilistic in-
ference
The next step involves analyzing how to address the multi-task
IRL problem to facilitate the learning of a multi-mode process
controller. Drawing from the MLE approach for MaxEnt IRL
outlined in Section II (Eq. (9)), and considering the previously
defined multi-task IRL problem based on latent context model,
the context-conditional trajectory distribution, parameterized
by the reward parameter θ, can be derived as follows:
τz
θ∼pθ(τ|z) =pθ(x1:T, u1:T|z)
=1
Zθ[η(x1)p(xt+1|xt, ut)] expTP
t=1rθ(xt, ut, z)(21)
where the conditional input zis inferred using an inference
model qψ(z|τ)parametrized by ψ,rθis the reward function in
the multi-task IRL, while Zθis the partition function. To de-
velop a multi-mode process controller, this framework ensures
that the context-conditional trajectory distribution accurately
captures the dependencies imposed by the latent context.
Therefore, the primary goal of multi-task IRL is to solve an
MLE problem:
arg min
θEp(z)[DKL(pπE(τ|z)||pθ(τ|z))]
= arg max
θEp(z),pπE(τ|z)[logpθ(τ|z)]
= arg max
θEz∼p(z),τ∼πz
E"TX
t=1rθ(xt, ut, z)#
−logZθ(22)
To achieve this, various IRL algorithms, such as AIRL, can be
employed to minimize the KL divergence between the optimal
or near-optimal expert trajectory distribution and the θ-induced
trajectory distribution. The underlying objective of IRL agent
is to match the trajectory distribution of τ={x1:T, u1:T}.6 IEEE TRANSACTIONS ON CYBERNETICS, APRIL 2025
However, the challenge arises because every trainable term
in Eq. (22) is conditioned on the latent context z, which
is not directly related to the IRL optimization process. This
limitation complicates the learning process, as the IRL agent
struggles to effectively optimize distinct modes without a
well-defined association between trajectory distributions and
latent contexts. In other words, an explicit correlation between
theθ-induced trajectory τand the latent variable zmust be
enforced to distinguish the conditional reward function and the
corresponding policy within each mode.
Building on the principles of InfoGAN [16], mutual infor-
mation (MI) between zandτcan be applied as a constraint
or correlation measure to enhance the dependency between
the latent context and the resulting trajectory. Within the
framework of multi-task IRL, a higher MI value indicates a
stronger correlation, thereby improving the interpretability of z
in relation to the trajectory τz. The MI under joint distribution
pθ(z, τ)is calculated as follows:
Ipθ(z;τ) =H(z)− H(z|τ)
=H(z) +Z
zZ
τpθ(z, τ) logpθ(z, τ)dzdτ
=H(z) +Z
zZ
τp(z)pθ(τ|z) logpθ(z|τ)dzdτ
=H(z) +Ez∼p(z),τ∼pθ(τ|z)[logpθ(z|τ)]
=Ez∼p(z),τ∼pθ(τ|z)[logpθ(z|τ)−logp(z)](23)
Since pθ(z|τ)is the posterior distribution that is unknown,
the probabilistic inference model qψ(z|τ)can serve as a
variational approximation of this posterior, and then Eq. (23)
can be reformulated as:
Ipθ(z;τ) =H(z) +Ez∼p(z),τ∼pθ(τ|z)[logpθ(z|τ)]
=H(z) +Ez∼p(z),τ∼pθ(τ|z)[DKL(pθ||qψ)|{z}
≥0+ log qψ(z|τ)]
≥ H(z) +Ez∼p(z),τ∼pθ(τ|z)[logqψ(z|τ)]
=Ez∼p(z),τ∼pθ(τ|z)[logqψ(z|τ)−logp(z)]
=LI(pθ, qψ)
(24)
where LI(pθ, qψ)is the variational lower bound of the MI.
The following analysis will outline the solution of the
multi-task IRL problem. In this context, Eq. (22) represents
the primary objective based on the MaxEnt principle, while
Eq. (24) introduces an additional regularization term for the
latent context variable. Accordingly, the overall optimization
objective can be formulated as:
min
θ,ψEp(z)[DKL(pπE(τ|z)||pθ(τ|z))]
−α·Ipθ(z;τ) +β·Epθ(τ)[DKL(pθ(z|τ)||qψ(z|τ))](25)
The first term aims to align the conditional distributions
between the closed-loop expert trajectories and the IRL trajec-
tories generated by the θ-parameterized reward function and
the corresponding RL policy. This alignment represents the
primary objective of the context-conditional multi-task IRL
problem within the MaxEnt reward learning framework. The
second term seeks to maximize the MI between the context
and the corresponding trajectory, ensuring that the informationembedded in the latent context zis retained throughout the
training process [16]. Finally, the third term addresses the
alignment of the variational inference approximation qψ(z|τ)
with the true posterior pθ(z|τ)of the latent context, which is
required to train the inference model qψ.
For simplicity and without loss of generality, the tunable
hyperparameters can be treated as constants α=β= 1.
Consequently, Eq. (25) can be expressed as:
min
θ,ψEp(z)[DKL(pπE(τ|z)||pθ(τ|z))]
+Ez∼p(z),τ∼pθ(τ|z)
logp(z)
pθ(z|τ)+ logpθ(z|τ)
qψ(z|τ)
≡max
θ,ψ−Ep(z)[DKL(pπE(τ|z)||pθ(τ|z))]
+Ez∼p(z),τ∼pθ(τ|z)[logqψ(z|τ)−logp(z)]| {z }
LI(pθ,qψ)
= max
θ,ψ−Ep(z)
DKL(pπE(τ|z)||pθ(τ|z))−logp(z)|{z}
regardless of θ,ψ

+Ez∼p(z),τ∼pθ(τ|z)logqψ(z|τ)
= max
θ,ψ−Ep(z)[DKL(pπE(τ|z)||pθ(τ|z))]
+Ez∼p(z),τ∼pθ(τ|z)logqψ(z|τ)
= max
θ,ψ−Ep(z)[DKL(pπE(τ|z)||pθ(τ|z))] +Linfo(θ, ψ).
(26)
D. Practical implementation for solving multi-task IRL
At this stage, the optimization objective in Eq. (26) remains
intractable, as it is not feasible to approximate the prior distri-
bution p(z)and the conditional trajectory distribution pθ(τ|z)
by directly sampling from the marginal distribution pπE(τ)
(i.e., expert trajectory distribution), which encompasses multi-
mode process characteristics.
Fortunately, with the latent context inference model qψ(z|τ),
an estimate over the sampled expert trajectory can be used to
approximate the prior distribution p(z)as follows:
τE∼pπE(τ), z∼p(z).=qψ(z|τE) (27)
And the conditional trajectory distribution pθ(τ|z)can be
sampled from trajectories generated during the training process
of the forward DRL agent within the inner loop of the multi-
task IRL algorithm. This is feasible because, if the forward
DRL policy πωis trained to optimality, the resulting trajectory
distribution pπ∗ω(τ|z)induced by the optimal policy π∗
ωwill
match the conditional trajectory distribution pθ(τ|z)[19].
With the approximately sampled p(z)andpθ(τ|z), the
second term in Eq. (26) can be optimized with respect to θ
andψ. For the first KL divergence minimization term, any
adversarial IRL algorithms such as AIRL can be applied.
The only adjustment needed is to augment the RL state
with an additional input dependency—the latent context z.
This modification allows the IRL policy to be conditioned to
πω(ut|xt, z), where ⟨x, z⟩serves as the augmented MDP state
in the practical implementation of the algorithm.LIN et al. : MULTI-MODE PROCESS CONTROL USING MULTI-TASK INVERSE REINFORCEMENT LEARNING 7
Fig. 2. Flowchart of the proposed multi-task inverse reinforcement learning
scheme.
Based upon the above analysis, the overall objective of the
context-conditional multi-task IRL algorithm can be expressed
as follows:
min
ωmax
θ,ψEpπE(τ),qψ(z|τE),ρπω(x,u|z)log(1−Dθ(x, u, z ))
+EτE∼pπE(τ),z∼qψ(z|τE)log(Dθ(x, u, m )) +Linfo(θ, ψ)
(28)
where
Dθ(x, u, z ) =exp{rθ(x, u, z )}
exp{rθ(x, u, z )}+πω(u|x, z)(29)
Therefore, the multi-task learning procedure for addressing
the multi-mode process control problem can be summarized
in Fig. 2 and Algorithm 1. This approach involves three neural
networks: the Generator (policy), the Discriminator (reward)
and the Inference Network (mode indicator). Unlike conven-
tional IRL, the proposed approach incorporates an additional
input dependency, i.e., the latent context z, resulting in an
augmented MDP state ⟨x, z⟩that is used to train both the
multi-mode policy and the reward. The process begins with the
Inference Network estimating the mode-specific latent context
of sampled trajectories. This inferred context is then fed into
both the policy and the reward function. The GAN-inspired
multi-task IRL framework utilizes a Generator to create virtual
data corresponding to the current multi-mode policy, while the
Discriminator defines the reward function by distinguishing
expert data from the generated virtual data. During training,
the outermost loop follows a GAN structure, where the Gener-
ator, acting as a DRL agent, updates the weights of the context-
conditional policy based on the latest reward. The internal
loop consists of any standard DRL algorithm, coupled with
a parallel mode inference module, ensuring efficient learning
across multiple modes. From the process control perspective,
once the training on multi-mode historical closed-loop data is
completed, the multi-task IRL-based controller can serve as an
initialized multi-mode controller, enabling adaptability across
those unseen operating modes in transfer learning settings.
V. R ESULTS &DISCUSSION
In this section, the proposed multi-mode controller learning
method is applied to two distinct cases. The first case involves
a fed-batch bioreactor, where the modes are characterized
by unique system dynamics. The second case examines a
continuous reactor, with modes defined by varying temperature
setpoints.Algorithm 1 Multi-task IRL training procedure
Input: Expert trajectories DE={τj
E}; Initial parameters of fθ, πω, qψ.
repeat
Sample two batches of unlabeled demonstrations: τE, τ′
E∼ DE
Infer a batch of latent context variables from the sampled demonstra-
tions: z∼qψ(z|τE)
Sample trajectories Dfrom πω(τ|z), with the latent context variable
fixed during each rollout and included in D.
Update ψto increase Linfo(θ, ψ)with gradients in Eq. (26), with
samples from D.
Update θto increase Linfo(θ, ψ)with gradients in Eq. (26), with samples
fromD.
Update θto decrease the binary classification loss:
E(x,u,z )∼D[∇θlogDθ(x, u, z )] +
Eτ′
E∼DE,z∼qψ(z|τ′
E)[∇θlog(1−Dθ(x, u, z ))]
Update ωwith “forward” RL to increase the following objective:
E(x,u,z )∼D[logDθ(x, u, z )]
until Convergence
Output: Learned inference model qψ(z|τ), reward function fθ(x, u, z )
and policy πω(u|x, z).
A. Case 1: A fed-batch bioreactor (batch profile optimization)
To validate the feasibility of the multi-task IRL solution
in recovering the reward function and addressing imitation
learning control policies, this case study uses the same photo-
production system as the numerical example presented in [20].
The process involves a fed-batch bioreactor that necessitates
solving a batch-to-batch optimization problem. Two distinct
operating modes are defined, each corresponding to a unique
set of internal system parameters. To implement this, the
dynamic model is modified to incorporate a mode variable:
dy1
dt=− 
u1+ 0.5u2
1
y1+u2
dy2
dt=u1y1−k·u2y1(30)
where u1, u2are the manipulated variables (i.e., light and an
inflow rate) and y1, y2are the outlet concentrations of the
reactant and product, respectively. The mode variable kis:
k=0.5,Mode 1
0.7,Mode 2(31)
The batch operation time course is normalized to 1, with
control actions constrained within the interval [0, 5].
The objective is to design a control policy that adjusts the
system inputs u1, u2to maximize the product concentration y2
at the end of the batch operation. Positive reward feedback is
only provided at the end of the batch operation, with rewards
set to penalize excessive action changes at all other intervals.
The reward function is defined as follows:
rt=−0.01× ∥u(t+ 1)−u(t)∥1, t = 0,1,···, T−1
rT=y2(T).
(32)
The aim is to train an IRL agent to autonomously discover
the optimal control policy from expert trajectories, ensuring
that the recovered reward function aligns with the true reward
function as described.
It is important to note that the experimental setup is highly
challenging for both RL and IRL due to the sparsity of
rewards, since positive feedback is granted only at the end of
the episode. In this context, if the multi-task IRL agent fails to
accurately infer the reward structures across different modes, it
risks generating ineffective control policies. This may, in turn,8 IEEE TRANSACTIONS ON CYBERNETICS, APRIL 2025
Fig. 3. Typical batch optimization profiles of the TRPO expert demonstrations
(left: Mode 1 k= 0.5; right: Mode 2 k= 0.7).
Fig. 4. Batch optimization profiles of the successfully trained multi-task IRL
agent based on the TRPO expert demonstrations (left: Mode 1 k= 0.5; right:
Mode 2 k= 0.7).
lead to inaccurate reward estimates, perpetuating a feedback
loop that hinders learning. Therefore, this setup serves as an
ideal testbed for rigorously assessing the effectiveness of the
multi-task IRL approach.
First, a DRL-based expert policy is developed using the
Trust Region Policy Optimization (TRPO) algorithm to maxi-
mize the reward function outlined in Eq. (32). In the RL setup,
the state is represented as X∆= [y1, y2]T, and the action as
U∆= [u1, u2]T. During training, each episode randomly selects
an operating mode as the environment. The trained TRPO
agent then generates expert demonstrations across both modes.
Specifically, 2,112 expert trajectories are collected, with each
comprising 20 samples. These trajectories are shuffled to sim-
ulate the mixed data sources typical in industrial production
involving multiple devices. The typical expert trajectories for
the two modes, k= 0.5andk= 0.7, are illustrated in Fig.
3, while the trajectories generated by the successfully trained
multi-task IRL agent are displayed in Fig. 4.
The results indicate that the multi-task IRL agent effectively
recovers both the control policy and reward function from
historical multi-mode data. At the endpoint of the batch
trajectories, the product concentrations for each mode dif-
fer, reflecting the agent’s ability to learn the batch-to-batch
optimization patterns unique to each operating mode. It is
worth noting that, because the multi-task IRL method learns
the reward function from historical data, it assumes expert
demonstrations as optimal. Consequently, the recovered reward
values are slightly lower than those of the expert, which is
expected. This approach, centered on purely offline training
of the multi-mode controller, is designed to fully leverage
the prior knowledge embedded in expert behaviors. Such
learned controller prior(s) can provide a strong foundation for
subsequent transfer learning applications.
FC
TCVC
ρC
TCV
CA
T
AO
FC
TCiCoolantFeedF
CAi
Ti
F
CA
TProductTT
21b TC
21TC
21
TY
21
I
PmTsetFig. 5. Sketch of the CSTR control system.
B. Case 2: A benchmark CSTR process (continuous control)
1) System description and problem formulation: To demon-
strate the effectiveness of the proposed method in continuous
control scenarios, a continuous stirred tank reactor (CSTR)
process is selected as the test case, depicted in Fig. 5. This
reactor operates as a jacketed, non-adiabatic tank that facili-
tates a single irreversible and exothermic first-order reaction.
The primary control objective is to maintain the reaction
temperature Tnear the target setpoint Tsetby adjusting the
valve opening m, which modulates the coolant feed flow
rate. Further details on the first-principles model and system
parameters are provided in [3].
Unlike the batch-to-batch optimization case, continuous
processes require dynamic control solutions. Designing control
strategies for continuous systems is considerably more com-
plex than for batch profile optimization. In this scenario, the
IRL agent faces increased difficulty due to the slow dynamic
characteristics inherent to continuous process control. This
necessitates a larger number of samples to effectively learn
the reward function and establish a robust control policy, while
also increasing instability in the training process.
In the experiment, two distinct modes are introduced to
represent the multi-mode control scenario: Mode 1: Setpoint
88→90°C, and Mode 2: Setpoint 88→86°C. Expert trajec-
tories from these modes are combined and randomly shuffled
to mimic the multi-mode nature of industrial big data. The IRL
agent is tasked with learning the characteristics of the expert
controller from a multi-mode dataset with varied distributions,
aiming to recover a reward function that can explain expert
behavior(s). Two types of expert sources are used to validate
the effectiveness of the multi-task IRL approach. The first type
consists of expert demonstrations directly generated by a DRL
agent. The second type uses multi-mode industrial closed-loop
data as the basis for expert trajectories.
In this case, the RL state is represented as X∆=
[CA, T, T C, b, Tset−T]T, and the action as U∆= [m]. For the
multi-mode CSTR control problem, the temperature setpoint
serves as the mode-indicating variable, which is unknown to
the agent. Consequently, the inference model is critical for
distinguishing between different operating modes within the
encapsulated mode-specific information.LIN et al. : MULTI-MODE PROCESS CONTROL USING MULTI-TASK INVERSE REINFORCEMENT LEARNING 9
Fig. 6. Typical control performances of the TRPO expert demonstrations
(left: Mode 1 Tset= 90 ; right: Mode 2 Tset= 86 ).
Fig. 7. Control performances of the successfully trained multi-task IRL agent
based on the TRPO expert demonstrations (left: Mode 1 Tset= 90 ; right:
Mode 2 Tset= 86 ).
2) DRL agent as optimal policy for generating expert
demonstrations: As in Case 1, the TRPO algorithm and the
reward function outlined in [3], [7] are employed to train
the DRL-based expert. A total of 2,112 expert trajectories
are collected across both modes. The typical TRPO expert
trajectories for each mode are displayed in Fig. 6, while the
trajectories generated by the trained multi-task IRL agent are
shown in Fig. 7. These results are obtained by deploying
the trained IRL controller directly into the environment for
validation. While a small residual error remains after the
controller stabilizes, the overall performance meets acceptable
standards. Furthermore, in industrial applications, the pre-
trained controller can be fine-tuned in real-world settings
to facilitate Sim2Real transfer learning [4], [21], potentially
enhancing control performance further.
3) Historical closed-loop operation data as expert trajec-
tories: To demonstrate the practical engineering potential and
feasibility of using multi-task IRL for multi-mode process
control, unknown expert demonstrations derived from histori-
cal closed-loop operating data will now serve as the training
source. Compared to using RL agents as expert(s), adopting a
classical control scheme (such as PID) as the expert controller
offers a more compelling test. This is because the behavior
patterns of RL agents and IRL agents are relatively similar,
making the trajectory distribution generated by RL well-suited
for IRL learning. In contrast, traditional PI control operates as
a feedback control law based on error and cumulative error,
presenting a notable challenge for IRL in accurately capturing
its control characteristics.
In this experiment, a well-tuned PI controller for the CSTR
system, is employed to generate closed-loop operating data for
training the IRL agent. To introduce variability and stochas-
ticity in the expert trajectories, white noise is applied to the
Fig. 8. Typical control performances of the PI expert demonstrations (left:
Mode 1 Tset= 90 ; right: Mode 2 Tset= 86 ).
Fig. 9. Control performances of the successfully trained multi-task IRL agent
based on the PI expert demonstrations (left: Mode 1 Tset= 90 ; right: Mode
2Tset= 86 ).
inlet concentration. A total of 2,112 expert trajectories are
recorded, each containing 300 samples (sampling interval Ts
= 10 seconds, corresponding to a total duration of 3,000
seconds for the system). The typical PI controller trajectories
for the two modes are depicted in Fig. 8, indicating that the
PI controller maintains satisfactory control performance.
The closed-loop expert demonstrations described above are
used for multi-task IRL training. The control performance
of the trained multi-task IRL-based controller across the two
modes is presented in Fig. 9. The results indicate that the
multi-task IRL agent effectively imitates expert behaviors and
naturally adapts to varying modes. As in the previous case, if
additional refinement is necessary, Sim2Real transfer learning
can be employed to minimize model-plant mismatches.
VI. C ONCLUSION
This paper presents a novel multi-task IRL approach aimed
at addressing the multi-mode process control problem, with
the primary objective of extracting controller patterns and RL
value information from closed-loop big data encompassing
multiple operating modes. To accomplish this, latent variables,
commonly used in multi-mode modeling, are integrated to
identify different mode indicators. Specifically, by introducing
a latent context variable, the proposed method first establishes
a mathematical framework to represent the conditional policy
and trajectory distribution. Subsequently, techniques such as
MaxEnt IRL, mutual information regularization, and varia-
tional inference are employed to optimize context-conditional
rewards and policies. Experimental results demonstrate that the
proposed method effectively learns a universal controller that
can adapt to various scenarios based on multi-mode historical
closed-loop data. This promising approach offers a probabilis-
tic inference-based solution for data-driven controller design10 IEEE TRANSACTIONS ON CYBERNETICS, APRIL 2025
and underscores the potential of context-conditional latent
variable modeling techniques in the development of multi-
mode process controllers.
REFERENCES
[1] R. Nian, J. Liu, and B. Huang, “A review on reinforcement learning:
Introduction and applications in industrial process control,” Computers
& Chemical Engineering , vol. 139, p. 106886, 2020.
[2] J. Shin, T. A. Badgwell, K.-H. Liu, and J. H. Lee, “Reinforcement
learning – overview of recent progress and implications for process
control,” Computers & Chemical Engineering , vol. 127, pp. 282–294,
2019.
[3] R. Lin, J. Chen, L. Xie, and H. Su, “Accelerating reinforcement learning
with case-based model-assisted experience augmentation for process
control,” Neural Networks , vol. 158, pp. 197–215, 2023.
[4] R. Lin, Y . Luo, X. Wu, J. Chen, B. Huang, H. Su, and L. Xie, “Surrogate
empowered Sim2Real transfer of deep reinforcement learning for ORC
superheat control,” Applied Energy , vol. 356, p. 122310, 2024.
[5] L. Zhang, R. Lin, L. Xie, W. Dai, and H. Su, “Event-triggered con-
strained optimal control for organic rankine cycle systems via safe
reinforcement learning,” IEEE Transactions on Neural Networks and
Learning Systems , vol. 35, no. 5, pp. 7126–7137, 2024.
[6] Y . Shi, R. Lin, X. Wu, Z. Zhang, P. Sun, L. Xie, and H. Su, “Dual-mode
fast DMC algorithm for the control of ORC based waste heat recovery
system,” Energy , vol. 244, p. 122664, 2022.
[7] R. Lin, J. Chen, L. Xie, and H. Su, “Accelerating reinforcement
learning with local data enhancement for process control,” in 2021 China
Automation Congress (CAC) , Conference Proceedings, pp. 5690–5695.
[8] H. Chang, Q. Chen, R. Lin, Y . Shi, L. Xie, and H. Su, “Controlling
pressure of gas pipeline network based on mixed proximal policy
optimization,” in 2022 China Automation Congress (CAC) , Conference
Proceedings, pp. 4642–4647.
[9] R. Lin, J. Chen, B. Huang, L. Xie, and H. Su, Developing Purely Data-
Driven Multi-Mode Process Controllers Using Inverse Reinforcement
Learning . Elsevier, 2024, vol. 53, pp. 2731–2736.
[10] M. Mowbray, R. Smith, E. A. Del Rio-Chanona, and D. Zhang, “Using
process data to generate an optimal control policy via apprenticeship
and reinforcement learning,” AIChE Journal , vol. 67, no. 9, p. e17306,
2021.
[11] S. Adams, T. Cody, and P. A. Beling, “A survey of inverse reinforcement
learning,” Artificial Intelligence Review , 2022.
[12] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in
Proceedings of the 30th International Conference on Neural Information
Processing Systems , ser. NIPS’16. Red Hook, NY , USA: Curran
Associates Inc., 2016, p. 4572–4580.
[13] J. Fu, K. Luo, and S. Levine, “Learning Robust Rewards with
Adversarial Inverse Reinforcement Learning,” arXiv e-prints , p.
arXiv:1710.11248, Oct. 2017.
[14] S. Levine, “Reinforcement Learning and Control as Probabilistic Infer-
ence: Tutorial and Review,” arXiv e-prints , p. arXiv:1805.00909, May
2018.
[15] L. Yao, B. Shen, L. Cui, J. Zheng, and Z. Ge, “Semi-supervised
deep dynamic probabilistic latent variable model for multimode process
soft sensor application,” IEEE Transactions on Industrial Informatics ,
vol. 19, no. 4, pp. 6056–6068, 2023.
[16] X. Chen, Y . Duan, R. Houthooft, J. Schulman, I. Sutskever, and
P. Abbeel, “InfoGAN: Interpretable representation learning by infor-
mation maximizing generative adversarial nets,” in Advances in Neural
Information Processing Systems , vol. 29, 2016, Conference Proceedings.
[17] Y . Zhang and Q. Yang, “A survey on multi-task learning,” IEEE
Transactions on Knowledge and Data Engineering , vol. 34, no. 12, pp.
5586–5609, 2022.
[18] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey, “Maximum
entropy inverse reinforcement learning,” in Proceedings of the 23rd
national conference on Artificial intelligence (AAAI) , vol. 3. AAAI
Press, Conference Proceedings, p. 1433–1438.
[19] L. Yu, T. Yu, C. Finn, and S. Ermon, “Meta-inverse reinforcement
learning with probabilistic context variables,” in Advances in Neural
Information Processing Systems , vol. 32. Curran Associates, Inc., 2019.
[20] P. Petsagkourakis, I. O. Sandoval, E. Bradford, D. Zhang, and E. A. del
Rio-Chanona, “Reinforcement learning for batch bioprocess optimiza-
tion,” Computers & Chemical Engineering , vol. 133, p. 106649, 2020.
[21] R. Lin, J. Chen, L. Xie, H. Su, and B. Huang, “Facilitating Re-
inforcement Learning for Process Control Using Transfer Learning:
Perspectives,” arXiv e-prints , p. arXiv:2404.00247, Mar. 2024.
Runze Lin received the B.S. degree in automation
from the College of Control Science and Engineer-
ing, Zhejiang University, Hangzhou, China, in 2020,
where he is currently pursuing the Ph.D. degree in
control science and engineering with the State Key
Laboratory of Industrial Control Technology, China.
From 2022 to 2023, he was a Visiting Scholar with
the University of Alberta, Edmonton, AB, Canada.
His research interests include reinforcement learn-
ing, transfer learning, process control, data analytics,
industrial big data and its applications.
Junghui Chen received the B.S. degree from the
Department of Chemical Engineering, Chung Yuan
Christian University, Taoyuan, Taiwan, in 1982, the
M.S. degree from the Department of Chemical Engi-
neering, National Taiwan University, Taipei, Taiwan,
in 1984, and the Ph.D. degree from the Department
of Chemical Engineering, The University of Ten-
nessee at Knoxville, Knoxville, TN, USA, in 1995.
He is currently a Full Professor with Chung Yuan
Christian University. His research interests are pro-
cess system engineering, including process design
for operability, nonlinear control, process monitoring and diagnosis, control
loop performance assessment, batch control, model predictive control, data
mining and analytics, and iterative learning design.
Biao Huang (Fellow, IEEE) received the B.S. and
M.S. degrees in automatic control from the Beijing
University of Aeronautics and Astronautics, Beijing,
China, in 1983 and 1986, respectively, and the Ph.D.
degree in process control from the University of
Alberta, Edmonton, AB, Canada, in 1997.
He joined the University of Alberta in 1997 as an
Assistant Professor with the Department of Chemi-
cal and Materials Engineering, where he is currently
a Full Professor. He was an NSERC Industrial
Research Chair in control of oil sands processes and
the AITF Industry Chair in process control from 2013 to 2018. His research
interests include data analytics, process control, system identification, control
performance assessment, Bayesian methods, and state estimation. He has
applied his expertise extensively in industrial practice.
Lei Xie received the B.S. and Ph.D. degrees from
Zhejiang University, China, in 2000 and 2005, re-
spectively.
From 2005 to 2006, he was a Postdoctoral Re-
searcher with the Berlin University of Technology
and an Assistant Professor from 2005 to 2008. He is
currently a Professor with the Department of Control
Science and Engineering, Zhejiang University. His
research interests focus on the interdisciplinary area
of statistics and system control theory.
Hongye Su (Senior Member, IEEE) received the
B.S. degree in industrial automation from the Nan-
jing University of Chemical Technology, Jiangsu,
China, in 1990, and the M.S. and Ph.D. degrees
in industrial automation from Zhejiang University,
Hangzhou, China, in 1993 and 1995, respectively.
From 1995 to 1997, he was a Lecturer with
the Department of Chemical Engineering, Zhejiang
University. From 1998 to 2000, he was an Associate
Professor with the Institute of Advanced Process
Control, Zhejiang University, where he is currently
a Professor with the Institute of Cyber-Systems and Control. His current
research interests include robust control, time-delay systems, and advanced
process control theory and applications.